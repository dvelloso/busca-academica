{"columns":["doi","abstract","year","title","author"],"index":[0,1,0,1,2,0,1,2,0,1,2,0,1,0,1],"data":[["10.1002\/9781119690962.ch1","This chapter discusses the different aspects of designing Big Data platforms, in order to define what makes a big platform and to set expectations for these platforms. The solutions for Big Data processing vary based on the company strategy. A modern Big Data platform has several requirements, and to meet them correctly, expectations with regard to data should be set. Securing data has become a crucial aspect of a modern Big Data platform. The data quality depends on factors such as accuracy, consistency, reliability, and visibility. One of the hard problems of Big Data is backups as the vast amount of storage needed is overwhelming for backups. The Big Data platform should provide an extract, transform, and load (ETL) solution\/s that manages the experience end to end. ETL developers should be able to develop, test, stage, and deploy their changes. Big Data platforms are quite complex as they are built based on distributed systems.","2021","An Introduction: What's a Modern Big Data Platform","Aytas, Yusuf"],["10.1109\/BigData.2017.8258380","The railways worldwide are increasingly looking to the integration of their data resources coupled with advanced analytics to enhance traffic management, to provide new insights on the health of infrastructure assets, to provide soft linkages to other transport modes, and ultimately to enable them to better serve their customers. As in many industrial sectors, over the past decade the rail industry has been investing heavily in sensing technologies that record every aspect of the operation of the railway network. However, as any data scientist knows, it does not matter how good an algorithm is, if you put rubbish in, you get rubbish out; and as the traditional industry model of working with data only within the system that it was collected by becomes increasingly fragile, the industry is discovering that it knows less than it thought about the data it is gathering. When coupled with legacy data resources of unknown accuracy, such as design diagrams for assets that in many cases are decades old, the rail industry now faces a crisis in which its data may become essentially worthless due to a poor understanding of the quality of its data. This paper reports the findings of the first phase of a three-phase systematic review of literature about how data quality can be managed and evaluated in the rail domain. It begins by discussing why data quality matters in a rail context, before going on to define the quality, introduce and expand the concept of a data quality schema.","2017","Understanding data quality: Ensuring data quality by design in the rail industry","Fu, Qian and Easton, John M."],["10.1109\/WISA.2017.29","Since a low-quality data may influence the effectiveness and reliability of applications, data quality is required to be guaranteed. Data quality assessment is considered as the foundation of the promotion of data quality, so it is essential to access the data quality before any other data related activities. In the electric power industry, more and more electric power data is continuously accumulated, and many electric power applications have been developed based on these data. In China, the power grid has many special characteristic, traditional big data assessment frameworks cannot be directly applied. Therefore, a big data framework for electric power data quality assessment is proposed. Based on big data techniques, the framework can accumulate both the real-time data and the history data, provide an integrated computation environment for electric power big data assessment, and support the storage of different types of data.","2017","A Big Data Framework for Electric Power Data Quality Assessment","Liu, He and Huang, Fupeng and Li, Han and Liu, Weiwei and Wang, Tongxun"],["10.1109\/BigDataCongress.2017.73","In the Big Data Era, data is the core for any governmental, institutional, and private organization. Efforts were geared towards extracting highly valuable insights that cannot happen if data is of poor quality. Therefore, data quality (DQ) is considered as a key element in Big data processing phase. In this stage, low quality data is not penetrated to the Big Data value chain. This paper, addresses the data quality rules discovery (DQR) after the evaluation of quality and prior to Big Data pre-processing. We propose a DQR discovery model to enhance and accurately target the pre-processing activities based on quality requirements. We defined, a set of pre-processing activities associated with data quality dimensions (DQD's) to automatize the DQR generation process. Rules optimization are applied on validated rules to avoid multi-passes pre-processing activities and eliminates duplicate rules. Conducted experiments showed an increased quality scores after applying the discovered and optimized DQR's on data.","2017","Big Data Pre-Processing: Closing the Data Quality Enforcement Loop","Taleb, Ikbal and Serhani, Mohamed Adel"],["10.1109\/IBDAP50342.2020.9245455","Data Profiling and data quality management become a more significant part of data engineering, which an essential part of ensuring that the system delivers quality information to users. In the last decade, data quality was considered to need more managing. Especially in the big data era that the data comes from many sources, many data types, and an enormous amount. Thus it makes the managing of data quality is more difficult and complicated. The traditional system was unable to respond as needed. The data quality managing software for big data was developed but often found in a high-priced, difficult to customize as needed, and mostly provide as GUI, which is challenging to integrate with other systems. From this problem, we have developed an opensource package for data quality managing. By using Python programming language, Which is a programming language that is widely used in the scientific and engineering field today. Because it is a programming language that is easy to read syntax, small, and has many additional packages to integrate. The software developed here is called \u201cSakdas\u201d this package has been divided into three parts. The first part deals with data profiling provide a set of data analyses to generate a data profile, and this profile will help to define the data quality rules. The second part deals with data quality auditing that users can set their own data quality rules for data quality measurement. The final part deals with data visualizing that provides data profiling and data auditing report to improve the data quality. The results of the profiling and auditing services, the user can specify both the form of a report for self-review. Or in the form of JSON for use in post-process automation.","2020","Sakdas: A Python Package for Data Profiling and Data Quality Auditing","Loetpipatwanich, Sakda and Vichitthamaros, Preecha"],["10.1145\/3341620.3341629","In a world increasingly connected, and in which information flows quickly and affects a very large number of people, sentiment analysis has seen a spectacular development over the past ten years. This is due to the fact that the explosion of social networks has allowed anyone with internet access to publicly express his opinion. Moreover, the emergence of big data has brought enormous opportunities and powerful storage and analytics tools to the field of sentiment analysis. However, big data introduces new variables and constraints that could radically affect the traditional models of sentiment analysis. Therefore, new concerns, such as big data quality, have to be addressed to get the most out of big data. To the best of our knowledge, no contributions have been published so far which address big data quality in SA throughout its different processes. In this paper, we first highlight the most important big data quality metrics to consider in any big data project. Then, we show how these metrics could be specifically considered in SA approaches and this for each phase in the big data value chain.","2019","Big Data Quality Metrics for Sentiment Analysis Approaches","El Alaoui, Imane and Gahi, Youssef and Messoussi, Rochdi"],["10.1145\/3419604.3419803","In recent years, as more and more data sources have become available and the volumes of data potentially accessible have increased, the assessment of data quality has taken a central role whether at the academic, professional or any other sector. Given that users are often concerned with the need to filter a large amount of data to better satisfy their requirements and needs, and that data analysis can be based on inaccurate, incomplete, ambiguous, duplicated and of poor quality, it makes everyone wonder what the results of these analyses will really be like. However, there is a very complex process involved in the identification of new, valid, potentially useful and meaningful data from a large data collection and various information systems, and is critically dependent on a number of measures to be developed to ensure data quality. To this end, the main objective of this paper is to introduce a general study on data quality related with big data, by providing what other researchers came up with on that subject. The paper will be finalized by a comparative study between the different existing data quality models.","2020","Towards a Data Quality Assessment in Big Data","Reda, Oumaima and Sassi, Imad and Zellou, Ahmed and Anter, Samir"],["10.1145\/3010089.3010090","As Big Data becomes better understood, there is a need for a comprehensive definition of Big Data to support work in fields such as data quality for Big Data. Existing definitions of Big Data define Big Data by comparison with existing, usually relational, definitions, or define Big Data in terms of data characteristics or use an approach which combines data characteristics with the Big Data environment. In this paper we examine existing definitions of Big Data and discuss the strengths and limitations of the different approaches, with particular reference to issues related to data quality in Big Data. We identify the issues presented by incomplete or inconsistent definitions. We propose an alternative definition and relate this definition to our work on quality in Big Data.","2016","Defining Big Data","Emmanuel, Isitor and Stanier, Clare"],["https:\/\/doi.org\/10.1016\/j.future.2015.11.024","Beyond the hype of Big Data, something within business intelligence projects is indeed changing. This is mainly because Big Data is not only about data, but also about a complete conceptual and technological stack including raw and processed data, storage, ways of managing data, processing and analytics. A challenge that becomes even trickier is the management of the quality of the data in Big Data environments. More than ever before the need for assessing the Quality-in-Use gains importance since the real contribution\u2013business value\u2013of data can be only estimated in its context of use. Although there exists different Data Quality models for assessing the quality of regular data, none of them has been adapted to Big Data. To fill this gap, we propose the \u201c3As Data Quality-in-Use model\u201d, which is composed of three Data Quality characteristics for assessing the levels of Data Quality-in-Use in Big Data projects: Contextual Adequacy, Operational Adequacy and Temporal Adequacy. The model can be integrated into any sort of Big Data project, as it is independent of any pre-conditions or technologies. The paper shows the way to use the model with a working example. The model accomplishes every challenge related to Data Quality program aimed for Big Data. The main conclusion is that the model can be used as an appropriate way to obtain the Quality-in-Use levels of the input data of the Big Data analysis, and those levels can be understood as indicators of trustworthiness and soundness of the results of the Big Data analysis.","2016","A Data Quality in Use model for Big Data","Jorge Merino and Ismael Caballero and Bibiano Rivas and Manuel Serrano and Mario Piattini"],["https:\/\/doi.org\/10.1016\/j.isprsjprs.2015.11.006","The recent explosive publications of big data studies have well documented the rise of big data and its ongoing prevalence. Different types of \u201cbig data\u201d have emerged and have greatly enriched spatial information sciences and related fields in terms of breadth and granularity. Studies that were difficult to conduct in the past time due to data availability can now be carried out. However, big data brings lots of \u201cbig errors\u201d in data quality and data usage, which cannot be used as a substitute for sound research design and solid theories. We indicated and summarized the problems faced by current big data studies with regard to data collection, processing and analysis: inauthentic data collection, information incompleteness and noise of big data, unrepresentativeness, consistency and reliability, and ethical issues. Cases of empirical studies are provided as evidences for each problem. We propose that big data research should closely follow good scientific practice to provide reliable and scientific \u201cstories\u201d, as well as explore and develop techniques and methods to mitigate or rectify those \u2018big-errors\u2019 brought by big data.","2016","Rethinking big data: A review on the data quality and usage issues","Jianzheng Liu and Jie Li and Weifeng Li and Jiansheng Wu"],["https:\/\/doi.org\/10.1016\/j.heliyon.2022.e10312","Background\nActivating prior medical knowledge in diagnosis and treatment is an important basis for clinicians to improve their care ability. However, it has not been systematically explained whether and how various big data resources affect the activation of prior knowledge in the big data environment faced by clinicians.\nObjective\nThe aim of this study is to contribute to a better understanding on how the activation of prior knowledge of clinicians is affected by a wide range of shared and private big data resources, to reveal the impact of big data resources on clinical competence and professional development of clinicians.\nMethod\nThrough the comprehensive analysis of extant research results, big data resources are classified as big data itself, big data technology and big data services at the public and institutional levels. A survey was conducted on clinicians and IT personnel in Chinese hospitals. A total of 616 surveys are completed, involving 308 medical institutions. Each medical institution includes a clinician and an IT personnel. SmartPLS version 2.0 software package was used to test the direct impact of big data resources on the activation of prior knowledge. We further analyze their indirect impact of those big data resources without direct impact.\nResults\n(1) Big data quality environment at the institutional level and the big data sharing environment at the public level directly affect activation of prior medical knowledge; (2) Big data service environment at the institutional level directly affects activation of prior medical knowledge; (3) Big data deployment environment at the institutional level and big data service environment at the public level have no direct impact on activation of prior knowledge of clinicians, but they have an indirect impact through big data quality environment and service environment at the institutional level and the big data sharing environment at the public level.\nConclusions\nBig data technology, big data itself and big data service at the public level and institutional level interact and influence each other to activate prior medical knowledge. This study highlights the implications of big data resources on improvement of clinicians\u2019 diagnosis and treatment ability.","2022","Impact of big data resources on clinicians\u2019 activation of prior medical knowledge","Sufen Wang and Junyi Yuan and Changqing Pan"],["10.1145\/3281022.3281026","Big Data (BD) solutions are designed to better support decision-making processes in order to optimize organizational performance. These BD solutions use company\u2019s core business data, using typically large datasets. However, data that doesn\u2019t meet adequate quality levels will lead to BD solutions that will not produce useful results, and consequently may not be used to make adequate business decisions. For a long time, companies have collected and stored large amounts of data without being able to exploit the advantage of exploring it. Nowadays, and thanks to the Big Data explosion, organizations have begun to recognize the need for estimating the value of their data and, vice-versa, managing data accordingly to their value. This need of managing the Value of data has led to the concept of Smart Data. It not only involves the datasets, but also the set of technologies, tools, processes and methodologies that enable all the Values from the data to the End-users (Business, data scientist, BI\u2026). Consequently, Smart data is data actionable. We discovered that data quality is one of the most important issues when it comes to \u201csmartizing\u201d data. In this paper, we introduce a methodology to make data smarter, taking as a reference point, the quality level of the data itself.","2018","From Big Data to Smart Data: A Data Quality Perspective","Baldassarre, Maria Teresa and Caballero, Ismael and Caivano, Danilo and Rivas Garcia, Bibiano and Piattini, Mario"],["10.1145\/3010089.3010143","In healthcare sector huge quantities of data about patients and their medical conditions have been gathered through clinical databases and various other healthcare processes. Currently, it records nearly all aspects of care, including patient personal information, clinical trials, hospital records, diagnosis, medication, test results, imaging data, costs, administrative reports, etc. Like in other application domains, the big data revolution holds also great promise in the area of healthcare, as the available data about individual patients is very rich, and hides crucial knowledge that can be exploited to improve patients' care while reducing its cost. For instance, in 2012 worldwide collected healthcare data was estimated to be in the region of 500 petabytes and it is expected to grow 50 times more in 2020 (25 Exabytes). Turning this massive amount of data into knowledge that can be used to identify needs, predict and prevent critical patients' conditions, and help practitioners to make rapid and accurate decisions is not only a desire but is of urgent and crucial necessity. Therefore, healthcare organisations must have the ability to manage and analyse their data in a rapid and efficient manner to answer several critical questions related to diseases, treatments, patients' behaviours, and care management. However, building such system faces huge challenges: 1) data complexity, 2) Privacy, security, ethical, legal, and social issues, and 3) Interoperability, portability, and compatibility. We will discuss all these challenges and the requirements of healthcare ecosystem. This will lead us to describe some innovative methodologies of how to build such ecosystem to face the healthcare challenges of the next decade or so.","2016","Healthcare Big Data: Challenges and Opportunities","Kechadi, M-Tahar"],["https:\/\/doi.org\/10.1016\/j.ijinfomgt.2014.02.002","Big data analytics associated with database searching, mining, and analysis can be seen as an innovative IT capability that can improve firm performance. Even though some leading companies are actively adopting big data analytics to strengthen market competition and to open up new business opportunities, many firms are still in the early stage of the adoption curve due to lack of understanding of and experience with big data. Hence, it is interesting and timely to understand issues relevant to big data adoption. In this study, a research model is proposed to explain the acquisition intention of big data analytics mainly from the theoretical perspectives of data quality management and data usage experience. Our empirical investigation reveals that a firm's intention for big data analytics can be positively affected by its competence in maintaining the quality of corporate data. Moreover, a firm's favorable experience (i.e., benefit perceptions) in utilizing external source data could encourage future acquisition of big data analytics. Surprisingly, a firm's favorable experience (i.e., benefit perceptions) in utilizing internal source data could hamper its adoption intention for big data analytics.","2014","Data quality management, data usage experience and acquisition intention of big data analytics","Ohbyung Kwon and Namyeon Lee and Bongsik Shin"],["https:\/\/doi.org\/10.1016\/j.future.2018.07.014","Big data changed the way in which we collect and analyze data. In particular, the amount of available information is constantly growing and organizations rely more and more on data analysis in order to achieve their competitive advantage. However, such amount of data can create a real value only if combined with quality: good decisions and actions are the results of correct, reliable and complete data. In such a scenario, methods and techniques for the Data Quality assessment can support the identification of suitable data to process. If for traditional database numerous assessment methods are proposed, in the Big Data scenario new algorithms have to be designed in order to deal with novel requirements related to variety, volume and velocity issues. In particular, in this paper we highlight that dealing with heterogeneous sources requires an adaptive approach able to trigger the suitable quality assessment methods on the basis of the data type and context in which data have to be used. Furthermore, we show that in some situations it is not possible to evaluate the quality of the entire dataset due to performance and time constraints. For this reason, we suggest to focus the Data Quality assessment only on a portion of the dataset and to take into account the consequent loss of accuracy by introducing a confidence factor as a measure of the reliability of the quality assessment procedure. We propose a methodology to build a Data Quality adapter module, which selects the best configuration for the Data Quality assessment based on the user main requirements: time minimization, confidence maximization, and budget minimization. Experiments are performed by considering real data gathered from a smart city case study.","2018","Context-aware data quality assessment for big data","Danilo Ardagna and Cinzia Cappiello and Walter Sam\u00e1 and Monica Vitali"]]}