,sourceid,title_x,type,issn,sjr,sjr best quartile,h index,total docs. (2020),total docs. (3years),total refs.,total cites (3years),citable docs. (3years),cites / doc. (2years),ref. / doc.,country,region,publisher,coverage,categories,campo_join,title_y,total cites,journal impact factor,eigenfactor score,abstract,doi,year,author,title,type_publication
0,22401,JOURNAL OF THE AMERICAN COLLEGE OF CARDIOLOGY,journal,"15583597, 07351097","10,315",Q1,431,935,2960,22363,23475,1191,"7,44","23,92",United States,Northern America,Elsevier USA,1983-2020,Cardiology and Cardiovascular Medicine (Q1),JOURNAL OF THE AMERICAN COLLEGE OF CARDIOLOGY,JOURNAL OF THE AMERICAN COLLEGE OF CARDIOLOGY,"125,873",24.094,0.177,"Data science is likely to lead to major changes in cardiovascular imaging. Problems with timing, efficiency, and missed diagnoses occur at all stages of the imaging chain. The application of artificial intelligence (AI) is dependent on robust data; the application of appropriate computational approaches and tools; and validation of its clinical application to image segmentation, automated measurements, and eventually, automated diagnosis. AI may reduce cost and improve value at the stages of image acquisition, interpretation, and decision-making. Moreover, the precision now possible with cardiovascular imaging, combined with “big data” from the electronic health record and pathology, is likely to better characterize disease and personalize therapy. This review summarizes recent promising applications of AI in cardiology and cardiac imaging, which potentially add value to patient care.",https://doi.org/10.1016/j.jacc.2018.12.054,2019,Damini Dey and Piotr J. Slomka and Paul Leeson and Dorin Comaniciu and Sirish Shrestha and Partho P. Sengupta and Thomas H. Marwick,ARTIFICIAL INTELLIGENCE IN CARDIOVASCULAR IMAGING: JACC STATE-OF-THE-ART REVIEW,article
1,17600155011,MOLECULAR PLANT,journal,"16742052, 17529867","4,588",Q1,115,153,474,8872,4792,380,"9,60","57,99",United States,Northern America,Cell Press,2008-2020,Molecular Biology (Q1); Plant Science (Q1),MOLECULAR PLANT,MOLECULAR PLANT,"15,778",13.164,0.02686,"The first paradigm of plant breeding involves direct selection based phenotypic observation, followed by predictive breeding using statistical models constructed for quantitative traits based on genetic experimental design and more recently by incorporating molecular marker genotypes. However, plant performance or phenotype (P) is determined by the combining effects of genotype (G), envirotype (E) and genotype by environment interaction (GEI). Phenotypes can be predicted more precisely by training a model using data collected from multiple sources, including spatiotemporal omics (genomics, phenomics and enviromics across time and space). Integration of 3D information profiles (G-P-E), each with multidimensionality, provides predictive breeding with both tremendous opportunities and great challenges. Here, we first review innovative technologies for predictive breeding. We then evaluate multidimensional information profiles that can be integrated with a predictive breeding strategy, particularly envirotypic data, which have largely been neglected in data collection and nearly untouched in model construction. We propose a smart breeding scheme, integrated genomic-enviromic prediction (iGEP), as an extension of genomic prediction, using integrated multiomics information, big data technology and artificial intelligence (mainly focus on machine and deep learning). How to implement iGEP was discussed, including spatiotemporal models, environmental indices, factorial and spatiotemporal structure of plant breeding data, and cross-species prediction. A strategy is then proposed for prediction-based crop redesign at both the macro (individual, population and species) and micro (gene, metabolism and network) scales. Finally, we provide perspectives on translating the smart breeding into genetic gain through integrative breeding platforms and open-source breeding initiatives. We call for coordinated efforts in smart breeding through iGEP, institutional partnerships, and innovative technological support.",https://doi.org/10.1016/j.molp.2022.09.001,2022,Yunbi Xu and Xingping Zhang and Huihui Li and Hongjian Zheng and Jianan Zhang and Michael S. Olsen and Rajeev K. Varshney and Boddupalli M. Prasanna and Qian Qian,"SMART BREEDING DRIVEN BY BIG DATA, ARTIFICIAL INTELLIGENCE AND INTEGRATED GENOMIC-ENVIROMIC PREDICTION",article
2,17600155011,MOLECULAR PLANT,journal,"16742052, 17529867","4,588",Q1,115,153,474,8872,4792,380,"9,60","57,99",United States,Northern America,Cell Press,2008-2020,Molecular Biology (Q1); Plant Science (Q1),MOLECULAR PLANT,MOLECULAR PLANT,"15,778",13.164,0.02686,,https://doi.org/10.1016/j.molp.2019.01.008,2019,Yoshiaki Ueda and Shuichi Yanagisawa,DELINEATION OF NITROGEN SIGNALING NETWORKS: COMPUTATIONAL APPROACHES IN THE BIG DATA ERA,article
3,17809,TRENDS IN PLANT SCIENCE,journal,"13601385, 18784372","4,587",Q1,263,162,386,9428,4844,377,"11,39","58,20",United Kingdom,Western Europe,Elsevier Ltd.,1996-2020,Plant Science (Q1),TRENDS IN PLANT SCIENCE,TRENDS IN PLANT SCIENCE,"29,531",18.313,0.02277,"Optical sensors and sensing-based phenotyping techniques have become mainstream approaches in high-throughput phenotyping for improving trait selection and genetic gains in crops. We review recent progress and contemporary applications of optical sensing-based phenotyping (OSP) techniques in cereal crops and highlight optical sensing principles for spectral response and sensor specifications. Further, we group phenotypic traits determined by OSP into four categories – morphological, biochemical, physiological, and performance traits – and illustrate appropriate sensors for each extraction. In addition to the current status, we discuss the challenges of OSP and provide possible solutions. We propose that optical sensing-based traits need to be explored further, and that standardization of the language of phenotyping and worldwide collaboration between phenotyping researchers and other fields need to be established.",https://doi.org/10.1016/j.tplants.2021.07.015,2022,Dawei Sun and Kelly Robbins and Nicolas Morales and Qingyao Shu and Haiyan Cen,ADVANCES IN OPTICAL PHENOTYPING OF CEREAL CROPS,article
4,6400153137,JOURNAL OF THORACIC ONCOLOGY,journal,"15561380, 15560864","4,539",Q1,133,316,1028,6973,6688,581,"6,04","22,07",United States,Northern America,International Association for the Study of Lung Cancer,2006-2020,Medicine (miscellaneous) (Q1); Oncology (Q1); Pulmonary and Respiratory Medicine (Q1),JOURNAL OF THORACIC ONCOLOGY,JOURNAL OF THORACIC ONCOLOGY,"24,405",15.609,0.04278,,https://doi.org/10.1016/j.jtho.2022.02.010,2022,Ahmed Salem and Kevin Franks and Alastair Greystoke and Gerard G. Hanna and Stephen Harrow and Matthew Hatton and Crispin Hiley and Fiona McDonald and Corinne Faivre-Finn,UNACCOUNTED CONFOUNDERS LIMIT THE ABILITY TO DRAW CONCLUSIONS FROM BIG DATA ANALYSIS COMPARING RADIOTHERAPY FRACTIONATION REGIMENS IN NSCLC,article
5,21100943502,MATTER,journal,"25902393, 25902385","4,138",Q1,22,322,156,15168,1414,133,"9,06","47,11",United States,Northern America,Cell Press,2019-2020,Materials Science (miscellaneous) (Q1),MATTER,MATTER,"2,357",15.589,0.00396,"With thousands of publications per year, the volume of data published on perovskite solar cells since the spark of the “perovskite fever” in 2013 is enormous and far exceeds the amount that any individual researcher could digest. To tackle this issue, Jacobsson et al.1 have created The Perovskite Database, which is part of a larger trend to harness the power of big data and artificial intelligence to accelerate the commercialization of perovskite solar cells.",https://doi.org/10.1016/j.matt.2022.06.001,2022,Kameron R. Hansen and Luisa Whittaker-Brooks,FINDING THE FAIRNESS IN PEROVSKITE PHOTOVOLTAICS RESEARCH,article
6,24046,SEMINARS IN CANCER BIOLOGY,journal,"10963650, 1044579X","3,908",Q1,148,287,310,47920,3555,286,"11,39","166,97",United States,Northern America,Academic Press Inc.,1990-2020,Cancer Research (Q1),SEMINARS IN CANCER BIOLOGY,SEMINARS IN CANCER BIOLOGY,"11,552",15.707,0.01211,"Radiotherapy is a discipline closely integrated with computer science. Artificial intelligence (AI) has developed rapidly over the past few years. With the explosive growth of medical big data, AI promises to revolutionize the field of radiotherapy through highly automated workflow, enhanced quality assurance, improved regional balances of expert experiences, and individualized treatment guided by multi-omics. In addition to independent researchers, the increasing number of large databases, biobanks, and open challenges significantly facilitated AI studies on radiation oncology. This article reviews the latest research, clinical applications, and challenges of AI in each part of radiotherapy including image processing, contouring, planning, quality assurance, motion management, and outcome prediction. By summarizing cutting-edge findings and challenges, we aim to inspire researchers to explore more future possibilities and accelerate the arrival of AI radiotherapy.",https://doi.org/10.1016/j.semcancer.2022.08.005,2022,Guangqi Li and Xin Wu and Xuelei Ma,ARTIFICIAL INTELLIGENCE IN RADIOTHERAPY,article
7,15537,CURRENT BIOLOGY,journal,"18790445, 09609822","3,822",Q1,316,929,2432,38487,15523,2265,"5,99","41,43",United States,Northern America,Cell Press,1991-2020,"Agricultural and Biological Sciences (miscellaneous) (Q1); Biochemistry, Genetics and Molecular Biology (miscellaneous) (Q1); Neuroscience (miscellaneous) (Q1)",CURRENT BIOLOGY,CURRENT BIOLOGY,"78,289",10.834,0.1161,"Summary
Fungi have successfully established themselves across seemingly every possible niche, substrate, and biome. They are fundamental to biogeochemical cycling, interspecies interactions, food production, and drug bioprocessing, as well as playing less heroic roles as difficult to treat human infections and devastating plant pathogens. Despite community efforts to estimate and catalog fungal diversity, we have only named and described a minute fraction of the fungal world. The identification, characterization, and conservation of fungal diversity is paramount to preserving fungal bioresources, and to understanding and predicting ecosystem cycling and the evolution and epidemiology of fungal disease. Although species and ecosystem conservation are necessarily the foundation of preserving this diversity, there is value in expanding our definition of conservation to include the protection of biological collections, ecological metadata, genetic and genomic data, and the methods and code used for our analyses. These definitions of conservation are interdependent. For example, we need metadata on host specificity and biogeography to understand rarity and set priorities for conservation. To aid in these efforts, we need to draw expertise from diverse fields to tie traditional taxonomic knowledge to data obtained from modern -omics-based approaches, and support the advancement of diverse research perspectives. We also need new tools, including an updated framework for describing and tracking species known only from DNA, and the continued integration of functional predictions to link genetic diversity to functional and ecological diversity. Here, we review the state of fungal diversity research as shaped by recent technological advancements, and how changing viewpoints in taxonomy, -omics, and systematics can be integrated to advance mycological research and preserve fungal biodiversity.",https://doi.org/10.1016/j.cub.2021.06.083,2021,Lotus A. Lofgren and Jason E. Stajich,"FUNGAL BIODIVERSITY AND CONSERVATION MYCOLOGY IN LIGHT OF NEW TECHNOLOGY, BIG DATA, AND CHANGING ATTITUDES",article
8,28973,JOURNAL OF ECONOMETRICS,journal,03044076,"3,769",Q1,159,241,418,11045,1497,408,"3,38","45,83",Netherlands,Western Europe,Elsevier BV,1973-2020,Applied Mathematics (Q1); Economics and Econometrics (Q1); History and Philosophy of Science (Q1),JOURNAL OF ECONOMETRICS,JOURNAL OF ECONOMETRICS,"25,569",2.388,0.02085,"Nearly all studies that analyze the term structure of interest rates take a two-step approach. First, actual bond prices are summarized by interpolated synthetic zero-coupon yields, and second, some of these yields are used as the source data for further empirical examination. In contrast, we consider the advantages of a one-step approach that directly analyzes the universe of bond prices. To illustrate the feasibility and desirability of the one-step approach, we compare arbitrage-free dynamic term structure models estimated using both approaches. We also provide a simulation study showing that a one-step approach can extract the information in large panels of bond prices and avoid any arbitrary noise introduced from a first-stage interpolation of yields.",https://doi.org/10.1016/j.jeconom.2019.04.019,2019,Martin M. Andreasen and Jens H.E. Christensen and Glenn D. Rudebusch,TERM STRUCTURE ANALYSIS WITH BIG DATA: ONE-STEP ESTIMATION USING BOND PRICES,article
9,16547,TOURISM MANAGEMENT,journal,02615177,"3,328",Q1,199,159,690,11910,8354,687,"11,06","74,91",United Kingdom,Western Europe,Elsevier Ltd.,1982-2021,"Development (Q1); Strategy and Management (Q1); Tourism, Leisure and Hospitality Management (Q1); Transportation (Q1)",TOURISM MANAGEMENT,TOURISM MANAGEMENT,"37,117",10.967,0.02226,"Even at an early stage, diverse big data have been applied to tourism research and made an amazing improvement. This paper might be the first attempt to present a comprehensive literature review on different types of big data in tourism research. By data sources, the tourism-related big data fall into three primary categories: UGC data (generated by users), including online textual data and online photo data; device data (by devices), including GPS data, mobile roaming data, Bluetooth data, etc.; transaction data (by operations), including web search data, webpage visiting data, online booking data, etc. Carrying different information, different data types address different tourism issues. For each type, a systematical analysis is conducted from the perspectives of research focuses, data characteristics, analytic techniques, major challenges and further directions. This survey facilitates a thorough understanding of this sunrise research and offers valuable insights into its future prospects.",https://doi.org/10.1016/j.tourman.2018.03.009,2018,Jingjing Li and Lizhi Xu and Ling Tang and Shouyang Wang and Ling Li,BIG DATA IN TOURISM RESEARCH: A LITERATURE REVIEW,article
10,17495,NEUROIMAGE,journal,"10959572, 10538119","3,259",Q1,364,981,2802,73619,20526,2767,"6,82","75,04",United States,Northern America,Academic Press Inc.,"1970, 1992-2020",Cognitive Neuroscience (Q1); Neurology (Q1),NEUROIMAGE,NEUROIMAGE,"119,618",6.556,0.10582,"We analyzed factors that may hamper the advancement of computational cognitive neuroscience (CCN). These factors include a particular statistical mindset, which paves the way for the dominance of statistical power theory and a preoccupation with statistical replicability in the behavioral and neural sciences. Exclusive statistical concerns about sampling error occur at the cost of an inadequate representation of the problem of measurement error. We contrasted the manipulation of data quantity (sampling error, by varying the number of subjects) against the manipulation of data quality (measurement error, by varying the number of data per subject) in a simulated Bayesian model identifiability study. The results were clear-cut in showing that - across all levels of signal-to-noise ratios - varying the number of subjects was completely inconsequential, whereas the number of data per subject exerted massive effects on model identifiability. These results emphasize data quality over data quantity, and they call for the integration of statistics and measurement theory.",https://doi.org/10.1016/j.neuroimage.2018.01.005,2018,Antonio Kolossa and Bruno Kopp,DATA QUALITY OVER DATA QUANTITY IN COMPUTATIONAL COGNITIVE NEUROSCIENCE,article
11,17495,NEUROIMAGE,journal,"10959572, 10538119","3,259",Q1,364,981,2802,73619,20526,2767,"6,82","75,04",United States,Northern America,Academic Press Inc.,"1970, 1992-2020",Cognitive Neuroscience (Q1); Neurology (Q1),NEUROIMAGE,NEUROIMAGE,"119,618",6.556,0.10582,"Brain activity is a dynamic combination of the responses to sensory inputs and its own spontaneous processing. Consequently, such brain activity is continuously changing whether or not one is focusing on an externally imposed task. Previously, we have introduced an analysis method that allows us, using Hidden Markov Models (HMM), to model task or rest brain activity as a dynamic sequence of distinct brain networks, overcoming many of the limitations posed by sliding window approaches. Here, we present an advance that enables the HMM to handle very large amounts of data, making possible the inference of very reproducible and interpretable dynamic brain networks in a range of different datasets, including task, rest, MEG and fMRI, with potentially thousands of subjects. We anticipate that the generation of large and publicly available datasets from initiatives such as the Human Connectome Project and UK Biobank, in combination with computational methods that can work at this scale, will bring a breakthrough in our understanding of brain function in both health and disease.",https://doi.org/10.1016/j.neuroimage.2017.06.077,2018,Diego Vidaurre and Romesh Abeysuriya and Robert Becker and Andrew J. Quinn and Fidel Alfaro-Almagro and Stephen M. Smith and Mark W. Woolrich,DISCOVERING DYNAMIC BRAIN NETWORKS FROM BIG DATA IN REST AND TASK,article
12,16146,TRENDS IN BIOTECHNOLOGY,journal,"01677799, 18793096","3,192",Q1,219,167,397,11206,4561,346,"11,86","67,10",United Kingdom,Western Europe,Elsevier Ltd.,1983-2020,Bioengineering (Q1); Biotechnology (Q1),TRENDS IN BIOTECHNOLOGY,TRENDS IN BIOTECHNOLOGY,"20,693",19.536,0.01817,"Batch effects (BEs) are technical biases that may confound analysis of high-throughput biotechnological data. BEs are complex and effective mitigation is highly context-dependent. In particular, the advent of high-resolution technologies such as single-cell RNA sequencing presents new challenges. We first cover how BE modeling differs between traditional datasets and the new data landscape. We also discuss new approaches for measuring and mitigating BEs, including whether a BE is significant enough to warrant correction. Even with the advent of machine learning and artificial intelligence, the increased complexity of next-generation biotechnological data means increased complexities in BE management. We forecast that BEs will not only remain relevant in the age of big data but will become even more important.",https://doi.org/10.1016/j.tibtech.2022.02.005,2022,Wilson Wen Bin Goh and Chern Han Yong and Limsoon Wong,ARE BATCH EFFECTS STILL RELEVANT IN THE AGE OF BIG DATA?,article
13,22990,JOURNAL OF RETAILING,journal,00224359,"3,184",Q1,136,66,98,4311,839,87,"4,51","65,32",United Kingdom,Western Europe,Elsevier BV,1993-2020,Marketing (Q1),JOURNAL OF RETAILING,JOURNAL OF RETAILING,"10,594",5.245,0.00458,"The paper examines the opportunities in and possibilities arising from big data in retailing, particularly along five major data dimensions—data pertaining to customers, products, time, (geo-spatial) location and channel. Much of the increase in data quality and application possibilities comes from a mix of new data sources, a smart application of statistical tools and domain knowledge combined with theoretical insights. The importance of theory in guiding any systematic search for answers to retailing questions, as well as for streamlining analysis remains undiminished, even as the role of big data and predictive analytics in retailing is set to rise in importance, aided by newer sources of data and large-scale correlational techniques. The Statistical issues discussed include a particular focus on the relevance and uses of Bayesian analysis techniques (data borrowing, updating, augmentation and hierarchical modeling), predictive analytics using big data and a field experiment, all in a retailing context. Finally, the ethical and privacy issues that may arise from the use of big data in retailing are also highlighted.",https://doi.org/10.1016/j.jretai.2016.12.004,2017,Eric T. Bradlow and Manish Gangwar and Praveen Kopalle and Sudhir Voleti,THE ROLE OF BIG DATA AND PREDICTIVE ANALYTICS IN RETAILING,article
14,18795,WATER RESEARCH,journal,"00431354, 18792448","3,099",Q1,303,1142,2603,70129,29848,2588,"11,32","61,41",United Kingdom,Western Europe,Elsevier Ltd.,1967-2020,Civil and Structural Engineering (Q1); Ecological Modeling (Q1); Environmental Engineering (Q1); Pollution (Q1); Waste Management and Disposal (Q1); Water Science and Technology (Q1),WATER RESEARCH,WATER RESEARCH,"120,695",11.236,0.07878,"Recent advancements in data-driven process control and performance analysis could provide the wastewater treatment industry with an opportunity to reduce costs and improve operations. However, big data in wastewater treatment plants (WWTP) is widely underutilized, due in part to a workforce that lacks background knowledge of data science required to fully analyze the unique characteristics of WWTP. Wastewater treatment processes exhibit nonlinear, nonstationary, autocorrelated, and co-correlated behavior that (i) is very difficult to model using first principals and (ii) must be considered when implementing data-driven methods. This review provides an overview of data-driven methods of achieving fault detection, variable prediction, and advanced control of WWTP. We present how big data has been used in the context of WWTP, and much of the discussion can also be applied to water treatment. Due to the assumptions inherent in different data-driven modeling approaches (e.g., control charts, statistical process control, model predictive control, neural networks, transfer functions, fuzzy logic), not all methods are appropriate for every goal or every dataset. Practical guidance is given for matching a desired goal with a particular methodology along with considerations regarding the assumed data structure. References for further reading are provided, and an overall analysis framework is presented.",https://doi.org/10.1016/j.watres.2019.03.030,2019,Kathryn B. Newhart and Ryan W. Holloway and Amanda S. Hering and Tzahi Y. Cath,DATA-DRIVEN PERFORMANCE ANALYSES OF WASTEWATER TREATMENT PLANTS: A REVIEW,article
15,98982,TRANSPORT REVIEWS,journal,"01441647, 14645327","3,046",Q1,82,52,124,3838,1233,106,"9,34","73,81",United Kingdom,Western Europe,Routledge,1981-2020,Transportation (Q1),TRANSPORT REVIEWS,TRANSPORT REVIEWS,"4,598",9.643,0.00476,"ABSTRACT
The information-rich vessel movement data provided by the Automatic Identification System (AIS) has gained much popularity over the past decade, during which the employment of satellite-based receivers has enabled wide coverage and improved data quality. The application of AIS data has developed from simply navigation-oriented research to now include trade flow estimation, emission accounting, and vessel performance monitoring. The AIS now provides high frequency, real-time positioning and sailing patterns for almost the whole world's commercial fleet, and therefore, in combination with supplementary databases and analyses, AIS data has arguably kickstarted the era of digitisation in the shipping industry. In this study, we conduct a comprehensive review of the literature regarding AIS applications by dividing it into three development stages, namely, basic application, extended application, and advanced application. Each stage contains two to three application fields, and in total we identified seven application fields, including (1) AIS data mining, (2) navigation safety, (3) ship behaviour analysis, (4) environmental evaluation, (5) trade analysis, (6) ship and port performance, and (7) Arctic shipping. We found that the original application of AIS data to navigation safety has, with the improvement of data accessibility, evolved into diverse applications in various directions. Moreover, we summarised the major methodologies in the literature into four categories, these being (1) data processing and mining, (2) index measurement, (3) causality analysis, and (4) operational research. Undoubtedly, the applications of AIS data will be further expanded in the foreseeable future. This will not only provide a more comprehensive understanding of voyage performance and allow researchers to examine shipping market dynamics from the micro level, but also the abundance of AIS data may also open up the rather opaque aspect of how shipping companies release information to external authorities, including the International Maritime Organization, port states, scientists and researchers. It is expected that more multi-disciplinary AIS studies will emerge in the coming years. We believe that this study will shed further light on the future development of AIS studies.",https://doi.org/10.1080/01441647.2019.1649315,2019,Dong Yang and Lingxiao Wu and Shuaian Wang and Haiying Jia and Kevin X. Li,HOW BIG DATA ENRICHES MARITIME RESEARCH – A CRITICAL REVIEW OF AUTOMATIC IDENTIFICATION SYSTEM (AIS) DATA APPLICATIONS,article
16,28801,APPLIED ENERGY,journal,03062619,"3,035",Q1,212,1729,5329,100144,56804,5304,"10,59","57,92",United Kingdom,Western Europe,Elsevier BV,1975-2020,"Building and Construction (Q1); Energy (miscellaneous) (Q1); Management, Monitoring, Policy and Law (Q1); Mechanical Engineering (Q1)",APPLIED ENERGY,APPLIED ENERGY,"122,712",9.746,0.15329,"Building heat demand is responsible for a significant share of the total global final energy consumption. Building stock models with a high spatio-temporal resolution are a powerful tool to investigate the effects of new building policies aimed at increasing energy efficiency, the introduction of new heating technologies or the integration of buildings within an energy system based on renewable energy sources. Therefore, building stock models have to be able to model the improvements and variation of used materials in buildings. In this paper, we propose a method based on generalized large-scale geographic information system (GIS) to model building heat demand of large regions with a high temporal resolution. In contrast to existing building stock models, our approach allows to derive the envelope of all buildings from digital elevation models and to model location dependent effects such as shadowing due to the topography and climate conditions. We integrate spatio-temporal climate data for temperature and solar radiation to model climate effects of complex terrain. The model is validated against a database containing the measured energy demand of 1845 buildings of the city of St. Gallen, Switzerland and 120 buildings of the Alpine village of Zernez, Switzerland. The proposed model is able to assess and investigate large regions by using spatial data describing natural and anthropogenic land features. The validation resulted in an average goodness of fit (R2) of 0.6.",https://doi.org/10.1016/j.apenergy.2017.10.041,2017,René Buffat and Andreas Froemelt and Niko Heeren and Martin Raubal and Stefanie Hellweg,BIG DATA GIS ANALYSIS FOR NOVEL APPROACHES IN BUILDING STOCK MODELLING,article
17,28801,APPLIED ENERGY,journal,03062619,"3,035",Q1,212,1729,5329,100144,56804,5304,"10,59","57,92",United Kingdom,Western Europe,Elsevier BV,1975-2020,"Building and Construction (Q1); Energy (miscellaneous) (Q1); Management, Monitoring, Policy and Law (Q1); Mechanical Engineering (Q1)",APPLIED ENERGY,APPLIED ENERGY,"122,712",9.746,0.15329,"Building energy data has been used for decades to understand energy flows in buildings and plan for future energy demand. Recent market, technology and policy drivers have resulted in widespread data collection by stakeholders across the buildings industry. Consolidation of independently collected and maintained datasets presents a cost-effective opportunity to build a database of unprecedented size. Applications of the data include peer group analysis to evaluate building performance, and data-driven algorithms that use empirical data to estimate energy savings associated with building retrofits. This paper discusses technical considerations in compiling such a database using the DOE Buildings Performance Database (BPD) as a case study. We gathered data on over 750,000 residential and commercial buildings. We describe the process and challenges of mapping and cleansing data from disparate sources. We analyze the distributions of buildings in the BPD relative to the Commercial Building Energy Consumption Survey (CBECS) and Residential Energy Consumption Survey (RECS), evaluating peer groups of buildings that are well or poorly represented, and discussing how differences in the distributions of the three datasets impact use-cases of the data. Finally, we discuss the usefulness and limitations of the current dataset and the outlook for increasing its size and applications.",https://doi.org/10.1016/j.apenergy.2014.11.042,2015,Paul A. Mathew and Laurel N. Dunn and Michael D. Sohn and Andrea Mercado and Claudine Custudio and Travis Walter,BIG-DATA FOR BUILDING ENERGY PERFORMANCE: LESSONS FROM ASSEMBLING A VERY LARGE NATIONAL DATABASE OF BUILDING ENERGY USE,article
18,28801,APPLIED ENERGY,journal,03062619,"3,035",Q1,212,1729,5329,100144,56804,5304,"10,59","57,92",United Kingdom,Western Europe,Elsevier BV,1975-2020,"Building and Construction (Q1); Energy (miscellaneous) (Q1); Management, Monitoring, Policy and Law (Q1); Mechanical Engineering (Q1)",APPLIED ENERGY,APPLIED ENERGY,"122,712",9.746,0.15329,"Battery is one of the most important and costly devices in electric vehicles (EVs). Developing an efficient battery management method is of great significance to enhancing vehicle safety and economy. Recently developed big-data and cloud platform computing technologies bring a bright perspective for efficient utilization and protection of vehicle batteries. However, a reliable data transmission network and a high-quality cloud battery dataset are indispensable to enable this benefit. This paper makes the first effort to systematically solve data quality problems in cloud-based vehicle battery monitoring and management by developing a novel integrated battery data cleaning framework. In the first stage, the outlier samples are detected by analyzing the temporal features in the battery data time series. The outlier data in the dataset can be accurately detected to avoid their impacts on battery monitoring and management. Then, the abnormal samples, including the noise polluted data and missing value, are restored by a novel future fusion data restoring model. The real electric bus operation data collected by a cloud-based battery monitoring and management platform are used to verify the performance of the developed data cleaning method. More than 93.3% of outlier samples can be detected, and the data restoring error can be limited to 2.11%, which validates the effectiveness of the developed methods. The proposed data cleaning method provides an effective data quality assessment tool in cloud-based vehicle battery management, which can further boost the practical application of the vehicle big data platform and Internet of vehicle.",https://doi.org/10.1016/j.apenergy.2022.119292,2022,Shuangqi Li and Hongwen He and Pengfei Zhao and Shuang Cheng,DATA CLEANING AND RESTORING METHOD FOR VEHICLE BATTERY BIG DATA PLATFORM,article
19,28801,APPLIED ENERGY,journal,03062619,"3,035",Q1,212,1729,5329,100144,56804,5304,"10,59","57,92",United Kingdom,Western Europe,Elsevier BV,1975-2020,"Building and Construction (Q1); Energy (miscellaneous) (Q1); Management, Monitoring, Policy and Law (Q1); Mechanical Engineering (Q1)",APPLIED ENERGY,APPLIED ENERGY,"122,712",9.746,0.15329,"As one of the bottleneck technologies of electric vehicles (EVs), the battery hosts complex and hardly observable internal chemical reactions. Therefore, a precise mathematical model is crucial for the battery management system (BMS) to ensure the secure and stable operation of the battery in a multi-variable environment. First, a Cloud-based BMS (C-BMS) is established based on a database containing complete battery status information. Next, a data cleaning method based on machine learning is applied to the big data of batteries. Meanwhile, to improve the model stability under dynamic conditions, an F-divergence-based data distribution quality assessment method and a sampling-based data preprocess method is designed. Then, a lithium-ion battery temperature-dependent model is built based on Stacked Denoising Autoencoders- Extreme Learning Machine (SDAE-ELM) algorithm, and a new training method combined with data preprocessing is also proposed to improve the model accuracy. Finally, to improve reliability, a conjunction working mode between the C-BMS and the BMS in vehicles (V-BMS) is also proposed, providing as an applied case of the model. Using the battery data extracted from electric buses, the effectiveness and accuracy of the model are validated. The error of the estimated battery terminal voltage is within 2%, and the error of the estimated State of Charge (SoC) is within 3%.",https://doi.org/10.1016/j.apenergy.2019.03.154,2019,Shuangqi Li and Hongwen He and Jianwei Li,BIG DATA DRIVEN LITHIUM-ION BATTERY MODELING METHOD BASED ON SDAE-ELM ALGORITHM AND DATA PRE-PROCESSING TECHNOLOGY,article
20,28801,APPLIED ENERGY,journal,03062619,"3,035",Q1,212,1729,5329,100144,56804,5304,"10,59","57,92",United Kingdom,Western Europe,Elsevier BV,1975-2020,"Building and Construction (Q1); Energy (miscellaneous) (Q1); Management, Monitoring, Policy and Law (Q1); Mechanical Engineering (Q1)",APPLIED ENERGY,APPLIED ENERGY,"122,712",9.746,0.15329,"Energy economy models are central to decision making on energy and climate issues in the 21st century, such as informing the design of deep decarbonisation strategies under the Paris Agreement. Designing policies that are aimed at achieving such radical transitions in the energy system will require ever more in-depth modelling of end-use demand, efficiency and fuel switching, as well as an increasing need for regional, sectoral, and agent disaggregation to capture technological, jurisdictional and policy detail. Building and using these models entails complex trade-offs between the level of detail, the size of the system boundary, and the available computing resources. The availability of data to characterise key energy system sectors and interactions is also a key driver of model structure and parameterisation, and there are many blind spots and design compromises that are caused by data scarcity. We may soon, however, live in a world of data abundance, potentially enabling previously impossible levels of resolution and coverage in energy economy models. But while big data concepts and platforms have already begun to be used in a number of selected energy research applications, their potential to improve or even completely revolutionise energy economy modelling has been almost completely overlooked in the existing literature. In this paper, we explore the challenges and possibilities of this emerging frontier. We identify critical gaps and opportunities for the field, as well as developing foundational concepts for guiding the future application of big data to energy economy modelling, with reference to the existing literature on decision making under uncertainty, scenario analysis and the philosophy of science.",https://doi.org/10.1016/j.apenergy.2019.02.002,2019,Francis G.N. Li and Chris Bataille and Steve Pye and Aidan O'Sullivan,"PROSPECTS FOR ENERGY ECONOMY MODELLING WITH BIG DATA: HYPE, ELIMINATING BLIND SPOTS, OR REVOLUTIONISING THE STATE OF THE ART?",article
21,28801,APPLIED ENERGY,journal,03062619,"3,035",Q1,212,1729,5329,100144,56804,5304,"10,59","57,92",United Kingdom,Western Europe,Elsevier BV,1975-2020,"Building and Construction (Q1); Energy (miscellaneous) (Q1); Management, Monitoring, Policy and Law (Q1); Mechanical Engineering (Q1)",APPLIED ENERGY,APPLIED ENERGY,"122,712",9.746,0.15329,"Data play an essential role in asset management decisions. The amount of data is increasing through accumulating historical data records, new measuring devices, and communication technology, notably with the evolution toward smart grids. Consequently, the management of data quantity and quality is becoming even more relevant for asset managers to meet efficiency and reliability requirements for power grids. In this work, we propose an innovative data quality management framework enabling asset managers (i) to quantify the impact of poor data quality, and (ii) to determine the conditions under which an investment in data quality improvement is required. To this end, an algorithm is used to determine the optimal year for component replacement based on three scenarios, a Reference scenario, an Imperfect information scenario, and an Investment in higher data quality scenario. Our results indicate that (i) the impact on the optimal year of replacement is the highest for middle-aged components; (ii) the profitability of investments in data quality improvement depends on various factors, including data quality, and the cost of investment in data quality improvement. Finally, we discuss the implementation of the proposed models to control data quality in practice, while taking into account real-world technological and economic limitations.",https://doi.org/10.1016/j.apenergy.2020.116057,2021,Sylvie Koziel and Patrik Hilber and Per Westerlund and Ebrahim Shayesteh,INVESTMENTS IN DATA QUALITY: EVALUATING IMPACTS OF FAULTY DATA ON ASSET MANAGEMENT IN POWER SYSTEMS,article
22,15857,JOURNAL OF CLINICAL EPIDEMIOLOGY,journal,"08954356, 18785921","2,993",Q1,212,314,726,8870,3242,528,"4,25","28,25",United States,Northern America,Elsevier USA,1988-2020,Epidemiology (Q1),JOURNAL OF CLINICAL EPIDEMIOLOGY,JOURNAL OF CLINICAL EPIDEMIOLOGY,"36,224",6.437,0.02836,"Background and Objectives
To highlight the potential of multiple file record linkage. Linkage increases the value of existing information by supplying missing data or correcting errors in existing data, through generating important covariates, and by using family information to control for unmeasured variables and expand research opportunities.
Methods
Recent Manitoba papers highlight the use of linkage to produce better studies. Specific ways in which linkage helps deal with different substantive issues are described.
Results
Wide data files—files containing considerable amounts of information on each individual—generated by linkage improve research by facilitating better design. Nonexperimental work in particular benefits from such linkages. Population registries are especially valuable in supplying family data to facilitate work across different substantive fields.
Conclusion
Several examples show how record linkage magnifies the value of information from individual projects. The results of observational studies become more defensible through the better designs facilitated by such linkage.",https://doi.org/10.1016/j.jclinepi.2022.06.006,2022,Leslie L. Roos and Elizabeth Wall-Wieler and Charles Burchill and Naomi C. Hamm and Amani F. Hamad and Lisa M. Lix,RECORD LINKAGE AND BIG DATA—ENHANCING INFORMATION AND IMPROVING DESIGN,article
23,29161,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,journal,09242716,"2,960",Q1,138,264,677,16114,7306,668,"10,56","61,04",Netherlands,Western Europe,Elsevier,1989-2020,"Atomic and Molecular Physics, and Optics (Q1); Computer Science Applications (Q1); Computers in Earth Sciences (Q1); Engineering (miscellaneous) (Q1); Geography, Planning and Development (Q1)",ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,"18,026",8.979,0.02145,"Spatially contiguous aerosol type grids were rarely available for air quality management in the past. To bridge the gap, we developed an integrated remote sensing and big data analytics framework to generate spatially gap-free aerosol type grids between 2000 and 2020 in China. The effect of emission control via environmental management on haze reduction was fully realized for the first time with the aid of satellite-based gap-free aerosol type data. Daily gap-free aerosol fine mode fraction (FMF) data were first derived via a data-driven regression model based on remote sensing big data. According to empirically determined FMF probability distributions over regions with typical emission sources, aerosols in China were classified into eight major types, including typical/atypical/mixed anthropogenic aerosols, typical/atypical/mixed dust, and typical mixed and multiple modes. The results indicated that the gridded FMF estimates derived in this study agreed well with FMF retrievals from AERONET, with correlation coefficient of 0.81 and root mean square error of 0.13. The long-term variations in major aerosol types showed that in China the territory covered by typical anthropogenic aerosols was reduced from 21.38% to 11.76% over the past two decades, while dust aerosols decreased from 6.99% to 2.15%. The declining trend in anthropogenic aerosols could be attributed to reduced coal consumption and/or favorable dispersion conditions, whereas decreasing dust aerosols were largely associated with increased vegetation cover and/or weakened wind speed in the west. Overall, such advancements provide fresh evidence to improve our understanding of the emission control effect on haze pollution variations in China.",https://doi.org/10.1016/j.isprsjprs.2022.09.001,2022,Ke Li and Kaixu Bai and Mingliang Ma and Jianping Guo and Zhengqiang Li and Gehui Wang and Ni-Bin Chang,SPATIALLY GAP FREE ANALYSIS OF AEROSOL TYPE GRIDS IN CHINA: FIRST RETRIEVAL VIA SATELLITE REMOTE SENSING AND BIG DATA ANALYTICS,article
24,29161,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,journal,09242716,"2,960",Q1,138,264,677,16114,7306,668,"10,56","61,04",Netherlands,Western Europe,Elsevier,1989-2020,"Atomic and Molecular Physics, and Optics (Q1); Computer Science Applications (Q1); Computers in Earth Sciences (Q1); Engineering (miscellaneous) (Q1); Geography, Planning and Development (Q1)",ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,"18,026",8.979,0.02145,"The recent explosive publications of big data studies have well documented the rise of big data and its ongoing prevalence. Different types of “big data” have emerged and have greatly enriched spatial information sciences and related fields in terms of breadth and granularity. Studies that were difficult to conduct in the past time due to data availability can now be carried out. However, big data brings lots of “big errors” in data quality and data usage, which cannot be used as a substitute for sound research design and solid theories. We indicated and summarized the problems faced by current big data studies with regard to data collection, processing and analysis: inauthentic data collection, information incompleteness and noise of big data, unrepresentativeness, consistency and reliability, and ethical issues. Cases of empirical studies are provided as evidences for each problem. We propose that big data research should closely follow good scientific practice to provide reliable and scientific “stories”, as well as explore and develop techniques and methods to mitigate or rectify those ‘big-errors’ brought by big data.",https://doi.org/10.1016/j.isprsjprs.2015.11.006,2016,Jianzheng Liu and Jie Li and Weifeng Li and Jiansheng Wu,RETHINKING BIG DATA: A REVIEW ON THE DATA QUALITY AND USAGE ISSUES,article
25,29161,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,journal,09242716,"2,960",Q1,138,264,677,16114,7306,668,"10,56","61,04",Netherlands,Western Europe,Elsevier,1989-2020,"Atomic and Molecular Physics, and Optics (Q1); Computer Science Applications (Q1); Computers in Earth Sciences (Q1); Engineering (miscellaneous) (Q1); Geography, Planning and Development (Q1)",ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,"18,026",8.979,0.02145,"Accurate information about the location, extent, and type of Land Cover (LC) is essential for various applications. The only recent available country-wide LC map of Iran was generated in 2016 by the Iranian Space Agency (ISA) using Moderate Resolution Imaging Spectroradiometer (MODIS) images with a considerably low accuracy. Therefore, the production of an up-to-date and accurate Iran-wide LC map using the most recent remote sensing, machine learning, and big data processing algorithms is required. Moreover, it is important to develop an efficient method for automatic LC generation for various time periods without the need to collect additional ground truth data from this immense country. Therefore, this study was conducted to fulfill two objectives. First, an improved Iranian LC map with 13 LC classes and a spatial resolution of 10 m was produced using multi-temporal synergy of Sentinel-1 and Sentinel-2 satellite datasets applied to an object-based Random forest (RF) algorithm. For this purpose, 2,869 Sentinel-1 and 11,994 Sentinel-2 scenes acquired in 2017 were processed and classified within the Google Earth Engine (GEE) cloud computing platform allowing big geospatial data analysis. The Overall Accuracy (OA) and Kappa Coefficient (KC) of the final Iran-wide LC map for 2017 was 95.6% and 0.95, respectively, indicating the considerable potential of the proposed big data processing method. Second, an efficient automatic method was developed based on Sentinel-2 images to migrate ground truth samples from a reference year to automatically generate an LC map for any target year. The OA and KC for the LC map produced for the target year 2019 were 91.35% and 0.91, respectively, demonstrating the efficiency of the proposed method for automatic LC mapping. Based on the obtained accuracies, this method can potentially be applied to other regions of interest for LC mapping without the need for ground truth data from the target year.",https://doi.org/10.1016/j.isprsjprs.2020.07.013,2020,Arsalan Ghorbanian and Mohammad Kakooei and Meisam Amani and Sahel Mahdavi and Ali Mohammadzadeh and Mahdi Hasanlou,IMPROVED LAND COVER MAP OF IRAN USING SENTINEL IMAGERY WITHIN GOOGLE EARTH ENGINE AND A NOVEL AUTOMATIC WORKFLOW FOR LAND COVER CLASSIFICATION USING MIGRATED TRAINING SAMPLES,article
26,29161,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,journal,09242716,"2,960",Q1,138,264,677,16114,7306,668,"10,56","61,04",Netherlands,Western Europe,Elsevier,1989-2020,"Atomic and Molecular Physics, and Optics (Q1); Computer Science Applications (Q1); Computers in Earth Sciences (Q1); Engineering (miscellaneous) (Q1); Geography, Planning and Development (Q1)",ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,"18,026",8.979,0.02145,"Discrete global grid systems (DGGS) have been proposed as a data model for a digital earth framework. We introduce a new data model and analytics system called IDEAS – integrated discrete environmental analysis system to create an operational DGGS-based GIS which is suitable for large scale environmental modelling and analysis. Our analysis demonstrates that DGGS-based GIS is feasible within a relational database environment incorporating common data analytics tools. Common GIS operations implemented in our DGGS data model outperformed the same operations computed using traditional geospatial data types. A case study into wildfire modelling demonstrates the capability for data integration and supporting big data geospatial analytics. These results indicate that DGGS data models have significant capability to solve some of the key outstanding problems related to geospatial data analytics, providing a common representation upon which fast and scalable algorithms can be built.",https://doi.org/10.1016/j.isprsjprs.2020.02.009,2020,Colin Robertson and Chiranjib Chaudhuri and Majid Hojati and Steven A. Roberts,AN INTEGRATED ENVIRONMENTAL ANALYTICS SYSTEM (IDEAS) BASED ON A DGGS,article
27,29161,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,journal,09242716,"2,960",Q1,138,264,677,16114,7306,668,"10,56","61,04",Netherlands,Western Europe,Elsevier,1989-2020,"Atomic and Molecular Physics, and Optics (Q1); Computer Science Applications (Q1); Computers in Earth Sciences (Q1); Engineering (miscellaneous) (Q1); Geography, Planning and Development (Q1)",ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,"18,026",8.979,0.02145,"Big data has now become a strong focus of global interest that is increasingly attracting the attention of academia, industry, government and other organizations. Big data can be situated in the disciplinary area of traditional geospatial data handling theory and methods. The increasing volume and varying format of collected geospatial big data presents challenges in storing, managing, processing, analyzing, visualizing and verifying the quality of data. This has implications for the quality of decisions made with big data. Consequently, this position paper of the International Society for Photogrammetry and Remote Sensing (ISPRS) Technical Commission II (TC II) revisits the existing geospatial data handling methods and theories to determine if they are still capable of handling emerging geospatial big data. Further, the paper synthesises problems, major issues and challenges with current developments as well as recommending what needs to be developed further in the near future.",https://doi.org/10.1016/j.isprsjprs.2015.10.012,2016,Songnian Li and Suzana Dragicevic and Francesc Antón Castro and Monika Sester and Stephan Winter and Arzu Coltekin and Christopher Pettit and Bin Jiang and James Haworth and Alfred Stein and Tao Cheng,GEOSPATIAL BIG DATA HANDLING THEORY AND METHODS: A REVIEW AND RESEARCH CHALLENGES,article
28,22647,GONDWANA RESEARCH,journal,1342937X,"2,859",Q1,135,243,535,31951,3698,517,"5,81","131,49",United States,Northern America,Elsevier Inc.,1997-2020,Geology (Q1),GONDWANA RESEARCH,GONDWANA RESEARCH,"18,040",6.051,0.01984,"The so-called Fourth Paradigm has witnessed a boom during the past two decades, with large volumes of observational data becoming available to scientists and engineers. Big data is characterized by the rule of the five Vs: Volume, Variety, Value, Velocity and Veracity. The concept of big data naturally matches well with the features of geoengineering and geoscience. Large-scale, comprehensive, multidirectional and multifield geotechnical data analysis is becoming a trend. On the other hand, Machine learning (ML), Deep Learning (DL) and Optimization Algorithm (OA) provide the ability to learn from data and deliver in-depth insight into geotechnical problems. Researchers use different ML, DL and OA models to solve various problems associated with geoengineering and geoscience. Consequently, there is a need to extend its research with big data research through integrating the use of ML, DL and OA techniques. This work focuses on a systematic review on the state-of-the-art application of ML, DL and OA algorithms in geoengineering and geoscience. Various ML, DL, and OA approaches are firstly concisely introduced, concerning mainly the supervised learning, unsupervised learning, deep learning and optimization algorithms. Then their representative applications in the geoengineering and geoscience are summarized via VOSviewer demonstration. The authors also provided their own thoughts learnt from these applications as well as work ongoing and future recommendations. This review paper aims to make a comprehensive summary and provide fundamental guidelines for researchers and engineers in the discipline of geoengineering and geoscience or similar research areas on how to integrate and apply ML, DL and OA methods.",https://doi.org/10.1016/j.gr.2022.03.015,2022,Wengang Zhang and Xin Gu and Libin Tang and Yueping Yin and Dongsheng Liu and Yanmei Zhang,"APPLICATION OF MACHINE LEARNING, DEEP LEARNING AND OPTIMIZATION ALGORITHMS IN GEOENGINEERING AND GEOSCIENCE: COMPREHENSIVE REVIEW AND FUTURE CHALLENGE",article
29,15579,CURRENT OPINION IN BIOTECHNOLOGY,journal,"18790429, 09581669","2,843",Q1,202,193,475,11888,4236,444,"9,14","61,60",United Kingdom,Western Europe,Elsevier Ltd.,1990-2020,Bioengineering (Q1); Biomedical Engineering (Q1); Biotechnology (Q1),CURRENT OPINION IN BIOTECHNOLOGY,CURRENT OPINION IN BIOTECHNOLOGY,"18,835",9.740,0.0197,"Modern agriculture and food production systems are facing increasing pressures from climate change, land and water availability, and, more recently, a pandemic. These factors are threatening the environmental and economic sustainability of current and future food supply systems. Scientific and technological innovations are needed more than ever to secure enough food for a fast-growing global population. Scientific advances have led to a better understanding of how various components of the agricultural system interact, from the cell to the field level. Despite incredible advances in genetic tools over the past few decades, our ability to accurately assess crop status in the field, at scale, has been severely lacking until recently. Thanks to recent advances in remote sensing and Artificial Intelligence (AI), we can now quantify field scale phenotypic information accurately and integrate the big data into predictive and prescriptive management tools. This review focuses on the use of recent technological advances in remote sensing and AI to improve the resilience of agricultural systems, and we will present a unique opportunity for the development of prescriptive tools needed to address the next decade’s agricultural and human nutrition challenges.",https://doi.org/10.1016/j.copbio.2020.09.003,2021,Jinha Jung and Murilo Maeda and Anjin Chang and Mahendra Bhandari and Akash Ashapure and Juan Landivar-Bowles,THE POTENTIAL OF REMOTE SENSING AND ARTIFICIAL INTELLIGENCE AS TOOLS TO IMPROVE THE RESILIENCE OF AGRICULTURE PRODUCTION SYSTEMS,article
30,26099,INFORMATION FUSION,journal,"18726305, 15662535","2,776",Q1,107,168,318,13659,5599,312,"15,73","81,30",Netherlands,Western Europe,Elsevier,2000-2021,Hardware and Architecture (Q1); Information Systems (Q1); Signal Processing (Q1); Software (Q1),INFORMATION FUSION,INFORMATION FUSION,"9,059",12.975,0.0126,"Urban big data fusion creates huge values for urban computing in solving urban problems. In recent years, various models and algorithms based on deep learning have been proposed to unlock the power of knowledge from urban big data. To clarify the methodologies of urban big data fusion based on deep learning (DL), this paper classifies them into three categories: DL-output-based fusion, DL-input-based fusion and DL-double-stage-based fusion. These methods use deep learning to learn feature representation from multi-source big data. Then each category of fusion methods is introduced and some examples are shown. The difficulties and ideas of dealing with urban big data will also be discussed.",https://doi.org/10.1016/j.inffus.2019.06.016,2020,Jia Liu and Tianrui Li and Peng Xie and Shengdong Du and Fei Teng and Xin Yang,URBAN BIG DATA FUSION BASED ON DEEP LEARNING: AN OVERVIEW,article
31,26099,INFORMATION FUSION,journal,"18726305, 15662535","2,776",Q1,107,168,318,13659,5599,312,"15,73","81,30",Netherlands,Western Europe,Elsevier,2000-2021,Hardware and Architecture (Q1); Information Systems (Q1); Signal Processing (Q1); Software (Q1),INFORMATION FUSION,INFORMATION FUSION,"9,059",12.975,0.0126,"The advancement of various research sectors such as Internet of Things (IoT), Machine Learning, Data Mining, Big Data, and Communication Technology has shed some light in transforming an urban city integrating the aforementioned techniques to a commonly known term - Smart City. With the emergence of smart city, plethora of data sources have been made available for wide variety of applications. The common technique for handling multiple data sources is data fusion, where it improves data output quality or extracts knowledge from the raw data. In order to cater evergrowing highly complicated applications, studies in smart city have to utilize data from various sources and evaluate their performance based on multiple aspects. To this end, we introduce a multi-perspectives classification of the data fusion to evaluate the smart city applications. Moreover, we applied the proposed multi-perspectives classification to evaluate selected applications in each domain of the smart city. We conclude the paper by discussing potential future direction and challenges of data fusion integration.",https://doi.org/10.1016/j.inffus.2019.05.004,2019,Billy Pik Lik Lau and Sumudu Hasala Marakkalage and Yuren Zhou and Naveed Ul Hassan and Chau Yuen and Meng Zhang and U-Xuan Tan,A SURVEY OF DATA FUSION IN SMART CITY APPLICATIONS,article
32,26099,INFORMATION FUSION,journal,"18726305, 15662535","2,776",Q1,107,168,318,13659,5599,312,"15,73","81,30",Netherlands,Western Europe,Elsevier,2000-2021,Hardware and Architecture (Q1); Information Systems (Q1); Signal Processing (Q1); Software (Q1),INFORMATION FUSION,INFORMATION FUSION,"9,059",12.975,0.0126,"Deep learning, as one of the most currently remarkable machine learning techniques, has achieved great success in many applications such as image analysis, speech recognition and text understanding. It uses supervised and unsupervised strategies to learn multi-level representations and features in hierarchical architectures for the tasks of classification and pattern recognition. Recent development in sensor networks and communication technologies has enabled the collection of big data. Although big data provides great opportunities for a broad of areas including e-commerce, industrial control and smart medical, it poses many challenging issues on data mining and information processing due to its characteristics of large volume, large variety, large velocity and large veracity. In the past few years, deep learning has played an important role in big data analytic solutions. In this paper, we review the emerging researches of deep learning models for big data feature learning. Furthermore, we point out the remaining challenges of big data deep learning and discuss the future topics.",https://doi.org/10.1016/j.inffus.2017.10.006,2018,Qingchen Zhang and Laurence T. Yang and Zhikui Chen and Peng Li,A SURVEY ON DEEP LEARNING FOR BIG DATA,article
33,26099,INFORMATION FUSION,journal,"18726305, 15662535","2,776",Q1,107,168,318,13659,5599,312,"15,73","81,30",Netherlands,Western Europe,Elsevier,2000-2021,Hardware and Architecture (Q1); Information Systems (Q1); Signal Processing (Q1); Software (Q1),INFORMATION FUSION,INFORMATION FUSION,"9,059",12.975,0.0126,"The landscape of mental health has undergone tremendous changes within the last two decades, but the research on mental health is still at the initial stage with substantial knowledge gaps and the lack of precise diagnosis. Nowadays, big data and artificial intelligence offer new opportunities for the screening and prediction of mental problems. In this review paper, we outline the vision of digital phenotyping of mental health (DPMH) by fusing the enriched data from ubiquitous sensors, social media and healthcare systems, and present a broad overview of DPMH from sensing and computing perspectives. We first conduct a systematical literature review and propose the research framework, which highlights the key aspects related with mental health, and discuss the challenges elicited by the enriched data for digital phenotyping. Next, five key research strands including affect recognition, cognitive analytics, behavioral anomaly detection, social analytics, and biomarker analytics are unfolded in the psychiatric context. Finally, we discuss various open issues and the corresponding solutions to underpin the digital phenotyping of mental health.",https://doi.org/10.1016/j.inffus.2019.04.001,2019,Yunji Liang and Xiaolong Zheng and Daniel D. Zeng,A SURVEY ON BIG DATA-DRIVEN DIGITAL PHENOTYPING OF MENTAL HEALTH,article
34,26099,INFORMATION FUSION,journal,"18726305, 15662535","2,776",Q1,107,168,318,13659,5599,312,"15,73","81,30",Netherlands,Western Europe,Elsevier,2000-2021,Hardware and Architecture (Q1); Information Systems (Q1); Signal Processing (Q1); Software (Q1),INFORMATION FUSION,INFORMATION FUSION,"9,059",12.975,0.0126,"Big data has become an important issue for a large number of research areas such as data mining, machine learning, computational intelligence, information fusion, the semantic Web, and social networks. The rise of different big data frameworks such as Apache Hadoop and, more recently, Spark, for massive data processing based on the MapReduce paradigm has allowed for the efficient utilisation of data mining methods and machine learning algorithms in different domains. A number of libraries such as Mahout and SparkMLib have been designed to develop new efficient applications based on machine learning algorithms. The combination of big data technologies and traditional machine learning algorithms has generated new and interesting challenges in other areas as social media and social networks. These new challenges are focused mainly on problems such as data processing, data storage, data representation, and how data can be used for pattern mining, analysing user behaviours, and visualizing and tracking data, among others. In this paper, we present a revision of the new methodologies that is designed to allow for efficient data mining and information fusion from social media and of the new applications and frameworks that are currently appearing under the “umbrella” of the social networks, social media and big data paradigms.",https://doi.org/10.1016/j.inffus.2015.08.005,2016,Gema Bello-Orgaz and Jason J. Jung and David Camacho,SOCIAL BIG DATA: RECENT ACHIEVEMENTS AND NEW CHALLENGES,article
35,15631,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,journal,02684012,"2,770",Q1,114,224,382,20318,5853,373,"16,16","90,71",United Kingdom,Western Europe,Elsevier Ltd.,"1970, 1986-2021",Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1),INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,"12,245",14.098,0.01167,"Open data aims to unlock the innovation potential of businesses, governments, and entrepreneurs, yet it also harbours significant challenges for its effective use. While numerous innovation successes exist that are based on the open data paradigm, there is uncertainty over the data quality of such datasets. This data quality uncertainty is a threat to the value that can be generated from such data. Data quality has been studied extensively over many decades and many approaches to data quality management have been proposed. However, these approaches are typically based on datasets internal to organizations, with known metadata, and domain knowledge of the data semantics. Open data, on the other hand, are often unfamiliar to the user and may lack metadata. The aim of this research note is to outline the challenges in dealing with data quality of open datasets, and to set an agenda for future research to address this risk to deriving value from open data investments.",https://doi.org/10.1016/j.ijinfomgt.2017.01.003,2017,Shazia Sadiq and Marta Indulska,OPEN DATA: QUALITY OVER QUANTITY,article
36,15631,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,journal,02684012,"2,770",Q1,114,224,382,20318,5853,373,"16,16","90,71",United Kingdom,Western Europe,Elsevier Ltd.,"1970, 1986-2021",Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1),INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,"12,245",14.098,0.01167,"The social housing sector has yet to realise the potential of high data quality. While other businesses, mainly in the private sector, reap the benefits of data quality, the social housing sector seems paralysed, as it is still struggling with recent government regulations and steep revenue reduction. This paper offers a succinct review of relevant literature on data quality and how it relates to social housing. The Housing and Development Board in Singapore offers a great example on how to integrate data quality initiatives in the social housing sector. Taking this example, the research presented in this paper is extrapolating cross-disciplinarily recommendations on how to implement data quality initiatives in social housing providers in the UK.",https://doi.org/10.1016/j.ijinfomgt.2017.09.008,2018,Caroline Duvier and Daniel Neagu and Crina Oltean-Dumbrava and Dave Dickens,DATA QUALITY CHALLENGES IN THE UK SOCIAL HOUSING SECTOR,article
37,15631,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,journal,02684012,"2,770",Q1,114,224,382,20318,5853,373,"16,16","90,71",United Kingdom,Western Europe,Elsevier Ltd.,"1970, 1986-2021",Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1),INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,"12,245",14.098,0.01167,"The success or failure of a RIS in a scientific institution is largely related to the quality of the data available as a basis for the RIS applications. The most beautiful Business Intelligence (BI) tools (reporting, etc.) are worthless when displaying incorrect, incomplete, or inconsistent data. An integral part of every RIS is thus the integration of data from the operative systems. Before starting the integration process (ETL) of a source system, a rich analysis of source data is required. With the support of a data quality check, causes of quality problems can usually be detected. Corresponding analyzes are performed with data profiling to provide a good picture of the state of the data. In this paper, methods of data profiling are presented in order to gain an overview of the quality of the data in the source systems before their integration into the RIS. With the help of data profiling, the scientific institutions can not only evaluate their research information and provide information about their quality, but also examine the dependencies and redundancies between data fields and better correct them within their RIS.",https://doi.org/10.1016/j.ijinfomgt.2018.02.007,2018,Otmane Azeroual and Gunter Saake and Eike Schallehn,ANALYZING DATA QUALITY ISSUES IN RESEARCH INFORMATION SYSTEMS VIA DATA PROFILING,article
38,15631,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,journal,02684012,"2,770",Q1,114,224,382,20318,5853,373,"16,16","90,71",United Kingdom,Western Europe,Elsevier Ltd.,"1970, 1986-2021",Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1),INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,"12,245",14.098,0.01167,"This is the first systematic literature review concerning the interconnections between big data (BD) and co-innovation. It uses BD as a common perspective of analysis as well as a concept aggregating different research streams (open innovation, co-creation and collaborative innovation). The review is based on the results of a bibliographic coupling analysis performed with 51 peer-reviewed papers published before the end of 2019. Three thematic clusters were discovered, which respectively focused on BD as a knowledge creation enabler within co-innovation contexts, BD as a driver of co-innovation processes based on customer engagement, and the impact of BD on co-innovation within service ecosystems. The paper theoretically argues that the use of BD, in addition to enhancing intentional and direct collaborative innovation processes, allows the development of passive and unintentional co-innovation that can be implemented through indirect relationships between the collaborative actors. This study also makes eleven unique research propositions concerning further theoretical developments and managerial implementations in the field of BD-driven co-innovation.",https://doi.org/10.1016/j.ijinfomgt.2021.102347,2021,Stefano Bresciani and Francesco Ciampi and Francesco Meli and Alberto Ferraris,"USING BIG DATA FOR CO-INNOVATION PROCESSES: MAPPING THE FIELD OF DATA-DRIVEN INNOVATION, PROPOSING THEORETICAL DEVELOPMENTS AND PROVIDING A RESEARCH AGENDA",article
39,15631,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,journal,02684012,"2,770",Q1,114,224,382,20318,5853,373,"16,16","90,71",United Kingdom,Western Europe,Elsevier Ltd.,"1970, 1986-2021",Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1),INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,"12,245",14.098,0.01167,"Big data analytics associated with database searching, mining, and analysis can be seen as an innovative IT capability that can improve firm performance. Even though some leading companies are actively adopting big data analytics to strengthen market competition and to open up new business opportunities, many firms are still in the early stage of the adoption curve due to lack of understanding of and experience with big data. Hence, it is interesting and timely to understand issues relevant to big data adoption. In this study, a research model is proposed to explain the acquisition intention of big data analytics mainly from the theoretical perspectives of data quality management and data usage experience. Our empirical investigation reveals that a firm's intention for big data analytics can be positively affected by its competence in maintaining the quality of corporate data. Moreover, a firm's favorable experience (i.e., benefit perceptions) in utilizing external source data could encourage future acquisition of big data analytics. Surprisingly, a firm's favorable experience (i.e., benefit perceptions) in utilizing internal source data could hamper its adoption intention for big data analytics.",https://doi.org/10.1016/j.ijinfomgt.2014.02.002,2014,Ohbyung Kwon and Namyeon Lee and Bongsik Shin,"DATA QUALITY MANAGEMENT, DATA USAGE EXPERIENCE AND ACQUISITION INTENTION OF BIG DATA ANALYTICS",article
40,15631,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,journal,02684012,"2,770",Q1,114,224,382,20318,5853,373,"16,16","90,71",United Kingdom,Western Europe,Elsevier Ltd.,"1970, 1986-2021",Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1),INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,"12,245",14.098,0.01167,"Data governance refers to the exercise of authority and control over the management of data. The purpose of data governance is to increase the value of data and minimize data-related cost and risk. Despite data governance gaining in importance in recent years, a holistic view on data governance, which could guide both practitioners and researchers, is missing. In this review paper, we aim to close this gap and develop a conceptual framework for data governance, synthesize the literature, and provide a research agenda. We base our work on a structured literature review including 145 research papers and practitioner publications published during 2001-2019. We identify the major building blocks of data governance and decompose them along six dimensions. The paper supports future research on data governance by identifying five research areas and displaying a total of 15 research questions. Furthermore, the conceptual framework provides an overview of antecedents, scoping parameters, and governance mechanisms to assist practitioners in approaching data governance in a structured manner.",https://doi.org/10.1016/j.ijinfomgt.2019.07.008,2019,Rene Abraham and Johannes Schneider and Jan {vom Brocke},"DATA GOVERNANCE: A CONCEPTUAL FRAMEWORK, STRUCTURED REVIEW, AND RESEARCH AGENDA",article
41,15631,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,journal,02684012,"2,770",Q1,114,224,382,20318,5853,373,"16,16","90,71",United Kingdom,Western Europe,Elsevier Ltd.,"1970, 1986-2021",Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1),INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,"12,245",14.098,0.01167,"The critical factors in the big data era are collection, analysis, and dissemination of information to improve an organization’s competitive position and enhance its products and services. In this scenario, it is imperative that organizations use Intelligence, which is understood as a process of gathering, analyzing, interpreting, and disseminating high-value data and information at the right time for use in the decision-making process. Earlier, the concept of Intelligence was associated with the military and national security sector; however, in present times, and as organizations evolve, Intelligence has been defined in several ways for the purposes of different applications. Given that the purpose of Intelligence is to obtain real value from data, information, and the dynamism of the organizations, the study of this discipline provides an opportunity to analyze the core trends related to data collection and processing, information management, decision-making process, and organizational capabilities. Therefore, the present study makes a conceptual analysis of the existing definitions of intelligence in the literature by quantifying the main bibliometric performance indicators, identifying the main authors and research areas, and evaluating the development of the field using SciMAT as a bibliometric analysis software.",https://doi.org/10.1016/j.ijinfomgt.2019.01.013,2019,J.R. López-Robles and J.R. Otegi-Olaso and I. {Porto Gómez} and M.J. Cobo,30 YEARS OF INTELLIGENCE MODELS IN MANAGEMENT AND BUSINESS: A BIBLIOMETRIC REVIEW,article
42,15631,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,journal,02684012,"2,770",Q1,114,224,382,20318,5853,373,"16,16","90,71",United Kingdom,Western Europe,Elsevier Ltd.,"1970, 1986-2021",Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1),INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,"12,245",14.098,0.01167,"Agile methodologies were introduced in 2001. Since this time, practitioners have applied Agile methodologies to many delivery disciplines. This article explores the application of Agile methodologies and principles to business intelligence delivery and how Agile has changed with the evolution of business intelligence. Business intelligence has evolved because the amount of data generated through the internet and smart devices has grown exponentially altering how organizations and individuals use information. The practice of business intelligence delivery with an Agile methodology has matured; however, business intelligence has evolved altering the use of Agile principles and practices. The Big Data phenomenon, the volume, variety, and velocity of data, has impacted business intelligence and the use of information. New trends such as fast analytics and data science have emerged as part of business intelligence. This paper addresses how Agile principles and practices have evolved with business intelligence, as well as its challenges and future directions.",https://doi.org/10.1016/j.ijinfomgt.2016.04.013,2016,Deanne Larson and Victor Chang,"A REVIEW AND FUTURE DIRECTION OF AGILE, BUSINESS INTELLIGENCE, ANALYTICS AND DATA SCIENCE",article
43,15631,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,journal,02684012,"2,770",Q1,114,224,382,20318,5853,373,"16,16","90,71",United Kingdom,Western Europe,Elsevier Ltd.,"1970, 1986-2021",Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1),INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,"12,245",14.098,0.01167,"Analysis of data by humans can be a time-consuming activity and thus use of sophisticated cognitive systems can be utilized to crunch this enormous amount of data. Cognitive computing can be utilized to reduce the shortcomings of the concerns faced during big data analytics. The aim of the study is to provide readers a complete understanding of past, present and future directions in the domain big data and cognitive computing. A systematic literature review has been adopted for this study by using the Scopus, DBLP and Web of Science databases. The work done in the field of big data and cognitive computing is currently at the nascent stage and this is evident from the publication record. The characteristics of cognitive computing, namely observation, interpretation, evaluation and decision were mapped to the five V’s of big data namely volume, variety, veracity, velocity and value. Perspectives which touch all these parameters are yet to be widely explored in existing literature.",https://doi.org/10.1016/j.ijinfomgt.2018.06.005,2018,Shivam Gupta and Arpan Kumar Kar and Abdullah Baabdullah and Wassan A.A. Al-Khowaiter,BIG DATA WITH COGNITIVE COMPUTING: A REVIEW FOR THE FUTURE,article
44,15631,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,journal,02684012,"2,770",Q1,114,224,382,20318,5853,373,"16,16","90,71",United Kingdom,Western Europe,Elsevier Ltd.,"1970, 1986-2021",Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1),INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,"12,245",14.098,0.01167,"People, devices, infrastructures and sensors can constantly communicate exchanging data and generating new data that trace many of these exchanges. This leads to vast volumes of data collected at ever increasing velocities and of different variety, a phenomenon currently known as Big Data. In particular, recent developments in Information and Communications Technologies are pushing the fourth industrial revolution, Industry 4.0, being data generated by several sources like machine controllers, sensors, manufacturing systems, among others. Joining volume, variety and velocity of data, with Industry 4.0, makes the opportunity to enhance sustainable innovation in the Factories of the Future. In this, the collection, integration, storage, processing and analysis of data is a key challenge, being Big Data systems needed to link all the entities and data needs of the factory. Thereby, this paper addresses this key challenge, proposing and implementing a Big Data Analytics architecture, using a multinational organisation (Bosch Car Multimedia – Braga) as a case study. In this work, all the data lifecycle, from collection to analysis, is handled, taking into consideration the different data processing speeds that can exist in the real environment of a factory (batch or stream).",https://doi.org/10.1016/j.ijinfomgt.2017.07.012,2017,Maribel Yasmina Santos and Jorge {Oliveira e Sá} and Carina Andrade and Francisca {Vale Lima} and Eduarda Costa and Carlos Costa and Bruno Martinho and João Galvão,A BIG DATA SYSTEM SUPPORTING BOSCH BRAGA INDUSTRY 4.0 STRATEGY,article
45,15631,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,journal,02684012,"2,770",Q1,114,224,382,20318,5853,373,"16,16","90,71",United Kingdom,Western Europe,Elsevier Ltd.,"1970, 1986-2021",Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1),INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,"12,245",14.098,0.01167,"Big data is a potential research area receiving considerable attention from academia and IT communities. In the digital world, the amounts of data generated and stored have expanded within a short period of time. Consequently, this fast growing rate of data has created many challenges. In this paper, we use structuralism and functionalism paradigms to analyze the origins of big data applications and its current trends. This paper presents a comprehensive discussion on state-of-the-art big data technologies based on batch and stream data processing. Moreover, strengths and weaknesses of these technologies are analyzed. This study also discusses big data analytics techniques, processing methods, some reported case studies from different vendors, several open research challenges, and the opportunities brought about by big data. The similarities and differences of these techniques and technologies based on important parameters are also investigated. Emerging technologies are recommended as a solution for big data problems.",https://doi.org/10.1016/j.ijinfomgt.2016.07.009,2016,Ibrar Yaqoob and Ibrahim Abaker Targio Hashem and Abdullah Gani and Salimah Mokhtar and Ejaz Ahmed and Nor Badrul Anuar and Athanasios V. Vasilakos,BIG DATA: FROM BEGINNING TO FUTURE,article
46,15631,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,journal,02684012,"2,770",Q1,114,224,382,20318,5853,373,"16,16","90,71",United Kingdom,Western Europe,Elsevier Ltd.,"1970, 1986-2021",Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1),INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,"12,245",14.098,0.01167,"The purpose of this study is to enrich the existing state-of-the-art literature on the impact of big data on business growth by examining how dozens of organizational theories can be applied to enhance the understanding of the effects of big data on organizational performance. While the majority of management disciplines have had research dedicated to the conceptual discussion of how to link a variety of organizational theories to empirically quantified research topics, the body of research into big data so far lacks an academic work capable of systematising the organizational theories supporting big data domain. The three main contributions of this work are: (a) it addresses the application of dozens of organizational theories to big data research; (b) it offers a research agenda on how to link organizational theories to empirical research in big data; and (c) it foresees promising linkages between organizational theories and the effects of big data on organizational performance, with the aim of contributing to further research in this field. This work concludes by presenting implications for researchers and managers, and by highlighting intrinsic limitations of the research.",https://doi.org/10.1016/j.ijinfomgt.2018.07.005,2018,Paula {de Camargo Fiorini} and Bruno Michel {Roman Pais Seles} and Charbel Jose {Chiappetta Jabbour} and Enzo {Barberio Mariano} and Ana Beatriz Lopes {de Sousa Jabbour},MANAGEMENT THEORY AND BIG DATA LITERATURE: FROM A REVIEW TO A RESEARCH AGENDA,article
47,15631,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,journal,02684012,"2,770",Q1,114,224,382,20318,5853,373,"16,16","90,71",United Kingdom,Western Europe,Elsevier Ltd.,"1970, 1986-2021",Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1),INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,"12,245",14.098,0.01167,"Recently, patient safety and healthcare have gained high attention in professional and health policy-makers. This rapid growth causes generating a high amount of data, which is known as big data. Therefore, handling and processing of this data are attracted great attention. Cloud computing is one of the main choices for handling and processing of this type of data. But, as far as we know, the detailed review and deep discussion in this filed are very rare. Therefore, this paper reviews and discusses the recently introduced mechanisms in this field as well as providing a deep analysis of their applied mechanisms. Moreover, the drawbacks and benefits of the reviewed mechanisms have been discussed and the main challenges of these mechanisms are highlighted for developing more efficient healthcare big data processing techniques over cloud computing in the future.",https://doi.org/10.1016/j.ijinfomgt.2019.05.017,2019,Lila Rajabion and Abdusalam Abdulla Shaltooki and Masoud Taghikhah and Amirhossein Ghasemi and Arshad Badfar,HEALTHCARE BIG DATA PROCESSING MECHANISMS: THE ROLE OF CLOUD COMPUTING,article
48,15631,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,journal,02684012,"2,770",Q1,114,224,382,20318,5853,373,"16,16","90,71",United Kingdom,Western Europe,Elsevier Ltd.,"1970, 1986-2021",Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1),INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,"12,245",14.098,0.01167,"The advent of connected devices and omnipresence of Internet have paved way for intruders to attack networks, which leads to cyber-attack, financial loss, information theft in healthcare, and cyber war. Hence, network security analytics has become an important area of concern and has gained intensive attention among researchers, off late, specifically in the domain of anomaly detection in network, which is considered crucial for network security. However, preliminary investigations have revealed that the existing approaches to detect anomalies in network are not effective enough, particularly to detect them in real time. The reason for the inefficacy of current approaches is mainly due the amassment of massive volumes of data though the connected devices. Therefore, it is crucial to propose a framework that effectively handles real time big data processing and detect anomalies in networks. In this regard, this paper attempts to address the issue of detecting anomalies in real time. Respectively, this paper has surveyed the state-of-the-art real-time big data processing technologies related to anomaly detection and the vital characteristics of associated machine learning algorithms. This paper begins with the explanation of essential contexts and taxonomy of real-time big data processing, anomalous detection, and machine learning algorithms, followed by the review of big data processing technologies. Finally, the identified research challenges of real-time big data processing in anomaly detection are discussed.",https://doi.org/10.1016/j.ijinfomgt.2018.08.006,2019,Riyaz Ahamed {Ariyaluran Habeeb} and Fariza Nasaruddin and Abdullah Gani and Ibrahim Abaker {Targio Hashem} and Ejaz Ahmed and Muhammad Imran,REAL-TIME BIG DATA PROCESSING FOR ANOMALY DETECTION: A SURVEY,article
49,15631,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,journal,02684012,"2,770",Q1,114,224,382,20318,5853,373,"16,16","90,71",United Kingdom,Western Europe,Elsevier Ltd.,"1970, 1986-2021",Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1),INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,"12,245",14.098,0.01167,"Patients face difficulties identifying appropriate doctors owing to the sizeable quantity and uneven quality of information in online healthcare communities. In studying physician searches, researchers often focus on expertise similarity matches and sentiment analyses of reviews. However, the quality is often ignored. To address patients' information needs holistically, we propose a four-dimensional IT framework based on signaling theory. The model takes expertise knowledge, online reviews, profile descriptions (e.g., hospital reputation, number of patients, city) and service quality (e.g., response speed, interaction frequency, cost) as signals that distinguish high-quality physicians. It uses machine learning approaches to derive similarity matches and sentiment analysis. It also measures the relative importance of the signals by multi-criterion analysis and derives the physician rankings through the aggregated scores. Our study revealed that the proposed approach performs better compared with the other two recommend techniques. This research expands the boundary of signaling theory to healthcare management and enriches the literature on IT use and inter-organizational systems. The proposed IT model may improve patient care, alleviate the physician-patient relationship and reduce lawsuits against hospitals; it also has practical implications for healthcare management.",https://doi.org/10.1016/j.ijinfomgt.2019.01.005,2019,Yan Ye and Yang Zhao and Jennifer Shang and Liyi Zhang,A HYBRID IT FRAMEWORK FOR IDENTIFYING HIGH-QUALITY PHYSICIANS USING BIG DATA ANALYTICS,article
50,15631,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,journal,02684012,"2,770",Q1,114,224,382,20318,5853,373,"16,16","90,71",United Kingdom,Western Europe,Elsevier Ltd.,"1970, 1986-2021",Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1),INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,"12,245",14.098,0.01167,"Value creation is a major sustainability factor for enterprises, in addition to profit maximization and revenue generation. Modern enterprises collect big data from various inbound and outbound data sources. The inbound data sources handle data generated from the results of business operations, such as manufacturing, supply chain management, marketing, and human resource management, among others. Outbound data sources handle customer-generated data which are acquired directly or indirectly from customers, market analysis, surveys, product reviews, and transactional histories. However, cloud service utilization costs increase because of big data analytics and value creation activities for enterprises and customers. This article presents a novel concept of big data reduction at the customer end in which early data reduction operations are performed to achieve multiple objectives, such as (a) lowering the service utilization cost, (b) enhancing the trust between customers and enterprises, (c) preserving privacy of customers, (d) enabling secure data sharing, and (e) delegating data sharing control to customers. We also propose a framework for early data reduction at customer end and present a business model for end-to-end data reduction in enterprise applications. The article further presents a business model canvas and maps the future application areas with its nine components. Finally, the article discusses the technology adoption challenges for value creation through big data reduction in enterprise applications.",https://doi.org/10.1016/j.ijinfomgt.2016.05.013,2016,Muhammad Habib ur Rehman and Victor Chang and Aisha Batool and Teh Ying Wah,BIG DATA REDUCTION FRAMEWORK FOR VALUE CREATION IN SUSTAINABLE ENTERPRISES,article
51,15631,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,journal,02684012,"2,770",Q1,114,224,382,20318,5853,373,"16,16","90,71",United Kingdom,Western Europe,Elsevier Ltd.,"1970, 1986-2021",Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1),INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,"12,245",14.098,0.01167,"The expansion of big data and the evolution of Internet of Things (IoT) technologies have played an important role in the feasibility of smart city initiatives. Big data offer the potential for cities to obtain valuable insights from a large amount of data collected through various sources, and the IoT allows the integration of sensors, radio-frequency identification, and Bluetooth in the real-world environment using highly networked services. The combination of the IoT and big data is an unexplored research area that has brought new and interesting challenges for achieving the goal of future smart cities. These new challenges focus primarily on problems related to business and technology that enable cities to actualize the vision, principles, and requirements of the applications of smart cities by realizing the main smart environment characteristics. In this paper, we describe the state-of-the-art communication technologies and smart-based applications used within the context of smart cities. The visions of big data analytics to support smart cities are discussed by focusing on how big data can fundamentally change urban populations at different levels. Moreover, a future business model of big data for smart cities is proposed, and the business and technological research challenges are identified. This study can serve as a benchmark for researchers and industries for the future progress and development of smart cities in the context of big data.",https://doi.org/10.1016/j.ijinfomgt.2016.05.002,2016,Ibrahim Abaker Targio Hashem and Victor Chang and Nor Badrul Anuar and Kayode Adewole and Ibrar Yaqoob and Abdullah Gani and Ejaz Ahmed and Haruna Chiroma,THE ROLE OF BIG DATA IN SMART CITY,article
52,15631,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,journal,02684012,"2,770",Q1,114,224,382,20318,5853,373,"16,16","90,71",United Kingdom,Western Europe,Elsevier Ltd.,"1970, 1986-2021",Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1),INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,"12,245",14.098,0.01167,"The increased availability of social media big data has created a unique challenge for marketing decision-makers; turning this data into useful information. One of the significant areas of opportunity in digital marketing is influencer marketing, but identifying these influencers from big data sets is a continual challenge. This research illustrates how one type of influencer, the market maven, can be identified using big data. Using a mixed-method combination of both self-report survey data and publicly accessible big data, we gathered 556,150 tweets from 370 active Twitter users. We then proposed and tested a range of social-media-based metrics to identify market mavens. Findings show that market mavens (when compared to non-mavens) have more followers, post more often, have less readable posts, use more uppercase letters, use less distinct words, and use hashtags more often. These metrics are openly available from public Twitter accounts and could integrate into a broad-scale decision support system for marketing and information systems managers. These findings have the potential to improve influencer identification effectiveness and efficiency, and thus improve influencer marketing.",https://doi.org/10.1016/j.ijinfomgt.2020.102246,2021,Paul Harrigan and Timothy M. Daly and Kristof Coussement and Julie A. Lee and Geoffrey N. Soutar and Uwana Evers,IDENTIFYING INFLUENCERS ON SOCIAL MEDIA,article
53,15631,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,journal,02684012,"2,770",Q1,114,224,382,20318,5853,373,"16,16","90,71",United Kingdom,Western Europe,Elsevier Ltd.,"1970, 1986-2021",Artificial Intelligence (Q1); Computer Networks and Communications (Q1); Information Systems (Q1); Information Systems and Management (Q1); Library and Information Sciences (Q1); Management Information Systems (Q1); Marketing (Q1),INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT,"12,245",14.098,0.01167,"While the use of big data tends to add value for business throughout the entire value chain, the integration of big data analytics (BDA) to the decision-making process remains a challenge. This study, based on a systematic literature review, thematic analysis and qualitative interview findings, proposes a set of six-steps to establish both rigor and relevance in the process of analytics-driven decision-making. Our findings illuminate the key steps in this decision process including problem definition, review of past findings, model development, data collection, data analysis as well as actions on insights in the context of service systems. Although findings have been discussed in a sequence of steps, the study identifies them as interdependent and iterative. The proposed six-step analytics-driven decision-making process, practical evidence from service systems, and future research agenda, provide altogether the foundation for future scholarly research and can serve as a step-wise guide for industry practitioners.",https://doi.org/10.1016/j.ijinfomgt.2019.01.020,2019,Shahriar Akter and Ruwan Bandara and Umme Hani and Samuel {Fosso Wamba} and Cyril Foropon and Thanos Papadopoulos,ANALYTICS-BASED DECISION-MAKING FOR SERVICE SYSTEMS: A QUALITATIVE STUDY AND AGENDA FOR FUTURE RESEARCH,article
54,13266,AMERICAN JOURNAL OF OPHTHALMOLOGY,journal,"00029394, 18791891","2,704",Q1,186,428,1146,11979,4453,848,"3,73","27,99",United States,Northern America,Elsevier USA,1918-2020,Ophthalmology (Q1),AMERICAN JOURNAL OF OPHTHALMOLOGY,AMERICAN JOURNAL OF OPHTHALMOLOGY,"32,566",5.258,0.0263,"Purpose
To characterize the role of Big Data in evaluating quality of care in ophthalmology, to highlight opportunities for studying quality improvement using data available in the American Academy of Ophthalmology Intelligent Research in Sight (IRIS) Registry, and to show how Big Data informs us about rare events such as endophthalmitis after cataract surgery.
Design
Review of published studies, analysis of public-use Medicare claims files from 2010 to 2013, and analysis of IRIS Registry from 2013 to 2014.
Methods
Statistical analysis of observational data.
Results
The overall rate of endophthalmitis after cataract surgery was 0.14% in 216 703 individuals in the Medicare database. In the IRIS Registry the endophthalmitis rate after cataract surgery was 0.08% among 511 182 individuals. Endophthalmitis rates tended to be higher in eyes with combined cataract surgery and anterior vitrectomy (P = .051), although only 0.08% of eyes had this combined procedure. Visual acuity (VA) in the IRIS Registry in eyes with and without postoperative endophthalmitis measured 1–7 days postoperatively were logMAR 0.58 (standard deviation [SD]: 0.84) (approximately Snellen acuity of 20/80) and logMAR 0.31 (SD: 0.34) (approximately Snellen acuity of 20/40), respectively. In 33 547 eyes with postoperative VA after cataract surgery, 18.3% had 1-month-postoperative VA worse than 20/40.
Conclusions
Big Data drawing on Medicare claims and IRIS Registry records can help identify additional areas for quality improvement, such as in the 18.3% of eyes in the IRIS Registry having 1-month-postoperative VA worse than 20/40. The ability to track patient outcomes in Big Data sets provides opportunities for further research on rare complications such as postoperative endophthalmitis and outcomes from uncommon procedures such as cataract surgery combined with anterior vitrectomy. But privacy and data-security concerns associated with Big Data should not be taken lightly.",https://doi.org/10.1016/j.ajo.2015.09.028,2015,Anne Louise Coleman,HOW BIG DATA INFORMS US ABOUT CATARACT SURGERY: THE LXXII EDWARD JACKSON MEMORIAL LECTURE,article
55,26371,NATURAL PRODUCT REPORTS,journal,"02650568, 14604752","2,703",Q1,177,92,246,12696,2233,239,"8,45","138,00",United Kingdom,Western Europe,Royal Society of Chemistry,1984-2020,Biochemistry (Q1); Drug Discovery (Q1); Organic Chemistry (Q1),NATURAL PRODUCT REPORTS,NATURAL PRODUCT REPORTS,"13,293",13.423,0.01116,"ABSTRACT
Systematic, large-scale, studies at the genomic, metabolomic, and functional level have transformed the natural product sciences. Improvements in technology and reduction in cost for obtaining spectroscopic, chromatographic, and genomic data coupled with the creation of readily accessible curated and functionally annotated data sets have altered the practices of virtually all natural product research laboratories. Gone are the days when the natural products researchers were expected to devote themselves exclusively to the isolation, purification, and structure elucidation of small molecules. We now also engage with big data in taxonomic, genomic, proteomic, and/or metabolomic collections, and use these data to generate and test hypotheses. While the oft stated aim for the use of large-scale -omics data in the natural products sciences is to achieve a rapid increase in the rate of discovery of new drugs, this has not yet come to pass. At the same time, new technologies have provided unexpected opportunities for natural products chemists to ask and answer new and different questions. With this viewpoint, we discuss the evolution of big data as a part of natural products research and provide a few examples of how discoveries have been enabled by access to big data. We also draw attention to some of the limitations in our existing engagement with large datasets and consider what would be necessary to overcome them.",https://doi.org/10.1039/d1np00061f,2021,Nadja B. Cech and Marnix H. Medema and Jon Clardy,BENEFITING FROM BIG DATA IN NATURAL PRODUCTS: IMPORTANCE OF PRESERVING FOUNDATIONAL SKILLS AND PRIORITIZING DATA QUALITY,article
56,19366,AMERICAN JOURNAL OF KIDNEY DISEASES,journal,"15236838, 02726386","2,677",Q1,214,264,857,7570,3775,538,"4,45","28,67",United Kingdom,Western Europe,W.B. Saunders Ltd,1981-2020,Nephrology (Q1),AMERICAN JOURNAL OF KIDNEY DISEASES,AMERICAN JOURNAL OF KIDNEY DISEASES,"27,640",8.860,0.0271,,https://doi.org/10.1053/j.ajkd.2017.04.008,2017,Rajiv Saran and Diane Steffick and Jennifer Bragg-Gresham,THE CHINA KIDNEY DISEASE NETWORK (CK-NET): “BIG DATA—BIG DREAMS”,article
57,20689,AUTOIMMUNITY REVIEWS,journal,"15689972, 18730183","2,621",Q1,122,177,465,12128,3509,408,"7,66","68,52",Netherlands,Western Europe,Elsevier,2002-2020,Immunology (Q1); Immunology and Allergy (Q1),AUTOIMMUNITY REVIEWS,AUTOIMMUNITY REVIEWS,"13,493",9.754,0.01481,"The past decade has seen tremendous development in digital health, including in innovative new technologies such as Electronic Health Records, telemedicine, virtual visits, wearable technology and sophisticated analytical tools such as artificial intelligence (AI) and machine learning for the deep-integration of big data. In the field of rare connective tissue diseases (rCTDs), these opportunities include increased access to scarce and remote expertise, improved patient monitoring, increased participation and therapeutic adherence, better patient outcomes and patient empowerment. In this review, we discuss opportunities and key-barriers to improve application of digital health technologies in the field of autoimmune diseases. We also describe what could be the fully digital pathway of rCTD patients. Smart technologies can be used to provide real-world evidence about the natural history of rCTDs, to determine real-life drug utilization, advanced efficacy and safety data for rare diseases and highlight significant unmet needs. Yet, digitalization remains one of the most challenging issues faced by rCTD patients, their physicians and healthcare systems. Digital health technologies offer enormous potential to improve autoimmune rCTD care but this potential has so far been largely unrealized due to those significant obstacles. The need for robust assessments of the efficacy, affordability and scalability of AI in the context of digital health is crucial to improve the care of patients with rare autoimmune diseases.",https://doi.org/10.1016/j.autrev.2021.102864,2021,Hugo Bergier and Loïc Duron and Christelle Sordet and Lou Kawka and Aurélien Schlencker and François Chasset and Laurent Arnaud,"DIGITAL HEALTH, BIG DATA AND SMART TECHNOLOGIES FOR THE CARE OF PATIENTS WITH SYSTEMIC AUTOIMMUNE DISEASES: WHERE DO WE STAND?",article
58,21858,BRITISH JOURNAL OF ANAESTHESIA,journal,"00070912, 14716771","2,589",Q1,181,486,1260,12638,5321,684,"4,33","26,00",United Kingdom,Western Europe,Elsevier Ltd.,1923-2020,Anesthesiology and Pain Medicine (Q1),BRITISH JOURNAL OF ANAESTHESIA,BRITISH JOURNAL OF ANAESTHESIA,"27,510",9.166,0.02901,,https://doi.org/10.1016/j.bja.2020.01.012,2020,Daniel I. McIsaac,REAL-WORLD EVALUATION OF ENHANCED RECOVERY AFTER SURGERY: BIG DATA UNDER THE MICROSCOPE,article
59,21858,BRITISH JOURNAL OF ANAESTHESIA,journal,"00070912, 14716771","2,589",Q1,181,486,1260,12638,5321,684,"4,33","26,00",United Kingdom,Western Europe,Elsevier Ltd.,1923-2020,Anesthesiology and Pain Medicine (Q1),BRITISH JOURNAL OF ANAESTHESIA,BRITISH JOURNAL OF ANAESTHESIA,"27,510",9.166,0.02901,"Advances in computer technology, patient monitoring systems, and electronic health record systems have enabled rapid accumulation of patient data in electronic form (i.e. big data). Organizations such as the Anesthesia Quality Institute and Multicenter Perioperative Outcomes Group have spearheaded large-scale efforts to collect anaesthesia big data for outcomes research and quality improvement. Analytics—the systematic use of data combined with quantitative and qualitative analysis to make decisions—can be applied to big data for quality and performance improvements, such as predictive risk assessment, clinical decision support, and resource management. Visual analytics is the science of analytical reasoning facilitated by interactive visual interfaces, and it can facilitate performance of cognitive activities involving big data. Ongoing integration of big data and analytics within anaesthesia and health care will increase demand for anaesthesia professionals who are well versed in both the medical and the information sciences.",https://doi.org/10.1093/bja/aeu552,2015,A.F. Simpao and L.M. Ahumada and M.A. Rehman,BIG DATA AND VISUAL ANALYTICS IN ANAESTHESIA AND HEALTH CARE†,article
60,29374,ENERGY ECONOMICS,journal,"01409883, 18736181","2,500",Q1,152,378,1081,21992,8195,1072,"7,10","58,18",Netherlands,Western Europe,Elsevier,1979-2019,Economics and Econometrics (Q1); Energy (miscellaneous) (Q1),ENERGY ECONOMICS,ENERGY ECONOMICS,"26,186",7.042,0.02456,"The authenticity and quality of industrial statistical data directly affects all types of systematic research based on it. Considering the limitations of extant data quality evaluation literature on research objects and evaluation methods, we constructed a new data quality comprehensive inspection and evaluation model based on Benford's Law (BL) and the technique for order of preference by similarity to ideal solution (TOPSIS), selected coal-related industries as the research object, and conducted an empirical test along the research path of “Industry→Province→Indicator”. The results showed that, at industry level, the quality of statistical data for China's coal-related industries from 2001 to 2016 was generally poor. Among the eight sample industries selected, the data quality for five industries (including coal, electricity, and steel) was assessed as poor or slightly poor. Furthermore, at the provincial level, there is significant spatial heterogeneity in the quality of statistical data for various industries affected by factors such as economic structure, marketization level, and industrial diversity. Compared with other types of statistical indicators, industry financial indicators are more prone to data quality problems at the indicator level, and the suspicious indicators of different industries show certain common characteristics and some industry differences. To improve the quality of industrial statistical data and reduce the possible adverse impacts of data quality problems, based on the research findings, we propose targeted countermeasures and suggestions on how to prevent data fraud and effectively identify and rationally use suspicious data.",https://doi.org/10.1016/j.eneco.2022.106310,2022,Delu Wang and Fan Chen and Jingqi Mao and Nannan Liu and Fangyu Rong,ARE THE OFFICIAL NATIONAL DATA CREDIBLE? EMPIRICAL EVIDENCE FROM STATISTICS QUALITY EVALUATION OF CHINA'S COAL AND ITS DOWNSTREAM INDUSTRIES,article
61,29160,CANCER LETTERS,journal,"03043835, 18727980","2,470",Q1,182,430,1423,27650,11608,1409,"7,85","64,30",Ireland,Western Europe,Elsevier Ireland Ltd,1975-2020,Cancer Research (Q1); Oncology (Q1),CANCER LETTERS,CANCER LETTERS,"42,174",8.679,0.04013,"Precision medicine relies on an increasing amount of heterogeneous data. Advances in radiation oncology, through the use of CT Scan, dosimetry and imaging performed before each fraction, have generated a considerable flow of data that needs to be integrated. In the same time, Electronic Health Records now provide phenotypic profiles of large cohorts of patients that could be correlated to this information. In this review, we describe methods that could be used to create integrative predictive models in radiation oncology. Potential uses of machine learning methods such as support vector machine, artificial neural networks, and deep learning are also discussed.",https://doi.org/10.1016/j.canlet.2016.05.033,2016,Jean-Emmanuel Bibault and Philippe Giraud and Anita Burgun,BIG DATA AND MACHINE LEARNING IN RADIATION ONCOLOGY: STATE OF THE ART AND FUTURE PROSPECTS,article
62,19165,INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS,journal,09255273,"2,406",Q1,185,327,891,21712,8124,881,"8,31","66,40",Netherlands,Western Europe,Elsevier,1991-2021,"Business, Management and Accounting (miscellaneous) (Q1); Economics and Econometrics (Q1); Industrial and Manufacturing Engineering (Q1); Management Science and Operations Research (Q1)",INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS,INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS,"32,606",7.885,0.0228,"Today׳s supply chain professionals are inundated with data, motivating new ways of thinking about how data are produced, organized, and analyzed. This has provided an impetus for organizations to adopt and perfect data analytic functions (e.g. data science, predictive analytics, and big data) in order to enhance supply chain processes and, ultimately, performance. However, management decisions informed by the use of these data analytic methods are only as good as the data on which they are based. In this paper, we introduce the data quality problem in the context of supply chain management (SCM) and propose methods for monitoring and controlling data quality. In addition to advocating for the importance of addressing data quality in supply chain research and practice, we also highlight interdisciplinary research topics based on complementary theory.",https://doi.org/10.1016/j.ijpe.2014.04.018,2014,Benjamin T. Hazen and Christopher A. Boone and Jeremy D. Ezell and L. Allison Jones-Farmer,"DATA QUALITY FOR DATA SCIENCE, PREDICTIVE ANALYTICS, AND BIG DATA IN SUPPLY CHAIN MANAGEMENT: AN INTRODUCTION TO THE PROBLEM AND SUGGESTIONS FOR RESEARCH AND APPLICATIONS",article
63,19165,INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS,journal,09255273,"2,406",Q1,185,327,891,21712,8124,881,"8,31","66,40",Netherlands,Western Europe,Elsevier,1991-2021,"Business, Management and Accounting (miscellaneous) (Q1); Economics and Econometrics (Q1); Industrial and Manufacturing Engineering (Q1); Management Science and Operations Research (Q1)",INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS,INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS,"32,606",7.885,0.0228,"This study adopts the diffusion of innovation theory as to develop the smart product service system model in banking industry due to prior studies are lacking in identifying the attributes. The smart product service system functions are bearing high uncertainty and system complexity; hence, the hybrid method of fuzzy Delphi method and fuzzy decision-making trial and evaluation laboratory to construct a valid hierarchical model and identified the causal interrelationships among the attributes. The smart product service system hierarchical model with eight aspects and 41 criteria are proposed enriching the existing literature and that identify appropriate strategies to achieve operational performance. The results show that seven aspects and 22 criteria are determined as the valid hierarchical model. The institutional compression, digital platform operation, and e-knowledge management are the causing aspects helps to form smart product service system operational performance in high uncertainty. For practices, the banking decision-makers should develop innovative actions relied on the forcible compression, cyber-physical systems, industrial big data, cloud service allocation and sharing, and transparency improvement as they are most importance criteria playing a decisive role in a successful SPSS. This provides guidelines for banking industry practice in Taiwan encouraging the miscellany of digital technology accomplishment for sustainable target.",https://doi.org/10.1016/j.ijpe.2021.108244,2021,Ming-Lang Tseng and Tat-Dat Bui and Shulin Lan and Ming K. Lim and Abu Hashan Md Mashud,SMART PRODUCT SERVICE SYSTEM HIERARCHICAL MODEL IN BANKING INDUSTRY UNDER UNCERTAINTIES,article
64,19165,INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS,journal,09255273,"2,406",Q1,185,327,891,21712,8124,881,"8,31","66,40",Netherlands,Western Europe,Elsevier,1991-2021,"Business, Management and Accounting (miscellaneous) (Q1); Economics and Econometrics (Q1); Industrial and Manufacturing Engineering (Q1); Management Science and Operations Research (Q1)",INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS,INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS,"32,606",7.885,0.0228,"In recent years, big data has emerged as one of the prominent buzzwords in business and management. In spite of the mounting body of research on big data across the social science disciplines, scholars have offered little synthesis on the current state of knowledge. To take stock of academic research that contributes to the big data revolution, this paper tracks scholarly work's perspectives on big data in the management domain over the past decade. We identify key themes emerging in management studies and develop an integrated framework to link the multiple streams of research in fields of organisation, operations, marketing, information management and other relevant areas. Our analysis uncovers a growing awareness of big data's business values and managerial changes led by data-driven approach. Stemming from the review is the suggestion for research that both structured and unstructured big data should be harnessed to advance understanding of big data value in informing organisational decisions and enhancing firm competitiveness. To discover the full value, firms need to formulate and implement a data-driven strategy. In light of these, the study identifies and outlines the implications and directions for future research.",https://doi.org/10.1016/j.ijpe.2017.06.006,2017,Jie Sheng and Joseph Amankwah-Amoah and Xiaojun Wang,A MULTIDISCIPLINARY PERSPECTIVE OF BIG DATA IN MANAGEMENT RESEARCH,article
65,19165,INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS,journal,09255273,"2,406",Q1,185,327,891,21712,8124,881,"8,31","66,40",Netherlands,Western Europe,Elsevier,1991-2021,"Business, Management and Accounting (miscellaneous) (Q1); Economics and Econometrics (Q1); Industrial and Manufacturing Engineering (Q1); Management Science and Operations Research (Q1)",INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS,INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS,"32,606",7.885,0.0228,"Big data has the potential to revolutionize the art of management. Despite the high operational and strategic impacts, there is a paucity of empirical research to assess the business value of big data. Drawing on a systematic review and case study findings, this paper presents an interpretive framework that analyzes the definitional perspectives and the applications of big data. The paper also provides a general taxonomy that helps broaden the understanding of big data and its role in capturing business value. The synthesis of the diverse concepts within the literature on big data provides deeper insights into achieving value through big data strategy and implementation.",https://doi.org/10.1016/j.ijpe.2014.12.031,2015,Samuel {Fosso Wamba} and Shahriar Akter and Andrew Edwards and Geoffrey Chopin and Denis Gnanzou,HOW ‘BIG DATA’ CAN MAKE BIG IMPACT: FINDINGS FROM A SYSTEMATIC REVIEW AND A LONGITUDINAL CASE STUDY,article
66,19165,INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS,journal,09255273,"2,406",Q1,185,327,891,21712,8124,881,"8,31","66,40",Netherlands,Western Europe,Elsevier,1991-2021,"Business, Management and Accounting (miscellaneous) (Q1); Economics and Econometrics (Q1); Industrial and Manufacturing Engineering (Q1); Management Science and Operations Research (Q1)",INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS,INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS,"32,606",7.885,0.0228,"Servitization has become a pervasive business strategy among manufacturers, enabling them to undergird their competitive advantage. However, it has at least one weakness. While it is used worldwide also in economies with lower production costs, services in manufacturing are slowly becoming commoditized and will become a necessary, though not sufficient, condition for reaching an above average competitive advantage. Consequently, in this article we propose a new basis for competitive advantage for manufacturing enterprises called a Big Data Strategy in servitization. We scrutinize how manufacturers can exploit the opportunity arising from combined Big Data and servitization. Therefore, the concept of a Big Data Strategy framework in servitization is proposed. The findings are benchmarked against established frameworks in the Big Data and servitization literature. Its impact on competitive advantage is assessed through three theoretical perspectives that increase the validity of the results. The main finding is that, through the proposed strategy, new revenue streams can be created, while opening the possibility to decrease prices for product–services. Through the proposed strategy manufacturers can differentiate themselves from the ones that are already servitizing. This article introduces the possibility of influencing the most important of the five “Vs” in Big Data–Value, in addition to the other four “Vs”—Volume, Variety, Velocity and Verification. As in regards to servitization, the article adds a third layer of added value— “information”, beside the two existing ones: product and service. The results have strategic implications for managers.",https://doi.org/10.1016/j.ijpe.2014.12.036,2015,David Opresnik and Marco Taisch,THE VALUE OF BIG DATA IN SERVITIZATION,article
67,19165,INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS,journal,09255273,"2,406",Q1,185,327,891,21712,8124,881,"8,31","66,40",Netherlands,Western Europe,Elsevier,1991-2021,"Business, Management and Accounting (miscellaneous) (Q1); Economics and Econometrics (Q1); Industrial and Manufacturing Engineering (Q1); Management Science and Operations Research (Q1)",INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS,INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS,"32,606",7.885,0.0228,"Today, firms can access to big data (tweets, videos, click streams, and other unstructured sources) to extract new ideas or understanding about their products, customers, and markets. Thus, managers increasingly view data as an important driver of innovation and a significant source of value creation and competitive advantage. To get the most out of the big data (in combination with a firm׳s existing data), a more sophisticated way of handling, managing, analysing and interpreting data is necessary. However, there is a lack of data analytics techniques to assist firms to capture the potential of innovation afforded by data and to gain competitive advantage. This research aims to address this gap by developing and testing an analytic infrastructure based on the deduction graph technique. The proposed approach provides an analytic infrastructure for firms to incorporate their own competence sets with other firms. Case studies results indicate that the proposed data analytic approach enable firms to utilise big data to gain competitive advantage by enhancing their supply chain innovation capabilities.",https://doi.org/10.1016/j.ijpe.2014.12.034,2015,Kim Hua Tan and YuanZhu Zhan and Guojun Ji and Fei Ye and Chingter Chang,HARVESTING BIG DATA TO ENHANCE SUPPLY CHAIN INNOVATION CAPABILITIES: AN ANALYTIC INFRASTRUCTURE BASED ON DEDUCTION GRAPH,article
68,19165,INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS,journal,09255273,"2,406",Q1,185,327,891,21712,8124,881,"8,31","66,40",Netherlands,Western Europe,Elsevier,1991-2021,"Business, Management and Accounting (miscellaneous) (Q1); Economics and Econometrics (Q1); Industrial and Manufacturing Engineering (Q1); Management Science and Operations Research (Q1)",INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS,INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS,"32,606",7.885,0.0228,"The recent interest in big data has led many companies to develop big data analytics capability (BDAC) in order to enhance firm performance (FPER). However, BDAC pays off for some companies but not for others. It appears that very few have achieved a big impact through big data. To address this challenge, this study proposes a BDAC model drawing on the resource-based theory (RBT) and the entanglement view of sociomaterialism. The findings show BDAC as a hierarchical model, which consists of three primary dimensions (i.e., management, technology, and talent capability) and 11 subdimensions (i.e., planning, investment, coordination, control, connectivity, compatibility, modularity, technology management knowledge, technical knowledge, business knowledge and relational knowledge). The findings from two Delphi studies and 152 online surveys of business analysts in the U.S. confirm the value of the entanglement conceptualization of the higher-order BDAC model and its impact on FPER. The results also illuminate the significant moderating impact of analytics capability–business strategy alignment on the BDAC–FPER relationship.",https://doi.org/10.1016/j.ijpe.2016.08.018,2016,Shahriar Akter and Samuel Fosso Wamba and Angappa Gunasekaran and Rameshwar Dubey and Stephen J. Childe,HOW TO IMPROVE FIRM PERFORMANCE USING BIG DATA ANALYTICS CAPABILITY AND BUSINESS STRATEGY ALIGNMENT?,article
69,28686,INTERNATIONAL JOURNAL OF HOSPITALITY MANAGEMENT,journal,02784319,"2,321",Q1,122,321,509,25314,4796,490,"8,98","78,86",United Kingdom,Western Europe,Elsevier Ltd.,1982-2020,"Strategy and Management (Q1); Tourism, Leisure and Hospitality Management (Q1)",INTERNATIONAL JOURNAL OF HOSPITALITY MANAGEMENT,INTERNATIONAL JOURNAL OF HOSPITALITY MANAGEMENT,"17,219",9.237,0.01134,"The main purpose of this study is based on qualitative and quantitative research procedures, and integrates the key service factors for the online food delivery (OFD) industry extracted by Internet Big Data Analytics (IBDA) to construct a OFD service quality scale (OFD-SERV). This study takes OFD customers in Taipei City as the objects. The results show that 20 key service factors for the OFD industry are extracted through IBDA. The OFD-SERV scale contains six dimensions including reliability, maintenance of meal quality and hygiene, assurance, security, system operation and traceability, a total of 28 items. The results from the structural equation modeling showed that the reliability, assurance and system operation have a positive impact on customer satisfaction. Finally, the findings provide knowledge and inspiration for the current OFD, and enable OFD operators and future researchers to more accurately identify the deficiency of service quality.",https://doi.org/10.1016/j.ijhm.2021.102938,2021,Ching-Chan Cheng and Ya-Yuan Chang and Cheng-Ta Chen,CONSTRUCTION OF A SERVICE QUALITY SCALE FOR THE ONLINE FOOD DELIVERY INDUSTRY,article
70,14966,JOURNAL OF MANUFACTURING SYSTEMS,journal,02786125,"2,310",Q1,70,155,294,8906,2949,288,"10,88","57,46",Netherlands,Western Europe,Elsevier,1982-2020,Control and Systems Engineering (Q1); Hardware and Architecture (Q1); Industrial and Manufacturing Engineering (Q1); Software (Q1),JOURNAL OF MANUFACTURING SYSTEMS,JOURNAL OF MANUFACTURING SYSTEMS,"5,413",8.633,0.00561,"Digital twin takes Industrial Internet as a carrier deeply coordinating and integrating virtual spaces with physical spaces, which effectively promotes smart factory development. Digital twin-based big data learning and analysis (BDLA) deepens virtual and real fusion, interaction and closed-loop iterative optimization in smart factories. This paper proposes a digital twin-based big data virtual and real fusion (DT-BDVRL) reference framework supported by Industrial Internet towards smart manufacturing. The reference framework is synthetically designed from three perspectives. The first one is an overall framework of DT-BDVRL supported by Industrial Internet. The second one is the establishment method and flow of BDLA models based on digital twin. The final one is digital thread of DT-BDVRL in virtual and real fusion analysis, iteration and closed-loop feedback in product full life cycle processes. For different virtual scenes, iterative optimization and verification methods and processes of BDLA models in virtual spaces are established. Moreover, the BDLA results can drive digital twin running in virtual spaces. By this, the BDLA results can be validated iteratively multiple times in virtual spaces. At same time, the BDLA results that run in virtual spaces are synchronized and executed in physical spaces through Industrial Internet platforms, effectively improving the physical execution effect of BDLA models. Finally, the above contents were applied and verified in the actual production case study of power switchgear equipment.",https://doi.org/10.1016/j.jmsy.2020.11.012,2021,Pei Wang and Ming Luo,A DIGITAL TWIN-BASED BIG DATA VIRTUAL AND REAL FUSION LEARNING REFERENCE FRAMEWORK SUPPORTED BY INDUSTRIAL INTERNET TOWARDS SMART MANUFACTURING,article
71,14966,JOURNAL OF MANUFACTURING SYSTEMS,journal,02786125,"2,310",Q1,70,155,294,8906,2949,288,"10,88","57,46",Netherlands,Western Europe,Elsevier,1982-2020,Control and Systems Engineering (Q1); Hardware and Architecture (Q1); Industrial and Manufacturing Engineering (Q1); Software (Q1),JOURNAL OF MANUFACTURING SYSTEMS,JOURNAL OF MANUFACTURING SYSTEMS,"5,413",8.633,0.00561,"Industrial big data technology has become one of the important driving forces to intelligent manufacturing in the steel industry. In this study, the characteristics of data in steel production are analyzed and an industrial big data platform for steeling process is developed to extract the quality-related parameters. A data-driven approach to construct prediction intervals (PIs) of mechanical performances for hot-rolling strips is proposed to represent the uncertainty and reliability of the prediction results. The proposed method employs a new manifold visualization method, SLISEMAP, to reduce the feature dimensions with interpretability, utilizes lower upper bound estimation (LUBE) method to obtain the PIs, in which the broad learning system (BLS) is used as the basic training network model and the artificial bee colony (ABC) algorithm is applied to optimize the weighting parameters of BLS under the LUBE framework. A hot-rolling steel coil dataset consisting of 39 variables and 1335 coil samples is used to validate the proposed method. Two Delta-based approaches, namely back propagation neural network (BPNN) and extreme learning machine (ELM); and three LUBE-based approaches, namely ABC-BPNN, ABC-ELM, and ABC-support vector regression (SVR) are compared with the proposed method. Results show that the proposed ABC-BLS in LUBE is effective and efficient in constructing the PIs with a higher coverage probability and a narrower interval width.",https://doi.org/10.1016/j.jmsy.2022.08.014,2022,Gongzhuang Peng and Yinliang Cheng and Yufei Zhang and Jian Shao and Hongwei Wang and Weiming Shen,INDUSTRIAL BIG DATA-DRIVEN MECHANICAL PERFORMANCE PREDICTION FOR HOT-ROLLING STEEL USING LOWER UPPER BOUND ESTIMATION METHOD,article
72,14966,JOURNAL OF MANUFACTURING SYSTEMS,journal,02786125,"2,310",Q1,70,155,294,8906,2949,288,"10,88","57,46",Netherlands,Western Europe,Elsevier,1982-2020,Control and Systems Engineering (Q1); Hardware and Architecture (Q1); Industrial and Manufacturing Engineering (Q1); Software (Q1),JOURNAL OF MANUFACTURING SYSTEMS,JOURNAL OF MANUFACTURING SYSTEMS,"5,413",8.633,0.00561,"The fourth industrial revolution is derived from advances in digitization and prognostic and health management (PHM) disciplines to make plants smarter and more efficient. However, an adapted approach for data-driven PHM process implementation in small and medium-sized enterprises (SMEs) has not been yet discussed. This research gap is due to the specificities of SMEs and the lack of documentation. In this paper, we examine existing standards for implementing PHM in the industrial field and discuss the limitations within SMEs. Based on that, a novel strategy to implement a data-driven PHM approach in SMEs is proposed. Accordingly, the data management process and the impact of data quality are reviewed to address some critical data problems in SMEs (e.g., data volume and data accuracy). A first set of simulations was carried out to study the impact of the data volume and percentage of missing data on classification problems in PHM. A general model of the evolution of the results accuracy in function of data volume and missing data is then generated, and an economic data volume notion is proposed for data infrastructure resizing. The proposed strategy and the developed models are then applied to the Scoder enterprise, which is a French SME. The feedback on the first results of this application is reported and discussed.",https://doi.org/10.1016/j.jmsy.2020.04.002,2020,N. Omri and Z. {Al Masry} and N. Mairot and S. Giampiccolo and N. Zerhouni,INDUSTRIAL DATA MANAGEMENT STRATEGY TOWARDS AN SME-ORIENTED PHM,article
73,14966,JOURNAL OF MANUFACTURING SYSTEMS,journal,02786125,"2,310",Q1,70,155,294,8906,2949,288,"10,88","57,46",Netherlands,Western Europe,Elsevier,1982-2020,Control and Systems Engineering (Q1); Hardware and Architecture (Q1); Industrial and Manufacturing Engineering (Q1); Software (Q1),JOURNAL OF MANUFACTURING SYSTEMS,JOURNAL OF MANUFACTURING SYSTEMS,"5,413",8.633,0.00561,"Manufacturing companies struggle to be efficient and effective when conducting root cause analyses of production disturbances; a fact which hinders them from creating and developing resilient production systems. This article aims to describe the challenges and enablers identified in current research relating to the different phases of root cause analysis. A systematic literature review was conducted, in which a total of 14 challenges and 17 enablers are identified and described. These correlate to the different phases of root cause analysis. Examples of challenges are “need for expertise”, “employee bias”, “poor data quality” and “lack of data integration”, among others. Examples of enablers are “visualisation tools”, “collaborative platforms”, “thesaurus” and “machine learning techniques”. Based on these findings, the authors also propose potential areas for further research and then design inputs for new solutions to improve root cause analysis. This article provides a theoretical contribution in that it describes the challenges and enablers of root cause analysis and their correlation to the creation of resilient production systems. The article also provides practical contributions, with an overview of current research to support practitioners in gaining insights into potential solutions to be implemented and further developed, with the aim of improving root cause analysis in production systems.",https://doi.org/10.1016/j.jmsy.2022.07.015,2022,Adriana Ito and Malin Hagström and Jon Bokrantz and Anders Skoogh and Mario Nawcki and Kanika Gandhi and Dag Bergsjö and Maja Bärring,IMPROVED ROOT CAUSE ANALYSIS SUPPORTING RESILIENT PRODUCTION SYSTEMS,article
74,14966,JOURNAL OF MANUFACTURING SYSTEMS,journal,02786125,"2,310",Q1,70,155,294,8906,2949,288,"10,88","57,46",Netherlands,Western Europe,Elsevier,1982-2020,Control and Systems Engineering (Q1); Hardware and Architecture (Q1); Industrial and Manufacturing Engineering (Q1); Software (Q1),JOURNAL OF MANUFACTURING SYSTEMS,JOURNAL OF MANUFACTURING SYSTEMS,"5,413",8.633,0.00561,"The current task scheduling mainly concerns the availability of machining resources, rather than the potential errors after scheduling. To minimise such errors in advance, this paper presents a big data analytics based fault prediction approach for shop floor scheduling. Within the context, machining tasks, machining resources, and machining processes are represented by data attributes. Based on the available data on the shop floor, the potential fault/error patterns, referring to machining errors, machine faults and maintenance states, are mined for unsuitable scheduling arrangements before machining as well as upcoming errors during machining. Comparing the data-represented tasks with the mined error patterns, their similarities or differences are calculated. Based on the calculated similarities, the fault probabilities of the scheduled tasks or the current machining tasks can be obtained, and they provide a reference of decision making for scheduling and rescheduling the tasks. By rescheduling high-risk tasks carefully, the potential errors can be avoided. In this paper, the architecture of the approach consisting of three steps in three levels is proposed. Furthermore, big data are considered in three levels, i.e. local data, local network data and cloud data. In order to implement this idea, several key techniques are illustrated in detail, e.g. data attribute, data cleansing, data integration of databases in different levels, and big data analytic algorithms. Finally, a simplified case study is described to show the prediction process of the proposed method.",https://doi.org/10.1016/j.jmsy.2017.03.008,2017,Wei Ji and Lihui Wang,BIG DATA ANALYTICS BASED FAULT PREDICTION FOR SHOP FLOOR SCHEDULING,article
75,14726,TECHNOVATION,journal,01664972,"2,300",Q1,130,53,102,5179,837,93,"6,66","97,72",United Kingdom,Western Europe,Elsevier Ltd.,1981-2020,Engineering (miscellaneous) (Q1); Management of Technology and Innovation (Q1),TECHNOVATION,TECHNOVATION,"8,486",6.606,0.00351,"The recent development and diffusion of next-generation digital technologies (NGDTs) such as artificial intelligence, the Internet of Things, big data, 3D printing, and so on are expected to have an immense impact on businesses, innovation, and society. While we know from extant research that a firm's R&D investment, intangible assets, and productivity are factors that influence technology use more generally, to date there is little known about the factors that determine how these emerging tools are used, and by who. Using Probit and OLS modeling on a survey of 12,579 South Korean firms in 2017, we conduct one of the first comprehensive examinations highlighting various firm characteristics that drive NGDT implementation. While much of the literature assesses the use of individual technologies, our research attempts to unveil the extent to which firms implement NGDTs in bundles. Our investigation shows that more than half of the firms that use NGDTs deployed multiple technologies simultaneously. One of the insightful complementarities identified in this research exists amongst technologies that generate, facilitate and demand large sums of data, including big data, IoT, cloud computing and AI. Such technologies also appear important for innovative tools such as 3D printing and robotics.",https://doi.org/10.1016/j.technovation.2022.102477,2022,Jaehan Cho and Timothy DeStefano and Hanhin Kim and Inchul Kim and Jin Hyun Paik,WHAT'S DRIVING THE DIFFUSION OF NEXT-GENERATION DIGITAL TECHNOLOGIES?,article
76,21080,MECHANICAL SYSTEMS AND SIGNAL PROCESSING,journal,"08883270, 10961216","2,275",Q1,167,538,2031,23327,16068,2017,"7,95","43,36",United States,Northern America,Elsevier,1987-2021,Aerospace Engineering (Q1); Civil and Structural Engineering (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Mechanical Engineering (Q1); Signal Processing (Q1),MECHANICAL SYSTEMS AND SIGNAL PROCESSING,MECHANICAL SYSTEMS AND SIGNAL PROCESSING,"30,686",6.823,0.03775,"The onset of the Internet of Things enables machines to be outfitted with always-on sensors that can provide health information to cloud-based monitoring systems for prognostics and health management (PHM), which greatly improves reliability and avoids downtime of machines and processes on the shop floor. On the other hand, real-time monitoring produces large amounts of data, leading to significant challenges for efficient and effective data transmission (from the shop floor to the cloud) and analysis (in the cloud). Restricted by industrial hardware capability, especially Internet bandwidth, most solutions approach data transmission from the perspective of data compression (before transmission, at local computing devices) coupled with data reconstruction (after transmission, in the cloud). However, existing data compression techniques may not adapt to domain-specific characteristics of data, and hence have limitations in addressing high compression ratios where full restoration of signal details is important for revealing machine conditions. This study integrates Deep Convolutional Autoencoders (DCAE) with local structure and physics-informed loss terms that incorporate PHM domain knowledge such as the importance of frequency content for machine fault diagnosis. Furthermore, Fault Division Autoencoder Multiplexing (FDAM) is proposed to mitigate the negative effects of multiple disjoint operating conditions on reconstruction fidelity. The proposed methods are evaluated on two case studies, and autocorrelation-based noise analysis provides insight into the relative performance across machine health and operating conditions. Results indicate that physically-informed DCAE compression outperforms prevalent data compression approaches, such as compressed sensing, Principal Component Analysis (PCA), Discrete Cosine Transform (DCT), and DCAE with a standard loss function. FDAM can further improve the data reconstruction quality for certain machine conditions.",https://doi.org/10.1016/j.ymssp.2021.108709,2022,Matthew Russell and Peng Wang,PHYSICS-INFORMED DEEP LEARNING FOR SIGNAL COMPRESSION AND RECONSTRUCTION OF BIG DATA IN INDUSTRIAL CONDITION MONITORING,article
77,14704,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,journal,00401625,"2,226",Q1,117,448,1099,35581,10127,1061,"9,01","79,42",United States,Northern America,Elsevier Inc.,1970-2020,Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1),TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,"21,116",8.593,0.02416,"Economic, social and environmental requirements make planning for a sustainable electricity generation mix a demanding endeavour. Technological innovation offers a range of renewable generation and energy management options which require fine tuning and accurate control to be successful, which calls for the use of large-scale, detailed datasets. In this paper, we focus on the UK and use Multi-Criteria Decision Making (MCDM) to evaluate electricity generation options against technical, environmental and social criteria. Data incompleteness and redundancy, usual in large-scale datasets, as well as expert opinion ambiguity are dealt with using a comprehensive grey TOPSIS model. We used evaluation scores to develop a multi-objective optimization model to maximize the technical, environmental and social utility of the electricity generation mix and to enable a larger role for innovative technologies. Demand uncertainty was handled with an interval range and we developed our problem with multi-objective grey linear programming (MOGLP). Solving the mathematical model provided us with the electricity generation mix for every 5 min of the period under study. Our results indicate that nuclear and renewable energy options, specifically wind, solar, and hydro, but not biomass energy, perform better against all criteria indicating that interindustry architectural innovation in the power generation mix is key to sustainable UK electricity production and supply.",https://doi.org/10.1016/j.techfore.2018.04.031,2019,Konstantinos J. Chalvatzis and Hanif Malekpoor and Nishikant Mishra and Fiona Lettice and Sonal Choudhary,SUSTAINABLE RESOURCE ALLOCATION FOR POWER GENERATION: THE ROLE OF BIG DATA IN ENABLING INTERINDUSTRY ARCHITECTURAL INNOVATION,article
78,14704,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,journal,00401625,"2,226",Q1,117,448,1099,35581,10127,1061,"9,01","79,42",United States,Northern America,Elsevier Inc.,1970-2020,Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1),TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,"21,116",8.593,0.02416,"Faced with internal and external pressure to adapt and implement environmental friendly business activities, it is becoming crucial for firms to identify practices that enhance their competitive advantage, economic, and environmental performance. Green innovation, green technologies, and the implementation of green supply chain management are examples of such practices. Green innovation and the adoption of the combination of green product innovation and green process innovation involve reduction in consumption of energy and pollution emission, recycling of wastes, sustainable utilization of resources, and green product designs. Although the extent research in this area is substantial, research on the importance of considering corporate environmental ethics, stakeholders view of green product, and demand for green products as drivers of green innovation must be conducted. Moreover, the role of large scale data, management commitment, and human resource practices play to overcome the technological challenges, achieve competitive advantage, and enhance the economic and environmental performance have yet to be addressed. This paper develops and tests a holistic model that depicts and examines the relationships among green innovation, its drivers, as well as factors that help overcome the technological challenges and influence the performance and competitive advantage of the firm. This paper is among the first works to deal with such a complex framework which considers the interrelationships among numerous constructs and their effects on competitive advantage as well as overall organizational performance. A questionnaire was designed to measure the influence of green innovation adoption/implementation and its drivers on performance and competitive advantage while taking into consideration the impact of management commitment and HR practices, as well as the use of large data on these relationships. Data collected from a sample of 215 respondents working in Middle East and North Africa (MENA) region and Golf-Cooperation Countries (GCC) were used to test the proposed relationships. The proposed model proved to be fit. The hypotheses were supported, and implications were discussed.",https://doi.org/10.1016/j.techfore.2017.12.016,2019,Abdul-Nasser El-Kassar and Sanjay Kumar Singh,GREEN INNOVATION AND ORGANIZATIONAL PERFORMANCE: THE INFLUENCE OF BIG DATA AND THE MODERATING ROLE OF MANAGEMENT COMMITMENT AND HR PRACTICES,article
79,14704,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,journal,00401625,"2,226",Q1,117,448,1099,35581,10127,1061,"9,01","79,42",United States,Northern America,Elsevier Inc.,1970-2020,Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1),TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,"21,116",8.593,0.02416,"Big Data is one of the recent technological advances with the strong applicability in almost every industry, including manufacturing. However, despite business opportunities offered by this technology, its adoption is still in early stage in many industries. Thus, this study aimed to identify and rank the significant factors influencing adoption of big data and in turn to predict the influence of big data adoption on manufacturing companies' performance using a hybrid approach of decision-making trial and evaluation laboratory (DEMATEL)- adaptive neuro-fuzzy inference systems (ANFIS). This study identified the critical adoption factors from literature review and categorized them into technological, organizational and environmental dimensions. Data was collected from 234 industrial managers who were involved in the decision-making process regarding IT procurement in Malaysian manufacturing companies. Research results showed that technological factors (perceived benefits, complexity, technology resources, big data quality and integration) have the highest influence on the big data adoption and firms' performance. This study is one of the pioneers in using DEMATEL-ANFIS approach in the big data adoption context. In addition to the academic contribution, findings of this study can hopefully assist manufacturing industries, big data service providers, and governments to precisely focus on vital factors found in this study in order to improve firm performance by adopting big data.",https://doi.org/10.1016/j.techfore.2018.07.043,2018,Elaheh Yadegaridehkordi and Mehdi Hourmand and Mehrbakhsh Nilashi and Liyana Shuib and Ali Ahani and Othman Ibrahim,INFLUENCE OF BIG DATA ADOPTION ON MANUFACTURING COMPANIES' PERFORMANCE: AN INTEGRATED DEMATEL-ANFIS APPROACH,article
80,14704,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,journal,00401625,"2,226",Q1,117,448,1099,35581,10127,1061,"9,01","79,42",United States,Northern America,Elsevier Inc.,1970-2020,Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1),TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,"21,116",8.593,0.02416,"This study examines the role of big data contractual and relational governance in big data decision-making performance of firms based in China. It investigates the mediation of big data analytics (BDA) capability in the association of contractual and relational governance with decision-making performance. Furthermore, moderating role of data-driven culture in the relationship of BDA capability and decision-making performance is examined. Data are collected from 108 Chinese firms engaged in big data-related activities. Structural equation modeling is employed to test the hypotheses. This study contributes towards the literature on big data management and governance mechanisms, by establishing the relationship of decision-making performance with big data contractual and relational governance directly and through the mediation of BDA capabilities. It also contributes towards knowledge based dynamic capabilities (KBDCs) view of firms, arguing that dynamic capabilities such as BDA capabilities can be influenced through knowledge sources and activities. We add to the discussions on whether contractual and relational governance are alternatives or they complement each other, by establishing the moderating role of big data relational governance in the relationship of contractual governance and decision-making performance. Finally, we argue that social capital can enhance KBDCs through contractual and relational governance in big data context.",https://doi.org/10.1016/j.techfore.2020.120315,2020,Saqib Shamim and Jing Zeng and Zaheer Khan and Najam Ul Zia,BIG DATA ANALYTICS CAPABILITY AND DECISION MAKING PERFORMANCE IN EMERGING MARKET FIRMS: THE ROLE OF CONTRACTUAL AND RELATIONAL GOVERNANCE MECHANISMS,article
81,14704,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,journal,00401625,"2,226",Q1,117,448,1099,35581,10127,1061,"9,01","79,42",United States,Northern America,Elsevier Inc.,1970-2020,Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1),TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,"21,116",8.593,0.02416,"ABSTRACT
The significance of big data analytics-powered artificial intelligence has grown in recent years. The literature indicates that big data analytics-powered artificial intelligence has the ability to enhance supply chain performance, but there is limited research concerning the reasons for which firms engaging in manufacturing activities adopt big data analytics-powered artificial intelligence. To address this gap, our study employs institutional theory and resource-based view theory to elucidate the way in which automotive firms configure tangible resources and workforce skills to drive technological enablement and improve sustainable manufacturing practices and furthermore develop circular economy capabilities. We tested the research hypothesis using primary data collected from 219 automotive and allied manufacturing companies operating in South Africa. The contribution of this work lies in the statistical validation of the theoretical framework, which provides insight regarding the role of institutional pressures on resources and their effects on the adoption of big data analytics-powered artificial intelligence, and how this affects sustainable manufacturing and circular economy capabilities under the moderating effects of organizational flexibility and industry dynamism.",https://doi.org/10.1016/j.techfore.2020.120420,2021,Surajit Bag and Jan Ham Christiaan Pretorius and Shivam Gupta and Yogesh K. Dwivedi,"ROLE OF INSTITUTIONAL PRESSURES AND RESOURCES IN THE ADOPTION OF BIG DATA ANALYTICS POWERED ARTIFICIAL INTELLIGENCE, SUSTAINABLE MANUFACTURING PRACTICES AND CIRCULAR ECONOMY CAPABILITIES",article
82,14704,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,journal,00401625,"2,226",Q1,117,448,1099,35581,10127,1061,"9,01","79,42",United States,Northern America,Elsevier Inc.,1970-2020,Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1),TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,"21,116",8.593,0.02416,"The access of machine learning techniques in popular programming languages and the exponentially expanding big data from social media, news, surveys, and markets provide exciting challenges and invaluable opportunities for organizations and individuals to explore implicit information for decision making. Nevertheless, the users of machine learning usually find that these sophisticated techniques could incur a high level of tensions caused by the selection of the appropriate size of the training data set among other factors. In this paper, we provide a systematic way of resolving such tensions by examining practical examples of predicting popularity and sentiment of posts on Twitter and Facebook, blogs on Mashable, news on Google and Yahoo, the US house survey, and Bitcoin prices. Interesting results show that for the case of big data, using around 20% of the full sample often leads to a better prediction accuracy than opting for the full sample. Our conclusion is found to be consistent across a series of experiments. The managerial implication is that using more is not necessarily the best and users need to be cautious about such an important sensitivity as the simplistic approach may easily lead to inferior solutions with potentially detrimental consequences.",https://doi.org/10.1016/j.techfore.2020.120175,2020,Huamao Wang and Yumei Yao and Said Salhi,TENSION IN BIG DATA USING MACHINE LEARNING: ANALYSIS AND APPLICATIONS,article
83,14704,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,journal,00401625,"2,226",Q1,117,448,1099,35581,10127,1061,"9,01","79,42",United States,Northern America,Elsevier Inc.,1970-2020,Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1),TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,"21,116",8.593,0.02416,"This study aims to develop accounting standards, curriculums, and research to cope with the rapid development of big data. The study presents several potential convergence points between big data and different accounting techniques and theories. The study discusses how big data can overcome the data limitations of six accounting issues: financial reporting, performance measurement, audit evidence, risk management, corporate budgeting and activity-based techniques. It presents six exciting research questions for future research. Then, the study explains the potential convergence between big data and agency theory, stakeholders theory, and legitimacy theory. This theoretical study develops new convergence points between big data and accounting by reviewing the literature and proposing new ideas and research questions. The conclusion indicates a significant convergence between big data and accounting on the premise that data is the heart of accounting. Big data and advanced analytics have the potential to overcome the data limitations of accounting techniques that require estimations and predictions. A remarkable convergence is argued between big data and three accounting theories. Overall, the study presents helpful insights to members of the accounting and auditing community on the potential of big data.",https://doi.org/10.1016/j.techfore.2021.121171,2021,Awad Elsayed Awad Ibrahim and Ahmed A. Elamer and Amr Nazieh Ezat,THE CONVERGENCE OF BIG DATA AND ACCOUNTING: INNOVATIVE RESEARCH OPPORTUNITIES,article
84,14704,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,journal,00401625,"2,226",Q1,117,448,1099,35581,10127,1061,"9,01","79,42",United States,Northern America,Elsevier Inc.,1970-2020,Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1),TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,"21,116",8.593,0.02416,"The introduction and use of ‘big data and analytics’ is an on-going issue of discussion in health sectors globally. Healthcare systems of developed countries are trying to create more value and better healthcare through data and use of big data technologies. With an increasing number of articles identifying the value creation of big data and analytics for clinical decision-making, this paper examines how big data is applied, or not applied, in clinical practice. Using social representation theory as a theoretical foundation the paper explores people's perceptions of big data across all levels (policy making, planning, funding, and clinical care) of the New Zealand healthcare sector. The findings show that although adoption of big data technologies is planned for population health and health management, the potential of big data for clinical care has yet to be explored in the New Zealand context. The findings also highlight concern over data quality. The paper provides recommendations for policy and practice particularly around the need for engagement and participation of all levels to discuss data quality as well as big-data-based changes such as precision medicine and technology-assisted clinical decision-making tools. Future avenues of research are suggested.",https://doi.org/10.1016/j.techfore.2021.121222,2022,Kasuni Weerasinghe and Shane L. Scahill and David J. Pauleen and Nazim Taskin,BIG DATA ANALYTICS FOR CLINICAL DECISION-MAKING: UNDERSTANDING HEALTH SECTOR PERCEPTIONS OF POLICY AND PRACTICE,article
85,14704,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,journal,00401625,"2,226",Q1,117,448,1099,35581,10127,1061,"9,01","79,42",United States,Northern America,Elsevier Inc.,1970-2020,Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1),TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,"21,116",8.593,0.02416,"A universal trend in advanced manufacturing countries is defining Industry 4.0, industrialized internet and future factories as a recent wave, which may transform the production and its related services. Further, big data analytics has emerged as a game changer in the business world due to its uses for increasing accuracy in decision-making and enhancing performance of sustainable industry 4.0 applications. This study intends to emphasize on how to support Industry 4.0 with knowledge based view. For the same, a conceptual model is framed and presented with essential components that are required for a real world implementation. The study used qualitative analysis and was guided by a knowledge-based theoretical framework. Thematic analysis resulted in the identification of a number of emergent categories. Key findings highlight significant gaps in conventional decision-making systems and demonstrate how big data enhances firms’ strategic and operational decisions as well as facilitates informational access for improved marketing performance. The resulting proposed model can provide managers with a reference point for using big data to line up firms’ activities for more effective marketing efforts and presents a conceptual basis for further empirical studies in this area.",https://doi.org/10.1016/j.techfore.2021.120986,2021,Shivam Gupta and Théo Justy and Shampy Kamboj and Ajay Kumar and Eivind Kristoffersen,BIG DATA AND FIRM MARKETING PERFORMANCE: FINDINGS FROM KNOWLEDGE-BASED VIEW,article
86,14704,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,journal,00401625,"2,226",Q1,117,448,1099,35581,10127,1061,"9,01","79,42",United States,Northern America,Elsevier Inc.,1970-2020,Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1),TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,"21,116",8.593,0.02416,"Big data initiatives are critical for transforming traditional organizational decision making into data-driven decision making. However, prior information systems research has not paid enough attention to the impact of big data analytics usage on decision-making quality. Drawing on the dynamic capability theory, this study investigated the impact of big data analytics usage on decision-making quality and tested the mediating effect of data analytics capabilities. We collected data from 240 agricultural firms in China. The empirical results showed that big data analytics usage had a positive impact on decision-making quality and that data analytics capabilities played a mediating role in the relationship between big data analytics usage and decision-making quality. Hence, firms should not only popularize big data analytics usage in their business activities but also take measures to improve their data analytics capabilities, which will improve their decision-making quality toward competitive advantages.",https://doi.org/10.1016/j.techfore.2021.121355,2022,Lei Li and Jiabao Lin and Ye Ouyang and Xin (Robert) Luo,EVALUATING THE IMPACT OF BIG DATA ANALYTICS USAGE ON THE DECISION-MAKING QUALITY OF ORGANIZATIONS,article
87,14704,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,journal,00401625,"2,226",Q1,117,448,1099,35581,10127,1061,"9,01","79,42",United States,Northern America,Elsevier Inc.,1970-2020,Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1),TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,"21,116",8.593,0.02416,"Digital technologies are growing in importance for accelerating firms’ circular economy transition. However, so far, the focus has primarily been on the technical aspects of implementing these technologies with limited research on the organizational resources and capabilities required for successfully leveraging digital technologies for circular economy. To address this gap, this paper explores the business analytics resources firms should develop and how these should be orchestrated towards a firm-wide capability. The paper proposes a conceptual model highlighting eight business analytics resources that, in combination, build a business analytics capability for the circular economy and how this relates to firms’ circular economy implementation, resource orchestration capability, and competitive performance. The model is based on the results of a thematic analysis of 15 semi-structured expert interviews with key positions in industry. Our approach is informed by and further develops, the theory of the resource-based view and the resource orchestration view. Based on the results, we develop a deeper understanding of the importance of taking a holistic approach to business analytics when leveraging data and analytics towards a more efficient and effective digital-enabled circular economy, the smart circular economy.",https://doi.org/10.1016/j.techfore.2021.120957,2021,Eivind Kristoffersen and Patrick Mikalef and Fenna Blomsma and Jingyue Li,TOWARDS A BUSINESS ANALYTICS CAPABILITY FOR THE CIRCULAR ECONOMY,article
88,14704,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,journal,00401625,"2,226",Q1,117,448,1099,35581,10127,1061,"9,01","79,42",United States,Northern America,Elsevier Inc.,1970-2020,Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1),TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,"21,116",8.593,0.02416,"Leveraging data science can enable businesses to exploit data for competitive advantage by generating valuable insights. However, many industries cannot effectively incorporate data science into their business processes, as there is no comprehensive approach that allows strategic planning for organization-wide data science efforts and data assets. Accordingly, this study explores the Data Science Roadmapping (DSR) to guide organizations in aligning their business strategies with data-related, technological, and organizational resources. The proposed approach is built on the widely adopted technology roadmapping framework and customizes its context, architecture, and process by synthesizing data science, big data, and data-driven organization literature. Based on industry collaborations, the framework provides a hybrid and agile methodology comprising the recommended steps. We applied DSR with a research group with sector experience to create a comprehensive data science roadmap to validate and refine the framework. The results indicate that the framework facilitates DSR initiatives by creating a comprehensive roadmap capturing strategy, data, technology, and organizational perspectives. The contemporary literature illustrates prebuilt roadmaps to help businesses become data-driven. However, becoming data-driven also necessitates significant social change toward openness and trust. The DSR initiative can facilitate this social change by opening communication channels, aligning perspectives, and generating consensus among stakeholders.",https://doi.org/10.1016/j.techfore.2021.121264,2022,Kerem Kayabay and Mert Onuralp Gökalp and Ebru Gökalp and P. {Erhan Eren} and Altan Koçyiğit,DATA SCIENCE ROADMAPPING: AN ARCHITECTURAL FRAMEWORK FOR FACILITATING TRANSFORMATION TOWARDS A DATA-DRIVEN ORGANIZATION,article
89,14704,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,journal,00401625,"2,226",Q1,117,448,1099,35581,10127,1061,"9,01","79,42",United States,Northern America,Elsevier Inc.,1970-2020,Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1),TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,"21,116",8.593,0.02416,"This study aims to fill a gap in the literature by identifying, defining, and evaluating the critical success factors that impact the implementation of data intelligence in the public sector. Fourteen factors were identified, and then divided into three categories: organization, process, and technology. We used the analytical hierarchy process, a quantitative method of decision-making, to evaluate the importance of the factors presented in the study using data collected from nine experts. The results showed that technology, as a category, is the most important. The analysis also indicated that project management, information systems & data, and data quality are the most important factors among all fourteen critical success factors. We discuss the implications of the analysis for practitioners and researchers in the paper.",https://doi.org/10.1016/j.techfore.2021.121180,2021,Mohammad I. Merhi,EVALUATING THE CRITICAL SUCCESS FACTORS OF DATA INTELLIGENCE IMPLEMENTATION IN THE PUBLIC SECTOR USING ANALYTICAL HIERARCHY PROCESS,article
90,14704,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,journal,00401625,"2,226",Q1,117,448,1099,35581,10127,1061,"9,01","79,42",United States,Northern America,Elsevier Inc.,1970-2020,Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1),TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,"21,116",8.593,0.02416,"The importance and relevance of the discipline of statistics with the merits of the evolving field of data science continues to be debated in academia and industry. Following a narrative literature review with over 100 scholarly and practitioner-oriented publications from statistics and data science, this article generates a pragmatic perspective on the relationships and differences between statistics and data science. Some data scientists argue that statistics is not necessary for data science as statistics delivers simple explanations and data science delivers results. Therefore, this article aims to stimulate debate and discourse among both academics and practitioners in these fields. The findings reveal the need for stakeholders to accept the inherent advantages and disadvantages within the science of statistics and data science. The science of statistics enables data science (aiding its reliability and validity), and data science expands the application of statistics to Big Data. Data scientists should accept the contribution and importance of statistics and statisticians must humbly acknowledge the novel capabilities made possible through data science and support this field of study with their theoretical and pragmatic expertise. Indeed, the emergence of data science does pose a threat to statisticians, but the opportunities for synergies are far greater.",https://doi.org/10.1016/j.techfore.2021.121111,2021,Hossein Hassani and Christina Beneki and Emmanuel Sirimal Silva and Nicolas Vandeput and Dag Øivind Madsen,THE SCIENCE OF STATISTICS VERSUS DATA SCIENCE: WHAT IS THE FUTURE?,article
91,14704,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,journal,00401625,"2,226",Q1,117,448,1099,35581,10127,1061,"9,01","79,42",United States,Northern America,Elsevier Inc.,1970-2020,Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1),TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,"21,116",8.593,0.02416,"To date, health care industry has not fully grasped the potential benefits to be gained from big data analytics. While the constantly growing body of academic research on big data analytics is mostly technology oriented, a better understanding of the strategic implications of big data is urgently needed. To address this lack, this study examines the historical development, architectural design and component functionalities of big data analytics. From content analysis of 26 big data implementation cases in healthcare, we were able to identify five big data analytics capabilities: analytical capability for patterns of care, unstructured data analytical capability, decision support capability, predictive capability, and traceability. We also mapped the benefits driven by big data analytics in terms of information technology (IT) infrastructure, operational, organizational, managerial and strategic areas. In addition, we recommend five strategies for healthcare organizations that are considering to adopt big data analytics technologies. Our findings will help healthcare organizations understand the big data analytics capabilities and potential benefits and support them seeking to formulate more effective data-driven analytics strategies.",https://doi.org/10.1016/j.techfore.2015.12.019,2018,Yichuan Wang and LeeAnn Kung and Terry Anthony Byrd,BIG DATA ANALYTICS: UNDERSTANDING ITS CAPABILITIES AND POTENTIAL BENEFITS FOR HEALTHCARE ORGANIZATIONS,article
92,14704,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,journal,00401625,"2,226",Q1,117,448,1099,35581,10127,1061,"9,01","79,42",United States,Northern America,Elsevier Inc.,1970-2020,Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1),TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,"21,116",8.593,0.02416,"Big data is an important driver of disruptive innovation that may increase organizations' competitive advantage. To create innovative data combinations and decrease investments, big data is often shared among organizations, crossing organizational boundaries. However, these big data collaborations need to balance disruptive innovation and compliance to a strict data protection regime in the EU. This paper investigates how inter-organizational big data collaborations arrange and govern their activities in the context of this dilemma. We conceptualize big data as inter-organizational systems and build on IS and Organization Theory literature to develop four archetypical governance arrangements: Market, Hierarchy, Bazaar and Network. Subsequently, these arrangements are investigated in four big data collaboration use cases. The contributions of this study to literature are threefold. First, we conceptualize the organization behind big data collaborations as IOS governance. Second, we show that the choice for an inter-organizational governance arrangement highly depends on the institutional pressure from regulation and the type of data that is shared. In this way, we contribute to the limited body of research on the antecedents of IOS governance. Last, we highlight with four use cases how the principles of big data, specifically data maximization, clash with the principles of EU data protection regulation. Practically, our study provides guidelines for IT and innovation managers how to arrange and govern the sharing of data among multiple organizations.",https://doi.org/10.1016/j.techfore.2017.09.040,2018,Tijs {van den Broek} and Anne Fleur {van Veenstra},GOVERNANCE OF BIG DATA COLLABORATIONS: HOW TO BALANCE REGULATORY COMPLIANCE AND DISRUPTIVE INNOVATION,article
93,14704,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,journal,00401625,"2,226",Q1,117,448,1099,35581,10127,1061,"9,01","79,42",United States,Northern America,Elsevier Inc.,1970-2020,Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1),TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,"21,116",8.593,0.02416,"This paper draws on data from three organisational case studies and expert interviews to propose that persuasive practices are the precursors and enablers of analytical capability development. A bundle of seven practices was identified and observed to bridge multiple gaps between technical and non-technical colleagues on big data analytics (BDA) projects. The deployment of these practices varied according to the level of BDA maturity and featured a host of socio-material elements. This paper complements existing technical case studies with a fine-grained qualitative account of the managerial and human elements of BDA implementation. Effective deployment of persuasive practices potentially both embeds the benefits and mitigates the risks of BDA, sowing the seeds of many different forms of value.",https://doi.org/10.1016/j.techfore.2020.120300,2020,Jeffrey Hughes and Kirstie Ball,SOWING THE SEEDS OF VALUE? PERSUASIVE PRACTICES AND THE EMBEDDING OF BIG DATA ANALYTICS,article
94,14704,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,journal,00401625,"2,226",Q1,117,448,1099,35581,10127,1061,"9,01","79,42",United States,Northern America,Elsevier Inc.,1970-2020,Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1),TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,"21,116",8.593,0.02416,"The analysis of the transformation and changes in scientific disciplines has always been a critical path for policymakers and researchers. The current study examines the changes in the research areas of data and information quality (DIQ). The aim of this study was to detect different types of changes occurring in the scientific areas including birth, death, growth, decline, merge, and splitting. A model has been developed for this data mining. To test the model, all DIQ articles published in online scientific citation indexing service or Web of Science (WOS) between 1970 and 2016 were extracted and analyzed using the given model. The study is related to the Big Data as well as the integration methods in Big Data which is the most important area in DIQ. It is demonstrated that the first and second emerging research areas are sub-disciplines of entity resolution and record linkage. Accordingly, linkage and privacy are the first emerging research area and the entity resolution using ontology is the second in DIQ. This is followed by the social media issues and genetic related DIQ issues.",https://doi.org/10.1016/j.techfore.2018.08.003,2018,Babak Sohrabi and Ahmad Khalilijafarabad,SYSTEMATIC METHOD FOR FINDING EMERGENCE RESEARCH AREAS AS DATA QUALITY,article
95,14704,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,journal,00401625,"2,226",Q1,117,448,1099,35581,10127,1061,"9,01","79,42",United States,Northern America,Elsevier Inc.,1970-2020,Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1),TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,"21,116",8.593,0.02416,"The Data Big Bang that the development of the ICTs has raised is providing us with a stream of fresh and digitized data related to how people, companies and other organizations interact. To turn these data into knowledge about the underlying behavior of the social and economic agents, organizations and researchers must deal with such amount of unstructured and heterogeneous data. Succeeding in this task requires to carefully plan and organize the whole process of data analysis taking into account the particularities of the social and economic analyses, which include the wide variety of heterogeneous sources of information and a strict governance policy. Grounded on the data lifecycle approach, this paper develops a Big Data architecture that properly integrates most of the non-traditional information sources and data analysis methods in order to provide a specifically designed system for forecasting social and economic behaviors, trends and changes.",https://doi.org/10.1016/j.techfore.2017.07.027,2018,Desamparados Blazquez and Josep Domenech,BIG DATA SOURCES AND METHODS FOR SOCIAL AND ECONOMIC ANALYSES,article
96,14704,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,journal,00401625,"2,226",Q1,117,448,1099,35581,10127,1061,"9,01","79,42",United States,Northern America,Elsevier Inc.,1970-2020,Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1),TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,"21,116",8.593,0.02416,"The business concept of the circular economy (CE) has gained significant momentum among practitioners and researchers alike. However, successful adoption and implementation of this paradigm of managing business remains a challenge. In this article, we build a case for utilizing big data analytics (BDA) as a fundamental basis for informed and data driven decision making in supply chain networks supporting CE. We view this from a stakeholder perspective and argue that a collaborative association among all supply chain members can positively affect CE implementation. We propose a model highlighting the facilitating role of big data analytics for achieving shared sustainability goals. The model is based on integrating thematic categories coming out of 10 semi-structured interviews with key position holders in industry. We argue that mutual support and coordination driven by a stakeholder perspective coupled with holistic information processing and sharing along the entire supply chain network can effectively create a basis for achieving the triple bottom line of economic, ecological and social benefits. The proposed model is useful for managers in that it provides a reference point for aligning activities with the circular economy paradigm. The conceptual model provides a theoretical basis for future empirical research in this domain.",https://doi.org/10.1016/j.techfore.2018.06.030,2019,Shivam Gupta and Haozhe Chen and Benjamin T. Hazen and Sarabjot Kaur and Ernesto D.R. {Santibañez Gonzalez},CIRCULAR ECONOMY AND BIG DATA ANALYTICS: A STAKEHOLDER PERSPECTIVE,article
97,14704,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,journal,00401625,"2,226",Q1,117,448,1099,35581,10127,1061,"9,01","79,42",United States,Northern America,Elsevier Inc.,1970-2020,Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1),TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,"21,116",8.593,0.02416,"While there is a general recognition that breakthrough innovation is non-linear and requires an alignment between producers (supply) and users (demand), there is still a need for strategic intelligence about the emerging supply chains of new technological innovations. This technology delivery system (TDS) is an updated form of the TDS model and provides a promising chain-link approach to the supply side of innovation. Building on early research into supply-side TDS studies, we present a systematic approach to building a TDS model that includes four phases: (1) identifying the macroeconomic and policy environment, including market competition, financial investment, and industrial policy; (2) specifying the key public and private institutions; (3) addressing the core technical complements and their owners, then tracing their interactions through information linkages and technology transfers; and (4) depicting the market prospects and evaluating the potential profound influences on technological change and social developments. Our TDS methodology is illustrated using the field of Big Data & Analytics (“BDA”).",https://doi.org/10.1016/j.techfore.2017.09.012,2018,Ying Huang and Alan L. Porter and Scott W. Cunningham and Douglas K.R. Robinson and Jianhua Liu and Donghua Zhu,A TECHNOLOGY DELIVERY SYSTEM FOR CHARACTERIZING THE SUPPLY SIDE OF TECHNOLOGY EMERGENCE: ILLUSTRATED FOR BIG DATA & ANALYTICS,article
98,14704,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,journal,00401625,"2,226",Q1,117,448,1099,35581,10127,1061,"9,01","79,42",United States,Northern America,Elsevier Inc.,1970-2020,Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1),TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,"21,116",8.593,0.02416,"As society continues its rapid change to a digitized individual, corporate, and government environment it is prudent for researchers to investigate the zeitgeist of the global citizenry. The technological changes brought about by big data analytics are changing the way we gather and view data. This big data analytics sentiment research examines how Chinese and American respondents may view big data collection and analytics differently. The paper follows with an analysis of reported attitudes toward possible viewpoints from each country on various big data analytics topics ranging from individual to business and governmental foci. Hofstede's cultural dimensions are used to inform and frame our research hypotheses. Findings suggest that Chinese and American perspectives differ on individual data values, with the Chinese being more open to data collection and analytic techniques targeted toward individuals. Furthermore, support is found that US respondents have a more favorable view of businesses' use of data analytics. Finally, there is a strong difference in the attitudes toward governmental use of data, where US respondents do not favor governmental big data analytics usage and the Chinese respondents indicated a greater acceptance of governmental data usage. These findings are helpful in better understanding appropriate technological change and adoption from a societal perspective. Specifically, this research provides insights for corporate business and government entities suggesting how they might adjust their approach to big data collection and management in order to better support and sustain their organization's services and products.",https://doi.org/10.1016/j.techfore.2017.06.029,2018,Ryan C. LaBrie and Gerhard H. Steinke and Xiangmin Li and Joseph A. Cazier,BIG DATA ANALYTICS SENTIMENT: US-CHINA REACTION TO DATA COLLECTION BY BUSINESS AND GOVERNMENT,article
99,14704,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,journal,00401625,"2,226",Q1,117,448,1099,35581,10127,1061,"9,01","79,42",United States,Northern America,Elsevier Inc.,1970-2020,Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1),TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,"21,116",8.593,0.02416,"Although big data, big data analytics (BDA) and business intelligence have attracted growing attention of both academics and practitioners, a lack of clarity persists about how BDA has been applied in business and management domains. In reflecting on Professor Ayre's contributions, we want to extend his ideas on technological change by incorporating the discourses around big data, BDA and business intelligence. With this in mind, we integrate the burgeoning but disjointed streams of research on big data, BDA and business intelligence to develop unified frameworks. Our review takes on both technical and managerial perspectives to explore the complex nature of big data, techniques in big data analytics and utilisation of big data in business and management community. The advanced analytics techniques appear pivotal in bridging big data and business intelligence. The study of advanced analytics techniques and their applications in big data analytics led to identification of promising avenues for future research.",https://doi.org/10.1016/j.techfore.2018.06.009,2019,Jie Sheng and Joseph Amankwah-Amoah and Xiaojun Wang,TECHNOLOGY IN THE 21ST CENTURY: NEW CHALLENGES AND OPPORTUNITIES,article
100,14704,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,journal,00401625,"2,226",Q1,117,448,1099,35581,10127,1061,"9,01","79,42",United States,Northern America,Elsevier Inc.,1970-2020,Applied Psychology (Q1); Business and International Management (Q1); Management of Technology and Innovation (Q1),TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE,"21,116",8.593,0.02416,"This study investigates the literary corpus of the role and potential of data intelligence and analytics through the lenses of artificial intelligence (AI), big data, and the human–AI interface to improve overall decision-making processes. It investigates how data intelligence and analytics improve decision-making processes in the public sector. A bibliometric analysis of a database containing 161 English-language articles published between 2017 and 2021 is performed, providing a map of the knowledge produced and disseminated in previous studies. It provides insights into key topics, citation patterns, publication activities, the status of collaborations between contributors over past studies, aggregated data intelligence, and analytics research contributions. The study provides a retrospective review of published content in the field of data intelligence and analytics. The findings indicate that field research has been concentrated mainly on emerging technologies' intelligence capabilities rather than on human–artificial intelligence in decision-making performance in the public sector. This study extends an ambidexterity theory in decision support, which enlightens how this ambidexterity can be encouraged and how it affects decision outcomes. The study emphasises the importance of the public sector adoption of data intelligence and analytics, as well as its efficiency. Furthermore, this study expands how researchers and practitioners interpret and understand data intelligence and analytics, AI, and big data for effective public sector decision-making.",https://doi.org/10.1016/j.techfore.2021.121201,2022,Assunta {Di Vaio} and Rohail Hassan and Claude Alavoine,DATA INTELLIGENCE AND ANALYTICS: A BIBLIOMETRIC ANALYSIS OF HUMAN–ARTIFICIAL INTELLIGENCE IN PUBLIC SECTOR DECISION-MAKING EFFECTIVENESS,article
101,17466,NEUROBIOLOGY OF DISEASE,journal,"1095953X, 09699961","2,205",Q1,166,330,722,29350,4074,710,"5,44","88,94",United States,Northern America,Academic Press Inc.,1994-2020,Neurology (Q1),NEUROBIOLOGY OF DISEASE,NEUROBIOLOGY OF DISEASE,"21,360",5.996,0.02068,"We describe the infrastructure and functionality for a centralized preclinical and clinical data repository and analytic platform to support importing heterogeneous multi-modal data, automatically and manually linking data across modalities and sites, and searching content. We have developed and applied innovative image and electrophysiology processing methods to identify candidate biomarkers from MRI, EEG, and multi-modal data. Based on heterogeneous biomarkers, we present novel analytic tools designed to study epileptogenesis in animal model and human with the goal of tracking the probability of developing epilepsy over time.",https://doi.org/10.1016/j.nbd.2018.05.026,2019,Dominique Duncan and Paul Vespa and Asla Pitkänen and Adebayo Braimah and Niina Lapinlampi and Arthur W. Toga,BIG DATA SHARING AND ANALYSIS TO ADVANCE RESEARCH IN POST-TRAUMATIC EPILEPSY,article
102,20209,BUSINESS HORIZONS,journal,00076813,"2,174",Q1,87,64,278,2475,2066,227,"6,68","38,67",United Kingdom,Western Europe,Elsevier Ltd.,1957-2020,Business and International Management (Q1); Marketing (Q1),BUSINESS HORIZONS,BUSINESS HORIZONS,"7,443",6.361,0.00571,"The use of big data to help explain fluctuations in the broader economy and key business performance indicators is now so commonplace that in some instances it has even begun to rival more traditional measures. Big data sources can very often provide advantages when compared with these more traditional data sources, but with these advantages also come potential pitfalls. We lay out a checklist called SMALL that we have developed in order to help interested parties as they navigate the big data minefield. Based on a set of five questions, the SMALL checklist should help users of big data draw justifiable conclusions and avoid making mistakes in matters of interpretation. To demonstrate, we provide several case studies that demonstrate the subtle nuances of several of these new big data sets and show how the problems they face often closely relate to age-old concerns that more traditional data sources are also forced to tackle.",https://doi.org/10.1016/j.bushor.2021.06.004,2022,Scott A. Brave and R. Andrew Butters and Michael Fogarty,"THE PERILS OF WORKING WITH BIG DATA, AND A SMALL CHECKLIST YOU CAN USE TO RECOGNIZE THEM",article
103,20209,BUSINESS HORIZONS,journal,00076813,"2,174",Q1,87,64,278,2475,2066,227,"6,68","38,67",United Kingdom,Western Europe,Elsevier Ltd.,1957-2020,Business and International Management (Q1); Marketing (Q1),BUSINESS HORIZONS,BUSINESS HORIZONS,"7,443",6.361,0.00571,"Machine learning holds great promise for lowering product and service costs, speeding up business processes, and serving customers better. It is recognized as one of the most important application areas in this era of unprecedented technological development, and its adoption is gaining momentum across almost all industries. In view of this, we offer a brief discussion of categories of machine learning and then present three types of machine-learning usage at enterprises. We then discuss the trade-off between the accuracy and interpretability of machine-learning algorithms, a crucial consideration in selecting the right algorithm for the task at hand. We next outline three cases of machine-learning development in financial services. Finally, we discuss challenges all managers must confront in deploying machine-learning applications.",https://doi.org/10.1016/j.bushor.2019.10.005,2020,In Lee and Yong Jae Shin,"MACHINE LEARNING FOR ENTERPRISES: APPLICATIONS, ALGORITHM SELECTION, AND CHALLENGES",article
104,20209,BUSINESS HORIZONS,journal,00076813,"2,174",Q1,87,64,278,2475,2066,227,"6,68","38,67",United Kingdom,Western Europe,Elsevier Ltd.,1957-2020,Business and International Management (Q1); Marketing (Q1),BUSINESS HORIZONS,BUSINESS HORIZONS,"7,443",6.361,0.00571,"As companies become increasingly digital, growth hacking emerged as a new way of scaling businesses. While the term is fashionable in business, many executives remain confused about the concept. Even if firms have an idea of what growth hacking is, they may still be puzzled as to how to do it, creating a strategy-execution gap. Our article assists firms by bridging the growth hacking strategy-execution gap. First, we provide a growth hacking framework and deconstruct its building blocks: marketing, data analysis, coding, and the lean startup philosophy. We then present a taxonomy of 34 growth hacking patterns along the customer lifecycle of acquisition, activation, revenue, retention, and referral; categorize them on the two dimensions of resource intensity and time lag; and provide an example of how to apply the taxonomy in the case of a fitness application. Finally, we discuss seven opportunities and challenges of growth hacking that firms should keep in mind.",https://doi.org/10.1016/j.bushor.2019.09.001,2019,René Bohnsack and Meike Malena Liesner,WHAT THE HACK? A GROWTH HACKING TAXONOMY AND PRACTICAL APPLICATIONS FOR FIRMS,article
105,20209,BUSINESS HORIZONS,journal,00076813,"2,174",Q1,87,64,278,2475,2066,227,"6,68","38,67",United Kingdom,Western Europe,Elsevier Ltd.,1957-2020,Business and International Management (Q1); Marketing (Q1),BUSINESS HORIZONS,BUSINESS HORIZONS,"7,443",6.361,0.00571,"With the explosion of the digital universe, it is becoming increasingly important to understand how organizational decision making (i.e., the business-oriented perspective) is intertwined with an understanding of enterprise data assets (i.e., the data-oriented perspective). This article first compares the business- and data-oriented perspectives to describe how the two views mesh with each other. It then presents three elements in the data-oriented perspective that are collectively referred to as the data triad: (1) use, (2) design and storage, and (3) processes and people. In describing the data triad, this article highlights practices, architectural techniques, and example tools that are used to manage, access, analyze, and deliver data. By presenting different elements of the data-oriented perspective, this article broadly and concretely describes the data triad and how it can play a role in the redefined scope of work for data-driven business managers.",https://doi.org/10.1016/j.bushor.2016.06.001,2016,Vijay Khatri,MANAGERIAL WORK IN THE REALM OF THE DIGITAL UNIVERSE: THE ROLE OF THE DATA TRIAD,article
106,20209,BUSINESS HORIZONS,journal,00076813,"2,174",Q1,87,64,278,2475,2066,227,"6,68","38,67",United Kingdom,Western Europe,Elsevier Ltd.,1957-2020,Business and International Management (Q1); Marketing (Q1),BUSINESS HORIZONS,BUSINESS HORIZONS,"7,443",6.361,0.00571,"Big data represents a new technology paradigm for data that are generated at high velocity and high volume, and with high variety. Big data is envisioned as a game changer capable of revolutionizing the way businesses operate in many industries. This article introduces an integrated view of big data, traces the evolution of big data over the past 20 years, and discusses data analytics essential for processing various structured and unstructured data. This article illustrates the application of data analytics using merchant review data. The impacts of big data on key business performances are then evaluated. Finally, six technical and managerial challenges are discussed.",https://doi.org/10.1016/j.bushor.2017.01.004,2017,In Lee,"BIG DATA: DIMENSIONS, EVOLUTION, IMPACTS, AND CHALLENGES",article
107,20209,BUSINESS HORIZONS,journal,00076813,"2,174",Q1,87,64,278,2475,2066,227,"6,68","38,67",United Kingdom,Western Europe,Elsevier Ltd.,1957-2020,Business and International Management (Q1); Marketing (Q1),BUSINESS HORIZONS,BUSINESS HORIZONS,"7,443",6.361,0.00571,"User-generated content, such as online product reviews, is a valuable source of consumer insight. Such unstructured big data is generated in real-time, is easily accessed, and contains messages consumers want managers to hear. Analyzing such data has potential to revolutionize market research and competitive analysis, but how can the messages be extracted? How can the vast amount of data be condensed into insights to help steer businesses’ strategy? We describe a non-proprietary technique that can be applied by anyone with statistical training. Latent Dirichlet Allocation (LDA) can analyze huge amounts of text and describe the content as focusing on unseen attributes in a specific weighting. For example, a review of a graphic novel might be analyzed to focus 70% on the storyline and 30% on the graphics. Aggregating the content from numerous consumers allows us to understand what is, collectively, on consumers’ minds, and from this we can infer what consumers care about. We can even highlight which attributes are seen positively or negatively. The value of this technique extends well beyond the CMO's office as LDA can map the relative strategic positions of competitors where they matter most: in the minds of consumers.",https://doi.org/10.1016/j.bushor.2015.10.001,2016,Neil T. Bendle and Xin (Shane) Wang,UNCOVERING THE MESSAGE FROM THE MESS OF BIG DATA,article
108,14642,TUNNELLING AND UNDERGROUND SPACE TECHNOLOGY,journal,08867798,"2,172",Q1,98,375,946,16500,6440,942,"6,53","44,00",United Kingdom,Western Europe,Elsevier Ltd.,1986-2020,Building and Construction (Q1); Geotechnical Engineering and Engineering Geology (Q1),TUNNELLING AND UNDERGROUND SPACE TECHNOLOGY,TUNNELLING AND UNDERGROUND SPACE TECHNOLOGY,"16,336",5.915,0.0149,"The perception of surrounding rock geological conditions ahead the tunnel face is essential for TBM safe and efficient tunnelling. This paper developed a perception approach of surrounding rock class based on TBM operational big data and combined unsupervised-supervised learning. In data preprocessing, four data mining techniques (i.e., Z-score, K-NN, Kalman filtering, and wavelet packet decomposition) were used to detect outliers, substitute outliers, suppress noise, and extract features, respectively. Then, GMM was used to revise the original surrounding rock class through clustering TBM load parameters and performance parameters in view of the shortcomings of the HC method in the TBM-excavated tunnel. After that, five various ensemble learning classification models were constructed to identify the surrounding rock class, in which model hyper-parameters were automatically tuned by Bayes optimization. In order to evaluate model performance, balanced accuracy, Kappa, F1-score, and training time were taken into account, and a novel multi-metric comprehensive ranking system was designed. Engineering application results indicated that LightGBM achieved the most superior performance with the highest comprehensive score of 6.9066, followed by GBDT (5.9228), XGBoost (5.4964), RF (3.7581), and AdaBoost (0.9946). Through the weighted purity reduction algorithm, the contributions of input features on the five models were quantitatively analyzed. Finally, the impact of class imbalance on model performance was discussed using the ADASYN algorithm, showing that eliminating class imbalance can further improve the model's perception ability.",https://doi.org/10.1016/j.tust.2021.104285,2022,Xin Yin and Quansheng Liu and Xing Huang and Yucong Pan,PERCEPTION MODEL OF SURROUNDING ROCK GEOLOGICAL CONDITIONS BASED ON TBM OPERATIONAL BIG DATA AND COMBINED UNSUPERVISED-SUPERVISED LEARNING,article
109,14642,TUNNELLING AND UNDERGROUND SPACE TECHNOLOGY,journal,08867798,"2,172",Q1,98,375,946,16500,6440,942,"6,53","44,00",United Kingdom,Western Europe,Elsevier Ltd.,1986-2020,Building and Construction (Q1); Geotechnical Engineering and Engineering Geology (Q1),TUNNELLING AND UNDERGROUND SPACE TECHNOLOGY,TUNNELLING AND UNDERGROUND SPACE TECHNOLOGY,"16,336",5.915,0.0149,"This work explores the potential for predicting TBM performance using deep learning. It focuses on a 17.5-km-long tunnel excavated for the Yingsong Water Diversion Project in Northeastern China with its 728 days’ continuous monitoring of mechanical data. The prediction uses the deep belief network (DBN) proposed by Hinton et al. (2006),on the penetration rate, cutter rotation speed, torque, and thrust force. Field Penetration Index (FPI) is introduced to quantify TBM performance in the field. The DBN algorithm trains on nth number of preceding elements and predicts the performance of the n + 1th element. Prior to the implementation of the DBN, a pilot test was performed to find the optimal values for the network structural parameters (number of input nodes, number of hidden layers, number of nodes in the hidden layers, and learning rate). Predictions on FPIs in all the three rock types were then proceeded with good agreement with the field measured data. The mean relative errors for the predicted measured FPIs are generally less than 0.15 and the correlation coefficients (R) can be higher than 0.78. The predicted and measured FPI values along the length of the tunnel graphically follow the same trends. These results confirm the usefulness of big data and the deep learning in predicting TBM performance.",https://doi.org/10.1016/j.tust.2020.103636,2021,Shangxin Feng and Zuyu Chen and Hua Luo and Shanyong Wang and Yufei Zhao and Lipeng Liu and Daosheng Ling and Liujie Jing,TUNNEL BORING MACHINES (TBM) PERFORMANCE PREDICTION: A CASE STUDY USING BIG DATA AND DEEP LEARNING,article
110,22489,EUROPEAN JOURNAL OF OPERATIONAL RESEARCH,journal,03772217,"2,161",Q1,260,657,2080,32022,13341,2068,"6,02","48,74",Netherlands,Western Europe,Elsevier,1977-2021,Computer Science (miscellaneous) (Q1); Information Systems and Management (Q1); Management Science and Operations Research (Q1); Modeling and Simulation (Q1),EUROPEAN JOURNAL OF OPERATIONAL RESEARCH,EUROPEAN JOURNAL OF OPERATIONAL RESEARCH,"64,756",5.334,0.04764,"The popularity of big data and business analytics has increased tremendously in the last decade and a key challenge for organizations is in understanding how to leverage them to create business value. However, while the literature acknowledges the importance of these topics little work has addressed them from the organization's point of view. This paper investigates the challenges faced by organizational managers seeking to become more data and information-driven in order to create value. Empirical research comprised a mixed methods approach using (1) a Delphi study with practitioners through various forums and (2) interviews with business analytics managers in three case organizations. The case studies reinforced the Delphi findings and highlighted several challenge focal areas: organizations need a clear data and analytics strategy, the right people to effect a data-driven cultural change, and to consider data and information ethics when using data for competitive advantage. Further, becoming data-driven is not merely a technical issue and demands that organizations firstly organize their business analytics departments to comprise business analysts, data scientists, and IT personnel, and secondly align that business analytics capability with their business strategy in order to tackle the analytics challenge in a systemic and joined-up way. As a result, this paper presents a business analytics ecosystem for organizations that contributes to the body of scholarly knowledge by identifying key business areas and functions to address to achieve this transformation.",https://doi.org/10.1016/j.ejor.2017.02.023,2017,Richard Vidgen and Sarah Shaw and David B. Grant,MANAGEMENT CHALLENGES IN CREATING VALUE FROM BUSINESS ANALYTICS,article
111,23916,ENVIRONMENTAL POLLUTION,journal,"02697491, 18736424","2,136",Q1,227,2109,4204,129684,35383,4169,"8,04","61,49",United Kingdom,Western Europe,Elsevier Ltd.,"1970-1980, 1986-2020","Health, Toxicology and Mutagenesis (Q1); Medicine (miscellaneous) (Q1); Pollution (Q1); Toxicology (Q1)",ENVIRONMENTAL POLLUTION,ENVIRONMENTAL POLLUTION,"84,491",8.071,0.07982,"An accurate estimation of population exposure to particulate matter with an aerodynamic diameter <2.5 μm (PM2.5) is crucial to hazard assessment and epidemiology. This study integrated annual data from 1146 in-home air monitors, air quality monitoring network, public applications, and traffic smart cards to determine the pattern of PM2.5 concentrations and activities in different microenvironments (including outdoors, indoors, subways, buses, and cars). By combining massive amounts of signaling data from cell phones, this study applied a spatio-temporally weighted model to improve the estimation of PM2.5 exposure. Using Shanghai as a case study, the annual average indoor PM2.5 concentration was estimated to be 29.3 ± 27.1 μg/m3 (n = 365), with an average infiltration factor of 0.63. The spatio-temporally weighted PM2.5 exposure was estimated to be 32.1 ± 13.9 μg/m3 (n = 365), with indoor PM2.5 contributing the most (85.1%), followed by outdoor (7.6%), bus (3.7%), subway (3.1%), and car (0.5%). However, considering that outdoor PM2.5 makes a significant contribution to indoor PM2.5, outdoor PM2.5 was responsible for most of the exposure in Shanghai. A heatmap of PM2.5 exposure indicated that the inner-city exposure index was significantly higher than that of the outskirts city, which demonstrated that the importance of spatial differences in population exposure estimation.",https://doi.org/10.1016/j.envpol.2019.07.034,2019,YuJie Ben and FuJun Ma and Hao Wang and Muhammad Azher Hassan and Romanenko Yevheniia and WenHong Fan and Yubiao Li and ZhaoMin Dong,A SPATIO-TEMPORALLY WEIGHTED HYBRID MODEL TO IMPROVE ESTIMATES OF PERSONAL PM2.5 EXPOSURE: INCORPORATING BIG DATA FROM MULTIPLE DATA SOURCES,article
112,23916,ENVIRONMENTAL POLLUTION,journal,"02697491, 18736424","2,136",Q1,227,2109,4204,129684,35383,4169,"8,04","61,49",United Kingdom,Western Europe,Elsevier Ltd.,"1970-1980, 1986-2020","Health, Toxicology and Mutagenesis (Q1); Medicine (miscellaneous) (Q1); Pollution (Q1); Toxicology (Q1)",ENVIRONMENTAL POLLUTION,ENVIRONMENTAL POLLUTION,"84,491",8.071,0.07982,"Cooking emission inventories always have poor spatial resolutions when applying with traditional methods, making their impacts on ambient air and human health remain obscure. In this study, we created a systematic dataset of cooking emission factors (CEFs) and applied it with a new data source, cooking-related point of interest (POI) data, to build up highly spatial resolved cooking emission inventories from the city scale. Averaged CEFs of six particulate and gaseous species (PM, OC, EC, NMHC, OVOCs, VOCs) were 5.92 ± 6.28, 4.10 ± 5.50, 0.05 ± 0.05, 22.54 ± 20.48, 1.56 ± 1.44, and 7.94 ± 6.27 g/h normalized in every cook stove, respectively. A three-field CEF index containing activity and emission factor species was created to identify and further build a connection with cooking-related POI data. A total of 95,034 cooking point sources were extracted from Beijing, as a study city. In downtown areas, four POI types were overlapped in the central part of the city and radiated into eight distinct directions from south to north. Estimated PM/VOC emissions caused by cooking activities in Beijing were 4.81/9.85 t per day. A 3D emission map showed an extremely unbalanced emission density in the Beijing region. Emission hotspots were seen in Central Business District (CBD), Sanlitun, and Wangjing in Chaoyang District and Willow and Zhongguancun in Haidian District. PM/VOC emissions could be as high as 16.6/42.0 kg/d in the searching radius of 2 km. For PM, the total emissions were 417.4, 389.0, 466.9, and 443.0 t between Q1 and Q4 2019 in Beijing, respectively. The proposed methodology is transferrable to other Chinese cities for deriving enhanced commercial cooking inventories and potentially highlighting the further importance of cooking emissions on air quality and human health.",https://doi.org/10.1016/j.envpol.2022.120320,2022,Pengchuan Lin and Jian Gao and Yisheng Xu and James J. Schauer and Jiaqi Wang and Wanqing He and Lei Nie,ENHANCED COMMERCIAL COOKING INVENTORIES FROM THE CITY SCALE THROUGH NORMALIZED EMISSION FACTOR DATASET AND BIG DATA,article
113,23916,ENVIRONMENTAL POLLUTION,journal,"02697491, 18736424","2,136",Q1,227,2109,4204,129684,35383,4169,"8,04","61,49",United Kingdom,Western Europe,Elsevier Ltd.,"1970-1980, 1986-2020","Health, Toxicology and Mutagenesis (Q1); Medicine (miscellaneous) (Q1); Pollution (Q1); Toxicology (Q1)",ENVIRONMENTAL POLLUTION,ENVIRONMENTAL POLLUTION,"84,491",8.071,0.07982,"Due to time- and expense- consuming of conventional indoor PM2.5 (particulate matter with aerodynamic diameter of less than 2.5 μm) sampling, the sample size in previous studies was generally small, which leaded to high heterogeneity in indoor PM2.5 exposure assessment. Based on 4403 indoor air monitors in Beijing, this study evaluated indoor PM2.5 exposure from 15th March 2016 to 14th March 2017. Indoor PM2.5 concentration in Beijing was estimated to be 38.6 ± 18.4 μg/m3. Specifically, the concentration in non-heating season was 34.9 ± 15.8 μg/m3, which was 24% lower than that in heating season (46.1 ± 21.2 μg/m3). A significant correlation between indoor and ambient PM2.5 (p < 0.05) was evident with an infiltration factor of 0.21, and the ambient PM2.5 contributed approximately 52% and 42% to indoor PM2.5 for non-heating and heating seasons, respectively. Meanwhile, the mean indoor/outdoor (I/O) ratio was estimated to be 0.73 ± 0.54. Finally, the adjusted PM2.5 exposure level integrating the indoor and outdoor impact was calculated to be 46.8 ± 27.4 μg/m3, which was approximately 42% lower than estimation only relied on ambient PM2.5 concentration. This study is the first attempt to employ big data from commercial air monitors to evaluate indoor PM2.5 exposure and risk in Beijing, which may be instrumental to indoor PM2.5 pollution control.",https://doi.org/10.1016/j.envpol.2018.05.030,2018,JinXing Zuo and Wei Ji and YuJie Ben and Muhammad Azher Hassan and WenHong Fan and Liam Bates and ZhaoMin Dong,USING BIG DATA FROM AIR QUALITY MONITORS TO EVALUATE INDOOR PM2.5 EXPOSURE IN BUILDINGS: CASE STUDY IN BEIJING,article
114,23916,ENVIRONMENTAL POLLUTION,journal,"02697491, 18736424","2,136",Q1,227,2109,4204,129684,35383,4169,"8,04","61,49",United Kingdom,Western Europe,Elsevier Ltd.,"1970-1980, 1986-2020","Health, Toxicology and Mutagenesis (Q1); Medicine (miscellaneous) (Q1); Pollution (Q1); Toxicology (Q1)",ENVIRONMENTAL POLLUTION,ENVIRONMENTAL POLLUTION,"84,491",8.071,0.07982,"In the past few decades, extensive epidemiological studies have focused on exploring the adverse effects of PM2.5 (particulate matters with aerodynamic diameters less than 2.5 μm) on public health. However, most of them failed to consider the dynamic changes of population distribution adequately and were limited by the accuracy of PM2.5 estimations. Therefore, in this study, location-based service (LBS) data from social media and satellite-derived high-quality PM2.5 concentrations were collected to perform highly spatiotemporal exposure assessments for thirteen cities in the Beijing-Tianjin-Hebei (BTH) region, China. The city-scale exposure levels and the corresponding health outcomes were first estimated. Then the uncertainties in exposure risk assessments were quantified based on in-situ PM2.5 observations and static population data. The results showed that approximately half of the population living in the BTH region were exposed to monthly mean PM2.5 concentration greater than 80 μg/m3 in 2015, and the highest risk was observed in December. In terms of all-cause, cardiovascular, and respiratory disease, the premature deaths attributed to PM2.5 were estimated to be 138,150, 80,945, and 18,752, respectively. A comparative analysis between five different exposure models further illustrated that the dynamic population distribution and accurate PM2.5 estimations showed great influence on environmental exposure and health assessments and need be carefully considered. Otherwise, the results would be considerably over- or under-estimated.",https://doi.org/10.1016/j.envpol.2019.06.057,2019,Yimeng Song and Bo Huang and Qingqing He and Bin Chen and Jing Wei and Rashed Mahmood,DYNAMIC ASSESSMENT OF PM2.5 EXPOSURE AND HEALTH RISK USING REMOTE SENSING AND GEO-SPATIAL BIG DATA,article
115,20107,SURVEY OF OPHTHALMOLOGY,journal,"00396257, 18793304","2,131",Q1,132,69,235,6991,1131,199,"4,75","101,32",United States,Northern America,Elsevier USA,"1956-1970, 1972-2020",Ophthalmology (Q1),SURVEY OF OPHTHALMOLOGY,SURVEY OF OPHTHALMOLOGY,"7,296",6.048,0.00487,"Large population-based health administrative databases, clinical registries, and data linkage systems are a rapidly expanding resource for health research. Ophthalmic research has benefited from the use of these databases in expanding the breadth of knowledge in areas such as disease surveillance, disease etiology, health services utilization, and health outcomes. Furthermore, the quantity of data available for research has increased exponentially in recent times, particularly as e-health initiatives come online in health systems across the globe. We review some big data concepts, the databases and data linkage systems used in eye research—including their advantages and limitations, the types of studies previously undertaken, and the future direction for big data in eye research.",https://doi.org/10.1016/j.survophthal.2016.01.003,2016,Antony Clark and Jonathon Q. Ng and Nigel Morlet and James B. Semmens,BIG DATA AND OPHTHALMIC RESEARCH,article
116,14735,GOVERNMENT INFORMATION QUARTERLY,journal,0740624X,"2,121",Q1,103,76,205,5894,1946,193,"8,39","77,55",United Kingdom,Western Europe,Elsevier Ltd.,1984-2020,E-learning (Q1); Law (Q1); Library and Information Sciences (Q1); Sociology and Political Science (Q1),GOVERNMENT INFORMATION QUARTERLY,GOVERNMENT INFORMATION QUARTERLY,"5,379",7.279,0.00502,"Despite great potential, high hopes and big promises, the actual impact of big data on the public sector is not always as transformative as the literature would suggest. In this paper, we ascribe this predicament to an overly strong emphasis the current literature places on technical-rational factors at the expense of political decision-making factors. We express these two different emphases as two archetypical narratives and use those to illustrate that some political decision-making factors should be taken seriously by critiquing some of the core ‘techno-optimist’ tenets from a more ‘policy-pessimist’ angle. In the conclusion we have these two narratives meet ‘eye-to-eye’, facilitating a more systematized interrogation of big data promises and shortcomings in further research, paying appropriate attention to both technical-rational and political decision-making factors. We finish by offering a realist rejoinder of these two narratives, allowing for more context-specific scrutiny and balancing both technical-rational and political decision-making concerns, resulting in more realistic expectations about using big data for policymaking in practice.",https://doi.org/10.1016/j.giq.2019.05.010,2019,Simon Vydra and Bram Klievink,TECHNO-OPTIMISM AND POLICY-PESSIMISM IN THE PUBLIC SECTOR BIG DATA DEBATE,article
117,14735,GOVERNMENT INFORMATION QUARTERLY,journal,0740624X,"2,121",Q1,103,76,205,5894,1946,193,"8,39","77,55",United Kingdom,Western Europe,Elsevier Ltd.,1984-2020,E-learning (Q1); Law (Q1); Library and Information Sciences (Q1); Sociology and Political Science (Q1),GOVERNMENT INFORMATION QUARTERLY,GOVERNMENT INFORMATION QUARTERLY,"5,379",7.279,0.00502,"It is estimated that by 2050, 70% of the population will be urban (Nations Unies, 2014). This massive urbanization has created unprecedented challenges for cities and city managers which has led many of them to look for technological solutions to address them, including the use of Big Data, which is among the most considered technological support to help improve the overall operational and service delivery of cities. It is estimated that around 7 billion connected objects will soon be implemented in cities worldwide which will produce an unprecedented and massive amount of real-time data that will have to be managed, used, and analyzed effectively. If this massive amount of data is effectively managed and used, it can provide important benefits and produce real positive impacts on the functioning of cities. Nonetheless, despite these benefits, only a few cities are able to use and exploit big data, and some studies have shown that less than 0.5% of all the available data has been explored. The objective of this study is to understand the factors that influence cities to use big data and the nature of such use. Based on a field survey involving 106 municipalities, this study investigates the antecedents of big data use by cities and shows how different sets of antecedents influence three different types of big data use by cities.",https://doi.org/10.1016/j.giq.2021.101600,2021,Hamza Ali and Ryad Titah,IS BIG DATA USED BY CITIES? UNDERSTANDING THE NATURE AND ANTECEDENTS OF BIG DATA USE BY MUNICIPALITIES,article
118,14735,GOVERNMENT INFORMATION QUARTERLY,journal,0740624X,"2,121",Q1,103,76,205,5894,1946,193,"8,39","77,55",United Kingdom,Western Europe,Elsevier Ltd.,1984-2020,E-learning (Q1); Law (Q1); Library and Information Sciences (Q1); Sociology and Political Science (Q1),GOVERNMENT INFORMATION QUARTERLY,GOVERNMENT INFORMATION QUARTERLY,"5,379",7.279,0.00502,"Smart cities are expected to improve the efficiency and effectiveness of urban management, including public services, public security, and environmental protection, and to ultimately achieve Sustainable Development Goal (SDG) 11 for making cities inclusive, safe, resilient, and sustainable. Big data have been identified as a key enabler in the development of smart cities. However, our understanding of how different data sources should be managed and integrated remains limited. By analyzing data applications in the development of a sustainable smart city, this case study identified three phases of development, each requiring a different approach to orchestrating diverse data sources. A framework identifying the phases, data-related issues, data orchestration and its interaction with other resources, focal capabilities, and development approaches is developed. This study benefits both researchers and practitioners by making theoretical contributions and by offering practical insights in the fields of smart cities and big data.",https://doi.org/10.1016/j.giq.2021.101626,2022,Dan Zhang and L.G. Pee and Shan L. Pan and Lili Cui,"BIG DATA ANALYTICS, RESOURCE ORCHESTRATION, AND DIGITAL SUSTAINABILITY: A CASE STUDY OF SMART CITY DEVELOPMENT",article
119,14735,GOVERNMENT INFORMATION QUARTERLY,journal,0740624X,"2,121",Q1,103,76,205,5894,1946,193,"8,39","77,55",United Kingdom,Western Europe,Elsevier Ltd.,1984-2020,E-learning (Q1); Law (Q1); Library and Information Sciences (Q1); Sociology and Political Science (Q1),GOVERNMENT INFORMATION QUARTERLY,GOVERNMENT INFORMATION QUARTERLY,"5,379",7.279,0.00502,"The growing Artificial Intelligence (AI) age has been flooded with several innovations in algorithmic machine learning that may bring significant impacts to industries such as healthcare, agriculture, education, manufacturing, retail etc. But challenges such as data quality, privacy and lack of a skilled workforce limit the scope of AI implementation in emerging economies, particularly in the Public Manufacturing Sector (PMS). Therefore, to enhance the body of relevant literature, this study examines the existing challenges of AI implementation in PMS of India and explores the inter-relationships among them. The study has utilized the DEMATEL method for identification of the cause-and-effect group factors. The findings reveal that poor data quality, managers' lack of understanding of cognitive technologies, data privacy, problems in integrating cognitive projects and expensive technologies are the main challenges for AI implementation in PMS of India. Moreover, a model is proposed for industrial decision-makers and managers to take appropriate decisions to develop intelligent AI enabled systems for manufacturing organizations in emerging economies.",https://doi.org/10.1016/j.giq.2021.101624,2022,Manu Sharma and Sunil Luthra and Sudhanshu Joshi and Anil Kumar,IMPLEMENTING CHALLENGES OF ARTIFICIAL INTELLIGENCE: EVIDENCE FROM PUBLIC MANUFACTURING SECTOR OF AN EMERGING ECONOMY,article
120,14735,GOVERNMENT INFORMATION QUARTERLY,journal,0740624X,"2,121",Q1,103,76,205,5894,1946,193,"8,39","77,55",United Kingdom,Western Europe,Elsevier Ltd.,1984-2020,E-learning (Q1); Law (Q1); Library and Information Sciences (Q1); Sociology and Political Science (Q1),GOVERNMENT INFORMATION QUARTERLY,GOVERNMENT INFORMATION QUARTERLY,"5,379",7.279,0.00502,"The rise of Big, Open and Linked Data (BOLD) enables Big Data Algorithmic Systems (BDAS) which are often based on machine learning, neural networks and other forms of Artificial Intelligence (AI). As such systems are increasingly requested to make decisions that are consequential to individuals, communities and society at large, their failures cannot be tolerated, and they are subject to stringent regulatory and ethical requirements. However, they all rely on data which is not only big, open and linked but varied, dynamic and streamed at high speeds in real-time. Managing such data is challenging. To overcome such challenges and utilize opportunities for BDAS, organizations are increasingly developing advanced data governance capabilities. This paper reviews challenges and approaches to data governance for such systems, and proposes a framework for data governance for trustworthy BDAS. The framework promotes the stewardship of data, processes and algorithms, the controlled opening of data and algorithms to enable external scrutiny, trusted information sharing within and between organizations, risk-based governance, system-level controls, and data control through shared ownership and self-sovereign identities. The framework is based on 13 design principles and is proposed incrementally, for a single organization and multiple networked organizations.",https://doi.org/10.1016/j.giq.2020.101493,2020,Marijn Janssen and Paul Brous and Elsa Estevez and Luis S. Barbosa and Tomasz Janowski,DATA GOVERNANCE: ORGANIZING DATA FOR TRUSTWORTHY ARTIFICIAL INTELLIGENCE,article
121,14735,GOVERNMENT INFORMATION QUARTERLY,journal,0740624X,"2,121",Q1,103,76,205,5894,1946,193,"8,39","77,55",United Kingdom,Western Europe,Elsevier Ltd.,1984-2020,E-learning (Q1); Law (Q1); Library and Information Sciences (Q1); Sociology and Political Science (Q1),GOVERNMENT INFORMATION QUARTERLY,GOVERNMENT INFORMATION QUARTERLY,"5,379",7.279,0.00502,"This paper aims to contribute to a better understanding of the literature on open data in three ways. The first is to develop a descriptive analysis of journals and authors to identify the knowledge areas in which open data are applied. The second is to analyse the conceptual structure of the field using a bibliometric technique. The co-word analysis enabled us to create a map of the main themes that have been studied, identifying their importance and relevance. These themes were analysed and grouped. The third is to propose future research trends. According to our results, the main knowledge areas are Engineering, Health, Public Administration, Management and Education. The main themes are big data, open-linked data and data reuse. Finally, several research questions are proposed according to knowledge area and theme.",https://doi.org/10.1016/j.giq.2018.10.008,2019,Diego Corrales-Garay and Marta Ortiz-de-Urbina-Criado and Eva-María Mora-Valentín,"KNOWLEDGE AREAS, THEMES AND FUTURE RESEARCH ON OPEN DATA: A CO-WORD ANALYSIS",article
122,14735,GOVERNMENT INFORMATION QUARTERLY,journal,0740624X,"2,121",Q1,103,76,205,5894,1946,193,"8,39","77,55",United Kingdom,Western Europe,Elsevier Ltd.,1984-2020,E-learning (Q1); Law (Q1); Library and Information Sciences (Q1); Sociology and Political Science (Q1),GOVERNMENT INFORMATION QUARTERLY,GOVERNMENT INFORMATION QUARTERLY,"5,379",7.279,0.00502,"Dashboards visualize a consolidated set data for a certain purpose which enables users to see what is happening and to initiate actions. Dashboards can be used by governments to support their decision-making and policy processes or to communicate and interact with the public. The objective of this paper is to understand and to support the design of dashboards for creating transparency and accountability. Two smart city cases are investigated showing that dashboards can improve transparency and accountability, however, realizing these benefits was cumbersome and encountered various risks and challenges. Challenges include insufficient data quality, lack of understanding of data, poor analysis, wrong interpretation, confusion about the outcomes, and imposing a pre-defined view. These challenges can easily result in misconceptions, wrong decision-making, creating a blurred picture resulting in less transparency and accountability, and ultimately in even less trust in the government. Principles guiding the design of dashboards are presented. Dashboards need to be complemented by mechanisms supporting citizens' engagement, data interpretation, governance and institutional arrangements.",https://doi.org/10.1016/j.giq.2018.01.006,2020,Ricardo Matheus and Marijn Janssen and Devender Maheshwari,DATA SCIENCE EMPOWERING THE PUBLIC: DATA-DRIVEN DASHBOARDS FOR TRANSPARENT AND ACCOUNTABLE DECISION-MAKING IN SMART CITIES,article
123,14735,GOVERNMENT INFORMATION QUARTERLY,journal,0740624X,"2,121",Q1,103,76,205,5894,1946,193,"8,39","77,55",United Kingdom,Western Europe,Elsevier Ltd.,1984-2020,E-learning (Q1); Law (Q1); Library and Information Sciences (Q1); Sociology and Political Science (Q1),GOVERNMENT INFORMATION QUARTERLY,GOVERNMENT INFORMATION QUARTERLY,"5,379",7.279,0.00502,"A theoretical framework for big data analytics-enabled customer agility and responsiveness was developed from extant IS research. In on-demand service environments, customer agility involves dynamic capabilities in sensing and responding to citizens. Using this framework, a case study examined a large city government's 311 on-demand services which had leveraged big data analytics. While we found the localized big data analytics use by some of the 22 departments for enhanced customer agility and on-demand 311 services, city-wide systemic change in on-demand service delivery through big data analytics use was not evident. From the case study we identified key institutional mechanisms for linking customer agility to public value creation through 311 services. We posit how systemic use of big data analytics embedded into critical processes enables the government to co-create public values with citizens through 311 on-demand services, indicating the importance of creating a culture of analytics driven by strong political leadership.",https://doi.org/10.1016/j.giq.2017.11.002,2018,Akemi Takeoka Chatfield and Christopher G. Reddick,CUSTOMER AGILITY AND RESPONSIVENESS THROUGH BIG DATA ANALYTICS FOR PUBLIC VALUE CREATION: A CASE STUDY OF HOUSTON 311 ON-DEMAND SERVICES,article
124,14735,GOVERNMENT INFORMATION QUARTERLY,journal,0740624X,"2,121",Q1,103,76,205,5894,1946,193,"8,39","77,55",United Kingdom,Western Europe,Elsevier Ltd.,1984-2020,E-learning (Q1); Law (Q1); Library and Information Sciences (Q1); Sociology and Political Science (Q1),GOVERNMENT INFORMATION QUARTERLY,GOVERNMENT INFORMATION QUARTERLY,"5,379",7.279,0.00502,"Big data promises to transform public decision-making for the better by making it more responsive to actual needs and policy effects. However, much recent work on big data in public decision-making assumes a rational view of decision-making, which has been much criticized in the public administration debate. In this paper, we apply this view, and a more political one, to the context of big data and offer a qualitative study. We question the impact of big data on decision-making, realizing that big data – including its new methods and functions – must inevitably encounter existing political and managerial institutions. By studying two illustrative cases of big data use processes, we explore how these two worlds meet. Specifically, we look at the interaction between data analysts and decision makers. In this we distinguish between a rational view and a political view, and between an information logic and a decision logic. We find that big data provides ample opportunities for both analysts and decision makers to do a better job, but this doesn't necessarily imply better decision-making, because big data also provides opportunities for actors to pursue their own interests. Big data enables both data analysts and decision makers to act as autonomous agents rather than as links in a functional chain. Therefore, big data's impact cannot be interpreted only in terms of its functional promise; it must also be acknowledged as a phenomenon set to impact our policymaking institutions, including their legitimacy.",https://doi.org/10.1016/j.giq.2018.10.011,2019,H.G. {van der Voort} and A.J. Klievink and M. Arnaboldi and A.J. Meijer,RATIONALITY AND POLITICS OF ALGORITHMS. WILL THE PROMISE OF BIG DATA SURVIVE THE DYNAMICS OF PUBLIC DECISION MAKING?,article
125,14735,GOVERNMENT INFORMATION QUARTERLY,journal,0740624X,"2,121",Q1,103,76,205,5894,1946,193,"8,39","77,55",United Kingdom,Western Europe,Elsevier Ltd.,1984-2020,E-learning (Q1); Law (Q1); Library and Information Sciences (Q1); Sociology and Political Science (Q1),GOVERNMENT INFORMATION QUARTERLY,GOVERNMENT INFORMATION QUARTERLY,"5,379",7.279,0.00502,"Automated decision-making (ADM) systems may affect multiple aspects of our lives. In particular, they can result in systematic discrimination of specific population groups, in violation of the EU Charter of Fundamental Rights. One of the potential causes of discriminative behavior, i.e., unfairness, lies in the quality of the data used to train such ADM systems. Using a data quality measurement approach combined with risk management, both defined in ISO standards, we focus on balance characteristics and we aim to understand how balance indexes (Gini, Simpson, Shannon, Imbalance Ratio) identify discrimination risk in six large datasets containing the classification output of ADM systems. The best result is achieved using the Imbalance Ratio index. Gini and Shannon indexes tend to assume high values and for this reason they have modest results in both aspects: further experimentation with different thresholds is needed. In terms of policies, the risk-based approach is a core element of the EU approach to regulate algorithmic systems: in this context, balance measures can be easily assumed as risk indicators of propagation – or even amplification – of bias in the input data of ADM systems.",https://doi.org/10.1016/j.giq.2021.101619,2021,Antonio Vetrò and Marco Torchiano and Mariachiara Mecati,A DATA QUALITY APPROACH TO THE IDENTIFICATION OF DISCRIMINATION RISK IN AUTOMATED DECISION MAKING SYSTEMS,article
126,19419,COMPUTERS IN HUMAN BEHAVIOR,journal,07475632,"2,108",Q1,178,385,1597,26867,14501,1573,"7,83","69,78",United Kingdom,Western Europe,Elsevier Ltd.,1985-2021,Arts and Humanities (miscellaneous) (Q1); Human-Computer Interaction (Q1); Psychology (miscellaneous) (Q1),COMPUTERS IN HUMAN BEHAVIOR,COMPUTERS IN HUMAN BEHAVIOR,"45,035",6.829,0.05973,"Despite the wide usage of big data in tourism and the hospitality sector, little research has been done to understand the role of organizations’ capability of managing big data in value creation. This study bridges this gap by investigating how big data management capabilities lead to service innovation and high online quality ratings. Instead of treating big data management as a whole, we access big data management capabilities at the strategic and operational level. Using a sample of 202 hotels in Pakistan, we collected the primary data for big data capabilities, knowledge creation and service innovation; the secondary data about quality rating were collected from Booking.com. Structural equation modelling through SmartPLS was used for data analysis. The results indicated that big data management capabilities lead to high online quality ratings through the mediation of knowledge creation and service innovation. We contribute to the current literature by empirically testing how strategic level big data capabilities enable the firm to add value in innovativeness and positive online quality ratings through acquiring, contextualizing, experimenting and applying big data.",https://doi.org/10.1016/j.chb.2021.106777,2021,Saqib Shamim and Yumei Yang and Najam Ul Zia and Mahmood Hussain Shah,BIG DATA MANAGEMENT CAPABILITIES IN THE HOSPITALITY SECTOR: SERVICE INNOVATION AND CUSTOMER GENERATED ONLINE QUALITY RATINGS,article
127,19419,COMPUTERS IN HUMAN BEHAVIOR,journal,07475632,"2,108",Q1,178,385,1597,26867,14501,1573,"7,83","69,78",United Kingdom,Western Europe,Elsevier Ltd.,1985-2021,Arts and Humanities (miscellaneous) (Q1); Human-Computer Interaction (Q1); Psychology (miscellaneous) (Q1),COMPUTERS IN HUMAN BEHAVIOR,COMPUTERS IN HUMAN BEHAVIOR,"45,035",6.829,0.05973,"Big data analytics has recently emerged as an important research area due to the popularity of the Internet and the advent of the Web 2.0 technologies. Moreover, the proliferation and adoption of social media applications have provided extensive opportunities and challenges for researchers and practitioners. The massive amount of data generated by users using social media platforms is the result of the integration of their background details and daily activities. This enormous volume of generated data known as “big data” has been intensively researched recently. A review of the recent works is presented to obtain a broad perspective of the social media big data analytics research topic. We classify the literature based on important aspects. This study also compares possible big data analytics techniques and their quality attributes. Moreover, we provide a discussion on the applications of social media big data analytics by highlighting the state-of-the-art techniques, methods, and the quality attributes of various studies. Open research challenges in big data analytics are described as well.",https://doi.org/10.1016/j.chb.2018.08.039,2019,Norjihan Abdul Ghani and Suraya Hamid and Ibrahim Abaker {Targio Hashem} and Ejaz Ahmed,SOCIAL MEDIA BIG DATA ANALYTICS: A SURVEY,article
128,19419,COMPUTERS IN HUMAN BEHAVIOR,journal,07475632,"2,108",Q1,178,385,1597,26867,14501,1573,"7,83","69,78",United Kingdom,Western Europe,Elsevier Ltd.,1985-2021,Arts and Humanities (miscellaneous) (Q1); Human-Computer Interaction (Q1); Psychology (miscellaneous) (Q1),COMPUTERS IN HUMAN BEHAVIOR,COMPUTERS IN HUMAN BEHAVIOR,"45,035",6.829,0.05973,"This article provides an overview of extant literature addressing consumer interaction with cutting-edge technologies. Six focal cutting-edge technologies are identified: artificial intelligence, augmented reality, virtual reality, wearable technology, robotics and big data analytics. Our analysis shows research on consumer interaction with cutting-edge technologies is at a nascent stage, and there are several gaps requiring attention. To further advance knowledge, our article offers avenues for future interdisciplinary research addressing implications of consumer interaction with cutting-edge technologies. More specifically, we propose six main areas for future research namely: rethinking consumer behaviour models, identifying behavioural differences among different generations of consumers, understanding how consumers interact with automated services, ethics, privacy and the blackbox, consumer security concerns and consumer interaction with new-age technologies during and after a major global crisis such as the COVID-19 pandemic.",https://doi.org/10.1016/j.chb.2021.106761,2021,Nisreen Ameen and Sameer Hosany and Ali Tarhini,CONSUMER INTERACTION WITH CUTTING-EDGE TECHNOLOGIES: IMPLICATIONS FOR FUTURE RESEARCH,article
129,19419,COMPUTERS IN HUMAN BEHAVIOR,journal,07475632,"2,108",Q1,178,385,1597,26867,14501,1573,"7,83","69,78",United Kingdom,Western Europe,Elsevier Ltd.,1985-2021,Arts and Humanities (miscellaneous) (Q1); Human-Computer Interaction (Q1); Psychology (miscellaneous) (Q1),COMPUTERS IN HUMAN BEHAVIOR,COMPUTERS IN HUMAN BEHAVIOR,"45,035",6.829,0.05973,"The scale and complexity of big data quickly exceed the reach of direct human comprehension and increasingly require machine assistance to semantically analyze, organize, and interpret vast and diverse sources of big data in order to unlock its strategic value. Due to its volume, velocity, variety, and veracity, big data integration challenges overwhelm traditional integration approaches leaving many integration possibilities out of reach. Unlocking the value of big data requires innovative technology. Organizations must have the innovativeness and data capability to adopt the technology and harness its potential value. The Semantic Web (SW) technology has demonstrated its potential for integrating big data and has become important technology for tackling big data. Despite its importance to manage big data, little research has examined the determinants affecting SW adoption. Drawing upon the technology–organization–environment framework as a theory base, this study develops a research model explaining the factors affecting the adoption of SW technology from IT professionals' perspective, specifically in the context of corporate computing enterprises. We validate the proposed model using a set of empirical data collected from IT professionals including IT managers, system architects, software developers, and web developers. The findings suggest that perceived usefulness, perceived ease of use, organization's innovativeness, organization's data capability, and applicability to data management are important drivers of SW adoption. This study provides new insights on theories of organizational IT adoption from IT professionals' perspectives tailored to the context of SW technology.",https://doi.org/10.1016/j.chb.2018.04.014,2018,Dan J. Kim and John Hebeler and Victoria Yoon and Fred Davis,"EXPLORING DETERMINANTS OF SEMANTIC WEB TECHNOLOGY ADOPTION FROM IT PROFESSIONALS' PERSPECTIVE: INDUSTRY COMPETITION, ORGANIZATION INNOVATIVENESS, AND DATA MANAGEMENT CAPABILITY",article
130,27467,GYNECOLOGIC ONCOLOGY,journal,"00908258, 10956859","2,105",Q1,162,480,1150,15477,5359,1087,"4,41","32,24",United States,Northern America,Academic Press Inc.,1972-2020,Obstetrics and Gynecology (Q1); Oncology (Q1),GYNECOLOGIC ONCOLOGY,GYNECOLOGIC ONCOLOGY,"29,012",5.482,0.02767,"Oncology is undergoing a data-driven metamorphosis. Armed with new and ever more efficient molecular and information technologies, we have entered an era where data is helping us spearhead the fight against cancer. This technology driven data explosion, often referred to as “big data”, is not only expediting biomedical discovery, but it is also rapidly transforming the practice of oncology into an information science. This evolution is critical, as results to-date have revealed the immense complexity and genetic heterogeneity of patients and their tumors, a sobering reminder of the challenge facing every patient and their oncologist. This can only be addressed through development of clinico-molecular data analytics that provide a deeper understanding of the mechanisms controlling the biological and clinical response to available therapeutic options. Beyond the exciting implications for improved patient care, such advancements in predictive and evidence-based analytics stand to profoundly affect the processes of cancer drug discovery and associated clinical trials.",https://doi.org/10.1016/j.ygyno.2016.02.022,2016,Guillaume Taglang and David B. Jackson,USE OF “BIG DATA” IN DRUG DISCOVERY AND CLINICAL TRIALS,article
131,29403,ENERGY POLICY,journal,03014215,"2,093",Q1,217,679,2165,40582,14111,2135,"6,29","59,77",United Kingdom,Western Europe,Elsevier BV,1973-2020,"Energy (miscellaneous) (Q1); Management, Monitoring, Policy and Law (Q1)",ENERGY POLICY,ENERGY POLICY,"60,369",6.142,0.04319,"The upstream oil and gas industry has been contending with massive data sets and monolithic files for many years, but “Big Data” is a relatively new concept that has the potential to significantly re-shape the industry. Despite the impressive amount of value that is being realized by Big Data technologies in other parts of the marketplace, however, much of the data collected within the oil and gas sector tends to be discarded, ignored, or analyzed in a very cursory way. This viewpoint examines existing data management practices in the upstream oil and gas industry, and compares them to practices and philosophies that have emerged in organizations that are leading the way in Big Data. The comparison shows that, in companies that are widely considered to be leaders in Big Data analytics, data is regarded as a valuable asset—but this is usually not true within the oil and gas industry insofar as data is frequently regarded there as descriptive information about a physical asset rather than something that is valuable in and of itself. The paper then discusses how the industry could potentially extract more value from data, and concludes with a series of policy-related questions to this end.",https://doi.org/10.1016/j.enpol.2015.02.020,2015,Robert K. Perrons and Jesse W. Jensen,DATA AS AN ASSET: WHAT THE OIL AND GAS SECTOR CAN LEARN FROM OTHER INDUSTRIES ABOUT “BIG DATA”,article
132,20550,JOURNAL OF BUSINESS RESEARCH,journal,01482963,"2,049",Q1,195,878,1342,73221,11507,1315,"7,38","83,40",United States,Northern America,Elsevier Inc.,1973-2021,Marketing (Q1),JOURNAL OF BUSINESS RESEARCH,JOURNAL OF BUSINESS RESEARCH,"46,935",7.550,0.03523,"This paper examines the challenges of leveraging big data in the humanitarian sector in support of UN Sustainable Development Goal 17 “Partnerships for the Goals”. The full promise of Big Data is underpinned by a tacit assumption that the heterogeneous ‘exhaust trail’ of data is contextually relevant and sufficiently granular to be mined for value. This promise, however, relies on relationality – that patterns can be derived from combining different pieces of data that are of corresponding detail or that there are effective mechanisms to resolve differences in detail. Here, we present empirical work integrating eight heterogeneous datasets from the humanitarian domain to provide evidence of the inherent challenge of complexity resulting from differing levels of data granularity. In clarifying this challenge, we explore the reasons why it is manifest, discuss strategies for addressing it and, as our principal contribution, identify five propositions to guide future research.",https://doi.org/10.1016/j.jbusres.2020.09.035,2021,David Bell and Mark Lycett and Alaa Marshan and Asmat Monaghan,EXPLORING FUTURE CHALLENGES FOR BIG DATA IN THE HUMANITARIAN DOMAIN,article
133,20550,JOURNAL OF BUSINESS RESEARCH,journal,01482963,"2,049",Q1,195,878,1342,73221,11507,1315,"7,38","83,40",United States,Northern America,Elsevier Inc.,1973-2021,Marketing (Q1),JOURNAL OF BUSINESS RESEARCH,JOURNAL OF BUSINESS RESEARCH,"46,935",7.550,0.03523,"Prior research articulated the importance of developing a big data analytics capability but did not show how to cultivate this development. Drawing on the literature on this topic, this study develops the concept of Big Data capability, which enhances our understanding of Big Data practice beyond that captured in previous literature on the concept of big data analytics capability. This study further highlights the strategic implications of the concept by testing its relationship to three strategic orientations and one aspect of organizational culture. Findings show that customer, entrepreneurial, and technology orientations, and developmental culture are important contributors to the development of Big Data capability.",https://doi.org/10.1016/j.jbusres.2019.07.016,2019,Canchu Lin and Anand Kunnathur,"STRATEGIC ORIENTATIONS, DEVELOPMENTAL CULTURE, AND BIG DATA CAPABILITY",article
134,20550,JOURNAL OF BUSINESS RESEARCH,journal,01482963,"2,049",Q1,195,878,1342,73221,11507,1315,"7,38","83,40",United States,Northern America,Elsevier Inc.,1973-2021,Marketing (Q1),JOURNAL OF BUSINESS RESEARCH,JOURNAL OF BUSINESS RESEARCH,"46,935",7.550,0.03523,"The lack of sufficient big data-based approaches impedes the development of human resource management (HRM) research and practices. Although scholars have realized the importance of applying a big data approach to HRM research, clear guidance is lacking regarding how to integrate the two. Using a clustering algorithm based on the big data research paradigm, we first conduct a bibliometric review to quantitatively assess and scientifically map the evolution of the current big data HRM literature. Based on this systematic review, we propose a general theoretical framework described as “Inductive (Prediction paradigm: Data mining/Theory building) vs. Deductive (Explanation paradigm: Theory testing)”. In this framework, we discuss potential research questions, their corresponding levels of analysis, relevant methods, data sources and software. We then summarize the general procedures for conducting big data research within HRM research. Finally, we propose a future agenda for applying big data approaches to HRM research and identify five promising HRM research topics at the micro, meso and macro levels along with three challenges and limitations that HRM scholars may face in the era of big data.",https://doi.org/10.1016/j.jbusres.2021.04.019,2021,Yucheng Zhang and Shan Xu and Long Zhang and Mengxi Yang,BIG DATA AND HUMAN RESOURCE MANAGEMENT RESEARCH: AN INTEGRATIVE REVIEW AND NEW DIRECTIONS FOR FUTURE RESEARCH,article
135,20550,JOURNAL OF BUSINESS RESEARCH,journal,01482963,"2,049",Q1,195,878,1342,73221,11507,1315,"7,38","83,40",United States,Northern America,Elsevier Inc.,1973-2021,Marketing (Q1),JOURNAL OF BUSINESS RESEARCH,JOURNAL OF BUSINESS RESEARCH,"46,935",7.550,0.03523,"Big data analytics (BDA) has recently gained importance as an emerging technology for handling big data. The use of advanced techniques with differing levels of intelligence, such as descriptive, predictive, prescriptive, and autonomous analytics, is expected to create value for firms. By viewing BDA as a sociotechnical system, we conduct a meta-analysis of 107 individual studies to integrate prior evidence on the role of the technical and social factors of BDA in creating BDA business value. The findings underline the predominant role of the social components in enhancing firm performance, such as the BDA system’s human factors and a nurturing organizational structure, in contrast to the minor role of the technological factors. However, both the technical and social factors are found to be strong determinants of BDA business value. Through the combined lens of sociotechnical theory and the IS business value framework, we contribute to research and practice by enhancing the understanding of the main technical and social determinants of BDA business value at the firm level.",https://doi.org/10.1016/j.jbusres.2022.08.028,2022,Thuy Duong Oesterreich and Eduard Anton and Frank Teuteberg and Yogesh K Dwivedi,THE ROLE OF THE SOCIAL AND TECHNICAL FACTORS IN CREATING BUSINESS VALUE FROM BIG DATA ANALYTICS: A META-ANALYSIS,article
136,20550,JOURNAL OF BUSINESS RESEARCH,journal,01482963,"2,049",Q1,195,878,1342,73221,11507,1315,"7,38","83,40",United States,Northern America,Elsevier Inc.,1973-2021,Marketing (Q1),JOURNAL OF BUSINESS RESEARCH,JOURNAL OF BUSINESS RESEARCH,"46,935",7.550,0.03523,"The use of social media for innovation requires firms to manage rapid information transfers, big data, and multiway communication. Yet managers lack clear insights on the way social media should be managed and current literature is dispersed across various research streams. In this article, the authors aim to develop a better understanding of how social media use should be leveraged for innovation. To achieve this objective, they build a systematic review of evidence from 177 scientific articles across four key management disciplines. They analyze research perspectives and conceptualizations of social media use for innovation and provide a framework of the drivers, contingencies and outcomes related to this topic. Next, they attempt to identify what is currently known about social media use for innovation. Last, they suggest critical areas for future inquiry on this important subject.",https://doi.org/10.1016/j.jbusres.2022.01.039,2022,Marie-Isabelle Muninger and Dominik Mahr and Wafa Hammedi,SOCIAL MEDIA USE: A REVIEW OF INNOVATION MANAGEMENT PRACTICES,article
137,20550,JOURNAL OF BUSINESS RESEARCH,journal,01482963,"2,049",Q1,195,878,1342,73221,11507,1315,"7,38","83,40",United States,Northern America,Elsevier Inc.,1973-2021,Marketing (Q1),JOURNAL OF BUSINESS RESEARCH,JOURNAL OF BUSINESS RESEARCH,"46,935",7.550,0.03523,"The emerging Big Data integration imposes diverse challenges, compromising the sustainable business research practice. Heterogeneity, multi-dimensionality, velocity, and massive volumes that challenge Big Data paradigm may preclude the effective data and system integration processes. Business alignments get affected within and across joint ventures as enterprises attempt to adapt to changes in industrial environments rapidly. In the context of the Oil and Gas industry, we design integrated artefacts for a resilient multidimensional warehouse repository. With access to several decades of resource data in upstream companies, we incorporate knowledge-based data models with spatial-temporal dimensions in data schemas to minimize ambiguity in warehouse repository implementation. The design considerations ensure uniqueness and monotonic properties of dimensions, maintaining the connectivity between artefacts and achieving the business alignments. The multidimensional attributes envisage Big Data analysts a scope of business research with valuable new knowledge for decision support systems and adding further business values in geographic scales.",https://doi.org/10.1016/j.jbusres.2018.04.029,2018,Shastri L. Nimmagadda and Torsten Reiners and Lincoln C. Wood,ON BIG DATA-GUIDED UPSTREAM BUSINESS RESEARCH AND ITS KNOWLEDGE MANAGEMENT,article
138,20550,JOURNAL OF BUSINESS RESEARCH,journal,01482963,"2,049",Q1,195,878,1342,73221,11507,1315,"7,38","83,40",United States,Northern America,Elsevier Inc.,1973-2021,Marketing (Q1),JOURNAL OF BUSINESS RESEARCH,JOURNAL OF BUSINESS RESEARCH,"46,935",7.550,0.03523,"Although big data analytics have tremendous benefits for healthcare organizations, extant research has paid insufficient attention to the exploration of its business value. In order to bridge this knowledge gap, this study proposes a big data analytics-enabled business value model in which we use the resource-based theory (RBT) and capability building view to explain how big data analytics capabilities can be developed and what potential benefits can be obtained by these capabilities in the health care industries. Using this model, we investigate 109 case descriptions, covering 63 healthcare organizations to explore the causal relationships between the big data analytics capabilities and business value and the path-to-value chains for big data analytics success. Our findings provide new insights to healthcare practitioners on how to constitute big data analytics capabilities for business transformation and offer an empirical basis that can stimulate a more detailed investigation of big data analytics implementation.",https://doi.org/10.1016/j.jbusres.2016.08.002,2017,Yichuan Wang and Nick Hajli,EXPLORING THE PATH TO BIG DATA ANALYTICS SUCCESS IN HEALTHCARE,article
139,20550,JOURNAL OF BUSINESS RESEARCH,journal,01482963,"2,049",Q1,195,878,1342,73221,11507,1315,"7,38","83,40",United States,Northern America,Elsevier Inc.,1973-2021,Marketing (Q1),JOURNAL OF BUSINESS RESEARCH,JOURNAL OF BUSINESS RESEARCH,"46,935",7.550,0.03523,"Consumer analytics is at the epicenter of a Big Data revolution. Technology helps capture rich and plentiful data on consumer phenomena in real time. Thus, unprecedented volume, velocity, and variety of primary data, Big Data, are available from individual consumers. To better understand the impact of Big Data on various marketing activities, enabling firms to better exploit its benefits, a conceptual framework that builds on resource-based theory is proposed. Three resources—physical, human, and organizational capital—moderate the following: (1) the process of collecting and storing evidence of consumer activity as Big Data, (2) the process of extracting consumer insight from Big Data, and (3) the process of utilizing consumer insight to enhance dynamic/adaptive capabilities. Furthermore, unique resource requirements for firms to benefit from Big Data are discussed.",https://doi.org/10.1016/j.jbusres.2015.07.001,2016,Sunil Erevelles and Nobuyuki Fukawa and Linda Swayne,BIG DATA CONSUMER ANALYTICS AND THE TRANSFORMATION OF MARKETING,article
140,20550,JOURNAL OF BUSINESS RESEARCH,journal,01482963,"2,049",Q1,195,878,1342,73221,11507,1315,"7,38","83,40",United States,Northern America,Elsevier Inc.,1973-2021,Marketing (Q1),JOURNAL OF BUSINESS RESEARCH,JOURNAL OF BUSINESS RESEARCH,"46,935",7.550,0.03523,"Grounded in gestalt insight learning theory and organizational learning theory, we collected data from 280 middle and top-level managers to investigate the impact of each big data characteristic (i.e., data volume, data velocity, data variety, and data veracity) on firm innovation competency (i.e., exploitation competency and exploration competency), mediated through data-driven insight generation (i.e., descriptive insight, predictive insight, and prescriptive insight). Findings show that while data velocity, variety, and veracity enhance data-driven insight generation, data volume does not impact it. Additionally, results of the post hoc analysis indicate that while descriptive and predictive insights improve innovation competency, prescriptive insight does not affect it. These results provide interesting and unique theoretical and practical insights.",https://doi.org/10.1016/j.jbusres.2019.07.006,2019,Maryam Ghasemaghaei and Goran Calic,DOES BIG DATA ENHANCE FIRM INNOVATION COMPETENCY? THE MEDIATING ROLE OF DATA-DRIVEN INSIGHTS,article
141,20550,JOURNAL OF BUSINESS RESEARCH,journal,01482963,"2,049",Q1,195,878,1342,73221,11507,1315,"7,38","83,40",United States,Northern America,Elsevier Inc.,1973-2021,Marketing (Q1),JOURNAL OF BUSINESS RESEARCH,JOURNAL OF BUSINESS RESEARCH,"46,935",7.550,0.03523,"Organizations are looking for ways to harness the power of big data (BD) to improve their decision making. Despite its significance the effects of BD on decision-making quality has been given scant attention in the literature. In this paper factors influencing decision-making based on BD are identified using a case study. BD is collected from different sources that have various data qualities and are processed by various organizational entities resulting in the creation of a big data chain. The veracity (manipulation, noise), variety (heterogeneity of data) and velocity (constantly changing data sources) amplified by the size of big data calls for relational and contractual governance mechanisms to ensure BD quality and being able to contextualize data. The case study reveals that taking advantage of big data is an evolutionary process in which the gradually understanding of the potential of big data and the routinization of processes plays a crucial role.",https://doi.org/10.1016/j.jbusres.2016.08.007,2017,Marijn Janssen and Haiko {van der Voort} and Agung Wahyudi,FACTORS INFLUENCING BIG DATA DECISION-MAKING QUALITY,article
142,20550,JOURNAL OF BUSINESS RESEARCH,journal,01482963,"2,049",Q1,195,878,1342,73221,11507,1315,"7,38","83,40",United States,Northern America,Elsevier Inc.,1973-2021,Marketing (Q1),JOURNAL OF BUSINESS RESEARCH,JOURNAL OF BUSINESS RESEARCH,"46,935",7.550,0.03523,"Although big data analytics (BDA) is considered the next “frontier” in data science by creating potential business opportunities, the way to extract those opportunities is unclear. This paper aims to understand the antecedents of BDA value at a firm level. The authors performed a study using a mixed methodology approach. First, by carrying out a Delphi study to explore and rank the antecedents affecting the creation of BDA value. Based on the Delphi results, we propose an empirically validated model supported by a survey conducted on 175 European firms to explain the antecedents of BDA sustained value. The results show that the proposed model explains 62% of BDA sustained value at the firm level, where the most critical contributor is BDA use. We provide directions for managers to support their decisions on BDA strategy definition and refinement. For academics, we extend BDA value literature and outline some potential research opportunities.",https://doi.org/10.1016/j.jbusres.2018.12.072,2019,Nadine Côrte-Real and Pedro Ruivo and Tiago Oliveira and Aleš Popovič,UNLOCKING THE DRIVERS OF BIG DATA ANALYTICS VALUE IN FIRMS,article
143,20550,JOURNAL OF BUSINESS RESEARCH,journal,01482963,"2,049",Q1,195,878,1342,73221,11507,1315,"7,38","83,40",United States,Northern America,Elsevier Inc.,1973-2021,Marketing (Q1),JOURNAL OF BUSINESS RESEARCH,JOURNAL OF BUSINESS RESEARCH,"46,935",7.550,0.03523,"Customer experience (CX) has emerged as a sustainable source of competitive differentiation. Recent developments in big data analytics (BDA) have exposed possibilities to unlock customer insights for customer experience management (CXM). Research at the intersection of these two fields is scarce and there is a need for conceptual work that (1) provides an overview of opportunities to use BDA for CXM and (2) guides management practice and future research. The purpose of this paper is therefore to develop a strategic framework for CXM based on CX insights resulting from BDA. Our conceptualisation is comprehensive and is particularly relevant for researchers and practitioners who are less familiar with the potential of BDA for CXM. For managers, we provide a step-by-step guide on how to kick-start or implement our strategic framework. For researchers, we propose some opportunities for future studies in this promising research area.",https://doi.org/10.1016/j.jbusres.2020.01.022,2020,Maria Holmlund and Yves {Van Vaerenbergh} and Robert Ciuchita and Annika Ravald and Panagiotis Sarantopoulos and Francisco Villarroel Ordenes and Mohamed Zaki,CUSTOMER EXPERIENCE MANAGEMENT IN THE AGE OF BIG DATA ANALYTICS: A STRATEGIC FRAMEWORK,article
144,20550,JOURNAL OF BUSINESS RESEARCH,journal,01482963,"2,049",Q1,195,878,1342,73221,11507,1315,"7,38","83,40",United States,Northern America,Elsevier Inc.,1973-2021,Marketing (Q1),JOURNAL OF BUSINESS RESEARCH,JOURNAL OF BUSINESS RESEARCH,"46,935",7.550,0.03523,"Big data analytics has been widely regarded as a breakthrough technological development in academic and business communities. Despite the growing number of firms that are launching big data initiatives, there is still limited understanding on how firms translate the potential of such technologies into business value. The literature argues that to leverage big data analytics and realize performance gains, firms must develop strong big data analytics capabilities. Nevertheless, most studies operate under the assumption that there is limited heterogeneity in the way firms build their big data analytics capabilities and that related resources are of similar importance regardless of context. This paper draws on complexity theory and investigates the configurations of resources and contextual factors that lead to performance gains from big data analytics investments. Our empirical investigation followed a mixed methods approach using survey data from 175 chief information officers and IT managers working in Greek firms, and three case studies to show that depending on the context, big data analytics resources differ in significance when considering performance gains. Applying a fuzzy-set qualitative comparative analysis (fsQCA) method on the quantitative data, we show that there are four different patterns of elements surrounding big data analytics that lead to high performance. Outcomes of the three case studies highlight the inter-relationships between these elements and outline challenges that organizations face when orchestrating big data analytics resources.",https://doi.org/10.1016/j.jbusres.2019.01.044,2019,Patrick Mikalef and Maria Boura and George Lekakos and John Krogstie,BIG DATA ANALYTICS AND FIRM PERFORMANCE: FINDINGS FROM A MIXED-METHOD APPROACH,article
145,20550,JOURNAL OF BUSINESS RESEARCH,journal,01482963,"2,049",Q1,195,878,1342,73221,11507,1315,"7,38","83,40",United States,Northern America,Elsevier Inc.,1973-2021,Marketing (Q1),JOURNAL OF BUSINESS RESEARCH,JOURNAL OF BUSINESS RESEARCH,"46,935",7.550,0.03523,"The advent and development of digital technologies have brought about a proliferation of online consumer reviews (OCRs), i.e., real-time customers’ evaluations of products, services, and brands. Increasingly, e-commerce platforms are using them to gain insights from customer feedback. Meanwhile, a new generation of big data analytics (BDA) companies are crowdsourcing large volumes of OCRs by means of controlled ad hoc online experiments and advanced machine learning (ML) techniques to forecast demand and determine the market potential for new products in several industries. We illustrate how this process is taking place for consumer goods companies by exploring the case of UK digital BDA company, SoundOut. Based on an in-depth qualitative analysis, we develop the consumer goods company innovation (CGCI) conceptual framework, which illustrates how digital BDA firms help consumer goods companies to test new products before they are launched on the market, and innovate. Theoretical and managerial implications are discussed.",https://doi.org/10.1016/j.jbusres.2020.09.012,2020,Marcello M. Mariani and Samuel {Fosso Wamba},EXPLORING HOW CONSUMER GOODS COMPANIES INNOVATE IN THE DIGITAL AGE: THE ROLE OF BIG DATA ANALYTICS COMPANIES,article
146,20550,JOURNAL OF BUSINESS RESEARCH,journal,01482963,"2,049",Q1,195,878,1342,73221,11507,1315,"7,38","83,40",United States,Northern America,Elsevier Inc.,1973-2021,Marketing (Q1),JOURNAL OF BUSINESS RESEARCH,JOURNAL OF BUSINESS RESEARCH,"46,935",7.550,0.03523,"Big Data (BD), with their potential to ascertain valued insights for enhanced decision-making process, have recently attracted substantial interest from both academics and practitioners. Big Data Analytics (BDA) is increasingly becoming a trending practice that many organizations are adopting with the purpose of constructing valuable information from BD. The analytics process, including the deployment and use of BDA tools, is seen by organizations as a tool to improve operational efficiency though it has strategic potential, drive new revenue streams and gain competitive advantages over business rivals. However, there are different types of analytic applications to consider. Therefore, prior to hasty use and buying costly BD tools, there is a need for organizations to first understand the BDA landscape. Given the significant nature of the BD and BDA, this paper presents a state-of-the-art review that presents a holistic view of the BD challenges and BDA methods theorized/proposed/employed by organizations to help others understand this landscape with the objective of making robust investment decisions. In doing so, systematically analysing and synthesizing the extant research published on BD and BDA area. More specifically, the authors seek to answer the following two principal questions: Q1 – What are the different types of BD challenges theorized/proposed/confronted by organizations? and Q2 – What are the different types of BDA methods theorized/proposed/employed to overcome BD challenges?. This systematic literature review (SLR) is carried out through observing and understanding the past trends and extant patterns/themes in the BDA research area, evaluating contributions, summarizing knowledge, thereby identifying limitations, implications and potential further research avenues to support the academic community in exploring research themes/patterns. Thus, to trace the implementation of BD strategies, a profiling method is employed to analyze articles (published in English-speaking peer-reviewed journals between 1996 and 2015) extracted from the Scopus database. The analysis presented in this paper has identified relevant BD research studies that have contributed both conceptually and empirically to the expansion and accrual of intellectual wealth to the BDA in technology and organizational resource management discipline.",https://doi.org/10.1016/j.jbusres.2016.08.001,2017,Uthayasankar Sivarajah and Muhammad Mustafa Kamal and Zahir Irani and Vishanth Weerakkody,CRITICAL ANALYSIS OF BIG DATA CHALLENGES AND ANALYTICAL METHODS,article
147,20550,JOURNAL OF BUSINESS RESEARCH,journal,01482963,"2,049",Q1,195,878,1342,73221,11507,1315,"7,38","83,40",United States,Northern America,Elsevier Inc.,1973-2021,Marketing (Q1),JOURNAL OF BUSINESS RESEARCH,JOURNAL OF BUSINESS RESEARCH,"46,935",7.550,0.03523,"Machine learning (ML) is expected to transform the business landscape in the near future completely. Hitherto, some successful ML case-stories have emerged. However, how organizations can derive business value (BV) from ML has not yet been substantiated. We assemble a conceptual model, grounded on the dynamic capabilities theory, to uncover key drivers of ML BV, in terms of financial and strategic performance. The proposed model was assessed by surveying 319 corporations. Our findings are that ML use, big data analytics maturity, platform maturity, top management support, and process complexity are, to some extent, drivers of ML BV. We also find that platform maturity has, to some degree, a moderator influence between ML use and ML BV, and between big data analytics maturity and ML BV. To the best of our knowledge, this is the first research to deliver such findings in the ML field.",https://doi.org/10.1016/j.jbusres.2020.05.053,2020,Carolina Reis and Pedro Ruivo and Tiago Oliveira and Paulo Faroleiro,ASSESSING THE DRIVERS OF MACHINE LEARNING BUSINESS VALUE,article
148,20550,JOURNAL OF BUSINESS RESEARCH,journal,01482963,"2,049",Q1,195,878,1342,73221,11507,1315,"7,38","83,40",United States,Northern America,Elsevier Inc.,1973-2021,Marketing (Q1),JOURNAL OF BUSINESS RESEARCH,JOURNAL OF BUSINESS RESEARCH,"46,935",7.550,0.03523,"Data analytics has gained importance in human resource management (HRM) for its ability to provide insights based on data-driven decision-making processes. However, integrating an analytics-based approach in HRM is a complex process, and hence, many organizations are unable to adopt HR Analytics (HRA). Using a framework synthesis approach, we first identify the challenges that hinder the practice of HRA and then develop a framework to explain the different factors that impact the adoption of HRA within organizations. This study identifies the key aspects related to the technological, organizational, environmental, data governance, and individual factors that influence the adoption of HRA. In addition, this paper determines 23 sub-dimensions of these five factors as the crucial aspects for successfully implementing and practicing HRA within organizations. We also discuss the implications of the framework for HR leaders, HR Managers, CEOs, IT Managers and consulting practitioners for effective adoption of HRA in organization.",https://doi.org/10.1016/j.jbusres.2021.03.054,2021,Sateesh.V. Shet and Tanuj Poddar and Fosso {Wamba Samuel} and Yogesh K. Dwivedi,EXAMINING THE DETERMINANTS OF SUCCESSFUL ADOPTION OF DATA ANALYTICS IN HUMAN RESOURCE MANAGEMENT – A FRAMEWORK FOR IMPLICATIONS,article
149,21100787106,JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION,journal,2452414X,"2,042",Q1,24,28,86,2152,1361,83,"12,26","76,86",Netherlands,Western Europe,Elsevier BV,2016-2020,Industrial and Manufacturing Engineering (Q1); Information Systems and Management (Q1),JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION,JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION,"1,149",10.063,0.00148,"Industry 4.0 (I4.0) defines a new paradigm to produce high-quality products at the low cost by reacting quickly and effectively to changing demands in the highly volatile global markets. In Industry 4.0, the adoption of Internet of Things (IoT)-enabled Wireless Sensors (WSs) in the manufacturing processes, such as equipment, machining, assembly, material handling, inspection, etc., generates a huge volume of data known as Industrial Big Data (IBD). However, the reliable and efficient gathering and transmission of this big data from the source sensors to the floor inspection system for the real-time monitoring of unexpected changes in the production and quality control processes is the biggest challenge for Industrial Wireless Sensor Networks (IWSNs). This is because of the harsh nature of the indoor industrial environment that causes high noise, signal fading, multipath effects, heat and electromagnetic interference, which reduces the transmission quality and trigger errors in the IWSNs. Therefore, this paper proposes a novel cross-layer data gathering approach called CBI4.0 for active monitoring and control of manufacturing processes in the Industry 4.0. The key aim of the proposed CBI4.0 scheme is to exploit the multi-channel and multi-radio architecture of the sensor network to guarantee quality of service (QoS) requirements, such as higher data rates, throughput, and low packet loss, corrupted packets, and latency by dynamically switching between different frequency bands in the Multichannel Wireless Sensor Networks (MWSNs). By performing several simulation experiments through EstiNet 9.0 simulator, the performance of the proposed CBI4.0 scheme is compared against existing studies in the automobile Industry 4.0. The experimental outcomes show that the proposed scheme outperforms existing schemes and is suitable for effective control and monitoring of various events in the automobile Industry 4.0.",https://doi.org/10.1016/j.jii.2021.100236,2021,Muhammad Faheem and Rizwan Aslam Butt and Rashid Ali and Basit Raza and Md. Asri Ngadi and Vehbi Cagri Gungor,CBI4.0: A CROSS-LAYER APPROACH FOR BIG DATA GATHERING FOR ACTIVE MONITORING AND MAINTENANCE IN THE MANUFACTURING INDUSTRY 4.0,article
150,21100787106,JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION,journal,2452414X,"2,042",Q1,24,28,86,2152,1361,83,"12,26","76,86",Netherlands,Western Europe,Elsevier BV,2016-2020,Industrial and Manufacturing Engineering (Q1); Information Systems and Management (Q1),JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION,JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION,"1,149",10.063,0.00148,"The application of blockchain in food supply chains does not resolve conventional IoT data quality issues. Data on a blockchain may simply be immutable garbage. In response, this paper reports our observations and learnings from an ongoing beef supply chain project that integrates Blockchain and IoT for supply chain event tracking and beef provenance assurance and proposes two solutions for data integrity and trust in the Blockchain and IoT-enabled food supply chain. Rather than aiming for absolute truth, we explain how applying the notion of ‘common knowledge’ fundamentally changes oracle identity and data validity practices. Based on the learnings derived from leading an IoT supply chain project with a focus on beef exports from Australia to China, our findings unshackle IoT and Blockchain from being used merely to collect lag indicators of past states and liberate their potential as lead indicators of desired future states. This contributes: (a) to limit the possibility of capricious claims on IoT data performance, and; (b) to utilise mechanism design as an approach by which supply chain behaviours that increase the probability of desired future states being realised can be encouraged.",https://doi.org/10.1016/j.jii.2021.100261,2022,Warwick Powell and Marcus Foth and Shoufeng Cao and Valéri Natanelov,GARBAGE IN GARBAGE OUT: THE PRECARIOUS LINK BETWEEN IOT AND BLOCKCHAIN IN FOOD SUPPLY CHAINS,article
151,21100787106,JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION,journal,2452414X,"2,042",Q1,24,28,86,2152,1361,83,"12,26","76,86",Netherlands,Western Europe,Elsevier BV,2016-2020,Industrial and Manufacturing Engineering (Q1); Information Systems and Management (Q1),JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION,JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION,"1,149",10.063,0.00148,"Disruptive innovations are usually identified as ideas that are created ‘outside the box’. They are expected to fundamentally change existing business models and processes founded on technological applications. Disruptive innovations can be challenging to define. Information technology (IT) solutions focus on collecting, processing, and reporting different types of data. Commonly, is the solutions are expected (in cybernetics or self-regulating processes) to provide feedback to original processes and to steer them based on the data. To achieve continuous improvement with regard to environmental responsibility and profitability, new thinking and, in particular, accurate and reliable data are needed for decision-making. Very large data storages, known as big data, contain an increasing mass of different types of homogenous and non-homogenous information, as well as extensive time-series. New, innovative algorithms are required to reveal relevant information and opportunities hidden in these data storages. Global environmental challenges and zero-emission responsible production issues can only be solved using relevant and reliable continuous data as the basis. The final goal should be the creation of scalable environmental solutions based on disruptive innovations and accurate data. The aim of this paper is to determine the explicit steps for replacing silo-based reporting with company-wide, refined information, which enables decision-makers in all industries the chance to make responsible choices.",https://doi.org/10.1016/j.jii.2019.100105,2019,Esa Hämäläinen and Tommi Inkinen,INDUSTRIAL APPLICATIONS OF BIG DATA IN DISRUPTIVE INNOVATIONS SUPPORTING ENVIRONMENTAL REPORTING,article
152,21100787106,JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION,journal,2452414X,"2,042",Q1,24,28,86,2152,1361,83,"12,26","76,86",Netherlands,Western Europe,Elsevier BV,2016-2020,Industrial and Manufacturing Engineering (Q1); Information Systems and Management (Q1),JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION,JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION,"1,149",10.063,0.00148,"Typically, only a smaller portion of the monitorable operational data (e.g. from sensors and environment) from Offshore Support Vessels (OSVs) are used at present. Operational data, in addition to equipment performance data, design and construction data, creates large volumes of data with high veracity and variety. In most cases, such data richness is not well understood as to how to utilize it better during design and operation. It is, very often, too time consuming and resource demanding to estimate the final operational performance of vessel concept design solution in early design by applying simulations and model tests. This paper argues that there is a significant potential to integrate ship lifecycle data from different phases of its operation in large data repository for deliberate aims and evaluations. It is disputed discretely in the paper, evaluating performance of real similar type vessels during early stages of the design process, helps substantially improving and fine-tuning the performance criterion of the next generations of vessel design solutions. Producing learning from such a ship lifecycle data repository to find useful patterns and relationships among design parameters and existing fleet real performance data, requires the implementation of modern data mining techniques, such as big data and clustering concepts, which are introduced and applied in this paper. The analytics model introduced suggests and reviews all relevant steps of data knowledge discovery, including pre-processing (integration, feature selection and cleaning), processing (data analyzing) and post processing (evaluating and validating results) in this context.",https://doi.org/10.1016/j.jii.2018.02.002,2018,Niki Sadat Abbasian and Afshin Salajegheh and Henrique Gaspar and Per Olaf Brett,IMPROVING EARLY OSV DESIGN ROBUSTNESS BY APPLYING ‘MULTIVARIATE BIG DATA ANALYTICS’ ON A SHIP'S LIFE CYCLE,article
153,21100787106,JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION,journal,2452414X,"2,042",Q1,24,28,86,2152,1361,83,"12,26","76,86",Netherlands,Western Europe,Elsevier BV,2016-2020,Industrial and Manufacturing Engineering (Q1); Information Systems and Management (Q1),JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION,JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION,"1,149",10.063,0.00148,"Data quality management (DQM) is one of the most critical aspects to ensure successful applications of the Internet of Things (IoT). So far, most of the approaches for assuring data quality are typically data-centric, i.e., mainly focus on fixing data issues for specific values. However, organizations can also benefit from improving their capabilities of their DQM processes by developing organizational best DQM practices. In this regard, our investigation addresses how well organizations perform their DQM processes in the IoT domain. The main contribution of this study is to establish a framework for IoT DQM maturity. This framework is compliant with ISO 8000-61 (DQM: process reference model) and ISO 8000-62 (DQM: organizational process maturity assessment) and can be used to assess and improve the capabilities of the DQM processes for IoT data. The framework is composed of two elements. First, a process reference model (PRM) for IoT DQM is proposed by extending the PRM for DQM defined in ISO 8000-61, tailoring some existing processes and adding new ones. Second, a maturity model suitable for IoT data is proposed based on the PRM for IoT DQM. The maturity model, named IoT DQM3, is proposed by extending the maturity model defined in ISO 8000-62. However, in order to increase the usability of IoT DQM3, we consider adequate the proposition of a simplification of the IoT DQM3, by introducing a lightweight version to reduce assessment indicators and facilitate its industrial adoption. A simplified method to measure the capability of a process is also suggested considering the relationship of process attributes with the measurement stack defined in ISO 8000-63. The empirical validation of the maturity model is twofold. First, the appropriateness of the two models is surveyed with data quality experts who are currently working in various organizations around the world. Second, in order to demonstrate the feasibility of the proposal, the light-weight version is applied to a manufacturing company as a case study.",https://doi.org/10.1016/j.jii.2021.100256,2022,Sunho Kim and Ricardo Pérez-Castillo and Ismael Caballero and Downgwoo Lee,ORGANIZATIONAL PROCESS MATURITY MODEL FOR IOT DATA QUALITY MANAGEMENT,article
154,21100787106,JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION,journal,2452414X,"2,042",Q1,24,28,86,2152,1361,83,"12,26","76,86",Netherlands,Western Europe,Elsevier BV,2016-2020,Industrial and Manufacturing Engineering (Q1); Information Systems and Management (Q1),JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION,JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION,"1,149",10.063,0.00148,"Driven by the innovative improvement of information and communication technologies (ICTs) and their applications into manufacturing industry, the big data era in manufacturing is correspondingly arising, and the developing data mining techniques (DMTs) pave the way for pursuing the aims of smart production with the real-time, dynamic, self-adaptive and precise control. However, lots of factors in the ever-changing environment of manufacturing industry, such as, various of complex production processes, larger scale and uncertainties, more complicated constrains, coupling of operational performance, and so on, make production management face with more and more big challenges. The dynamic inflow of a large number of raw data which is collected from the physical manufacturing sites or generated in various related information systems, caused the heavy information overload problems. Indeed, most of traditional DMTs are not yet sufficient to process such big data for smart production management. Therefore, this paper reviews the development of DMTs in the big data era, and makes discussion on the applications of DMTs in production management, by selecting and analyzing the relevant papers since 2010. In the meantime, we point out limitations and put forward some suggestions about the smartness and further applications of DMTs used in production management.",https://doi.org/10.1016/j.jii.2017.08.001,2018,Ying Cheng and Ken Chen and Hemeng Sun and Yongping Zhang and Fei Tao,DATA AND KNOWLEDGE MINING WITH BIG DATA TOWARDS SMART PRODUCTION,article
155,21100787106,JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION,journal,2452414X,"2,042",Q1,24,28,86,2152,1361,83,"12,26","76,86",Netherlands,Western Europe,Elsevier BV,2016-2020,Industrial and Manufacturing Engineering (Q1); Information Systems and Management (Q1),JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION,JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION,"1,149",10.063,0.00148,"Digital Twin was introduced over a decade ago, as an innovative all-encompassing tool, with perceived benefits including real-time monitoring, simulation, optimisation and accurate forecasting. However, the theoretical framework and practical implementations of digital twin (DT) are yet to fully achieve this vision at scale. Although an increasing number of successful implementations exist in research and industrial works, sufficient implementation details are not publicly available, making it difficult to fully assess their components and effectiveness, to draw comparisons, identify successful solutions, share lessons, and thus to jointly advance and benefit from the DT methodology. This work first presents a review of relevant DT research and industrial works, focusing on the key DT features, current approaches in different domains, and successful DT implementations, to infer the key DT components and properties, and to identify current limitations and reasons behind the delay in the widespread implementation and adoption of digital twin. This work identifies that the major reasons for this delay are: the fact the DT is still a fast evolving concept; the lack of a universal DT reference framework, e.g. DT standards are scarce and still evolving; problem- and domain-dependence; security concerns over shared data; lack of DT performance metrics; and reliance of digital twin on other fast-evolving technologies. Advancements in machine learning, Internet of Things (IoT) and big data have led to significant improvements in DT features such as real-time monitoring and accurate forecasting. Despite this progress and individual company-based efforts, certain research and implementation gaps exist in the field, which have so far prevented the widespread adoption of the DT concept and technology; these gaps are also discussed in this work. Based on reviews of past work and the identified gaps, this work then defines a conceptualisation of DT which includes its components and properties; these also validate the uniqueness of DT as a concept, when compared to similar concepts such as simulation, autonomous systems and optimisation. Real-life case studies are used to showcase the application of the conceptualisation. This work discusses the state-of-the-art in DT, addresses relevant and timely DT questions, and identifies novel research questions, thus contributing to a better understanding of the DT paradigm and advancing the theory and practice of DT and its allied technologies.",https://doi.org/10.1016/j.jii.2022.100383,2022,Angira Sharma and Edward Kosasih and Jie Zhang and Alexandra Brintrup and Anisoara Calinescu,"DIGITAL TWINS: STATE OF THE ART THEORY AND PRACTICE, CHALLENGES, AND OPEN RESEARCH QUESTIONS",article
156,25858,JOURNAL OF HAZARDOUS MATERIALS,journal,03043894,"2,034",Q1,284,2143,2994,126809,31399,2975,"10,39","59,17",Netherlands,Western Europe,Elsevier,1975-2021,"Environmental Chemistry (Q1); Environmental Engineering (Q1); Health, Toxicology and Mutagenesis (Q1); Pollution (Q1); Waste Management and Disposal (Q1)",JOURNAL OF HAZARDOUS MATERIALS,JOURNAL OF HAZARDOUS MATERIALS,"137,983",10.588,0.07094,"Over the past few decades, data-driven machine learning (ML) has distinguished itself from hypothesis-driven studies and has recently received much attention in environmental toxicology. However, the use of ML in environmental toxicology remains in the early stages, with knowledge gaps, technical bottlenecks in data quality, high-dimensional/heterogeneous/small-sample data analysis and model interpretability, and a lack of an in-depth understanding of environmental toxicology. Given the above problems, we review the recent progress in the literature and highlight state-of-the-art toxicological studies using ML (such as learning and predicting toxicity in complicated biosystems and multiple-factor environmental scenarios of long-term and large-scale pollution). Beyond predicting simple biological endpoints by integrating untargeted omics and adverse outcome pathways, ML development should focus on revealing toxicological mechanisms. The integration of data-driven ML with other methods (e.g., omics analysis and adverse outcome pathway frameworks) endows ML with widely promising application in revealing toxicological mechanisms. High-quality databases and interpretable algorithms are urgently needed for toxicology and environmental science. Addressing the core issues and future challenges for ML in this review may narrow the knowledge gap between environmental toxicity and computational science and facilitate the control of environmental risk in the future.",https://doi.org/10.1016/j.jhazmat.2022.129487,2022,Xiaotong Wu and Qixing Zhou and Li Mu and Xiangang Hu,"MACHINE LEARNING IN THE IDENTIFICATION, PREDICTION AND EXPLORATION OF ENVIRONMENTAL TOXICOLOGY: CHALLENGES AND PERSPECTIVES",article
157,22792,INDUSTRIAL MARKETING MANAGEMENT,journal,00198501,"2,022",Q1,136,314,465,27989,3787,450,"6,86","89,14",United States,Northern America,Elsevier Inc.,1971-2020,Marketing (Q1),INDUSTRIAL MARKETING MANAGEMENT,INDUSTRIAL MARKETING MANAGEMENT,"16,291",6.960,0.00901,"Big Data represents a promising area for value creation and frontier research. The potential to extract actionable insights from Big Data has gained increasing attention of both academics and practitioners operating in several industries. Marketing domain has become from the start a field for experiments with Big Data approaches, even if the adoption of Big Data solutions does not always generate effective value for the adopters. Therefore, the gap existing between the potential of value creation embedded in the Big Data paradigm and the current limited exploitation of this value represents an area of investigation that this paper aims to explore. In particular, by following a systematic literature review, this study aims at presenting a framework that outlines the multiple value directions that the Big Data paradigm can generate for the adopting organizations. Eleven distinct value directions have been identified and then grouped in five dimensions (Informational, Transactional, Transformational, Strategic, Infrastructural Value), which constitute the pillars of the proposed framework. Finally, the framework has been also preliminarily applied in three case studies conducted within three Italian based companies operating in different industries (e-commerce, fast-moving consumer goods, and banking) in the final aim to see its applicability in real business scenarios.",https://doi.org/10.1016/j.indmarman.2020.03.015,2020,Gianluca Elia and Gloria Polimeno and Gianluca Solazzo and Giuseppina Passiante,A MULTI-DIMENSION FRAMEWORK FOR VALUE CREATION THROUGH BIG DATA,article
158,22792,INDUSTRIAL MARKETING MANAGEMENT,journal,00198501,"2,022",Q1,136,314,465,27989,3787,450,"6,86","89,14",United States,Northern America,Elsevier Inc.,1971-2020,Marketing (Q1),INDUSTRIAL MARKETING MANAGEMENT,INDUSTRIAL MARKETING MANAGEMENT,"16,291",6.960,0.00901,"Big Data represents a promising area for value creation and frontier research. The potential to extract actionable insights from Big Data has gained increasing attention of both academics and practitioners operating in several industries. Marketing domain has become from the start a field for experiments with Big Data approaches, even if the adoption of Big Data solutions does not always generate effective value for the adopters. Therefore, the gap existing between the potential of value creation embedded in the Big Data paradigm and the current limited exploitation of this value represents an area of investigation that this paper aims to explore. In particular, by following a systematic literature review, this study aims at presenting a framework that outlines the multiple value directions that the Big Data paradigm can generate for the adopting organizations. Eleven distinct value directions have been identified and then grouped in five dimensions (Informational, Transactional, Transformational, Strategic, Infrastructural Value), which constitute the pillars of the proposed framework. Finally, the framework has been also preliminarily applied in three case studies conducted within three Italian based companies operating in different industries (e-commerce, fast-moving consumer goods, and banking) in the final aim to see its applicability in real business scenarios.",https://doi.org/10.1016/j.indmarman.2019.08.004,2020,Gianluca Elia and Gloria Polimeno and Gianluca Solazzo and Giuseppina Passiante,A MULTI-DIMENSION FRAMEWORK FOR VALUE CREATION THROUGH BIG DATA,article
159,22792,INDUSTRIAL MARKETING MANAGEMENT,journal,00198501,"2,022",Q1,136,314,465,27989,3787,450,"6,86","89,14",United States,Northern America,Elsevier Inc.,1971-2020,Marketing (Q1),INDUSTRIAL MARKETING MANAGEMENT,INDUSTRIAL MARKETING MANAGEMENT,"16,291",6.960,0.00901,"The complexity that characterises the dynamic nature of the various environmental factors makes it very compelling for firms to be capable of addressing the changing customers' needs. The current study examines the role of big data in new product success. We develop a qualitative research with case study approach to look at this. Specifically, we look at multiple cases to get in-depth understanding of customer agility for new product success with big data analytics. The findings of the study provide insight into the role of customer agility in new product success. This study unpacks the interconnectedness of the effective use of data aggregation tools, the effectiveness of data analysis tools and customer agility. It also explores the link between all of these factors and new product success. The study is reasonably telling in that it shows that the effective use of data aggregation and data analysis tools results in customer agility which in itself explains how an organisation senses and responds speedily to opportunities for innovation in the competitive marketing environment. The current study provides significant theoretical contributions by providing evidence for the role of big data analytics, big data aggregation tools, customer agility, organisational slack and environmental turbulence in new product success.",https://doi.org/10.1016/j.indmarman.2019.09.010,2020,Nick Hajli and Mina Tajvidi and Ayantunji Gbadamosi and Waqar Nadeem,UNDERSTANDING MARKET AGILITY FOR NEW PRODUCT SUCCESS WITH BIG DATA ANALYTICS,article
160,22792,INDUSTRIAL MARKETING MANAGEMENT,journal,00198501,"2,022",Q1,136,314,465,27989,3787,450,"6,86","89,14",United States,Northern America,Elsevier Inc.,1971-2020,Marketing (Q1),INDUSTRIAL MARKETING MANAGEMENT,INDUSTRIAL MARKETING MANAGEMENT,"16,291",6.960,0.00901,"The emergence of digitally connected products and big data analytics (BDA) in industrial marketing has attracted academic and managerial interest in smart services. However, suppliers' provision of smart services and customers' adoption of these services have received scarce attention in the literature, demonstrating the need to address the changing nature of customer-supplier interactions in the digital era. Responding to prior research calls, this study utilizes ethnographic research and a storytelling lens to advance our knowledge of how stories and BDA can enhance customers' attitudes toward suppliers' smart services, their behavioral intentions and their actual adoption of smart services. The study's findings demonstrate that storytelling is a collective sensemaking and sensegiving process that occurs in interactions between customers and suppliers in which both parties contribute to the story development. The use of BDA in storytelling enhances customer sensemaking of smart services by highlighting the business value extracted from the digitized data of a reference customer. By synthesizing insights from servitization, storytelling, BDA and the customer reference literature, this study offers managers practical guidance regarding how to increase smart service sales. An example of a story used to facilitate customer adoption of a supplier's smart services in the manufacturing sector is provided.",https://doi.org/10.1016/j.indmarman.2019.12.004,2020,Valeriia Boldosova,TELLING STORIES THAT SELL: THE ROLE OF STORYTELLING AND BIG DATA ANALYTICS IN SMART SERVICE SALES,article
161,22792,INDUSTRIAL MARKETING MANAGEMENT,journal,00198501,"2,022",Q1,136,314,465,27989,3787,450,"6,86","89,14",United States,Northern America,Elsevier Inc.,1971-2020,Marketing (Q1),INDUSTRIAL MARKETING MANAGEMENT,INDUSTRIAL MARKETING MANAGEMENT,"16,291",6.960,0.00901,"This study investigates the driving forces of a firm's assimilation of big data analytical intelligence (BDAI) and how the assimilation of BDAI improve customer relationship management (CRM) performance. Drawing on the resource-based view, this study argues that a firm's data-driven culture and the competitive pressure it faces in the industry motivate a firm's assimilation of BDAI. As a firm resource, BDAI enables an organization to develop superior mass-customization capability, which in turn positively influences its CRM performance. In addition, this study proposes that a firm's marketing capability can moderate the impact of BDAI assimilation on its mass-customization capability. Using survey data collected from 147 business-to-business companies, this study finds support for most of the hypotheses. The findings of this study uncover compelling insights about the dynamics involved in the process of using BDAI to improve CRM performance.",https://doi.org/10.1016/j.indmarman.2020.10.012,2020,Chubing Zhang and Xinchun Wang and Annie Peng Cui and Shenghao Han,LINKING BIG DATA ANALYTICAL INTELLIGENCE TO CUSTOMER RELATIONSHIP MANAGEMENT PERFORMANCE,article
162,22792,INDUSTRIAL MARKETING MANAGEMENT,journal,00198501,"2,022",Q1,136,314,465,27989,3787,450,"6,86","89,14",United States,Northern America,Elsevier Inc.,1971-2020,Marketing (Q1),INDUSTRIAL MARKETING MANAGEMENT,INDUSTRIAL MARKETING MANAGEMENT,"16,291",6.960,0.00901,"Artificial Intelligence (AI) could be an important foundation of competitive advantage in the market for firms. As such, firms use AI to achieve deep market engagement when the firm's data are employed to make informed decisions. This study examines the role of computer-mediated AI agents in detecting crises related to events in a firm. A crisis threatens organizational performance; therefore, a data-driven strategy will result in an efficient and timely reflection, which increases the success of crisis management. The study extends the situational crisis communication theory (SCCT) and Attribution theory frameworks built on big data and machine learning capabilities for early detection of crises in the market. This research proposes a structural model composed of a statistical and sentimental big data analytics approach. The findings of our empirical research suggest that knowledge extracted from day-to-day data communications such as email communications of a firm can lead to the sensing of critical events related to business activities. To test our model, we use a publicly available dataset containing 517,401 items belonging to 150 users, mostly senior managers of Enron during 1999 through the 2001 crisis. The findings suggest that the model is plausible in the early detection of Enron's critical events, which can support decision making in the market.",https://doi.org/10.1016/j.indmarman.2020.09.015,2020,Aydin Farrokhi and Farid Shirazi and Nick Hajli and Mina Tajvidi,USING ARTIFICIAL INTELLIGENCE TO DETECT CRISIS RELATED TO EVENTS: DECISION MAKING IN B2B BY ARTIFICIAL INTELLIGENCE,article
163,12391,LUNG CANCER,journal,"01695002, 18728332","1,989",Q1,129,349,917,10507,4671,845,"4,71","30,11",Ireland,Western Europe,Elsevier Ireland Ltd,1985-2020,Cancer Research (Q1); Oncology (Q1); Pulmonary and Respiratory Medicine (Q1),LUNG CANCER,LUNG CANCER,"15,504",5.705,0.01966,"Introduction
Until the recent approval of immunotherapy after completing concurrent chemoradiotherapy (CCRT), there has been little progress in treating unresectable stage III non-small cell lung cancer (NSCLC). This prompted us to search real-world data (RWD) to better understand diagnosis and treatment patterns, and outcomes.
Methods
This non-interventional observational study used a unique, novel algorithm for big data analysis to collect and assess anonymized patient electronic medical records from a clinical data warehouse (CDW) over a 10-year period to capture real-world patterns of diagnosis, treatment, and outcomes of stage III NSCLC patients. We describe real-world patterns of diagnosis and treatment of patients with newly-diagnosed stage III NSCLC, and patients’ characteristics, and assessment of treatment outcomes.
Results
We analyzed clinical variables from 23,735 NSCLC patients. Stage III patients (N = 4138, 18.2 %) were diagnosed as IIIA (N = 2,547, 11.2 %) or IIIB (N = 1,591. 7.0 %). Treated stage III patients (N = 2530, 61.1 %) had a median age of 64.2 years, were mostly male (78.5 %) and had an ECOG performance status of 1 (65.2 %). Treatment comprised curative-intent surgery (N = 1,254, 49.6 %) with 705 receiving neoadjuvant therapy; definitive CRT (N = 648, 25.6 %); palliative CT (N = 270, 10.7 %), or thoracic RT (N = 170, 6.7 %). Median OS (range) for neoadjuvant, surgery, CRT, palliative chemotherapy, lung RT alone, and supportive care was 49.2 (42.0–56.5), 52.5 (43.1–61.9), 30.3 (26.6–34.0), 14.7 (13.0–16.4), 8.8 (6.2–11.3), and 2.0 (1.0–3.0) months, respectively.
Conclusions
This unique in-house algorithm enabled a rapid and comprehensive analysis of big data through a CDW, with daily automatic updates that documented real-world PFS and OS consistent with the published literature, and real-world treatment patterns and clinical outcomes in stage III NSCLC patients.",https://doi.org/10.1016/j.lungcan.2020.05.033,2020,Hyun Ae Jung and Jong-Mu Sun and Se-Hoon Lee and Jin Seok Ahn and Myung-Ju Ahn and Keunchil Park,"TEN-YEAR PATIENT JOURNEY OF STAGE III NON-SMALL CELL LUNG CANCER PATIENTS: A SINGLE-CENTER, OBSERVATIONAL, RETROSPECTIVE STUDY IN KOREA (REALTIME AUTOMATICALLY UPDATED DATA WAREHOUSE IN HEALTH CARE; UNIVERSE-ROOT STUDY)",article
164,21100405003,SCIENCE BULLETIN,journal,"20959273, 20959281","1,983",Q1,112,365,833,12806,5639,616,"7,21","35,08",Netherlands,Western Europe,Elsevier BV,2015-2020,Multidisciplinary (Q1),SCIENCE BULLETIN,SCIENCE BULLETIN,"8,832",11.780,0.0164,"The United Nations 2030 Agenda for Sustainable Development provides an important framework for economic, social, and environmental action. A comprehensive indicator system to aid in the systematic implementation and monitoring of progress toward the Sustainable Development Goals (SDGs) is unfortunately limited in many countries due to lack of data. The availability of a growing amount of multi-source data and rapid advancements in big data methods and infrastructure provide unique opportunities to mitigate these data shortages and develop innovative methodologies for comparatively monitoring SDGs. Big Earth Data, a special class of big data with spatial attributes, holds tremendous potential to facilitate science, technology, and innovation toward implementing SDGs around the world. Several programs and initiatives in China have invested in Big Earth Data infrastructure and capabilities, and have successfully carried out case studies to demonstrate their utility in sustainability science. This paper presents implementations of Big Earth Data in evaluating SDG indicators, including the development of new algorithms, indicator expansion (for SDG 11.4.1) and indicator extension (for SDG 11.3.1), introduction of a biodiversity risk index as a more effective analysis method for SDG 15.5.1, and several new high-quality data products, such as global net ecosystem productivity, high-resolution global mountain green cover index, and endangered species richness. These innovations are used to present a comprehensive analysis of SDGs 2, 6, 11, 13, 14, and 15 from 2010 to 2020 in China utilizing Big Earth Data, concluding that all six SDGs are on schedule to be achieved by 2030.",https://doi.org/10.1016/j.scib.2022.07.015,2022,Huadong Guo and Dong Liang and Zhongchang Sun and Fang Chen and Xinyuan Wang and Junsheng Li and Li Zhu and Jinhu Bian and Yanqiang Wei and Lei Huang and Yu Chen and Dailiang Peng and Xiaosong Li and Shanlong Lu and Jie Liu and Zeeshan Shirazi,MEASURING AND EVALUATING SDG INDICATORS WITH BIG EARTH DATA,article
165,21100405003,SCIENCE BULLETIN,journal,"20959273, 20959281","1,983",Q1,112,365,833,12806,5639,616,"7,21","35,08",Netherlands,Western Europe,Elsevier BV,2015-2020,Multidisciplinary (Q1),SCIENCE BULLETIN,SCIENCE BULLETIN,"8,832",11.780,0.0164,"Smart, real-time, low-cost, and distributed ecosystem monitoring is essential for understanding and managing rapidly changing ecosystems. However, new techniques in the big data era have rarely been introduced into operational ecosystem monitoring, particularly for fragile ecosystems in remote areas. We introduce the Internet of Things (IoT) techniques to establish a prototype ecosystem monitoring system by developing innovative smart devices and using IoT technologies for ecosystem monitoring in isolated environments. The developed smart devices include four categories: large-scale and nonintrusive instruments to measure evapotranspiration and soil moisture, in situ observing systems for CO2 and δ13C associated with soil respiration, portable and distributed devices for monitoring vegetation variables, and Bi-CMOS cameras and pressure trigger sensors for terrestrial vertebrate monitoring. These new devices outperform conventional devices and are connected to each other via wireless communication networks. The breakthroughs in the ecosystem monitoring IoT include new data loggers and long-distance wireless sensor network technology that supports the rapid transmission of data from devices to wireless networks. The applicability of this ecosystem monitoring IoT is verified in three fragile ecosystems, including a karst rocky desertification area, the National Park for Amur Tigers, and the oasis-desert ecotone in China. By integrating these devices and technologies with an ecosystem monitoring information system, a seamless data acquisition, transmission, processing, and application IoT is created. The establishment of this ecosystem monitoring IoT will serve as a new paradigm for ecosystem monitoring and therefore provide a platform for ecosystem management and decision making in the era of big data.",https://doi.org/10.1016/j.scib.2019.07.004,2019,Xin Li and Ning Zhao and Rui Jin and Shaomin Liu and Xiaomin Sun and Xuefa Wen and Dongxiu Wu and Yan Zhou and Jianwen Guo and Shiping Chen and Ziwei Xu and Mingguo Ma and Tianming Wang and Yonghua Qu and Xinwei Wang and Fangming Wu and Yuke Zhou,INTERNET OF THINGS TO NETWORK SMART DEVICES FOR ECOSYSTEM MONITORING,article
166,21100405003,SCIENCE BULLETIN,journal,"20959273, 20959281","1,983",Q1,112,365,833,12806,5639,616,"7,21","35,08",Netherlands,Western Europe,Elsevier BV,2015-2020,Multidisciplinary (Q1),SCIENCE BULLETIN,SCIENCE BULLETIN,"8,832",11.780,0.0164,,https://doi.org/10.1016/j.scib.2019.09.011,2019,Peng Jia and Hong Xue and Shiyong Liu and Hao Wang and Lijian Yang and Therese Hesketh and Lu Ma and Hongwei Cai and Xin Liu and Yaogang Wang and Youfa Wang,OPPORTUNITIES AND CHALLENGES OF USING BIG DATA FOR GLOBAL HEALTH,article
167,29348,ENERGY,journal,"18736785, 03605442","1,961",Q1,193,2315,6498,109602,49084,6476,"7,55","47,34",United Kingdom,Western Europe,Elsevier Ltd.,1976-2020,"Building and Construction (Q1); Civil and Structural Engineering (Q1); Electrical and Electronic Engineering (Q1); Energy Engineering and Power Technology (Q1); Energy (miscellaneous) (Q1); Fuel Technology (Q1); Industrial and Manufacturing Engineering (Q1); Management, Monitoring, Policy and Law (Q1); Mechanical Engineering (Q1); Modeling and Simulation (Q1); Pollution (Q1); Renewable Energy, Sustainability and the Environment (Q1)",ENERGY,ENERGY,"103,367",7.147,0.11575,"Measuring real-world fuel consumption of light duty vehicles can be challenging due to the limited collection of actual data. In this paper, we use big data retrieved from the record of real-world fuel consumptions of different brands of vehicles in different areas (n = 106,809 samples from 201 brands of vehicles and 34 cities) in China to build up a real-world fuel consumption rate (RFCR) model to estimate the fuel consumption given the driving conditions and figure out the main factors that affect actual fuel consumption in the real world. We find the average deviation of actual fuel consumptions and the fitting results of RFCR model is 4.22% , which does not significantly differ from zero, and the fuel consumptions calculated by RFCR model tend to be 1.40 L/100 km (about 25%) higher than the official reported data. Furthermore, we find that annual average temperature and altitude factors significantly influence the fuel consumption rate. The results indicate that there is a real world performance discrepancy between the theoretical fuel consumption released by authorities and that in the real world, and some green behaviors (choose light duty vehicles, reduce the use of air conditioning and change to manual transmission type) can reduce energy consumption of vehicles.",https://doi.org/10.1016/j.energy.2019.116388,2020,Tian Wu and Xiao Han and M. Mocarlo Zheng and Xunmin Ou and Hongbo Sun and Xiong Zhang,IMPACT FACTORS OF THE REAL-WORLD FUEL CONSUMPTION RATE OF LIGHT DUTY VEHICLES IN CHINA,article
168,78796,FIELD CROPS RESEARCH,journal,"18726852, 03784290","1,951",Q1,150,191,798,10669,4752,795,"5,39","55,86",Netherlands,Western Europe,Elsevier,1978-2020,Agronomy and Crop Science (Q1); Soil Science (Q1),FIELD CROPS RESEARCH,FIELD CROPS RESEARCH,"24,118",5.224,0.01607,"Yield gaps and water productivity are key indicators to monitor the progress towards more sustainable and productive cropping systems. Individual farmers are collecting increasing amounts of data (‘big data’), which can help monitor the process of sustainable intensification at local level. In this study, we build upon such data to quantify the magnitude and identify the biophysical and management determinants of on-farm yield gaps and water productivity for the main arable crops cultivated in the Netherlands. The analysis focused on ware, seed and starch potatoes, sugar beet, spring onion, winter wheat and spring barley and covered the period 2015–2017. A crop modelling approach based on crop coefficients (kc) and daily weather data was used to estimate the potential yield (Yp), radiation intercepted and potential evapotranspiration (ETP) for each crop. Yield gaps were estimated to be ca. 10% of Yp for sugar beet, 25–30% of Yp for ware, seed and starch potato and spring barley, and 35–40% of Yp for spring onion and winter wheat. Variation in actual yields was associated with water availability in key periods of the growing season as well as with sowing and harvest dates. However, the R2 of the fitted regressions was rather low (20–49%). Current levels of crop water productivity ranged between 13 kg DM ha−1 mm−1 for spring barley, ca. 15 kg DM ha−1 mm−1 for seed potato, spring onion and winter wheat, 23 kg DM ha−1 mm−1 for ware potato and ca. 25 kg DM ha−1 mm−1 for starch potato and sugar beet. These values are about half of their potential, but increasing actual water productivity further is restricted by rainfall amount and distribution. However, doing so should not be prioritized over reducing environmental impacts of these intensive cropping systems in the short-term and may require large investments from farm to regional levels in the long-term. Although these findings are most relevant to similar cropping systems in NW Europe, the underlying methods are generic and can be used to benchmark crop performance in other cropping systems. Based on this work, we argue that ‘big data’ are currently most useful to describe cropping systems at regional scale and derive benchmarks of farm performance but not as much to predict and explain crop yield variability in time and space.",https://doi.org/10.1016/j.fcr.2020.107828,2020,João Vasco Silva and Tomás R. Tenreiro and Léon Spätjens and Niels P.R. Anten and Martin K. {van Ittersum} and Pytrik Reidsma,CAN BIG DATA EXPLAIN YIELD VARIABILITY AND WATER PRODUCTIVITY IN INTENSIVE CROPPING SYSTEMS?,article
169,19167,JOURNAL OF CLEANER PRODUCTION,journal,"09596526, 18791786","1,937",Q1,200,5126,10603,304498,103295,10577,"9,56","59,40",United Kingdom,Western Europe,Elsevier Ltd.,1993-2021,"Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)",JOURNAL OF CLEANER PRODUCTION,JOURNAL OF CLEANER PRODUCTION,"170,352",9.297,0.18299,"Researches about the fusion application of Big Data and blockchain have appeared for a long time, many information service providers have launched information service business based on Big Data and blockchain (hereafter, ISBD). However, in the green agri-food area, the ISBD application does not popularized. A vital reason is that many decision makers do not know how to make an optimal investment decision and coordinate chain members after adopting ISBD. The core of this problem is to study the issue of investment decision and coordination in a green agri-food supply chain. To solve this problem, firstly, combining with the status of Chinese agricultural development, we proposed a more suitable supply chain structure in the fusion application environment of Big Data and blockchain. Then, we chose a green agri-food supply chain with one producer and one retailer as research object and revised the demand function. Afterwards, considering the changes of agri-food freshness and greenness, we built and analysed the benefit models of producer and retailer before and after using ISBD, and then a cost-sharing and revenue-sharing contract was put forward to coordinate the supply chain. Findings: 1) When the total investment cost payed by producer and retailer is in a certain range, using ISBD will help chain members gain more benefits. 2) If chain members want to gain more benefits after using ISBD, they should try their best to optimize costs by extracting valuable information. Results can offer a theoretical guidance for producer and retailer in investing in ISBD, pricing decision and supply chain coordination after applying ISBD.",https://doi.org/10.1016/j.jclepro.2020.123646,2020,Pan Liu and Yue Long and Hai-Cao Song and Yan-Dong He,INVESTMENT DECISION AND COORDINATION OF GREEN AGRI-FOOD SUPPLY CHAIN CONSIDERING INFORMATION SERVICE BASED ON BLOCKCHAIN AND BIG DATA,article
170,19167,JOURNAL OF CLEANER PRODUCTION,journal,"09596526, 18791786","1,937",Q1,200,5126,10603,304498,103295,10577,"9,56","59,40",United Kingdom,Western Europe,Elsevier Ltd.,1993-2021,"Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)",JOURNAL OF CLEANER PRODUCTION,JOURNAL OF CLEANER PRODUCTION,"170,352",9.297,0.18299,"With the development of the integrated energy Internet, energy structure optimization and emission reduction have led to higher requirements for developing various energy sources to enable coordinated and sustainable development. However, data-mining methods are rarely used to study the coordination of multi-energy generation in published research results. In this study, from the perspective of power industry emissions, coordinated generation of various energy sources, and balance of power generation and consumption, a data-mining algorithm was used to analyze the development of thermal power, hydropower, wind power, waste heat, gas, and other power sources. The chi-square automatic interaction detection tree (CHAID), logistic regression, and two-step clustering methods were applied. The results show that: a) CO2 and SO2 emissions were mainly affected by thermal power generation, whereas NOx emissions were jointly affected by thermal power, garbage power, and gas-fired power, and the emissions of various pollutants increased with an increase in power consumption. The optimal power-generation scheme under minimum emission can be obtained. b) There was a strong correlation between thermal power generation and residential electricity consumption, and renewable energy (wind energy, photovoltaic, hydropower) exhibited the highest correlation with the electricity consumption of the tertiary industry, which indicates that renewable energy generation can be promoted by managing electricity consumption in the tertiary industry. c) When the electricity demand of all users was small, the proportion of renewable energy power generation increased; in contrast, the thermal power generation was larger. This indicates the importance of improving the sustainable and stable power supply of renewable energy. This study provides a data analysis model for the coordinated development of multiple energies, which will contribute to the decision-making basis for controlling power emissions, improving the utilization rate of renewable energy, and optimizing the energy structure.",https://doi.org/10.1016/j.jclepro.2021.128154,2021,Dongfang Ren and Xiaopeng Guo and Cunbin Li,RESEARCH ON BIG DATA ANALYSIS MODEL OF MULTI ENERGY POWER GENERATION CONSIDERING POLLUTANT EMISSION—EMPIRICAL ANALYSIS FROM SHANXI PROVINCE,article
171,19167,JOURNAL OF CLEANER PRODUCTION,journal,"09596526, 18791786","1,937",Q1,200,5126,10603,304498,103295,10577,"9,56","59,40",United Kingdom,Western Europe,Elsevier Ltd.,1993-2021,"Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)",JOURNAL OF CLEANER PRODUCTION,JOURNAL OF CLEANER PRODUCTION,"170,352",9.297,0.18299,"The future of humanity depends increasingly on the performance of cities. Big data provide new and powerful ways of studying and improving coupled urban environmental, social, and economic systems to achieve urban sustainability. However, the term big data has been defined variably, and its urban applications have so far been sporadic in terms of research topic and location. A comprehensive review of big data-based urban environment, society, and sustainability (UESS) research is much needed. The aim of this study was to summarize the big data-based UESS research using a systematic review approach in combination with bibliometric and thematic analyses. The results showed that the numbers of publications and citations of related articles have been increasing exponentially in recent years. The most frequently used big data in UESS research are human behavior data, and the major analytical methods are of five types: classification, clustering, regression, association rules, and social network analysis. The major research topics of big data-based UESS research include urban mobility, urban land use and planning, environmental sustainability, public health and safety, social equity, tourism, resources and energy utilization, real estate, and retail, accommodation and catering. Big data benefit UESS research by proving a people-oriented perspective, timely and real-time information, and fine-resolution spatial dynamics. In addition, several obstacles were identified to applying big data in UESS research, which are related to data quality and acquisition, data storage and management, data security and privacy, data cleaning and preprocessing, and data analysis and information mining. To move forward, future research should integrate multiple big data sources, develop and utilize new methods such as deep learning and cloud computing, and expand the application fields to focus on the interactions between human activities and urban environments. This review can contribute to understanding the current situation of big data-based UESS research, and provide a reference for studies of this topic in the future.",https://doi.org/10.1016/j.jclepro.2020.123142,2020,Lingqiang Kong and Zhifeng Liu and Jianguo Wu,A SYSTEMATIC REVIEW OF BIG DATA-BASED URBAN SUSTAINABILITY RESEARCH: STATE-OF-THE-SCIENCE AND FUTURE DIRECTIONS,article
172,19167,JOURNAL OF CLEANER PRODUCTION,journal,"09596526, 18791786","1,937",Q1,200,5126,10603,304498,103295,10577,"9,56","59,40",United Kingdom,Western Europe,Elsevier Ltd.,1993-2021,"Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)",JOURNAL OF CLEANER PRODUCTION,JOURNAL OF CLEANER PRODUCTION,"170,352",9.297,0.18299,"Smart manufacturing has received increased attention from academia and industry in recent years, as it provides competitive advantage for manufacturing companies making industry more efficient and sustainable. As one of the most important technologies for smart manufacturing, big data analytics can uncover hidden knowledge and other useful information like relations between lifecycle decisions and process parameters helping industrial leaders to make more-informed business decisions in complex management environments. However, according to the literature, big data analytics and smart manufacturing were individually researched in academia and industry. To provide theoretical foundations for the research community to further develop scientific insights in applying big data analytics to smart manufacturing, it is necessary to summarize the existing research progress and weakness. In this paper, through combining the key technologies of smart manufacturing and the idea of ubiquitous servitization in the whole lifecycle, the term of sustainable smart manufacturing was coined. A comprehensive overview of big data in smart manufacturing was conducted, and a conceptual framework was proposed from the perspective of product lifecycle. The proposed framework allows analyzing potential applications and key advantages, and the discussion of current challenges and future research directions provides valuable insights for academia and industry.",https://doi.org/10.1016/j.jclepro.2018.11.025,2019,Shan Ren and Yingfeng Zhang and Yang Liu and Tomohiko Sakao and Donald Huisingh and Cecilia M.V.B. Almeida,"A COMPREHENSIVE REVIEW OF BIG DATA ANALYTICS THROUGHOUT PRODUCT LIFECYCLE TO SUPPORT SUSTAINABLE SMART MANUFACTURING: A FRAMEWORK, CHALLENGES AND FUTURE RESEARCH DIRECTIONS",article
173,19167,JOURNAL OF CLEANER PRODUCTION,journal,"09596526, 18791786","1,937",Q1,200,5126,10603,304498,103295,10577,"9,56","59,40",United Kingdom,Western Europe,Elsevier Ltd.,1993-2021,"Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)",JOURNAL OF CLEANER PRODUCTION,JOURNAL OF CLEANER PRODUCTION,"170,352",9.297,0.18299,"Big data analytics is becoming very popular concept in academia as well as in industry. It has come up with new decision tools to design data-driven supply chains. The manufacturing industry is under huge pressure to integrate sustainable practices into their overall business for sustainbale operations management. The purpose of this study is to analyse the predictors of sustainable business performance through big data analytics in the context of developing countries. Data was collected from manufacturing firms those have adopted sustainable practices. A hybrid Structural Equation Modelling - Artificial Neural Network model is used to analyse 316 responses of Indian professional experts. Factor analysis results shows that management and leadership style, state and central-government policy, supplier integration, internal business process, and customer integration have a significant influence on big data analytics and sustainability practices. Furthermore, the results obtained from structural equation modelling were feed as input to the artificial neural network model. The study findings shows that management and leadership style, state and central-government policy as the two most important predictors of big data analytics and sustainability practices. The results provide unique insights into manufacturing firms to improve their sustainable business performance from an operations management viewpoint. The study provides theoretical and practical insights into big data implementation issues in accomplishing sustainability practices in business organisations of emerging economies.",https://doi.org/10.1016/j.jclepro.2019.03.181,2019,Rakesh D. Raut and Sachin Kumar Mangla and Vaibhav S. Narwane and Bhaskar B. Gardas and Pragati Priyadarshinee and Balkrishna E. Narkhede,LINKING BIG DATA ANALYTICS AND OPERATIONAL SUSTAINABILITY PRACTICES FOR SUSTAINABLE BUSINESS MANAGEMENT,article
174,19167,JOURNAL OF CLEANER PRODUCTION,journal,"09596526, 18791786","1,937",Q1,200,5126,10603,304498,103295,10577,"9,56","59,40",United Kingdom,Western Europe,Elsevier Ltd.,1993-2021,"Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)",JOURNAL OF CLEANER PRODUCTION,JOURNAL OF CLEANER PRODUCTION,"170,352",9.297,0.18299,"Today, in many organizations, the debate about the difference in core capabilities has become an important factor for market competition. Companies, based on the field of activity, decide to strengthen some of their capabilities, capacities, and expertise. Therefore, the focus of an organization on the strengths and efforts to develop its sustainability will lead to a competitive advantage in the marketplace. Due to changes in environmental factors, organizations have focused on carbon emissions in procurement and transportation that have the highest carbon footprint. This paper proposes a multi-objective, eco-sustainability model for a supply chain. The objectives are to minimize overall costs, maximize the efficiency of transportation vehicles and minimize information fraud in the process of information sharing within supply chain elements. Big data is considered in the amount of information exchanged between customers and other elements of the proposed supply chain; since there are frauds in information sharing then using big data 5Vs the model is adapted to control the cost of information loss leading to customer dissatisfaction. Since uncertainty is inevitable in the real environments, in this research hybrid uncertainty is considered. Because two sources of uncertainty are considered in most of the parameters, thus it is necessary to robustify the decision-making process. The model is a mixed integer nonlinear program including big data for an optimal sustainable procurement and transportation decision. A heuristic method is used to solve the big data problem that makes use of a robust fuzzy stochastic programming approach. The proposed model can prevent disturbances by using a scenario-based stochastic programming approach. An effective hybrid robust fuzzy stochastic method is also employed for controlling uncertainty in parameters and risk taking out of outbound decisions. To solve the multi-objective model, augmented ε-constraint method is utilized. The model performance is investigated in a comprehensive computational study.",https://doi.org/10.1016/j.jclepro.2020.120640,2020,Hadi Gholizadeh and Hamed Fazlollahtabar and Mohammad Khalilzadeh,A ROBUST FUZZY STOCHASTIC PROGRAMMING FOR SUSTAINABLE PROCUREMENT AND LOGISTICS UNDER HYBRID UNCERTAINTY USING BIG DATA,article
175,19167,JOURNAL OF CLEANER PRODUCTION,journal,"09596526, 18791786","1,937",Q1,200,5126,10603,304498,103295,10577,"9,56","59,40",United Kingdom,Western Europe,Elsevier Ltd.,1993-2021,"Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)",JOURNAL OF CLEANER PRODUCTION,JOURNAL OF CLEANER PRODUCTION,"170,352",9.297,0.18299,"Remanufacturing is deemed to be an effective method for recycling resources, achieving sustainable production. However, little importance of remanufacturing has been attached in PLM. Surely, there are many problems in implementation of the remanufacturing strategy, such as inability to effectively reduce uncertainty, lack of product multi-life-cycle remanufacturing process tracking management, lack of smart enabling technology application in the full lifecycle that focusing on multi-life-cycle remanufacturing. After analyzing the reasons, through integrating smart enabling technologies, a new PLM paradigm focusing on the multi-life-cycle remanufacturing process: Big Data driven Hierarchical Digital Twin Predictive Remanufacturing (BDHDTPREMfg) is proposed. And the definition of BDHDTPREMfg is proposed. A big data driven layered architecture and the hierarchical CPS-Digital-Twin(CPSDT) reconfiguration control mechanism of BDHDTPREMfg are respectively developed. Then, this paper presents an application scenario of BDHDTPREMfg to validate the feasibility and effectiveness. Based on the above application analysis, the benefits of penetrating BDHDTPREMfg into the entire lifecycle are demonstrated. The summary of this paper and future research work is discussed in the end.",https://doi.org/10.1016/j.jclepro.2019.119299,2020,Yankai Wang and Shilong Wang and Bo Yang and Lingzi Zhu and Feng Liu,"BIG DATA DRIVEN HIERARCHICAL DIGITAL TWIN PREDICTIVE REMANUFACTURING PARADIGM: ARCHITECTURE, CONTROL MECHANISM, APPLICATION SCENARIO AND BENEFITS",article
176,19167,JOURNAL OF CLEANER PRODUCTION,journal,"09596526, 18791786","1,937",Q1,200,5126,10603,304498,103295,10577,"9,56","59,40",United Kingdom,Western Europe,Elsevier Ltd.,1993-2021,"Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)",JOURNAL OF CLEANER PRODUCTION,JOURNAL OF CLEANER PRODUCTION,"170,352",9.297,0.18299,"Big data has caused the scientific community to re-examine the scientific research methodologies and has triggered a revolution in scientific thinking. As a branch of scientific research, production safety management is also exploring methods to take advantage of big data. This research aims to provide a theoretical basis for promoting the application of big data in production safety management. First, four different types of production safety management paradigms were identified, namely small-data-based, static-oriented, interpretation-based and causal-oriented paradigm, and the challenges to these paradigms in the presence of big data were introduced. Second, the opportunities of employing big data in production safety management were identified from four aspects, including better predict the future production safety phenomena, promote production safety management highlight relevance, achieve the balance between deductive and inductive approaches and promote the interdisciplinary development of production safety management. Third, the paradigm shifting trend of production safety management was concluded, and the discipline foundation of the new paradigm was considered as the integration of data science, production management and safety science. Fourth, a new big-data-driven production safety management paradigm was developed, which consists of the logical line of production safety management, the macro-meso-micro data spectrum, the key big data analytics, and the four-dimensional morphology. At last, the strengths (e.g., supporting better-informed safety description, safety inquisition, safety prediction) and future research direction (e.g., theory research focuses on safety-related data mining/capturing/cleansing) of the new paradigm were discussed. The research results not only can provide theoretical and practical basis for big-data-driven production safety management, but also can offer advice to managerial consideration and scholarly investigation.",https://doi.org/10.1016/j.jclepro.2019.05.245,2019,Lang Huang and Chao Wu and Bing Wang,"CHALLENGES, OPPORTUNITIES AND PARADIGM OF APPLYING BIG DATA TO PRODUCTION SAFETY MANAGEMENT: FROM A THEORETICAL PERSPECTIVE",article
177,19167,JOURNAL OF CLEANER PRODUCTION,journal,"09596526, 18791786","1,937",Q1,200,5126,10603,304498,103295,10577,"9,56","59,40",United Kingdom,Western Europe,Elsevier Ltd.,1993-2021,"Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)",JOURNAL OF CLEANER PRODUCTION,JOURNAL OF CLEANER PRODUCTION,"170,352",9.297,0.18299,"This paper presents a multi-objective optimization model for a green supply chain management scheme that minimizes the inherent risk occurred by hazardous materials, associated carbon emission and economic cost. The model related parameters are capitalized on a big data analysis. Three scenarios are proposed to improve green supply chain management. The first scenario divides optimization into three options: the first involves minimizing risk and then dealing with carbon emissions (and thus economic cost); the second minimizes both risk and carbon emissions first, with the ultimate goal of minimizing overall cost; and the third option attempts to minimize risk, carbon emissions, and economic cost simultaneously. This paper provides a case study to verify the optimization model. Finally, the limitations of this research and approach are discussed to lay a foundation for further improvement.",https://doi.org/10.1016/j.jclepro.2016.03.006,2017,Rui Zhao and Yiyun Liu and Ning Zhang and Tao Huang,AN OPTIMIZATION MODEL FOR GREEN SUPPLY CHAIN MANAGEMENT BY USING A BIG DATA ANALYTIC APPROACH,article
178,19167,JOURNAL OF CLEANER PRODUCTION,journal,"09596526, 18791786","1,937",Q1,200,5126,10603,304498,103295,10577,"9,56","59,40",United Kingdom,Western Europe,Elsevier Ltd.,1993-2021,"Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)",JOURNAL OF CLEANER PRODUCTION,JOURNAL OF CLEANER PRODUCTION,"170,352",9.297,0.18299,"The purpose of this paper is to propose and test a theoretical framework to explain resilience in supply chain networks for sustainability using unstructured Big Data, based upon 36,422 items gathered in the form of tweets, news, Facebook, WordPress, Instagram, Google+, and YouTube, and structured data, via responses from 205 managers involved in disaster relief activities in the aftermath of Nepal earthquake in 2015. The paper uses Big Data analysis, followed by a survey which was analyzed using content analysis and confirmatory factor analysis (CFA). The results of the analysis suggest that swift trust, information sharing and public–private partnership are critical enablers of resilience in supply chain networks. The current study used cross-sectional data. However the hypotheses of the study can be tested using longitudinal data to attempt to establish causality. The article advances the literature on resilience in disaster supply chain networks for sustainability in that (i) it suggests the use of Big Data analysis to propose and test particular frameworks in the context of resilient supply chains that enable sustainability; (ii) it argues that swift trust, public private partnerships, and quality information sharing link to resilience in supply chain networks; and (iii) it uses the context of Nepal, at the moment of the disaster relief activities to provide contemporaneous perceptions of the phenomenon as it takes place.",https://doi.org/10.1016/j.jclepro.2016.03.059,2017,Thanos Papadopoulos and Angappa Gunasekaran and Rameshwar Dubey and Nezih Altay and Stephen J. Childe and Samuel Fosso-Wamba,THE ROLE OF BIG DATA IN EXPLAINING DISASTER RESILIENCE IN SUPPLY CHAINS FOR SUSTAINABILITY,article
179,19167,JOURNAL OF CLEANER PRODUCTION,journal,"09596526, 18791786","1,937",Q1,200,5126,10603,304498,103295,10577,"9,56","59,40",United Kingdom,Western Europe,Elsevier Ltd.,1993-2021,"Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)",JOURNAL OF CLEANER PRODUCTION,JOURNAL OF CLEANER PRODUCTION,"170,352",9.297,0.18299,"With continuous collaborative research in sensor technology, communication technology, plant science, computer science and engineering science, Internet of Things (IoT) in agriculture has made a qualitative leap through environmental sensor networks, non-destructive imaging, spectral analysis, robotics, machine vision and laser radar technology. Physical and chemical analysis can continuously obtain environmental data, experimental metadata (including text, image and spectral, 3D point cloud and real-time growth data) through integrated automation platform equipment and technical means. Based on data on multi-scale, multi-environmental and multi-mode plant traits that constitute big data on plant phenotypes, genotype–phenotype–envirotype relationship in the omics system can be explored deeply. Detailed information on the formation mechanism of specific biological traits can promote the process of functional genomics, plant molecular breeding and efficient cultivation. This study summarises the development background, research process and characteristics of high-throughput plant phenotypes. A systematic review of the research progress of IoT in agriculture and plant high-throughput phenotypes is conducted, including the acquisition and analysis of plant phenotype big data, phenotypic trait prediction and multi-recombination analysis based on plant phenomics. This study proposes key techniques for current plant phenotypes, and looks forward to the research on plant phenotype detection technology in the field environment, fusion and data mining of plant phenotype multivariate data, simultaneous observation of multi-scale phenotype platform and promotion of a comprehensive high-throughput phenotype technology.",https://doi.org/10.1016/j.jclepro.2020.123651,2021,Jiangchuan Fan and Ying Zhang and Weiliang Wen and Shenghao Gu and Xianju Lu and Xinyu Guo,THE FUTURE OF INTERNET OF THINGS IN AGRICULTURE: PLANT HIGH-THROUGHPUT PHENOTYPIC PLATFORM,article
180,19167,JOURNAL OF CLEANER PRODUCTION,journal,"09596526, 18791786","1,937",Q1,200,5126,10603,304498,103295,10577,"9,56","59,40",United Kingdom,Western Europe,Elsevier Ltd.,1993-2021,"Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)",JOURNAL OF CLEANER PRODUCTION,JOURNAL OF CLEANER PRODUCTION,"170,352",9.297,0.18299,"The energy industry is at a crossroads. Digital technological developments have the potential to change our energy supply, trade, and consumption dramatically. The new digitalization model is powered by the artificial intelligence (AI) technology. The integration of energy supply, demand, and renewable sources into the power grid will be controlled autonomously by smart software that optimizes decision-making and operations. AI will play an integral role in achieving this goal. This study focuses on the use of AI techniques in the energy sector. This study aims to present a realistic baseline that allows researchers and readers to compare their AI efforts, ambitions, new state-of-the-art applications, challenges, and global roles in policymaking. We covered three major aspects, including: i) the use of AI in solar and hydrogen power generation; (ii) the use of AI in supply and demand management control; and (iii) recent advances in AI technology. This study explored how AI techniques outperform traditional models in controllability, big data handling, cyberattack prevention, smart grid, IoT, robotics, energy efficiency optimization, predictive maintenance control, and computational efficiency. Big data, the development of a machine learning model, and AI will play an important role in the future energy market. Our study’s findings show that AI is becoming a key enabler of a complex, new and data-related energy industry, providing a key magic tool to increase operational performance and efficiency in an increasingly cut-throat environment. As a result, the energy industry, utilities, power system operators, and independent power producers may need to focus more on AI technologies if they want meaningful results to remain competitive. New competitors, new business strategies, and a more active approach to customers would require informed and flexible regulatory engagement with the associated complexities of customer safety, privacy, and information security. Given the pace of development in information technology, AI and data analysis, regulatory approvals for new services and products in the new Era of digital energy markets can be enforced as quickly and efficiently as possible.",https://doi.org/10.1016/j.jclepro.2021.125834,2021,Tanveer Ahmad and Dongdong Zhang and Chao Huang and Hongcai Zhang and Ningyi Dai and Yonghua Song and Huanxin Chen,"ARTIFICIAL INTELLIGENCE IN SUSTAINABLE ENERGY INDUSTRY: STATUS QUO, CHALLENGES AND OPPORTUNITIES",article
181,19167,JOURNAL OF CLEANER PRODUCTION,journal,"09596526, 18791786","1,937",Q1,200,5126,10603,304498,103295,10577,"9,56","59,40",United Kingdom,Western Europe,Elsevier Ltd.,1993-2021,"Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)",JOURNAL OF CLEANER PRODUCTION,JOURNAL OF CLEANER PRODUCTION,"170,352",9.297,0.18299,"Self-reported life satisfaction of China's population has not improved as much as expected during the economic boom, which was accompanied by a significant decline in environmental performance. Is environmental pollution the culprit for the lagging subjective well-being? To explore this issue, this paper adopts the sentiment analysis method to construct a real-time daily subjective well-being metric at the city level based on the big data of online search traces. Using daily data from 13 Chinese cities centred on Beijing between August 2014 and December 2019, we look at the corelation between subjective well-being and air pollution and the heterogeneity in this relationship based on two separate identification strategies. We find that air pollutants are negatively correlated with subjective well-being, and well-being tends to decline more from pollution during hot seasons. In addition, residents in wealthier regions tend to be more sensitive to air pollution. This result may be explained by the differences in the subjective perception of air pollution and personal preferences at different levels of income. These findings provide information about the concerns of the public to the central government, thereby helping it take appropriate actions to respond to the dynamics of subjective well-being.",https://doi.org/10.1016/j.jclepro.2022.134380,2022,Lu Cheng and Zhifu Mi and Yi-Ming Wei and Shidong Wang and Klaus Hubacek,DIRTY SKIES LOWER SUBJECTIVE WELL-BEING,article
182,19167,JOURNAL OF CLEANER PRODUCTION,journal,"09596526, 18791786","1,937",Q1,200,5126,10603,304498,103295,10577,"9,56","59,40",United Kingdom,Western Europe,Elsevier Ltd.,1993-2021,"Environmental Science (miscellaneous) (Q1); Industrial and Manufacturing Engineering (Q1); Renewable Energy, Sustainability and the Environment (Q1); Strategy and Management (Q1)",JOURNAL OF CLEANER PRODUCTION,JOURNAL OF CLEANER PRODUCTION,"170,352",9.297,0.18299,"In the age of “Internet+”, many Internet service platforms (ISPs) in China have been widely introduced to the closed-loop supply chain (CLSC). To further study the role of the Internet service platform, this paper considers a CLSC composed of a manufacturer, a retailer and an Internet service platform who invests in research and development (R&D), advertising and Big Data marketing, and develops the goodwill dynamic model based on the differential game theory. The construction of a goodwill dynamic model has two purposes, namely, to increase sales and the return rate. The optimal decisions for 3 players under two different cooperative scenarios are obtained, namely, the retailer payment scenario (scenario D) and the manufacturer cost-sharing scenario (scenario S). The supply chain members gain more profit or achieve a higher level of goodwill for products under certain conditions, i.e., a high residual value from remanufacturing, a high sharing rate of residual value from the retailer's recycled products, and a low recycling cost. Interestingly, the wholesale price increases with the residual value of recycled products when goodwill effectiveness is low, while the price declines when goodwill effectiveness is high. After comparing two cooperative scenarios, the result shows that an Internet service platform will invest more in Big Data marketing under the manufacturer cost-sharing scenario, and cooperation between the manufacturer and the Internet service platform can help improve the goodwill of enterprises or products. Moreover, the manufacturer cost-sharing scenario is payoff-Pareto-improving in most cases through the coordination of a cost-sharing rate, and the effectiveness of Big Data marketing exerts a positive effect on goodwill and the development of the industry. In addition, the retailer has “free rider” tendencies in the manufacturer cost-sharing scenario. The results encourage more enterprises to enhance the value of goodwill through cooperation with Internet service platforms because Internet service platforms conveniently utilize Big Data marketing to increase the sales of products and the collecting rate of used products, which in turn helps environmental sustainability.",https://doi.org/10.1016/j.jclepro.2019.01.310,2019,Zehua Xiang and Minli Xu,DYNAMIC COOPERATION STRATEGIES OF THE CLOSED-LOOP SUPPLY CHAIN INVOLVING THE INTERNET SERVICE PLATFORM,article
183,21100318415,COMPUTATIONAL AND STRUCTURAL BIOTECHNOLOGY JOURNAL,journal,20010370,"1,908",Q1,45,357,255,28093,2022,255,"7,89","78,69",Sweden,Western Europe,Research Network of Computational and Structural Biotechnology,2012-2020,Biochemistry (Q1); Biophysics (Q1); Biotechnology (Q1); Computer Science Applications (Q1); Genetics (Q1); Structural Biology (Q2),COMPUTATIONAL AND STRUCTURAL BIOTECHNOLOGY JOURNAL,COMPUTATIONAL AND STRUCTURAL BIOTECHNOLOGY JOURNAL,"3,620",7.271,0.00677,"We review the current applications of artificial intelligence (AI) in functional genomics. The recent explosion of AI follows the remarkable achievements made possible by “deep learning”, along with a burst of “big data” that can meet its hunger. Biology is about to overthrow astronomy as the paradigmatic representative of big data producer. This has been made possible by huge advancements in the field of high throughput technologies, applied to determine how the individual components of a biological system work together to accomplish different processes. The disciplines contributing to this bulk of data are collectively known as functional genomics. They consist in studies of: i) the information contained in the DNA (genomics); ii) the modifications that DNA can reversibly undergo (epigenomics); iii) the RNA transcripts originated by a genome (transcriptomics); iv) the ensemble of chemical modifications decorating different types of RNA transcripts (epitranscriptomics); v) the products of protein-coding transcripts (proteomics); and vi) the small molecules produced from cell metabolism (metabolomics) present in an organism or system at a given time, in physiological or pathological conditions. After reviewing main applications of AI in functional genomics, we discuss important accompanying issues, including ethical, legal and economic issues and the importance of explainability.",https://doi.org/10.1016/j.csbj.2021.10.009,2021,Claudia Caudai and Antonella Galizia and Filippo Geraci and Loredana {Le Pera} and Veronica Morea and Emanuele Salerno and Allegra Via and Teresa Colombo,AI APPLICATIONS IN FUNCTIONAL GENOMICS,article
184,21100318415,COMPUTATIONAL AND STRUCTURAL BIOTECHNOLOGY JOURNAL,journal,20010370,"1,908",Q1,45,357,255,28093,2022,255,"7,89","78,69",Sweden,Western Europe,Research Network of Computational and Structural Biotechnology,2012-2020,Biochemistry (Q1); Biophysics (Q1); Biotechnology (Q1); Computer Science Applications (Q1); Genetics (Q1); Structural Biology (Q2),COMPUTATIONAL AND STRUCTURAL BIOTECHNOLOGY JOURNAL,COMPUTATIONAL AND STRUCTURAL BIOTECHNOLOGY JOURNAL,"3,620",7.271,0.00677,"Infectious diseases, including vector-borne diseases transmitted by arthropods, are a leading cause of morbidity and mortality worldwide. In the era of big data, addressing broad-scale, fundamental questions regarding the complex dynamics of these diseases will increasingly require the integration of diverse datasets to produce new biological knowledge. This review provides a current snapshot of the systematic assessment of the relationships between microbial pathogens, arthropod vectors and mammalian hosts using data mining and machine learning. We employ PRISMA to identify 32 key papers relevant to this topic. Our analysis shows an increasing use of data mining and machine learning tasks and techniques, including prediction, classification, clustering, association rules mining, and deep learning, over the last decade. However, it also reveals a number of critical challenges in applying these to the study of vector-host-pathogen interactions at various systems biology levels. Here, relevant studies, current limitations and future directions are discussed. Furthermore, the quality of data in relevant papers was assessed using the FAIR (Findable, Accessible, Interoperable, Reusable) compliance criteria to evaluate and encourage reproducibility and shareability of research outcomes. Although shortcomings in their application remain, data mining and machine learning have significant potential to break new ground in understanding fundamental aspects of vector-host-pathogen relationships and their application in this field should be encouraged. In particular, while predictive modeling, feature engineering and supervised machine learning are already being used in the field, other data mining and machine learning methods such as deep learning and association rules analysis lag behind and should be implemented in combination with established methods to accelerate hypothesis and knowledge generation in the domain.",https://doi.org/10.1016/j.csbj.2020.06.031,2020,Diing D.M. Agany and Jose E. Pietri and Etienne Z. Gnimpieba,ASSESSMENT OF VECTOR-HOST-PATHOGEN RELATIONSHIPS USING DATA MINING AND MACHINE LEARNING,article
185,21100318415,COMPUTATIONAL AND STRUCTURAL BIOTECHNOLOGY JOURNAL,journal,20010370,"1,908",Q1,45,357,255,28093,2022,255,"7,89","78,69",Sweden,Western Europe,Research Network of Computational and Structural Biotechnology,2012-2020,Biochemistry (Q1); Biophysics (Q1); Biotechnology (Q1); Computer Science Applications (Q1); Genetics (Q1); Structural Biology (Q2),COMPUTATIONAL AND STRUCTURAL BIOTECHNOLOGY JOURNAL,COMPUTATIONAL AND STRUCTURAL BIOTECHNOLOGY JOURNAL,"3,620",7.271,0.00677,"Big Data pervades nearly all areas of life sciences, yet the analysis of large integrated data sets remains a major challenge. Moreover, the field of life sciences is highly fragmented and, consequently, so is its data, knowledge, and standards. This, in turn, makes integrated data analysis and knowledge gathering across sub-fields a demanding task. At the same time, the integration of various research angles and data types is crucial for modelling the complexity of organisms and biological processes in a holistic manner. This is especially valid in the context of drug development and chemical safety assessment where computational methods can provide solutions for the urgent need of fast, effective, and sustainable approaches. At the same time, such computational methods require the development of methodologies suitable for an integrated and data centred Big Data view. Here we discuss Knowledge Graphs (KG) as a solution to a data centred analysis approach for drug and chemical development and safety assessment. KGs are knowledge bases, data analysis engines, and knowledge discovery systems all in one, allowing them to be used from simple data retrieval, over meta-analysis to complex predictive and knowledge discovery systems. Therefore, KGs have immense potential to advance the data centred approach, the re-usability, and informativity of data. Furthermore, they can improve the power of analysis, and the complexity of modelled processes, all while providing knowledge in a natively human understandable network data model.",https://doi.org/10.1016/j.csbj.2022.08.061,2022,Alisa Pavel and Laura A. Saarimäki and Lena Möbus and Antonio Federico and Angela Serra and Dario Greco,THE POTENTIAL OF A DATA CENTRED APPROACH & KNOWLEDGE GRAPH DATA REPRESENTATION IN CHEMICAL SAFETY AND DRUG DESIGN,article
186,17876,RADIOTHERAPY AND ONCOLOGY,journal,"18790887, 01678140","1,892",Q1,157,470,1026,14846,5131,959,"5,14","31,59",Ireland,Western Europe,Elsevier Ireland Ltd,1983-2020,"Hematology (Q1); Oncology (Q1); Radiology, Nuclear Medicine and Imaging (Q1)",RADIOTHERAPY AND ONCOLOGY,RADIOTHERAPY AND ONCOLOGY,"22,462",6.280,0.02494,"Big data are no longer an obstacle; now, by using artificial intelligence (AI), previously undiscovered knowledge can be found in massive data collections. The radiation oncology clinic daily produces a large amount of multisource data and metadata during its routine clinical and research activities. These data involve multiple stakeholders and users. Because of a lack of interoperability, most of these data remain unused, and powerful insights that could improve patient care are lost. Changing the paradigm by introducing powerful AI analytics and a common vision for empowering big data in radiation oncology is imperative. However, this can only be achieved by creating a clinical data science community in radiation oncology. In this work, we present why such a community is needed to translate multisource data into clinical decision aids.",https://doi.org/10.1016/j.radonc.2020.09.054,2020,Joanna Kazmierska and Andrew Hope and Emiliano Spezi and Sam Beddar and William H. Nailon and Biche Osong and Anshu Ankolekar and Ananya Choudhury and Andre Dekker and Kathrine Røe Redalen and Alberto Traverso,FROM MULTISOURCE DATA TO CLINICAL DECISION AIDS IN RADIATION ONCOLOGY: THE NEED FOR A CLINICAL DATA SCIENCE COMMUNITY,article
187,22377,VALUE IN HEALTH,journal,"10983015, 15244733","1,859",Q1,103,211,572,8179,2291,532,"3,67","38,76",United Kingdom,Western Europe,Elsevier Ltd.,1998-2020,"Health Policy (Q1); Medicine (miscellaneous) (Q1); Public Health, Environmental and Occupational Health (Q1)",VALUE IN HEALTH,VALUE IN HEALTH,"12,642",5.725,0.01786,,https://doi.org/10.1016/j.jval.2021.11.1002,2022,A Miracolo and M Mills and P Kanavos,POSB319 PREDICTIVE ANALYTIC TECHNIQUES AND BIG DATA FOR IMPROVED HEALTH OUTCOMES IN THE CONTEXT OF VALUE BASED HEALTH CARE AND COVERAGE DECISIONS: A SCOPING REVIEW,article
188,22377,VALUE IN HEALTH,journal,"10983015, 15244733","1,859",Q1,103,211,572,8179,2291,532,"3,67","38,76",United Kingdom,Western Europe,Elsevier Ltd.,1998-2020,"Health Policy (Q1); Medicine (miscellaneous) (Q1); Public Health, Environmental and Occupational Health (Q1)",VALUE IN HEALTH,VALUE IN HEALTH,"12,642",5.725,0.01786,"Next-generation sequencing (NGS) is considered to be a prominent example of “big data” because of the quantity and complexity of data it produces and because it presents an opportunity to use powerful information sources that could reduce clinical and health economic uncertainty at a patient level. One obstacle to translating NGS into routine health care has been a lack of clinical trials evaluating NGS technologies, which could be used to populate cost-effectiveness analyses (CEAs). A key question is whether big data can be used to partially support CEAs of NGS. This question has been brought into sharp focus with the creation of large national sequencing initiatives. In this article we summarize the main methodological and practical challenges of using big data as an input into CEAs of NGS. Our focus is on the challenges of using large observational datasets and cohort studies and linking these data to the genomic information obtained from NGS, as is being pursued in the conduct of large genomic sequencing initiatives. We propose potential solutions to these key challenges. We conclude that the use of genomic big data to support and inform CEAs of NGS technologies holds great promise. Nevertheless, health economists face substantial challenges when using these data and must be cognizant of them before big data can be confidently used to produce evidence on the cost-effectiveness of NGS.",https://doi.org/10.1016/j.jval.2018.06.016,2018,Sarah Wordsworth and Brett Doble and Katherine Payne and James Buchanan and Deborah A. Marshall and Christopher McCabe and Dean A. Regier,USING “BIG DATA” IN THE COST-EFFECTIVENESS ANALYSIS OF NEXT-GENERATION SEQUENCING TECHNOLOGIES: CHALLENGES AND POTENTIAL SOLUTIONS,article
189,22377,VALUE IN HEALTH,journal,"10983015, 15244733","1,859",Q1,103,211,572,8179,2291,532,"3,67","38,76",United Kingdom,Western Europe,Elsevier Ltd.,1998-2020,"Health Policy (Q1); Medicine (miscellaneous) (Q1); Public Health, Environmental and Occupational Health (Q1)",VALUE IN HEALTH,VALUE IN HEALTH,"12,642",5.725,0.01786,,https://doi.org/10.1016/j.jval.2019.04.303,2019,Z.G. Hui and Q. Guo and W.Z. Shi and M.C. Gong and C. Liu and H. Xu and H. Li,PCN181 THE NATIONAL CANCER BIG DATA PLATFORM OF CHINA: VISION AND STATUS,article
190,22350,INFECTIOUS DISEASE CLINICS OF NORTH AMERICA,journal,"15579824, 08915520","1,854",Q1,96,57,190,4523,1000,168,"5,20","79,35",United Kingdom,Western Europe,W.B. Saunders Ltd,1987-2020,Infectious Diseases (Q1); Microbiology (medical) (Q1),INFECTIOUS DISEASE CLINICS OF NORTH AMERICA,INFECTIOUS DISEASE CLINICS OF NORTH AMERICA,"4,090",5.982,0.00687,,https://doi.org/10.1016/j.idc.2019.05.009,2019,Aadia I. Rana and Michael J. Mugavero,HOW BIG DATA SCIENCE CAN IMPROVE LINKAGE AND RETENTION IN CARE,article
191,24931,AUTOMATION IN CONSTRUCTION,journal,09265805,"1,837",Q1,121,364,779,20472,7433,778,"9,16","56,24",Netherlands,Western Europe,Elsevier,1992-2020,Building and Construction (Q1); Civil and Structural Engineering (Q1); Control and Systems Engineering (Q1),AUTOMATION IN CONSTRUCTION,AUTOMATION IN CONSTRUCTION,"16,738",7.700,0.01309,"A building's strategic asset management (SAM) capability has traditionally been limited by its site-based management. With the emergence of needs from clients about delivering a long-term portfolio-based building asset management plan that minimizes the asset risk and optimizes the value of their asset portfolios, SAM Units have emerged as a new business form to provide various SAM services to their clients. However, the quality of their current data model is still hindered by many issues, such as missing important attributes and the lack of customized information flow guidance. In addition, there is a gap in integrating their existing data collection with various data sources and Building Information Modeling (BIM) to enhance their data quality. By evaluating a SAM Unit's portfolio case study, this paper identifies the factors limiting the quality of SAM Units' data model and develops a guide to integrating various data sources better. We develop a BIM-integrated portfolio-based SAM information flow framework and a detailed hierarchical portfolio-based non-geometric data structure. The proposed framework and data structure will help SAM professionals, building asset owners, and other facilities management professionals embrace the benefits of managing the portfolio-based SAM data.",https://doi.org/10.1016/j.autcon.2021.104070,2022,Zigeng Fang and Yan Liu and Qiuchen Lu and Michael Pitt and Sean Hanna and Zhichao Tian,BIM-INTEGRATED PORTFOLIO-BASED STRATEGIC ASSET DATA QUALITY MANAGEMENT,article
192,13120,SEMINARS IN ONCOLOGY,journal,"00937754, 15328708","1,812",Q1,133,51,150,2969,690,124,"4,21","58,22",United Kingdom,Western Europe,W.B. Saunders Ltd,1974-2020,Hematology (Q1); Oncology (Q1),SEMINARS IN ONCOLOGY,SEMINARS IN ONCOLOGY,"5,713",4.929,0.00455,"Pediatric cancer is a rare disease with a low annual incidence, which presents a significant challenge in being able to collect enough data to fuel clinical discoveries. Big data registry trials hold promise to advance the study of pediatric cancers by allowing for the combination of traditional randomized controlled trials with the power of larger cohort sizes. The emergence of big data resources and data-sharing initiatives are becoming transformative for pediatric cancer diagnosis and treatment. This review discusses the uses of big data in pediatric cancer, existing pediatric cancer registry initiatives and research, the challenges in harmonizing these data to improve accessibility for study, and building pediatric data commons and other important future endeavors.",https://doi.org/10.1053/j.seminoncol.2020.02.006,2020,Ajay Major and Suzanne M. Cox and Samuel L. Volchenboum,USING BIG DATA IN PEDIATRIC ONCOLOGY: CURRENT APPLICATIONS AND FUTURE DIRECTIONS,article
193,29295,JOURNAL OF TRANSPORT GEOGRAPHY,journal,09666923,"1,809",Q1,108,264,515,15444,2931,514,"5,21","58,50",United Kingdom,Western Europe,Elsevier BV,1993-2020,"Environmental Science (miscellaneous) (Q1); Geography, Planning and Development (Q1); Transportation (Q1)",JOURNAL OF TRANSPORT GEOGRAPHY,JOURNAL OF TRANSPORT GEOGRAPHY,"11,719",4.986,0.01053,"Accessibility metrics are gaining momentum in public transportation planning and policy-making. However, critical user experience issues such as crowding discomfort and travel time unreliability are still not considered in those accessibility indicators. This paper aims to apply a methodology to build spatiotemporal crowding data and estimate travel time variability in a congested public transport network to improve accessibility calculations. It relies on using multiple big data sources available in most transit systems such as smart card and automatic vehicle location (AVL) data. São Paulo, Brazil, is used as a case study to show the impact of crowding and travel time variability on accessibility to jobs. Our results evidence a population-weighted average reduction of 56.8% in accessibility to jobs in a regular workday morning peak due to crowding discomfort, as well as reductions of 6.2% due to travel time unreliability and 59.2% when both are combined. The findings of this study can be of invaluable help to public transport planners and policymakers, as they show the importance of including both aspects in accessibility indicators for better decision making. Despite some limitations due to data quality and consistency throughout the study period, the proposed approach offers a new way to leverage big data in public transport to enhance policy decisions.",https://doi.org/10.1016/j.jtrangeo.2020.102671,2020,Renato Arbex and Claudio B. Cunha,ESTIMATING THE INFLUENCE OF CROWDING AND TRAVEL TIME VARIABILITY ON ACCESSIBILITY TO JOBS IN A LARGE PUBLIC TRANSPORT NETWORK USING SMART CARD BIG DATA,article
194,29295,JOURNAL OF TRANSPORT GEOGRAPHY,journal,09666923,"1,809",Q1,108,264,515,15444,2931,514,"5,21","58,50",United Kingdom,Western Europe,Elsevier BV,1993-2020,"Environmental Science (miscellaneous) (Q1); Geography, Planning and Development (Q1); Transportation (Q1)",JOURNAL OF TRANSPORT GEOGRAPHY,JOURNAL OF TRANSPORT GEOGRAPHY,"11,719",4.986,0.01053,"This paper aims to estimate the causal effect of accidents on traffic congestion and vice versa. In order to identify both effects of this two-way relationship, I use dynamic panel data techniques and open access ‘big data’ of highway traffic and accidents in England for the period 2012–2014. The research design is based on the daily-and-hourly specific mean reversion pattern of highway traffic, which can be used to define a recurrent congestion benchmark. Using this benchmark, I am able to identify the causal effect of accidents on non-recurrent traffic congestion. A positive relationship between traffic congestion and road accidents would yield multiplicative benefits for policies that aim at reducing either of these issues. Additionally, I explore the duration of the effect of an accident on congestion, the ‘rubbernecking’ effect, as well as heterogeneous effects in the most congested highway segments. Then, I test the use of methods which employ the bulk of information in big data and other methods using a very reduced sample. In my application, both approaches produce similar results. Finally, I find a non-linear negative effect of traffic congestion on the probability of an accident.",https://doi.org/10.1016/j.jtrangeo.2017.10.006,2019,Ilias Pasidis,CONGESTION BY ACCIDENT? A TWO-WAY RELATIONSHIP FOR HIGHWAYS IN ENGLAND,article
195,29295,JOURNAL OF TRANSPORT GEOGRAPHY,journal,09666923,"1,809",Q1,108,264,515,15444,2931,514,"5,21","58,50",United Kingdom,Western Europe,Elsevier BV,1993-2020,"Environmental Science (miscellaneous) (Q1); Geography, Planning and Development (Q1); Transportation (Q1)",JOURNAL OF TRANSPORT GEOGRAPHY,JOURNAL OF TRANSPORT GEOGRAPHY,"11,719",4.986,0.01053,"This paper considers the implications of so-called ‘big data’ for the analysis, modelling and planning of transport systems. The primary conceptual focus is on the needs of the practical context of medium-term planning and decision-making, from which perspective the paper seeks to achieve three goals: (i) to try to identify what is truly ‘special’ about big data; (ii) to provoke debate on the future relationship between transport planning and big data; and (iii) to try to identify promising themes for research and application. Differences in the information that can be derived from the data compared to more traditional surveys are discussed, and the respects in which they may impact on the role of models in supporting transport planning and decision-making are identified. It is argued that, over time, changes to the nature of data may lead to significant differences in both modelling approaches and in the expectations placed upon them. Furthermore, it is suggested that the potential widespread availability of data to commercial actors and travellers will affect the performance of the transport systems themselves, which might be expected to have knock-on effects for planning functions. We conclude by proposing a series of research challenges that we believe need to be addressed and warn against adaptations based on minimising change from the status quo.",https://doi.org/10.1016/j.jtrangeo.2017.11.004,2019,Dave Milne and David Watling,BIG DATA AND UNDERSTANDING CHANGE IN THE CONTEXT OF PLANNING TRANSPORT SYSTEMS,article
196,21100907125,ISCIENCE,journal,25890042,"1,805",Q1,27,1073,724,65016,3676,717,"5,08","60,59",United States,Northern America,Elsevier Inc.,2018-2020,Multidisciplinary (Q1),ISCIENCE,ISCIENCE,"5,235",5.458,0.0123,"Summary
Early quantitative structure-activity relationship (QSAR) technologies have unsatisfactory versatility and accuracy in fields such as drug discovery because they are based on traditional machine learning and interpretive expert features. The development of Big Data and deep learning technologies significantly improve the processing of unstructured data and unleash the great potential of QSAR. Here we discuss the integration of wet experiments (which provide experimental data and reliable verification), molecular dynamics simulation (which provides mechanistic interpretation at the atomic/molecular levels), and machine learning (including deep learning) techniques to improve QSAR models. We first review the history of traditional QSAR and point out its problems. We then propose a better QSAR model characterized by a new iterative framework to integrate machine learning with disparate data input. Finally, we discuss the application of QSAR and machine learning to many practical research fields, including drug development and clinical trials.",https://doi.org/10.1016/j.isci.2021.103052,2021,Jiashun Mao and Javed Akhtar and Xiao Zhang and Liang Sun and Shenghui Guan and Xinyu Li and Guangming Chen and Jiaxin Liu and Hyeon-Nae Jeon and Min Sung Kim and Kyoung Tai No and Guanyu Wang,COMPREHENSIVE STRATEGIES OF MACHINE-LEARNING-BASED QUANTITATIVE STRUCTURE-ACTIVITY RELATIONSHIP MODELS,article
197,25349,SCIENCE OF THE TOTAL ENVIRONMENT,journal,"00489697, 18791026","1,795",Q1,244,6929,13278,455970,106837,13125,"7,96","65,81",Netherlands,Western Europe,Elsevier,"1970, 1972-2021",Environmental Chemistry (Q1); Environmental Engineering (Q1); Pollution (Q1); Waste Management and Disposal (Q1),SCIENCE OF THE TOTAL ENVIRONMENT,SCIENCE OF THE TOTAL ENVIRONMENT,"210,143",7.963,0.23082,"Light-duty gasoline vehicles (LDGVs) have made up >90 % of vehicle fleets in China since 2019, moreover, with a high annual growth rate (> 10 %) since 2017. Hence, accurate estimates of air pollutant emissions of these fast-changing LDGVs are vital for air quality management, human healthcare, and ecological protection. However, this issue is poorly quantified due to insufficient reserves of timely updated LDGV emission factors, which are dependent on real-world activity levels. Here we constructed a big dataset of explicit emission profiles (e.g., emission factors and accumulated mileages) for 159,051 LDGVs based on an official I/M database by matching real-time traffic dynamics via real-world traffic monitoring (e.g., traffic volumes and speeds). Consequently, we provide robust evidence that the emission factors of these LDGVs follow a clear heavy-tailed distribution. The top 10 % emitters contributed >60 % to the total fleet emissions, while the bottom 50 % contributed <10 %. Such emission factors were effectively reduced by 75.7–86.2 % as official emission standards upgraded gradually (i.e., from China 2 to China 5) within 13 years from 2004 to 2017. Nevertheless, such achievements would be offset once traffic congestion occurred. In the real world, the typical traffic congestions (i.e., vehicle speed <5 km/h) can lead to emissions 5– 9 times higher than those on non-congested roads (i.e., vehicle speed >50 km/h). These empirical analyses enabled us to propose future traffic scenarios that could harmonize emission standards and traffic congestion. Practical approaches on vehicle emission controls under realistic conditions are proposed, which would provide new insights for future urban vehicle emission management.",https://doi.org/10.1016/j.scitotenv.2022.157581,2022,Xue Chen and Linhui Jiang and Yan Xia and Lu Wang and Jianjie Ye and Tangyan Hou and Yibo Zhang and Mengying Li and Zhen Li and Zhe Song and Jiali Li and Yaping Jiang and Pengfei Li and Xiaoye Zhang and Yang Zhang and Daniel Rosenfeld and John H. Seinfeld and Shaocai Yu,QUANTIFYING ON-ROAD VEHICLE EMISSIONS DURING TRAFFIC CONGESTION USING UPDATED EMISSION FACTORS OF LIGHT-DUTY GASOLINE VEHICLES AND REAL-WORLD TRAFFIC MONITORING BIG DATA,article
198,25349,SCIENCE OF THE TOTAL ENVIRONMENT,journal,"00489697, 18791026","1,795",Q1,244,6929,13278,455970,106837,13125,"7,96","65,81",Netherlands,Western Europe,Elsevier,"1970, 1972-2021",Environmental Chemistry (Q1); Environmental Engineering (Q1); Pollution (Q1); Waste Management and Disposal (Q1),SCIENCE OF THE TOTAL ENVIRONMENT,SCIENCE OF THE TOTAL ENVIRONMENT,"210,143",7.963,0.23082,"Climate change and environmental management are issues of global concern. The advent of the era of Big Data has created a new research platform for the assessment of environmental governance and policies. However, little is known about Big Data application to climate change and environmental management research. This paper adopts bibliometric analysis in conjunction with network analysis to systematically evaluate the publications on carbon emissions and environmental management based on Big Data and Streaming Data using R package and VOSviewer software. The analysis involves 274 articles after rigorous screening and includes citation analysis, co-citation analysis, and co-word analysis. Main findings include (1) Carbon emissions and environmental management based on big data and streaming data is an emerging multidisciplinary research topic, which has been applied in the fields of computer science, supply chain design, transportation, carbon price assessment, environmental policy evaluation, and CO2 emissions reduction. (2) This field has attracted the attention of nations which are major contributors to the world economy. In particular, European and American scholars have made the main contributions to this topic, and Chinese researchers have also had great impact. (3) The research content of this topic is primarily divided into four categories, including empirical studies of specific industries, air pollution governance, technological innovation, and low-carbon transportation. Our findings suggest that future research should bring greater depth of practical and modeling analysis to environmental policy assessment based on Big Data.",https://doi.org/10.1016/j.scitotenv.2020.138984,2020,Yuan Su and Yanni Yu and Ning Zhang,CARBON EMISSIONS AND ENVIRONMENTAL MANAGEMENT BASED ON BIG DATA AND STREAMING DATA: A BIBLIOMETRIC ANALYSIS,article
199,25349,SCIENCE OF THE TOTAL ENVIRONMENT,journal,"00489697, 18791026","1,795",Q1,244,6929,13278,455970,106837,13125,"7,96","65,81",Netherlands,Western Europe,Elsevier,"1970, 1972-2021",Environmental Chemistry (Q1); Environmental Engineering (Q1); Pollution (Q1); Waste Management and Disposal (Q1),SCIENCE OF THE TOTAL ENVIRONMENT,SCIENCE OF THE TOTAL ENVIRONMENT,"210,143",7.963,0.23082,"Anthropogenic marine debris is a persistent threat to oceans, imposing risks to ecosystems and the communities they support. Whilst an understanding of marine debris risks is steadily advancing, monitoring at spatial and temporal scales relevant to management remains limited. Citizen science projects address this shortcoming but are often critiqued on data accuracy and potential bias in sampling efforts. Here we present 10-years of Australia's largest marine debris database - the Australian Marine Debris Initiative (AMDI), in which we perform systematic data filtering, test for differences between collecting groups, and report patterns in marine debris. We defined five stages of data filtering to address issues in data quality and to limit inference to ocean-facing sandy beaches. Significant differences were observed in the average accumulation of items between filtered and remaining data. Further, differences in sampling were compared between collecting groups at the same site (e.g., government, NGOs, and schools), where no significant differences were observed. The filtering process removed 21% of events due to data quality issues and a further 42% of events to restrict analyses to ocean-facing sandy beaches. The remaining 7275 events across 852 sites allowed for an assessment of debris patterns at an unprecedented spatial and temporal resolution. Hard plastics were the most common material found on beaches both nationally and regionally, consisting of up to 75% of total debris. Nationally, land and sea-sourced items accounted for 48% and 7% of debris, respectively, with most debris found on the east coast of Australia. This study demonstrates the value of citizen science datasets with broad spatial and temporal coverage, and the importance of data filtering to improve data quality. The citizen science presented provides an understanding of debris patterns on Australia's ocean beaches and can serve as a foundation for future source reduction plans.",https://doi.org/10.1016/j.scitotenv.2021.150742,2022,Jordan Gacutan and Emma L. Johnston and Heidi Tait and Wally Smith and Graeme F. Clark,CONTINENTAL PATTERNS IN MARINE DEBRIS REVEALED BY A DECADE OF CITIZEN SCIENCE,article
200,25349,SCIENCE OF THE TOTAL ENVIRONMENT,journal,"00489697, 18791026","1,795",Q1,244,6929,13278,455970,106837,13125,"7,96","65,81",Netherlands,Western Europe,Elsevier,"1970, 1972-2021",Environmental Chemistry (Q1); Environmental Engineering (Q1); Pollution (Q1); Waste Management and Disposal (Q1),SCIENCE OF THE TOTAL ENVIRONMENT,SCIENCE OF THE TOTAL ENVIRONMENT,"210,143",7.963,0.23082,"Remote sensing image products (e.g. brightness of nighttime lights and land cover/land use types) have been widely used to disaggregate census data to produce gridded population maps for large geographic areas. The advent of the geospatial big data revolution has created additional opportunities to map population distributions at fine resolutions with high accuracy. A considerable proportion of the geospatial data contains semantic information that indicates different categories of human activities occurring at exact geographic locations. Such information is often lacking in remote sensing data. In addition, the remarkable progress in machine learning provides toolkits for demographers to model complex nonlinear correlations between population and heterogeneous geographic covariates. In this study, a typical type of geospatial big data, points-of-interest (POIs), was combined with multi-source remote sensing data in a random forests model to disaggregate the 2010 county-level census population data to 100 × 100 m grids. Compared with the WorldPop population dataset, our population map showed higher accuracy. The root mean square error for population estimates in Beijing, Shanghai, Guangzhou, and Chongqing for this method and WorldPop were 27,829 and 34,193, respectively. The large under-allocation of the population in urban areas and over-allocation in rural areas in the WorldPop dataset was greatly reduced in this new population map. Apart from revealing the effectiveness of POIs in improving population mapping, this study promises the potential of geospatial big data for mapping other socioeconomic parameters in the future.",https://doi.org/10.1016/j.scitotenv.2018.12.276,2019,Tingting Ye and Naizhuo Zhao and Xuchao Yang and Zutao Ouyang and Xiaoping Liu and Qian Chen and Kejia Hu and Wenze Yue and Jiaguo Qi and Zhansheng Li and Peng Jia,IMPROVED POPULATION MAPPING FOR CHINA USING REMOTELY SENSED AND POINTS-OF-INTEREST DATA WITHIN A RANDOM FORESTS MODEL,article
201,25349,SCIENCE OF THE TOTAL ENVIRONMENT,journal,"00489697, 18791026","1,795",Q1,244,6929,13278,455970,106837,13125,"7,96","65,81",Netherlands,Western Europe,Elsevier,"1970, 1972-2021",Environmental Chemistry (Q1); Environmental Engineering (Q1); Pollution (Q1); Waste Management and Disposal (Q1),SCIENCE OF THE TOTAL ENVIRONMENT,SCIENCE OF THE TOTAL ENVIRONMENT,"210,143",7.963,0.23082,"Insurance plays a crucial role in human efforts to adapt to environmental hazards. Effective insurance can serve as both a measure to distribute, and a method to communicate risk. In order for insurance to fulfil these roles successfully, policy pricing and cover choices must be risk-based and founded on accurate information. This is reliant on a robust evidence base forming the foundation of policy choices. This paper focuses on the evidence available to insurers and emergent innovation in the use of data. The main risk considered is coastal flooding, for which the insurance sector offers an option for potential adaptation, capable of increasing resilience. However, inadequate supply and analysis of data have been highlighted as factors preventing insurance from fulfilling this role. Research was undertaken to evaluate how data are currently, and could potentially, be used within risk evaluations for the insurance industry. This comprised of 50 interviews with those working and associated with the London insurance market. The research reveals new opportunities, which could facilitate improvements in risk-reflective pricing of policies. These relate to a new generation of data collection techniques and analytics, such as those associated with satellite-derived data, IoT (Internet of Things) sensors, cloud computing, and Big Data solutions. Such technologies present opportunities to reduce moral hazard through basing predictions and pricing of risk on large empirical datasets. The value of insurers' claims data is also revealed, and is shown to have the potential to refine, calibrate, and validate models and methods. The adoption of such data-driven techniques could enable insurers to re-evaluate risk ratings, and in some instances, extend coverage to locations and developments, previously rated as too high a risk to insure. Conversely, other areas may be revealed more vulnerable, which could generate negative impacts for residents in these regions, such as increased premiums. However, the enhanced risk awareness generated, by new technology, data and data analytics, could positively alter future planning, development and investment decisions.",https://doi.org/10.1016/j.scitotenv.2019.01.114,2019,Alexander G. Rumson and Stephen H. Hallett,INNOVATIONS IN THE USE OF DATA FACILITATING INSURANCE AS A RESILIENCE MECHANISM FOR COASTAL FLOOD RISK,article
202,25349,SCIENCE OF THE TOTAL ENVIRONMENT,journal,"00489697, 18791026","1,795",Q1,244,6929,13278,455970,106837,13125,"7,96","65,81",Netherlands,Western Europe,Elsevier,"1970, 1972-2021",Environmental Chemistry (Q1); Environmental Engineering (Q1); Pollution (Q1); Waste Management and Disposal (Q1),SCIENCE OF THE TOTAL ENVIRONMENT,SCIENCE OF THE TOTAL ENVIRONMENT,"210,143",7.963,0.23082,"An accurate characterization of spatial-temporal emission patterns and speciation of volatile organic compounds (VOCs) for multiple chemical mechanisms is important to improving the air quality ensemble modeling. In this study, we developed a 2017-based high-resolution (3 km × 3 km) model-ready emission inventory for Guangdong Province (GD) by updating estimation methods, emission factors, activity data, and allocation profiles. In particular, a full-localized speciation profile dataset mapped to five chemical mechanisms was developed to promote the determination of VOC speciation, and two dynamic approaches based on big data were used to improve the estimation of ship emissions and open fire biomass burning (OFBB). Compared with previous emissions, more VOC emissions were classified as oxygenated volatile organic compound (OVOC) species, and their contributions to the total ozone formation potential (OFP) in the Pearl River Delta (PRD) region increased by 17%. Formaldehyde became the largest OFP species in GD, accounting for 11.6% of the total OFP, indicating that the model-ready emission inventory developed in this study is more reactive. The high spatial-temporal variability of ship sources and OFBB, which were previously underestimated, was also captured by using big data. Ship emissions during typhoon days and holidays decreased by 23–55%. 95% of OFBB emissions were concentrated in 9% of the GD area and 31% of the days in 2017, demonstrating their strong spatial-temporal variability. In addition, this study revealed that GD emissions have changed rapidly in recent years due to the leap-forward control measures implemented, and thus, they needed to be updated regularly. All of these updates led to a 5–17% decrease in the emission uncertainty for most pollutants. The results of this study provide a reference for how to reduce uncertainties in developing model-ready emission inventories.",https://doi.org/10.1016/j.scitotenv.2020.144535,2021,Zhijiong Huang and Zhuangmin Zhong and Qinge Sha and Yuanqian Xu and Zhiwei Zhang and Lili Wu and Yuzheng Wang and Lihang Zhang and Xiaozhen Cui and MingShuang Tang and Bowen Shi and Chuanzeng Zheng and Zhen Li and Mingming Hu and Linlin Bi and Junyu Zheng and Min Yan,AN UPDATED MODEL-READY EMISSION INVENTORY FOR GUANGDONG PROVINCE BY INCORPORATING BIG DATA AND MAPPING ONTO MULTIPLE CHEMICAL MECHANISMS,article
203,27843,ANNUAL REVIEWS IN CONTROL,journal,13675788,"1,780",Q1,80,55,145,5080,1435,132,"9,15","92,36",United Kingdom,Western Europe,Elsevier Ltd.,1996-2020,Control and Systems Engineering (Q1); Software (Q1),ANNUAL REVIEWS IN CONTROL,ANNUAL REVIEWS IN CONTROL,"2,756",6.091,0.0039,"Industrial process data are usually mixed with missing data and outliers which can greatly affect the statistical explanation abilities for traditional data-driven modeling methods. In this sense, more attention should be paid on robust data mining methods so as to investigate those stable and reliable modeling prototypes for decision-making. This paper gives a systematic review of various state-of-the-art data preprocessing tricks as well as robust principal component analysis methods for process understanding and monitoring applications. Afterwards, comprehensive robust techniques have been discussed for various circumstances with diverse process characteristics. Finally, big data perspectives on potential challenges and opportunities have been highlighted for future explorations in the community.",https://doi.org/10.1016/j.arcontrol.2018.09.003,2018,Jinlin Zhu and Zhiqiang Ge and Zhihuan Song and Furong Gao,REVIEW AND BIG DATA PERSPECTIVES ON ROBUST DATA MINING APPROACHES FOR INDUSTRIAL PROCESS MODELING WITH OUTLIERS AND MISSING DATA,article
204,21196,DRUG DISCOVERY TODAY,journal,"18785832, 13596446","1,778",Q1,175,258,667,19752,5142,630,"7,17","76,56",United Kingdom,Western Europe,Elsevier Ltd.,1996-2020,Drug Discovery (Q1); Pharmacology (Q1),DRUG DISCOVERY TODAY,DRUG DISCOVERY TODAY,"18,695",7.851,0.0174,"The objective of this paper is to identify the extent to which real world data (RWD) is being utilized, or could be utilized, at scale in drug development. Through screening peer-reviewed literature, we have cited specific examples where RWD can be used for biomarker discovery or validation, gaining a new understanding of a disease or disease associations, discovering new markers for patient stratification and targeted therapies, new markers for identifying persons with a disease, and pharmacovigilance. None of the papers meeting our criteria was specifically geared toward novel targets or indications in the biopharmaceutical sector; the majority were focused on the area of public health, often sponsored by universities, insurance providers or in combination with public health bodies such as national insurers. The field is still in an early phase of practical application, and is being harnessed broadly where it serves the most direct need in public health applications in early, rare and novel disease incidents. However, these exemplars provide a valuable contribution to insights on the use of RWD to create novel, faster and less invasive approaches to advance disease understanding and biomarker discovery. We believe that pharma needs to invest in making better use of Electronic Health Records and the need for more precompetitive collaboration to grow the scale of this ‘big denominator’ capability, especially given the needs of precision medicine research.",https://doi.org/10.1016/j.drudis.2017.12.002,2018,Gurparkash Singh and Duane Schulthess and Nigel Hughes and Bart Vannieuwenhuyse and Dipak Kalra,REAL WORLD BIG DATA FOR CLINICAL RESEARCH AND DRUG DEVELOPMENT,article
205,21196,DRUG DISCOVERY TODAY,journal,"18785832, 13596446","1,778",Q1,175,258,667,19752,5142,630,"7,17","76,56",United Kingdom,Western Europe,Elsevier Ltd.,1996-2020,Drug Discovery (Q1); Pharmacology (Q1),DRUG DISCOVERY TODAY,DRUG DISCOVERY TODAY,"18,695",7.851,0.0174,"Advancing a new drug to market requires substantial investments in time as well as financial resources. Crucial bioactivities for drug candidates, including their efficacy, pharmacokinetics (PK), and adverse effects, need to be investigated during drug development. With advancements in chemical synthesis and biological screening technologies over the past decade, a large amount of biological data points for millions of small molecules have been generated and are stored in various databases. These accumulated data, combined with new machine learning (ML) approaches, such as deep learning, have shown great potential to provide insights into relevant chemical structures to predict in vitro, in vivo, and clinical outcomes, thereby advancing drug discovery and development in the big data era.",https://doi.org/10.1016/j.drudis.2020.07.005,2020,Linlin Zhao and Heather L. Ciallella and Lauren M. Aleksunes and Hao Zhu,ADVANCING COMPUTER-AIDED DRUG DISCOVERY (CADD) BY BIG DATA AND DATA-DRIVEN MACHINE LEARNING MODELING,article
206,16956,CITIES,journal,02642751,"1,771",Q1,90,419,732,28409,4866,723,"6,19","67,80",United Kingdom,Western Europe,Elsevier Ltd.,1983-2020,"Development (Q1); Sociology and Political Science (Q1); Tourism, Leisure and Hospitality Management (Q1); Urban Studies (Q1)",CITIES,CITIES,"11,076",5.835,0.01251,"While commerce is one of the key activities in cities, its spatial description still requires further attention, especially by considering the different dimensions of commercial space: physical, economic and socio-symbolic. The latter is becoming more and more important in an era where consumption is at the centre of social relations. Further, although data availability has been an enduring obstacle in commercial research, we are witnessing the advent of new data sources, and social-network big data is an opportunity to unveil the places to which consumers attribute prestige or symbolic capital, at the extent of entire metropolitan areas. This paper compares the physical, economic and socio-symbolic dimensions of commercial spaces through the analysis of three different commercial data sources: cadastral micro-data, business register and social-network big data. For the case of Madrid Metropolitan Area, the three databases are compared with correlation analysis and density maps, coming out as partly redundant and partly complementary. Getis-Ord's hotspot statistics integrated into a cluster analysis enable a comprehensive understanding of commercial environments, enriching previous spatial hierarchies. The spatial distribution of symbolic capital unveils a relation with socio-spatial segregation and paves the way to new reflections on the spatiality of consumption as a social practice.",https://doi.org/10.1016/j.cities.2020.102859,2020,José Carpio-Pinedo and Javier Gutiérrez,CONSUMPTION AND SYMBOLIC CAPITAL IN THE METROPOLITAN SPACE: INTEGRATING ‘OLD’ RETAIL DATA SOURCES WITH SOCIAL BIG DATA,article
207,16956,CITIES,journal,02642751,"1,771",Q1,90,419,732,28409,4866,723,"6,19","67,80",United Kingdom,Western Europe,Elsevier Ltd.,1983-2020,"Development (Q1); Sociology and Political Science (Q1); Tourism, Leisure and Hospitality Management (Q1); Urban Studies (Q1)",CITIES,CITIES,"11,076",5.835,0.01251,"Cities worldwide are attempting to transform themselves into smart cities. Recent cases and studies show that a key factor in this transformation is the use of urban big data from stakeholders and physical objects in cities. However, the knowledge and framework for data use for smart cities remain relatively unknown. This paper reports findings from an analysis of various use cases of big data in cities worldwide and the authors' four projects with government organizations toward developing smart cities. Specifically, this paper classifies the urban data use cases into four reference models and identifies six challenges in transforming data into information for smart cities. Furthermore, building upon the relevant literature, this paper proposes five considerations for addressing the challenges in implementing the reference models in real-world applications. The reference models, challenges, and considerations collectively form a framework for data use for smart cities. This paper will contribute to urban planning and policy development in the modern data-rich economy.",https://doi.org/10.1016/j.cities.2018.04.011,2018,Chiehyeon Lim and Kwang-Jae Kim and Paul P. Maglio,"SMART CITIES WITH BIG DATA: REFERENCE MODELS, CHALLENGES, AND CONSIDERATIONS",article
208,16956,CITIES,journal,02642751,"1,771",Q1,90,419,732,28409,4866,723,"6,19","67,80",United Kingdom,Western Europe,Elsevier Ltd.,1983-2020,"Development (Q1); Sociology and Political Science (Q1); Tourism, Leisure and Hospitality Management (Q1); Urban Studies (Q1)",CITIES,CITIES,"11,076",5.835,0.01251,"With current decentralization trends and polycentric planning efforts, the urban spatial structures of Chinese cities have been changing tremendously. To detect the true urban polycentric pattern of Chinese cities, this article analyzed the urban polycentricity characteristics of 294 cities. The natural cities were delineated by points of interest (POIs), and road networks constituted street blocks. Based on check-in data and new spatial units, centers within both metropolitan areas and central cities were identified and examined. We discovered that all Chinese cities have at least one natural city in their metropolitan areas because of rapid urban sprawl. Although a monocentric structure is still the most common urban spatial structure, 110 Chinese cities displayed different degrees of polycentricity at the metropolitan level. Many natural cities beyond central cities contribute to polycentric development at the metropolitan level. Central cities have maintained their original vitality and importance, most Chinese cities have dispersed urban structures in central cities, and 45 central cities are polycentric. The spatial structures in metropolitan areas are more polycentric than those in central cities. The only 36 cities with polycentric urban structures at both the metropolitan and central city levels are all national or regional central cities in eastern China.",https://doi.org/10.1016/j.cities.2021.103298,2021,Yongqiang Lv and Lin Zhou and Guobiao Yao and Xinqi Zheng,DETECTING THE TRUE URBAN POLYCENTRIC PATTERN OF CHINESE CITIES IN MORPHOLOGICAL DIMENSIONS: A MULTISCALE ANALYSIS BASED ON GEOSPATIAL BIG DATA,article
209,13121,SEMINARS IN RADIATION ONCOLOGY,journal,"10534296, 15329461","1,761",Q1,93,40,129,2575,654,118,"5,29","64,38",United Kingdom,Western Europe,W.B. Saunders Ltd,1991-2020,"Oncology (Q1); Radiology, Nuclear Medicine and Imaging (Q1); Cancer Research (Q2)",SEMINARS IN RADIATION ONCOLOGY,SEMINARS IN RADIATION ONCOLOGY,"2,837",5.934,0.00271,"In oncology, the term “big data” broadly describes the rapid acquisition and generation of massive amounts of information, typically from population cancer registries, electronic health records, or large-scale genetic sequencing studies. The challenge of using big data in cancer research lies in interdisciplinary collaboration and information processing to unify diverse data sources and provide valid analytics to harness meaningful information. This article provides an overview of how big data approaches can be applied in cancer research, and how they can be used to translate information into new ways to ultimately make informed decisions that improve cancer care and delivery.",https://doi.org/10.1016/j.semradonc.2019.05.002,2019,Chiaojung Jillian Tsai and Nadeem Riaz and Scarlett Lin Gomez,BIG DATA IN CANCER RESEARCH: REAL-WORLD RESOURCES FOR PRECISION ONCOLOGY TO IMPROVE CANCER CARE DELIVERY,article
210,13121,SEMINARS IN RADIATION ONCOLOGY,journal,"10534296, 15329461","1,761",Q1,93,40,129,2575,654,118,"5,29","64,38",United Kingdom,Western Europe,W.B. Saunders Ltd,1991-2020,"Oncology (Q1); Radiology, Nuclear Medicine and Imaging (Q1); Cancer Research (Q2)",SEMINARS IN RADIATION ONCOLOGY,SEMINARS IN RADIATION ONCOLOGY,"2,837",5.934,0.00271,"The application of big data to the quality assurance of radiation therapy is multifaceted. Big data can be used to detect anomalies and suboptimal quality metrics through both statistical means and more advanced machine learning and artificial intelligence. The application of these methods to clinical practice is discussed through examples of guideline adherence, contour integrity, treatment delivery mechanics, and treatment plan quality. The ultimate goal is to apply big data methods to direct measures of patient outcomes for care quality. The era of big data and machine learning is maturing and the implementation for quality assurance promises to improve the quality of care for patients.",https://doi.org/10.1016/j.semradonc.2019.05.006,2019,Todd R. McNutt and Kevin L. Moore and Binbin Wu and Jean L. Wright,USE OF BIG DATA FOR QUALITY ASSURANCE IN RADIATION THERAPY,article
211,29359,ENERGY AND BUILDINGS,journal,03787788,"1,737",Q1,184,705,2402,39338,15832,2394,"6,33","55,80",Netherlands,Western Europe,Elsevier BV,"1970, 1977-2020",Building and Construction (Q1); Civil and Structural Engineering (Q1); Electrical and Electronic Engineering (Q1); Mechanical Engineering (Q1),ENERGY AND BUILDINGS,ENERGY AND BUILDINGS,"51,254",5.879,0.04387,"Scientific literature about building occupants’ behaviour and the related energy performance analyses document about several strategies to monitor window operation, including different sensors and data series lengths. In this framework, the primary goal of this study is to propose effective guidelines for minimum experiment durations and their reliability. A six-year-long database from a living laboratory was used as a benchmark; and a recursive strategy enabled to split it into more than 2,500 subsets, supporting two main steps. First, information theory concepts were used to calculate uncertainty and subsets’ divergence were compared to the full database. Second, the subsets were used to train deep neural networks and evaluate the influence of monitoring lengths combined with different kinds of environmental data (i.e. indoor or outdoor). From the information-theoretic metrics, the results support that indoor-related variables can reduce most of the uncertainty related to window operation. Besides, subsets influenced by autumn and winter diverge the most compared to the full database. Considering the modelling approach, the results demonstrated that by including indoor-related variables, higher shares of reliably-performing models were achieved, and smaller subsets were needed. Seasonality has also played a major role along these lines. As a consequence, the conclusions supported the feasibility of nine-month-long field studies, starting in summer or spring, when indoor and outdoor variables are monitored.",https://doi.org/10.1016/j.enbuild.2022.112197,2022,Mateus Bavaresco and Ioannis Kousis and Ilaria Pigliautile and Anna {Laura Pisello} and Cristina Piselli and Enedir Ghisi,ARE YEARS-LONG FIELD STUDIES ABOUT WINDOW OPERATION EFFICIENT? A DATA-DRIVEN APPROACH BASED ON INFORMATION THEORY AND DEEP LEARNING,article
212,29359,ENERGY AND BUILDINGS,journal,03787788,"1,737",Q1,184,705,2402,39338,15832,2394,"6,33","55,80",Netherlands,Western Europe,Elsevier BV,"1970, 1977-2020",Building and Construction (Q1); Civil and Structural Engineering (Q1); Electrical and Electronic Engineering (Q1); Mechanical Engineering (Q1),ENERGY AND BUILDINGS,ENERGY AND BUILDINGS,"51,254",5.879,0.04387,"This study applies big data mining, machine learning analysis technique and uses the Waikato Environment for Knowledge Analysis (WEKA) as a tool to discuss the convenience stores energy consumption performance in Taiwan which consists of (a). Influential factors of architectural space environment and geographical conditions; (b). Influential factors of management type; (c). Influential factors of business equipment; (d). Influential factors of local climatic conditions; (e). Influential factors of service area socioeconomic conditions. The survey data of 1,052 chain convenience stores belong to 7-Eleven, Family Mart and Hi-Life groups by Taiwan Architecture and Building Center (TABC) in 2014. The implicit knowledge will be explored in order to improve the traditional analysis technique which is unlikely to build a model for complex, inexact and uncertain dynamic energy consumption system for convenience stores. The analysis process comprises of (a). Problem definition and objective setting; (b). Data source selection; (c). Data collection; (d). Data preprocessing/preparation; (e). Data attributes selection; (f). Data mining and model construction; (g). Results analysis and evaluation; (h). Knowledge discovery and dissemination. The key factors influencing the convenience stores energy consumption and the influence intensity order can be explored by data attributes selection. The numerical prediction model for energy consumption is built by applying regression analysis and classification techniques. The optimization thresholds of various influential factors are obtained. The different cluster data are compared by using clustering analysis to verify the correlation between the factors influencing the convenience stores energy consumption characteristic. The implicit knowledge of energy consumption characteristic obtained by the aforesaid analysis can be used to (a). Provide the owners with accurate predicted energy consumption performance to optimize architectural space, business equipment and operations management mode; (b). The design planners can obtain the optimum design proposal of Cost Performance Ratio (C/P) by planning the thresholds of various key factors and the validation of prediction model; (c). Provide decision support for government energy and environment departments, to make energy saving and carbon emission reduction policies, in order to estimate and set the energy consumption scenarios of convenience store industry.",https://doi.org/10.1016/j.enbuild.2018.03.021,2018,Chung-Feng {Jeffrey Kuo} and Chieh-Hung Lin and Ming-Hao Lee,ANALYZE THE ENERGY CONSUMPTION CHARACTERISTICS AND AFFECTING FACTORS OF TAIWAN'S CONVENIENCE STORES-USING THE BIG DATA MINING APPROACH,article
213,29359,ENERGY AND BUILDINGS,journal,03787788,"1,737",Q1,184,705,2402,39338,15832,2394,"6,33","55,80",Netherlands,Western Europe,Elsevier BV,"1970, 1977-2020",Building and Construction (Q1); Civil and Structural Engineering (Q1); Electrical and Electronic Engineering (Q1); Mechanical Engineering (Q1),ENERGY AND BUILDINGS,ENERGY AND BUILDINGS,"51,254",5.879,0.04387,"The rapid development of building energy consumption monitoring platforms makes engineering data more diverse, which facilitates the goal of reducing emissions. It is increasingly acknowledged that data preprocessing deserves the same attention as intelligent algorithms. In this work, the data quality issue of the engineering big data from non-demonstration complexes in China are analyzed thoroughly, and the analysis is based on clustering-based algorithms. We can conclude that the data of the hourly power of equipment groups are quality and stable, which is suitable for the benchmark to check other data. The quality of the data about pipes is acceptable. The number of data types about cooling towers is less, and the quality is worse. Regarding other data, the quality is unstable, so researchers should deal with those case-by-case. According to the above analysis, we proposed a convenient, rule-based data preprocessing framework that utilizes the law of physics, ensuring the strong coupling of multi-variants. After the data preprocessing, these engineering data are more reliable and can be used to improve performance or train models. Additionally, the proposed framework is more suitable for preprocessing multi-variant engineering data.",https://doi.org/10.1016/j.enbuild.2022.112372,2022,Ruikai He and Tong Xiao and Shunian Qiu and Jiefan Gu and Minchen Wei and Peng Xu,A RULE-BASED DATA PREPROCESSING FRAMEWORK FOR CHILLER ROOMS INSPIRED BY THE ANALYSIS OF ENGINEERING BIG DATA,article
214,29359,ENERGY AND BUILDINGS,journal,03787788,"1,737",Q1,184,705,2402,39338,15832,2394,"6,33","55,80",Netherlands,Western Europe,Elsevier BV,"1970, 1977-2020",Building and Construction (Q1); Civil and Structural Engineering (Q1); Electrical and Electronic Engineering (Q1); Mechanical Engineering (Q1),ENERGY AND BUILDINGS,ENERGY AND BUILDINGS,"51,254",5.879,0.04387,"With buildings consuming nearly 40% of energy in developed countries, it is important to accurately estimate and understand the building energy efficiency in a city. A better understanding of building energy efficiency is beneficial for reducing overall household energy use and providing guidance for future housing improvement and retrofit. In this research, we propose a deep learning-based multi-source data fusion framework to estimate building energy efficiency. We consider the traditional factors associated with the building energy efficiency from the Energy Performance Certificate (EPC) for 160,000 properties (30,000 buildings) in Glasgow, UK (e.g., property structural attributes and morphological attributes), as well as the Google Street View (GSV) building façade images as a complement. We compare the performance improvements between our data-fusion framework with traditional morphological attributes and image-only models. The results show that including the building façade images from GSV, the overall model accuracy increases from 79.7% to 86.8%. A further investigation and explanation of the deep learning model are conducted to understand the relationships between building features and building energy efficiency by using SHapley Additive exPlanations (SHAP). Our research demonstrates the potential of using multi-source data in building energy efficiency prediction with high accuracy and short inference time. Our paper also helps understand building energy efficiency at the city level to help achieve the net-zero target by 2050.",https://doi.org/10.1016/j.enbuild.2022.112331,2022,Maoran Sun and Changyu Han and Quan Nie and Jingying Xu and Fan Zhang and Qunshan Zhao,UNDERSTANDING BUILDING ENERGY EFFICIENCY WITH ADMINISTRATIVE AND EMERGING URBAN BIG DATA BY DEEP LEARNING IN GLASGOW,article
215,29359,ENERGY AND BUILDINGS,journal,03787788,"1,737",Q1,184,705,2402,39338,15832,2394,"6,33","55,80",Netherlands,Western Europe,Elsevier BV,"1970, 1977-2020",Building and Construction (Q1); Civil and Structural Engineering (Q1); Electrical and Electronic Engineering (Q1); Mechanical Engineering (Q1),ENERGY AND BUILDINGS,ENERGY AND BUILDINGS,"51,254",5.879,0.04387,"Building operations account for the largest proportion of energy use throughout the building life cycle. The energy saving potential is considerable taking into account the existence of a wide variety of building operation deficiencies. The advancement in information technologies has made modern buildings to be not only energy-intensive, but also information-intensive. Massive amounts of building operational data, which are in essence the reflection of actual building operating conditions, are available for knowledge discovery. It is very promising to extract potentially useful insights from big building operational data, based on which actionable measures for energy efficiency enhancement are devised. Data mining is an advanced technology for analyzing big data. It consists of two main types of data analytics, i.e., supervised and unsupervised analytics. Despite of the power of supervised analytics in predictive modeling, unsupervised analytics are more practical and promising in discovering novel knowledge given limited prior knowledge. This paper provides a comprehensive review on the current utilization of unsupervised data analytics in mining massive building operational data. The commonly used unsupervised analytics are summarized according to their knowledge representations and applications. The challenges and opportunities are elaborated as guidance for future research in this multi-disciplinary field.",https://doi.org/10.1016/j.enbuild.2017.11.008,2018,Cheng Fan and Fu Xiao and Zhengdao Li and Jiayuan Wang,UNSUPERVISED DATA ANALYTICS IN MINING BIG BUILDING OPERATIONAL DATA FOR ENERGY EFFICIENCY ENHANCEMENT: A REVIEW,article
216,29359,ENERGY AND BUILDINGS,journal,03787788,"1,737",Q1,184,705,2402,39338,15832,2394,"6,33","55,80",Netherlands,Western Europe,Elsevier BV,"1970, 1977-2020",Building and Construction (Q1); Civil and Structural Engineering (Q1); Electrical and Electronic Engineering (Q1); Mechanical Engineering (Q1),ENERGY AND BUILDINGS,ENERGY AND BUILDINGS,"51,254",5.879,0.04387,"Improving the energy efficiency of the buildings is a worldwide hot topic nowadays. To assist comprehensive analysis and smart management, high-quality historical data records of the energy consumption is one of the key bases. However, the energy data records in the real world always contain different kinds of problems. The most common problem is missing data. It is also one of the most frequently reported data quality problems in big data/machine learning/deep learning related literature in energy management. However, limited studied have been conducted to comprehensively discuss different kinds of missing data situations, including random missing, continuous missing, and large proportionally missing. Also, the methods used in previous literature often rely on linear statistical methods or traditional machine learning methods. Limited study has explored the feasibility of advanced deep learning and transfer learning techniques in this problem. To this end, this study proposed a methodology, namely the hybrid Long Short Term Memory model with Bi-directional Imputation and Transfer Learning (LSTM-BIT). It integrates the powerful modeling ability of deep learning networks and flexible transferability of transfer learning. A case study on the electric consumption data of a campus lab building was utilized to test the method. Results show that LSTM-BIT outperforms other methods with 4.24% to 47.15% lower RMSE under different missing rates.",https://doi.org/10.1016/j.enbuild.2020.109941,2020,Jun Ma and Jack C.P. Cheng and Feifeng Jiang and Weiwei Chen and Mingzhu Wang and Chong Zhai,A BI-DIRECTIONAL MISSING DATA IMPUTATION SCHEME BASED ON LSTM AND TRANSFER LEARNING FOR BUILDING ENERGY DATA,article
217,29359,ENERGY AND BUILDINGS,journal,03787788,"1,737",Q1,184,705,2402,39338,15832,2394,"6,33","55,80",Netherlands,Western Europe,Elsevier BV,"1970, 1977-2020",Building and Construction (Q1); Civil and Structural Engineering (Q1); Electrical and Electronic Engineering (Q1); Mechanical Engineering (Q1),ENERGY AND BUILDINGS,ENERGY AND BUILDINGS,"51,254",5.879,0.04387,"With the advances of information technologies, today's building automation systems (BASs) are capable of managing building operational performance in an efficient and convenient way. Meanwhile, the amount of real-time monitoring and control data in BASs grows continually in the building lifecycle, which stimulates an intense demand for powerful big data analysis tools in BASs. Existing big data analytics adopted in the building automation industry focus on mining cross-sectional relationships, whereas the temporal relationships, i.e., the relationships over time, are usually overlooked. However, building operations are typically dynamic and BAS data are essentially multivariate time series data. This paper presents a time series data mining methodology for temporal knowledge discovery in big BAS data. A number of time series data mining techniques are explored and carefully assembled, including the Symbolic Aggregate approXimation (SAX), motif discovery, and temporal association rule mining. This study also develops two methods for the efficient post-processing of knowledge discovered. The methodology has been applied to analyze the BAS data retrieved from a real building. The temporal knowledge discovered is valuable to identify dynamics, patterns and anomalies in building operations, derive temporal association rules within and between subsystems, assess building system performance and spot opportunities in energy conservation.",https://doi.org/10.1016/j.enbuild.2015.09.060,2015,Cheng Fan and Fu Xiao and Henrik Madsen and Dan Wang,TEMPORAL KNOWLEDGE DISCOVERY IN BIG BAS DATA FOR BUILDING ENERGY MANAGEMENT,article
218,26874,BUILDING AND ENVIRONMENT,journal,03601323,"1,736",Q1,154,754,1692,43938,12000,1687,"6,90","58,27",United Kingdom,Western Europe,Elsevier BV,1976-2020,"Building and Construction (Q1); Civil and Structural Engineering (Q1); Environmental Engineering (Q1); Geography, Planning and Development (Q1)",BUILDING AND ENVIRONMENT,BUILDING AND ENVIRONMENT,"38,699",6.456,0.02925,"Life cycle assessment (LCA) and life cycle cost (LCC) are two primary methods used to assess the environmental and economic feasibility of building construction. An estimation of the building's life span is essential to carrying out these methods. However, given the diverse factors that affect the building's life span, it was estimated typically based on its main structural type. However, different buildings have different life spans. Simply assuming that all buildings with the same structural type follow an identical life span can cause serious estimation errors. In this study, we collected 1,812,700 records describing buildings built and demolished in South Korea, analysed the actual life span of each building, and developed a building life-span prediction model using deep-learning and traditional machine learning. The prediction models examined in this study produced root mean square errors of 3.72–4.6 and the coefficients of determination of 0.932–0.955. Among those models, a deep-learning based prediction model was found the most powerful. As anticipated, the conventional method of determining a building's life expectancy using a discrete set of specific factors and associated assumptions of life span did not yield realistic results. This study demonstrates that an application of deep learning to the LCA and LCC of a building is a promising direction, effectively guiding business planning and critical decision making throughout the construction process.",https://doi.org/10.1016/j.buildenv.2021.108267,2021,Sukwon Ji and Bumho Lee and Mun Yong Yi,BUILDING LIFE-SPAN PREDICTION FOR LIFE CYCLE ASSESSMENT AND LIFE CYCLE COST USING MACHINE LEARNING: A BIG DATA APPROACH,article
219,26874,BUILDING AND ENVIRONMENT,journal,03601323,"1,736",Q1,154,754,1692,43938,12000,1687,"6,90","58,27",United Kingdom,Western Europe,Elsevier BV,1976-2020,"Building and Construction (Q1); Civil and Structural Engineering (Q1); Environmental Engineering (Q1); Geography, Planning and Development (Q1)",BUILDING AND ENVIRONMENT,BUILDING AND ENVIRONMENT,"38,699",6.456,0.02925,"The proliferation of urban sensing, IoT, and big data in cities provides unprecedented opportunities for a deeper understanding of occupant behaviour and energy usage patterns at the urban scale. This enables data-driven building and energy models to capture the urban dynamics, specifically the intrinsic occupant and energy use behavioural profiles that are not usually considered in traditional models. Although there are related reviews, none investigated urban data for use in modelling occupant behaviour and energy use at multiple scales, from buildings to neighbourhood to city. This survey paper aims to fill this gap by providing a critical summary and analysis of the works reported in the literature. We present the different sources of occupant-centric urban data that are useful for data-driven modelling and categorise the range of applications and recent data-driven modelling techniques for urban behaviour and energy modelling, along with the traditional stochastic and simulation-based approaches. Finally, we present a set of recommendations for future directions in data-driven modelling of occupant behaviour and energy in buildings at the urban scale.",https://doi.org/10.1016/j.buildenv.2020.106964,2020,Flora D. Salim and Bing Dong and Mohamed Ouf and Qi Wang and Ilaria Pigliautile and Xuyuan Kang and Tianzhen Hong and Wenbo Wu and Yapan Liu and Shakila Khan Rumi and Mohammad Saiedur Rahaman and Jingjing An and Hengfang Deng and Wei Shao and Jakub Dziedzic and Fisayo Caleb Sangogboye and Mikkel Baun Kjærgaard and Meng Kong and Claudia Fabiani and Anna Laura Pisello and Da Yan,"MODELLING URBAN-SCALE OCCUPANT BEHAVIOUR, MOBILITY, AND ENERGY IN BUILDINGS: A SURVEY",article
220,15061,AGRICULTURAL SYSTEMS,journal,"0308521X, 18732267","1,694",Q1,107,197,511,12414,3229,503,"5,52","63,02",United Kingdom,Western Europe,Elsevier BV,1976-2020,Agronomy and Crop Science (Q1); Animal Science and Zoology (Q1),AGRICULTURAL SYSTEMS,AGRICULTURAL SYSTEMS,"9,779",5.370,0.00937,"The COVID-19 outbreak was an unprecedented situation that uncovered forgotten interconnections and interdependencies between agriculture, society, and economy, whereas it also brought to the fore the vulnerability of agrifood production to external disturbances. Building upon the ongoing experience of the COVID-19 pandemic, in this short communication, we discuss three potential mechanisms that, in our opinion, can mitigate the impacts of major crises or disasters in agriculture: resilience-promoting policies, community marketing schemes, and smart farming technology. We argue that resilience-promoting policies should focus on the development of crisis management plans and enhance farmers' capacity to cope with external disturbances. We also stress the need to promote community marketing conduits that ensure an income floor for farmers while in parallel facilitating consumer access to agrifood products when mainstream distribution channels under-serve them. Finally, we discuss some issues that need to be solved to ensure that smart technology and big data can help farmers overcome external shocks.",https://doi.org/10.1016/j.agsy.2020.103023,2021,Evagelos D. Lioutas and Chrysanthi Charatsari,ENHANCING THE ABILITY OF AGRICULTURE TO COPE WITH MAJOR CRISES OR DISASTERS: WHAT THE EXPERIENCE OF COVID-19 TEACHES US,article
221,15061,AGRICULTURAL SYSTEMS,journal,"0308521X, 18732267","1,694",Q1,107,197,511,12414,3229,503,"5,52","63,02",United Kingdom,Western Europe,Elsevier BV,1976-2020,Agronomy and Crop Science (Q1); Animal Science and Zoology (Q1),AGRICULTURAL SYSTEMS,AGRICULTURAL SYSTEMS,"9,779",5.370,0.00937,"Smart Farming is a development that emphasizes the use of information and communication technology in the cyber-physical farm management cycle. New technologies such as the Internet of Things and Cloud Computing are expected to leverage this development and introduce more robots and artificial intelligence in farming. This is encompassed by the phenomenon of Big Data, massive volumes of data with a wide variety that can be captured, analysed and used for decision-making. This review aims to gain insight into the state-of-the-art of Big Data applications in Smart Farming and identify the related socio-economic challenges to be addressed. Following a structured approach, a conceptual framework for analysis was developed that can also be used for future studies on this topic. The review shows that the scope of Big Data applications in Smart Farming goes beyond primary production; it is influencing the entire food supply chain. Big data are being used to provide predictive insights in farming operations, drive real-time operational decisions, and redesign business processes for game-changing business models. Several authors therefore suggest that Big Data will cause major shifts in roles and power relations among different players in current food supply chain networks. The landscape of stakeholders exhibits an interesting game between powerful tech companies, venture capitalists and often small start-ups and new entrants. At the same time there are several public institutions that publish open data, under the condition that the privacy of persons must be guaranteed. The future of Smart Farming may unravel in a continuum of two extreme scenarios: 1) closed, proprietary systems in which the farmer is part of a highly integrated food supply chain or 2) open, collaborative systems in which the farmer and every other stakeholder in the chain network is flexible in choosing business partners as well for the technology as for the food production side. The further development of data and application infrastructures (platforms and standards) and their institutional embedment will play a crucial role in the battle between these scenarios. From a socio-economic perspective, the authors propose to give research priority to organizational issues concerning governance issues and suitable business models for data sharing in different supply chain scenarios.",https://doi.org/10.1016/j.agsy.2017.01.023,2017,Sjaak Wolfert and Lan Ge and Cor Verdouw and Marc-Jeroen Bogaardt,BIG DATA IN SMART FARMING – A REVIEW,article
222,20838,TRANSPORT POLICY,journal,"1879310X, 0967070X","1,687",Q1,96,201,494,11238,2674,486,"5,12","55,91",United Kingdom,Western Europe,Elsevier Ltd.,1993-2020,"Geography, Planning and Development (Q1); Law (Q1); Transportation (Q1)",TRANSPORT POLICY,TRANSPORT POLICY,"9,048",4.674,0.0091,"This study sets out to assess whether there is a knowledge gap between the research frontier and the consultation business in how transport data are collected, managed and analysed. The consulting business plays an important role in applying data and methods as they typically carry out public tasks in various parts of the transport system, which are becoming more and more specialised. At the same time, big data has emerged with the promise to provide new, more and better information to help understand society and execute policies more efficiently – what we refer to as the data driven transition. We conduct a literature review to identify the state of the art within international research and compare this with results from interviews and with a survey sent to representatives from the Norwegian consultation business. We find that there is a considerable gap between international researchers and the consulting business within the entire process of collection, management and analysis of traffic data, and that this gap is increasing with the emergence of the data driven transition. Finally, we argue that the results are applicable to other countries as well. Action should be taken to keep the consultants up to speed, which will require efforts from several actors, including governmental agencies, the education institutions, the consulting business and researchers.",https://doi.org/10.1016/j.tranpol.2019.05.016,2019,Hanne Seter and Petter Arnesen and Odd André Hjelkrem,THE DATA DRIVEN TRANSPORT RESEARCH TRAIN IS LEAVING THE STATION. CONSULTANTS ALL ABOARD?,article
223,50089,JOURNAL OF HYDROLOGY,journal,00221694,"1,684",Q1,226,1336,2587,87508,15250,2563,"5,76","65,50",Netherlands,Western Europe,Elsevier,"1949, 1963-2020",Water Science and Technology (Q1),JOURNAL OF HYDROLOGY,JOURNAL OF HYDROLOGY,"73,620",5.722,0.04972,"Flood susceptibility assessment for identifying flood-prone areas plays a significant role in flood hazard mitigation. Machine learning is an optional assessment method because of its high objectivity and computational efficiency, but how to get enough and accurate information of historical flood locations to train the machine learning models has been a key problem. In recent years, news media data from both news websites and social media accounts has emerged as a promising source for natural science studies. However, the application of news media data in urban flood susceptibility assessment is still inadequate. This study proposed an approach to fill this gap. Firstly, flood locations were extracted from news media data based on a named entity recognition (NER) model. Then, a frequency or distance-based data quality control method was employed to improve the representativeness of the extracted flooded locations. Finally, flood conditioning factors with information of historical flood locations were input into a Support Vector Machine (SVM) model for flood susceptibility assessment. We took the central city of Dalian, China as a case study. The T-test results show that there was no significant difference between the distributions of most flood conditioning factors at the flood locations from the news media data and the official planning report. In the obtained flood susceptibility map, the high flood susceptibility areas got a recall of 90% compared with the high flood hazard areas in the planning report. Performing data quality control in the frequency-based method can improve the precision of the flood susceptibility map by up to 5%, while the distance-based method is ineffective. This study provides an example and offers the value of applying new data sources and modern deep learning techniques for urban flood management.",https://doi.org/10.1016/j.jhydrol.2022.128312,2022,Shengnan Fu and Heng Lyu and Ze Wang and Xin Hao and Chi Zhang,EXTRACTING HISTORICAL FLOOD LOCATIONS FROM NEWS MEDIA DATA BY THE NAMED ENTITY RECOGNITION (NER) MODEL TO ASSESS URBAN FLOOD SUSCEPTIBILITY,article
224,24443,CONSTRUCTION AND BUILDING MATERIALS,journal,09500618,"1,662",Q1,170,3583,7707,175521,52599,7705,"6,50","48,99",United Kingdom,Western Europe,Elsevier Ltd.,1987-2020,Building and Construction (Q1); Civil and Structural Engineering (Q1); Materials Science (miscellaneous) (Q1),CONSTRUCTION AND BUILDING MATERIALS,CONSTRUCTION AND BUILDING MATERIALS,"123,941",6.141,0.09849,"Intensive big data nanoindentation (BDNi) characterization was performed to reveal the cross-scale mechanical properties of, and hence distinguish the different phases in, inorganic–organic hybrid oilwell cement-elastomer composites, hydrothermally cured at 160 °C and 20 MPa for 28 days. Totally-three emulsified and particulate elastomers, including styrene-butadiene latex (SBL) emulsion (6, 12, and 14 wt.%), polypropylene (PP) powder (12 wt.%), and nitrile rubber (NR) powder (6 wt.%), and a weighting agent, hematite (50 wt.%), were used as additives to finely adjust the mechanical properties and microstructure of the hybrid composites, which were respectively examined by the BDNi and mercury intrusion porosimetry and scanning electron microscopy. BDNi data were statistically deconvoluted by the Gaussian mixture modeling (GMM) to discern mechanically distinct phases and their Young’s moduli and hardness at the micro/nano scale and the bulk composites’ properties at the macro scale. Results show that the SBL emulsion can be more homogeneously dispersed into the cement matrix, due to its emulsified soft consistency and hydrophilicity, resulting in the formation of soft coatings on, and softer infills intermixed with, the cement hydration products (CHPs). In contrast, the two hydrophobic, inert, particulate elastomers, PP and NR powders, only act as isolated soft inclusions embedded in the hydrated cement matrix. The NR melts at high temperatures and permeates into the pores of the cement matrix, leading to the formation of complex intervened micromorphology and hence functions better than the PP. All elastomers can effectively reduce the composites’ Young’s moduli: with increasing the elastomer contents, while the modulus of a BDNi-identified major CHP phase decreases from 20.9 to 11.3 GPa, the bulk composites’ counterpart from 17.3 to 10.7 GPa. The BDNi enables the identification of multiple mechanically distinct phases in the hybrid composites and quantification of the property changes of these phases.",https://doi.org/10.1016/j.conbuildmat.2022.129190,2022,Yucheng Li and Yunhu Lu and Li Liu and Shengmin Luo and Li He and Yongfeng Deng and Guoping Zhang,BIG DATA NANOINDENTATION CHARACTERIZATION OF CROSS-SCALE MECHANICAL PROPERTIES OF OILWELL CEMENT-ELASTOMER COMPOSITES,article
225,8000153138,COMPUTER SCIENCE REVIEW,journal,15740137,"1,646",Q1,44,54,61,7789,891,61,"12,49","144,24",Ireland,Western Europe,Elsevier Ireland Ltd,2007-2020,Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1),COMPUTER SCIENCE REVIEW,COMPUTER SCIENCE REVIEW,"1,249",7.872,0.00195,"The rapid growth of urban populations worldwide imposes new challenges on citizens’ daily lives, including environmental pollution, public security, road congestion, etc. New technologies have been developed to manage this rapid growth by developing smarter cities. Integrating the Internet of Things (IoT) in citizens’ lives enables the innovation of new intelligent services and applications that serve sectors around the city, including healthcare, surveillance, agriculture, etc. IoT devices and sensors generate large amounts of data that can be analyzed to gain valuable information and insights that help to enhance citizens’ quality of life. Deep Learning (DL), a new area of Artificial Intelligence (AI), has recently demonstrated the potential for increasing the efficiency and performance of IoT big data analytics. In this survey, we provide a review of the literature regarding the use of IoT and DL to develop smart cities. We begin by defining the IoT and listing the characteristics of IoT-generated big data. Then, we present the different computing infrastructures used for IoT big data analytics, which include cloud, fog, and edge computing. After that, we survey popular DL models and review the recent research that employs both IoT and DL to develop smart applications and services for smart cities. Finally, we outline the current challenges and issues faced during the development of smart city services.",https://doi.org/10.1016/j.cosrev.2020.100303,2020,Safa Ben Atitallah and Maha Driss and Wadii Boulila and Henda Ben Ghézala,LEVERAGING DEEP LEARNING AND IOT BIG DATA ANALYTICS TO SUPPORT THE SMART CITIES DEVELOPMENT: REVIEW AND FUTURE DIRECTIONS,article
226,8000153138,COMPUTER SCIENCE REVIEW,journal,15740137,"1,646",Q1,44,54,61,7789,891,61,"12,49","144,24",Ireland,Western Europe,Elsevier Ireland Ltd,2007-2020,Computer Science (miscellaneous) (Q1); Theoretical Computer Science (Q1),COMPUTER SCIENCE REVIEW,COMPUTER SCIENCE REVIEW,"1,249",7.872,0.00195,"This survey presents the concept of Big Data. Firstly, a definition and the features of Big Data are given. Secondly, the different steps for Big Data data processing and the main problems encountered in big data management are described. Next, a general overview of an architecture for handling it is depicted. Then, the problem of merging Big Data architecture in an already existing information system is discussed. Finally this survey tackles semantics (reasoning, coreference resolution, entity linking, information extraction, consolidation, paraphrase resolution, ontology alignment) in the Big Data context.",https://doi.org/10.1016/j.cosrev.2015.05.002,2015,Cheikh {Kacfah Emani} and Nadine Cullot and Christophe Nicolle,UNDERSTANDABLE BIG DATA: A SURVEY,article
227,19700194105,SUSTAINABLE CITIES AND SOCIETY,journal,22106707,"1,645",Q1,61,705,1286,43818,10974,1284,"8,53","62,15",Netherlands,Western Europe,Elsevier BV,2011-2020,"Civil and Structural Engineering (Q1); Geography, Planning and Development (Q1); Renewable Energy, Sustainability and the Environment (Q1); Transportation (Q1)",SUSTAINABLE CITIES AND SOCIETY,SUSTAINABLE CITIES AND SOCIETY,"14,373",7.587,0.01684,"Smart Sustainable Cities (SSC) consist of multiple stakeholders, who must cooperate in order for SSCs to be successful. Housing is an important challenge and in many cities, therefore, a key stakeholder are social housing organisations. This paper introduces a qualitative case study of a social housing provider in the UK who implemented a business intelligence project (a method to assess data networks within an organisation) to increase data quality and data interoperability. Our analysis suggests that creating pathways for different information systems within an organisation to ‘talk to’ each other is the first step. Some of the issues during the project implementation include the lack of training and development, organisational reluctance to change, and the lack of a project plan. The challenges faced by the organisation during this project can be helpful for those implementing SSCs. Currently, many SSC frameworks and models exist, yet most seem to neglect localised challenges faced by the different stakeholders. This paper hopes to help bridge this gap in the SSC research agenda.",https://doi.org/10.1016/j.scs.2018.02.015,2018,Caroline Duvier and P.B. Anand and Crina Oltean-Dumbrava,DATA QUALITY AND GOVERNANCE IN A UK SOCIAL HOUSING INITIATIVE: IMPLICATIONS FOR SMART SUSTAINABLE CITIES,article
228,19700194105,SUSTAINABLE CITIES AND SOCIETY,journal,22106707,"1,645",Q1,61,705,1286,43818,10974,1284,"8,53","62,15",Netherlands,Western Europe,Elsevier BV,2011-2020,"Civil and Structural Engineering (Q1); Geography, Planning and Development (Q1); Renewable Energy, Sustainability and the Environment (Q1); Transportation (Q1)",SUSTAINABLE CITIES AND SOCIETY,SUSTAINABLE CITIES AND SOCIETY,"14,373",7.587,0.01684,"Smart City and IoT improves the performance of health, transportation, energy and reduce the consumption of resources. Among the smart city services, Big Data analytics is one of the imperative technologies that have a vast perspective to reach sustainability, enhanced resilience, effective quality of life and quick management of resources. This paper focuses on the privacy of big data in the context of smart health to support smart cities. Furthermore, the trade-off between the data privacy and utility in big data analytics is the foremost concern for the stakeholders of a smart city. The majority of smart city application databases focus on preserving the privacy of individuals with different disease data. In this paper, we propose a trust-based hybrid data privacy approach named as “MIDR-Angelization” to assure privacy and utility in big data analytics when sharing same disease data of patients in IoT industry. Above all, this study suggests that privacy-preserving policies and practices to share disease and health information of patients having the same disease should consider detailed disease information to enhance data utility. An extensive experimental study performed on a real-world dataset to measure instance disclosure risk which shows that the proposed scheme outperforms its counterpart in terms of data utility and privacy.",https://doi.org/10.1016/j.scs.2018.04.014,2018,Adeel Anjum and Tahir Ahmed and Abid Khan and Naveed Ahmad and Mansoor Ahmad and Muhammad Asif and Alavalapati Goutham Reddy and Tanzila Saba and Nayma Farooq,PRIVACY PRESERVING DATA BY CONCEPTUALIZING SMART CITIES USING MIDR-ANGELIZATION,article
229,19700194105,SUSTAINABLE CITIES AND SOCIETY,journal,22106707,"1,645",Q1,61,705,1286,43818,10974,1284,"8,53","62,15",Netherlands,Western Europe,Elsevier BV,2011-2020,"Civil and Structural Engineering (Q1); Geography, Planning and Development (Q1); Renewable Energy, Sustainability and the Environment (Q1); Transportation (Q1)",SUSTAINABLE CITIES AND SOCIETY,SUSTAINABLE CITIES AND SOCIETY,"14,373",7.587,0.01684,"The effects of human activities and land cover changes on urban thermal field patterns are closely related to the land surface temperature (LST) and air temperature. At present, the number of studies on the quantitative relationship between these two indexes and the effect of the observational scale on their influence is insufficient. In this study, spatial analysis methods such as geographic modeling were combined with remote sensing images, meteorological data, and points of insert and used to investigate the composition and scale of the factors influencing the temperature field in Beijing. The results showed that there are differences in the positive and negative correlations between LST and air temperature and various influencing factors. At a spatial resolution of 90 m, LST had a strong linear relationship with the average air temperature. Indicators reflecting elements of human activity, such as buildings, roads, and entertainment, were easily measured by meteorological stations at a small scale, and the natural green space ratio could also be easily captured by satellite thermal sensors at small scales. These results have substantial implications for environmental impact assessments in areas experiencing an increasing urban heat island effect due to rapid urbanization.",https://doi.org/10.1016/j.scs.2020.102024,2020,Huang Huanchun and Yang Hailin and Deng Xin and Hao Cui and Liu Zhifeng and Liu Wei and Zeng Peng,ANALYZING THE INFLUENCING FACTORS OF URBAN THERMAL FIELD INTENSITY USING BIG-DATA-BASED GIS,article
230,19700194105,SUSTAINABLE CITIES AND SOCIETY,journal,22106707,"1,645",Q1,61,705,1286,43818,10974,1284,"8,53","62,15",Netherlands,Western Europe,Elsevier BV,2011-2020,"Civil and Structural Engineering (Q1); Geography, Planning and Development (Q1); Renewable Energy, Sustainability and the Environment (Q1); Transportation (Q1)",SUSTAINABLE CITIES AND SOCIETY,SUSTAINABLE CITIES AND SOCIETY,"14,373",7.587,0.01684,"Undoubtedly, sustainable development has inspired a generation of scholars and practitioners in different disciplines into a quest for the immense opportunities created by the development of sustainable urban forms for human settlements that will enable built environments to function in a more constructive and efficient way. However, there are still significant challenges that need to be addressed and overcome. The issue of such forms has been problematic and difficult to deal with, particularly in relation to the evaluation and improvement of their contribution to the goals of sustainable development. As it is an urban world where the informational and physical landscapes are increasingly being merged, sustainable urban forms need to embrace and leverage what current and future ICT has to offer as innovative solutions and sophisticated methods so as to thrive—i.e. advance their contribution to sustainability. The need for ICT of the new wave of computing to be embedded in such forms is underpinned by the recognition that urban sustainability applications are deemed of high relevance to the contemporary research agenda of computing and ICT. To unlock and exploit the underlying potential, the field of sustainable urban planning is required to extend its boundaries and broaden its horizons beyond the ambit of the built form of cities to include technological innovation opportunities. This paper explores and substantiates the real potential of ICT of the new wave of computing to evaluate and improve the contribution of sustainable urban forms to the goals of sustainable development. This entails merging big data and context-aware technologies and their applications with the typologies and design concepts of sustainable urban forms to achieve multiple hitherto unrealized goals. In doing so, this paper identifies models of smart sustainable city and their technologies and applications and models of sustainable urban form and their design concepts and typologies. In addition, it addresses the question of how these technologies and applications can be amalgamated with these design concepts and typologies in ways that ultimately evaluate and improve the contribution of sustainable urban forms to the goals of sustainable development. The overall aim of this paper suits a mix of three methodologies: literature review, thematic analysis, and secondary (qualitative) data analysis to achieve different but related objectives. The study identifies four technologies and two classes of applications pertaining to models of smart sustainable city as well as three design concepts and four typologies related to models of sustainable urban form. Finally, this paper proposes a Matrix to help scholars and planners in understanding and analyzing how and to what extent the contribution of sustainable urban forms to sustainability can be improved through ICT of the new wave of computing as to the underlying novel technologies and their applications, as well as a data-centric approach into investigating and evaluating this contribution and a simulation method for strategically optimizing it.",https://doi.org/10.1016/j.scs.2017.04.012,2017,Simon Elias Bibri and John Krogstie,ICT OF THE NEW WAVE OF COMPUTING FOR SUSTAINABLE URBAN FORMS: THEIR BIG DATA AND CONTEXT-AWARE AUGMENTED TYPOLOGIES AND DESIGN CONCEPTS,article
231,24657,CHEMOSPHERE,journal,00456535,"1,632",Q1,248,3039,6460,173496,46391,6428,"7,04","57,09",United Kingdom,Western Europe,Elsevier Ltd.,1972-2021,"Chemistry (miscellaneous) (Q1); Environmental Chemistry (Q1); Environmental Engineering (Q1); Health, Toxicology and Mutagenesis (Q1); Medicine (miscellaneous) (Q1); Pollution (Q1); Public Health, Environmental and Occupational Health (Q1)",CHEMOSPHERE,CHEMOSPHERE,"127,067",7.086,0.0979,"According to Eurostat, the EU production of chemicals hazardous to health reached 211 million tonnes in 2019. Thus, the possibility that some of these chemical compounds interact negatively with the human endocrine system has received, especially in the last decade, considerable attention from the scientific community. It is obvious that given the large number of chemical compounds it is impossible to use in vitro/in vivo tests for identifying all the possible toxic interactions of these chemicals and their metabolites. In addition, the poor availability of highly curated databases from which to retrieve and download the chemical, structure, and regulative information about all food contact chemicals has delayed the application of in silico methods. To overcome these problems, in this study we use robust computational approaches, based on a combination of highly curated databases and molecular docking, in order to screen all food contact chemicals against the nuclear receptor family in a cost and time-effective manner.",https://doi.org/10.1016/j.chemosphere.2021.133422,2022,Pietro Cozzini and Francesca Cavaliere and Giulia Spaggiari and Gianluca Morelli and Marco Riani,COMPUTATIONAL METHODS ON FOOD CONTACT CHEMICALS: BIG DATA AND IN SILICO SCREENING ON NUCLEAR RECEPTORS FAMILY,article
232,39563,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,journal,15698432,"1,623",Q1,98,16,523,1076,3348,520,"6,62","67,25",Netherlands,Western Europe,Elsevier,1998-2020,"Computers in Earth Sciences (Q1); Earth-Surface Processes (Q1); Global and Planetary Change (Q1); Management, Monitoring, Policy and Law (Q1)",INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,"11,556",5.933,0.01275,"Remote Sensing (RS) has been used in urban mapping for a long time; however, the complexity and diversity of urban functional patterns are difficult to be captured by RS only. Emerging Geospatial Big Data (GBD) are considered as the supplement to RS data, and help to contribute to our understanding of urban lands from physical aspects (i.e., urban land cover) to socioeconomic aspects (i.e., urban land use). Integrating RS and GBD could be an effective way to combine physical and socioeconomic aspects with great potential for high-quality urban land use classification. In this study, we reviewed the existing literature and focused on the state-of-the-art and perspective of the urban land use categorization by integrating RS and GBD. Specifically, the commonly used RS features (e.g., spectral, textural, temporal, and spatial features) and GBD features (e.g., spatial, temporal, semantic, and sequence features) were identified and analyzed in urban land use classification. The integration strategies for RS and GBD features were categorized into feature-level integration (FI) and decision-level integration (DI). To be more specific, the FI method integrates the RS and GBD features and classifies urban land use types using the integrated feature sets; the DI method processes RS and GBD independently and then merges the classification results based on decision rules. We also discussed other critical issues, including analysis unit setting, parcel segmentation, parcel labeling of land use types, and data integration. Our findings provide a retrospect of different features from RS and GBD, strategies of RS and GBD integration, and their pros and cons, which could help to define the framework for future urban land use mapping and better support urban planning, urban environment assessment, urban disaster monitoring and urban traffic analysis.",https://doi.org/10.1016/j.jag.2021.102514,2021,Jiadi Yin and Jinwei Dong and Nicholas A.S. Hamm and Zhichao Li and Jianghao Wang and Hanfa Xing and Ping Fu,INTEGRATING REMOTE SENSING AND GEOSPATIAL BIG DATA FOR URBAN LAND USE MAPPING: A REVIEW,article
233,39563,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,journal,15698432,"1,623",Q1,98,16,523,1076,3348,520,"6,62","67,25",Netherlands,Western Europe,Elsevier,1998-2020,"Computers in Earth Sciences (Q1); Earth-Surface Processes (Q1); Global and Planetary Change (Q1); Management, Monitoring, Policy and Law (Q1)",INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,"11,556",5.933,0.01275,"Urban Geography studies forms, social fabrics, and economic structures of cities from a geographic perspective. Catalysed by the increasingly abundant spatial big data, Urban Geography seeks new models and research paradigms to explain urban phenomena and address urban issues. Recent years have witnessed significant advances in spatially-explicit geospatial artificial intelligence (GeoAI), which integrates spatial studies and AI, primarily focusing on incorporating spatial thinking and concept into deep learning models for urban studies. This paper provides an overview of techniques and applications of spatially-explicit GeoAI in Urban Geography based on 581 papers identified using a systematic review approach. We examined and screened papers in three scopes of Urban Geography (Urban Dynamics, Social Differentiation of Urban Areas, and Social Sensing) and found that although GeoAI is a trending topic in geography and the applications of deep neural network-based methods are proliferating, the development of spatially-explicit GeoAI models is still at their early phase. We identified three challenges of existing models and advised future research direction towards developing multi-scale explainable spatially-explicit GeoAI. This review paper acquaints beginners with the basics of GeoAI and state-of-the-art and serve as an inspiration to attract more research in exploring the potential of spatially-explicit GeoAI in studying the socio-economic dimension of the city and urban life.",https://doi.org/10.1016/j.jag.2022.102936,2022,Pengyuan Liu and Filip Biljecki,A REVIEW OF SPATIALLY-EXPLICIT GEOAI APPLICATIONS IN URBAN GEOGRAPHY,article
234,18965,MOLECULAR PHYLOGENETICS AND EVOLUTION,journal,"10959513, 10557903","1,612",Q1,159,208,978,16985,3802,968,"3,89","81,66",United States,Northern America,Academic Press Inc.,1992-2020,"Ecology, Evolution, Behavior and Systematics (Q1); Genetics (Q1); Molecular Biology (Q2)",MOLECULAR PHYLOGENETICS AND EVOLUTION,MOLECULAR PHYLOGENETICS AND EVOLUTION,"22,497",4.286,0.02216,"Assessing support for molecular phylogenies is difficult because the data is heterogeneous in quality and overwhelming in quantity. Traditionally, node support values (bootstrap frequency, Bayesian posterior probability) are used to assess confidence in tree topologies. Other analyses to assess the quality of phylogenetic data (e.g. Lento plots, saturation plots, trait consistency) and the resulting phylogenetic trees (e.g. internode certainty, parameter permutation tests, topological tests) exist but are rarely applied. Here we argue that a single qualitative analysis is insufficient to assess support of a phylogenetic hypothesis and relate data quality to tree quality. We use six molecular markers to infer the phylogeny of Blattodea and apply various tests to assess relationship support, locus quality, and the relationship between the two. We use internode-certainty calculations in conjunction with bootstrap scores, alignment permutations, and an approximately unbiased (AU) test to assess if the molecular data unambiguously support the phylogenetic relationships found. Our results show higher support for the position of Lamproblattidae, high support for the termite phylogeny, and low support for the position of Anaplectidae, Corydioidea and phylogeny of Blaberoidea. We use Lento plots in conjunction with mutation-saturation plots, calculations of locus homoplasy to assess locus quality, identify long branch attraction, and decide if the tree’s relationships are the result of data biases. We conclude that multiple tests and metrics need to be taken into account to assess tree support and data robustness.",https://doi.org/10.1016/j.ympev.2018.05.007,2018,Dominic Evangelista and France Thouzé and Manpreet Kaur Kohli and Philippe Lopez and Frédéric Legendre,TOPOLOGICAL SUPPORT AND DATA QUALITY CAN ONLY BE ASSESSED THROUGH MULTIPLE TESTS IN REVIEWING BLATTODEA PHYLOGENY,article
235,5100155103,JOURNAL OF INFORMETRICS,journal,17511577,"1,605",Q1,76,79,276,3786,1696,245,"5,66","47,92",Netherlands,Western Europe,Elsevier BV,2007-2020,Applied Mathematics (Q1); Computer Science Applications (Q1); Library and Information Sciences (Q1); Management Science and Operations Research (Q1); Modeling and Simulation (Q1); Statistics and Probability (Q1),JOURNAL OF INFORMETRICS,JOURNAL OF INFORMETRICS,"4,326",5.107,0.00554,"This study presents a unique approach in investigating the knowledge diffusion structure for the field of data quality through an analysis of the main paths. We study a dataset of 1880 papers to explore the knowledge diffusion path, using citation data to build the citation network. The main paths are then investigated and visualized via social network analysis. This paper takes three different main path analyses, namely local, global, and key-route, to depict the knowledge diffusion path and additionally implements the g-index and h-index to evaluate the most important journals and researchers in the data quality domain.",https://doi.org/10.1016/j.joi.2014.05.001,2014,Yu Xiao and Louis Y.Y. Lu and John S. Liu and Zhili Zhou,KNOWLEDGE DIFFUSION PATH ANALYSIS OF DATA QUALITY LITERATURE: A MAIN PATH ANALYSIS,article
236,15576,EUROPEAN NEUROPSYCHOPHARMACOLOGY,journal,"0924977X, 18737862","1,603",Q1,112,154,383,9717,1610,371,"3,85","63,10",Netherlands,Western Europe,Elsevier,1990-2020,Neurology (Q1); Neurology (clinical) (Q1); Pharmacology (Q1); Pharmacology (medical) (Q1); Psychiatry and Mental Health (Q1); Biological Psychiatry (Q2),EUROPEAN NEUROPSYCHOPHARMACOLOGY,EUROPEAN NEUROPSYCHOPHARMACOLOGY,"8,999",4.600,0.01119,"Current limitations impeding on data reproducibility are often poor statistical design, underpowered studies, lack of robust data, lack of methodological detail, biased reporting and lack of open data sharing, coupled with wrong research incentives. To improve data reproducibility, robustness and quality for brain disease research, a Preclinical Data Forum Network was formed under the umbrella of the European College of Neuropsychopharmacology (ECNP). The goal of this network, members of which met for the first time in October 2014, is to establish a forum to collaborate in precompetitive space, to exchange and develop best practices, and to bring together the members from academia, pharmaceutical industry, publishers, journal editors, funding organizations, public/private partnerships and non-profit advocacy organizations. To address the most pertinent issues identified by the Network, it was decided to establish a data sharing platform that allows open exchange of information in the area of preclinical neuroscience and to develop an educational scientific program. It is also planned to reach out to other organizations to align initiatives to enhance efficiency, and to initiate activities to improve the clinical relevance of preclinical data. Those Network activities should contribute to scientific rigor and lead to robust and relevant translational data. Here we provide a synopsis of the proceedings from the inaugural meeting.",https://doi.org/10.1016/j.euroneuro.2015.05.011,2015,Thomas Steckler and Katja Brose and Magali Haas and Martien J. Kas and Elena Koustova and Anton Bespalov,THE PRECLINICAL DATA FORUM NETWORK: A NEW ECNP INITIATIVE TO IMPROVE DATA QUALITY AND ROBUSTNESS FOR (PRECLINICAL) NEUROSCIENCE,article
237,20063,BIOCHEMICAL PHARMACOLOGY,journal,"18732968, 00062952","1,595",Q1,198,513,993,40185,5594,983,"5,29","78,33",United States,Northern America,Elsevier Inc.,1958-2020,Biochemistry (Q1); Pharmacology (Q1),BIOCHEMICAL PHARMACOLOGY,BIOCHEMICAL PHARMACOLOGY,"33,633",5.858,0.01736,"The tremendous expansion of data analytics and public and private big datasets presents an important opportunity for pre-clinical drug discovery and development. In the field of life sciences, the growth of genetic, genomic, transcriptomic and proteomic data is partly driven by a rapid decline in experimental costs as biotechnology improves throughput, scalability, and speed. Yet far too many researchers tend to underestimate the challenges and consequences involving data integrity and quality standards. Given the effect of data integrity on scientific interpretation, these issues have significant implications during preclinical drug development. We describe standardized approaches for maximizing the utility of publicly available or privately generated biological data and address some of the common pitfalls. We also discuss the increasing interest to integrate and interpret cross-platform data. Principles outlined here should serve as a useful broad guide for existing analytical practices and pipelines and as a tool for developing additional insights into therapeutics using big data.",https://doi.org/10.1016/j.bcp.2018.03.014,2018,John F. Brothers and Matthew Ung and Renan Escalante-Chong and Jermaine Ross and Jenny Zhang and Yoonjeong Cha and Andrew Lysaght and Jason Funt and Rebecca Kusko,"INTEGRITY, STANDARDS, AND QC-RELATED ISSUES WITH BIG DATA IN PRE-CLINICAL DRUG DISCOVERY",article
238,24772,KNOWLEDGE-BASED SYSTEMS,journal,09507051,"1,587",Q1,121,716,1187,36777,11094,1181,"9,42","51,36",Netherlands,Western Europe,Elsevier,1987-2020,Artificial Intelligence (Q1); Information Systems and Management (Q1); Management Information Systems (Q1); Software (Q1),KNOWLEDGE-BASED SYSTEMS,KNOWLEDGE-BASED SYSTEMS,"22,261",8.038,0.02794,"Information extraction using distributed sensors has been widely used to obtain information knowledge from various regions or areas. Vehicle traffic data extraction is one of the ways to gather information in order to get the traffic condition information. This research intends to predict and visualize the traffic conditions in a particular road region. Traffic data was obtained from Department of Transport UK. These data are collected using hundreds of sensors for 24 h. Thus, the size of data is very huge. In order to get the behavior of the traffic condition, we need to analyze the huge dataset which was obtained from the sensors. The uses of conventional data mining methods are not sufficient to use, due to the process of knowledge building that should store data temporary in the memory. The fact that data is continuously becoming larger over time, therefore we need to find a method that could automatically adapt to process data in the form of streams. We use method called FIMT-DD (Fast Incremental Model Trees-Drift Detection) to analyze and predict the very large traffic dataset. Based on the prediction system that we have developed, we also visualize the prediction of traffic flow condition within generated sensor point in the real map simulation.",https://doi.org/10.1016/j.knosys.2015.10.028,2016,Ari Wibisono and Wisnu Jatmiko and Hanief Arief Wisesa and Benny Hardjono and Petrus Mursanto,TRAFFIC BIG DATA PREDICTION AND VISUALIZATION USING FAST INCREMENTAL MODEL TREES-DRIFT DETECTION (FIMT-DD),article
239,24772,KNOWLEDGE-BASED SYSTEMS,journal,09507051,"1,587",Q1,121,716,1187,36777,11094,1181,"9,42","51,36",Netherlands,Western Europe,Elsevier,1987-2020,Artificial Intelligence (Q1); Information Systems and Management (Q1); Management Information Systems (Q1); Software (Q1),KNOWLEDGE-BASED SYSTEMS,KNOWLEDGE-BASED SYSTEMS,"22,261",8.038,0.02794,"Modern applications of Big Data are transcending from being scalable solutions of data processing and analysis, to now provide advanced functionalities with the ability to exploit and understand the underpinning knowledge. This change is promoting the development of tools in the intersection of data processing, data analysis, knowledge extraction and management. In this paper, we propose TITAN, a software platform for managing all the life cycle of science workflows from deployment to execution in the context of Big Data applications. This platform is characterised by a design and operation mode driven by semantics at different levels: data sources, problem domain and workflow components. The proposed platform is developed upon an ontological framework of meta-data consistently managing processes and models and taking advantage of domain knowledge. TITAN comprises a well-grounded stack of Big Data technologies including Apache Kafka for inter-component communication, Apache Avro for data serialisation and Apache Spark for data analytics. A series of use cases are conducted for validation, which comprises workflow composition and semantic meta-data management in academic and real-world fields of human activity recognition and land use monitoring from satellite images.",https://doi.org/10.1016/j.knosys.2021.107489,2021,Antonio Benítez-Hidalgo and Cristóbal Barba-González and José García-Nieto and Pedro Gutiérrez-Moncayo and Manuel Paneque and Antonio J. Nebro and María del Mar Roldán-García and José F. Aldana-Montes and Ismael Navas-Delgado,TITAN: A KNOWLEDGE-BASED PLATFORM FOR BIG DATA WORKFLOW MANAGEMENT,article
240,24772,KNOWLEDGE-BASED SYSTEMS,journal,09507051,"1,587",Q1,121,716,1187,36777,11094,1181,"9,42","51,36",Netherlands,Western Europe,Elsevier,1987-2020,Artificial Intelligence (Q1); Information Systems and Management (Q1); Management Information Systems (Q1); Software (Q1),KNOWLEDGE-BASED SYSTEMS,KNOWLEDGE-BASED SYSTEMS,"22,261",8.038,0.02794,"Today, big data processing has become a challenging task due to the amount of data collected using various sensors increasingly significantly. To build knowledge and predict the data, traditional data mining methods calculate all numerical attributes into the memory simultaneously. The data stream method is a solution for processing and calculating data. The method streams incrementally in batch form; therefore, infrastructure memory is sufficient to develop knowledge. The existing method for data stream prediction is FIMT-DD (Fast Incremental Model Tree-Drift Detection). Using this method, knowledge is developed in tree form for every instance. In this paper, enhanced FIMT-DD is proposed using ARDEV (Average Restrain Divider of Evaluation Value). ARDEV utilizes the Chernoff bound approach with error evaluation, improvement in learning rate, modification of perceptron rule calculation, and utilization of activation function. Standard FIMT-DD separates the tree formation process and perceptron prediction. The proposed method evaluates and connects the development of the tree for knowledge formation and the perceptron rule for prediction. The prediction accuracy of the proposed method is measured using MAE, RMSE and MAPE. From the experiment performed, the utilization of ARDEV enhancement shows significant improvement in terms of accuracy prediction. Statistically, the overall accuracy prediction improvement is approximately 6.99 % compared to standard FIMT-DD with a traffic dataset.",https://doi.org/10.1016/j.knosys.2019.03.019,2019,Ari Wibisono and Devvi Sarwinda,AVERAGE RESTRAIN DIVIDER OF EVALUATION VALUE (ARDEV) IN DATA STREAM ALGORITHM FOR BIG DATA PREDICTION,article
241,28611,GEOFORUM,journal,00167185,"1,584",Q1,116,284,773,20662,3032,725,"3,54","72,75",United Kingdom,Western Europe,Elsevier BV,1970-2020,Sociology and Political Science (Q1),GEOFORUM,GEOFORUM,"11,684",3.901,0.0159,"Global health threats such as the recent Ebola and Zika virus outbreaks require rapid and robust responses to prevent, reduce and recover from disease dispersion. As part of broader big data and digital humanitarianism discourses, there is an emerging interest in data produced through mobile phone communications for enhancing the data environment in such circumstances. This paper assembles user perspectives and critically examines existing evidence and future potential of mobile phone data derived from call detail records (CDRs) and two-way short message service (SMS) platforms, for managing and responding to humanitarian disasters caused by communicable disease outbreaks. We undertake a scoping review of relevant literature and in-depth interviews with key informants to ascertain the: (i) information that can be gathered from CDRs or SMS data; (ii) phase(s) in the disease disaster management cycle when mobile data may be useful; (iii) value added over conventional approaches to data collection and transfer; (iv) barriers and enablers to use of mobile data in disaster contexts; and (v) the social and ethical challenges. Based on this evidence we develop a typology of mobile phone data sources, types, and end-uses, and a decision-tree for mobile data use, designed to enable effective use of mobile data for disease disaster management. We show that mobile data holds great potential for improving the quality, quantity and timing of selected information required for disaster management, but that testing and evaluation of the benefits, constraints and limitations of mobile data use in a wider range of mobile-user and disaster contexts is needed to fully understand its utility, validity, and limitations.",https://doi.org/10.1016/j.geoforum.2016.07.019,2016,Jonathan Cinnamon and Sarah K. Jones and W. Neil Adger,EVIDENCE AND FUTURE POTENTIAL OF MOBILE PHONE DATA FOR DISEASE DISASTER MANAGEMENT,article
242,31405,JOURNAL OF ARCHAEOLOGICAL SCIENCE,journal,"03054403, 10959238","1,572",Q1,126,137,365,10414,1239,362,"3,04","76,01",United States,Northern America,Academic Press Inc.,1974-2020,Archeology (Q1); Archeology (arts and humanities) (Q1); History (Q1),JOURNAL OF ARCHAEOLOGICAL SCIENCE,JOURNAL OF ARCHAEOLOGICAL SCIENCE,"17,761",3.216,0.01111,"As spatial technology has evolved and become integrated in to archaeology, we face a new set of challenges posed by the sheer size and complexity of data we use and produce. In this paper I discuss the prospects and problems of Geospatial Big Data (GBD) – broadly defined as data sets with locational information that exceed the capacity of widely available hardware, software, and/or human resources. While the datasets we create today remain within available resources, we nonetheless face the same challenges as many other fields that use and create GBD, especially in apprehensions over data quality and privacy. After reviewing the kinds of archaeological geospatial data currently available I discuss the near future of GBD in writing culture histories, making decisions, and visualizing the past. I use a case study from New Zealand to argue for the value of taking a data quantity-in-use approach to GBD and requiring applications of GBD in archaeology be regularly accompanied by a Standalone Quality Report.",https://doi.org/10.1016/j.jas.2017.06.003,2017,Mark D. McCoy,GEOSPATIAL BIG DATA AND ARCHAEOLOGY: PROSPECTS AND PROBLEMS TOO GREAT TO IGNORE,article
243,22992,JOURNAL OF RETAILING AND CONSUMER SERVICES,journal,09696989,"1,568",Q1,89,346,555,27784,4555,550,"7,77","80,30",United Kingdom,Western Europe,Elsevier Ltd.,1994-2021,Marketing (Q1),JOURNAL OF RETAILING AND CONSUMER SERVICES,JOURNAL OF RETAILING AND CONSUMER SERVICES,"10,506",7.135,0.00876,"Big data analytics (BDA) adoption has gained attention in both practical and theoretical circles owing to the opportunities and advantages that can be reaped from it. In theory, the majority of researchers have evidenced the benefits of BDA, although barriers to its adoption have also been mentioned. This study draws upon the technology-organisation-environment framework and resource-based view theory to propose an integrated model that examines the drivers and impact of BDA adoption in the retail industry in Jordan. The proposed single model encapsulates the aspects of BDA adoption and performance. The study makes use of an online questionnaire survey to collect the required data, and the research model is eventually validated based on 132 responses gathered from the retail industry in Jordan. The findings highlight two major observations. The first is that relative advantage, organisational readiness, top management support, government support, data variety and data velocity all have a significant influence over BDA adoption. The second observation is that a significant association exists between BDA adoption and firm performance, providing information on the way firms can enhance their BDA adoption for enhanced performance. This study contributes to literature dedicated to examining BDA in terms of its drivers and impact on performance and can be used as a reference in developing nations.",https://doi.org/10.1016/j.jretconser.2022.103129,2023,Abdalwali Lutfi and Mahmaod Alrawad and Adi Alsyouf and Mohammed Amin Almaiah and Ahmad Al-Khasawneh and Akif Lutfi Al-Khasawneh and Ahmad Farhan Alshira'h and Malek Hamed Alshirah and Mohamed Saad and Nahla Ibrahim,DRIVERS AND IMPACT OF BIG DATA ANALYTIC ADOPTION IN THE RETAIL INDUSTRY: A QUANTITATIVE INVESTIGATION APPLYING STRUCTURAL EQUATION MODELING,article
244,22992,JOURNAL OF RETAILING AND CONSUMER SERVICES,journal,09696989,"1,568",Q1,89,346,555,27784,4555,550,"7,77","80,30",United Kingdom,Western Europe,Elsevier Ltd.,1994-2021,Marketing (Q1),JOURNAL OF RETAILING AND CONSUMER SERVICES,JOURNAL OF RETAILING AND CONSUMER SERVICES,"10,506",7.135,0.00876,"Big data analytics (BDA) has emerged as a significant area of research for both researchers and practitioners in the retail industry, indicating the importance and influence of solving data-related problems in contemporary business organization. The present study utilised a quantitative-methods approach to investigate factors affecting retailers' adoption of BDA across three countries. A survey questionnaire was used to collect data from managers and decision-makers in the retail industry. Data of 2278 respondents were analysed through structural equation modelling. The findings revealed that security concerns, external support, top management support, and rational decision making culture have a greater effect on BDA adoption in developed countries UK than in UAE and Egypt. However, competition intensity and firm size have a greater effect on BDA adoption in UAE and Egypt than in UK. Finally, human variables (competence of information system's staff and staff's information system knowledge) have a greater effect on BDA adoption in Egypt than UK and UAE. The findings indicate that a “one-size-fits-all” approach is insufficient in capturing the heterogeneity of managers across countries. Implications for practice and theory were demonstrated.",https://doi.org/10.1016/j.jretconser.2021.102827,2022,Mayada Abd El-Aziz Youssef and Riyad Eid and Gomaa Agag,CROSS-NATIONAL DIFFERENCES IN BIG DATA ANALYTICS ADOPTION IN THE RETAIL INDUSTRY,article
245,20896,TELEMATICS AND INFORMATICS,journal,07365853,"1,567",Q1,66,96,483,6939,3832,438,"7,45","72,28",United Kingdom,Western Europe,Elsevier Ltd.,1984-2020,Communication (Q1); Computer Networks and Communications (Q1); Electrical and Electronic Engineering (Q1); Law (Q1),TELEMATICS AND INFORMATICS,TELEMATICS AND INFORMATICS,"5,351",6.182,0.00899,"Social Networking Services (SNSs) connect people worldwide, where they communicate through sharing contents, photos, videos, posting their first-hand opinions, comments, and following their friends. Social networks are characterized by velocity, volume, value, variety, and veracity, the 5 V’s of big data. Hence, big data analytic techniques and frameworks are commonly exploited in Social Network Analysis (SNA). By the ever-increasing growth of social networks, the analysis of social data, to describe and find communication patterns among users and understand their behaviors, has attracted much attention. In this paper, we demonstrate how big data analytics meets social media, and a comprehensive review is provided on big data analytic approaches in social networks to search published studies between 2013 and August 2020, with 74 identified papers. The findings of this paper are presented in terms of main journals/conferences, yearly distributions, and the distribution of studies among publishers. Furthermore, the big data analytic approaches are classified into two main categories: Content-oriented approaches and network-oriented approaches. The main ideas, evaluation parameters, tools, evaluation methods, advantages, and disadvantages are also discussed in detail. Finally, the open challenges and future directions that are worth further investigating are discussed.",https://doi.org/10.1016/j.tele.2020.101517,2021,Sepideh {Bazzaz Abkenar} and Mostafa {Haghi Kashani} and Ebrahim Mahdipour and Seyed Mahdi Jameii,"BIG DATA ANALYTICS MEETS SOCIAL MEDIA: A SYSTEMATIC REVIEW OF TECHNIQUES, OPEN ISSUES, AND FUTURE DIRECTIONS",article
246,20896,TELEMATICS AND INFORMATICS,journal,07365853,"1,567",Q1,66,96,483,6939,3832,438,"7,45","72,28",United Kingdom,Western Europe,Elsevier Ltd.,1984-2020,Communication (Q1); Computer Networks and Communications (Q1); Electrical and Electronic Engineering (Q1); Law (Q1),TELEMATICS AND INFORMATICS,TELEMATICS AND INFORMATICS,"5,351",6.182,0.00899,"From the viewpoint of big data as a socio-technical phenomenon, this study examines the associated assumptions and biases critically and contextually. The research analyzes the big data phenomenon from a socio-technical systems theory perspective: cultural, technological, and scholarly phenomena that rest on the interplay of technology, analysis, and mythology provoking extensive utopian and dystopian rhetoric. It examines the development of big data by reviewing this theory, identifying key components of the big data ecosystem, and explaining how these components are likely to evolve over time. Despite extensive investment and proactive drive, uncertainty exists concerning the evolution of big data and the impact on the new information milieu. Significant concerns recently addressed are in the areas of privacy, data quality, access, curation, preservation, and use. This study provides insight into these challenges and opportunities through the lens of a socio-technical analysis of big data development, which includes social dynamics, political discourse, and technological choices inherent in the design and development of next-generation ICT ecology. The policy implications of big data are addressed using Korean information initiatives to highlight key considerations as the country progresses in this new ecology era.",https://doi.org/10.1016/j.tele.2014.09.006,2015,Dong-Hee Shin and Min Jae Choi,ECOLOGICAL VIEWS OF BIG DATA: PERSPECTIVES AND ISSUES,article
247,21933,DECISION SUPPORT SYSTEMS,journal,01679236,"1,564",Q1,151,115,342,7152,2672,338,"7,04","62,19",Netherlands,Western Europe,Elsevier,1985-2020,Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1),DECISION SUPPORT SYSTEMS,DECISION SUPPORT SYSTEMS,"13,580",5.795,0.0081,,https://doi.org/10.1016/j.dss.2019.113172,2019,James R. Marsden and David E. Pingry and Jason B. Thatcher,PERSPECTIVES ON NUMERICAL DATA QUALITY IN IS RESEARCH,article
248,21933,DECISION SUPPORT SYSTEMS,journal,01679236,"1,564",Q1,151,115,342,7152,2672,338,"7,04","62,19",Netherlands,Western Europe,Elsevier,1985-2020,Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1),DECISION SUPPORT SYSTEMS,DECISION SUPPORT SYSTEMS,"13,580",5.795,0.0081,"We present a probability-based metric for semantic consistency using a set of uncertain rules. As opposed to existing metrics for semantic consistency, our metric allows to consider rules that are expected to be fulfilled with specific probabilities. The resulting metric values represent the probability that the assessed dataset is free of internal contradictions with regard to the uncertain rules and thus have a clear interpretation. The theoretical basis for determining the metric values are statistical tests and the concept of the p-value, allowing the interpretation of the metric value as a probability. We demonstrate the practical applicability and effectiveness of the metric in a real-world setting by analyzing a customer dataset of an insurance company. Here, the metric was applied to identify semantic consistency problems in the data and to support decision-making, for instance, when offering individual products to customers.",https://doi.org/10.1016/j.dss.2018.03.011,2018,Bernd Heinrich and Mathias Klier and Alexander Schiller and Gerit Wagner,ASSESSING DATA QUALITY – A PROBABILITY-BASED METRIC FOR SEMANTIC CONSISTENCY,article
249,21933,DECISION SUPPORT SYSTEMS,journal,01679236,"1,564",Q1,151,115,342,7152,2672,338,"7,04","62,19",Netherlands,Western Europe,Elsevier,1985-2020,Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1),DECISION SUPPORT SYSTEMS,DECISION SUPPORT SYSTEMS,"13,580",5.795,0.0081,"Where a technology is its life cycle can make a difference in the data generated by or about adopting, using or implementing that technology. As a result, it is arguable that adoption, usage or implementation data generated early in a technology's life cycle is directly comparable to data generated later in the life cycle. In particular, comparisons of early and late data can result in a number of disparate results and limit replicability, because of biases in the data and non-stationary data. This paper suggests that it can be important for information systems researchers to disclose an estimate of the location in the technology's life cycle, as part of their analysis. The data life cycle discussion is then extended to the notion of “data phase triangulation,” which is contrasted with “methodology triangulation” and “data (collection) triangulation.” In addition, we discuss the importance of being able to use the findings from life cycle-based research to “push” a technology from one phase to another phase.",https://doi.org/10.1016/j.dss.2019.113139,2019,Daniel E. O'Leary,TECHNOLOGY LIFE CYCLE AND DATA QUALITY: ACTION AND TRIANGULATION,article
250,21933,DECISION SUPPORT SYSTEMS,journal,01679236,"1,564",Q1,151,115,342,7152,2672,338,"7,04","62,19",Netherlands,Western Europe,Elsevier,1985-2020,Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1),DECISION SUPPORT SYSTEMS,DECISION SUPPORT SYSTEMS,"13,580",5.795,0.0081,"We argue that there are major, persistent numerical data quality issues in IS academic research. These issues undermine the ability to replicate our research – a critical element of scientific investigation and analysis. In IS empirical and analytics research articles, the amount of space devoted to the details of data collection, validation, and/or quality pales in comparison to the space devoted to the evaluation and selection of relatively sophisticated model form(s) and estimation technique(s). Yet erudite modeling and estimation can yield no immediate value or be meaningfully replicated without high quality data inputs. The purpose of this paper is: 1) to detail potential quality issues with data types currently used in IS research, and 2) to start a wider and deeper discussion of data quality in IS research. No data type is inherently of low quality and no data type guarantees high quality. As researchers, our empirical research must always address data quality issues and provide the information necessary to determine What, When, Where, How, Who, and Which.",https://doi.org/10.1016/j.dss.2018.10.007,2018,James R. Marsden and David E. Pingry,NUMERICAL DATA QUALITY IN IS RESEARCH AND THE IMPLICATIONS FOR REPLICATION,article
251,21933,DECISION SUPPORT SYSTEMS,journal,01679236,"1,564",Q1,151,115,342,7152,2672,338,"7,04","62,19",Netherlands,Western Europe,Elsevier,1985-2020,Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1),DECISION SUPPORT SYSTEMS,DECISION SUPPORT SYSTEMS,"13,580",5.795,0.0081,"Anecdotal evidence suggests that, despite the large variety of data, the huge volume of generated data, and the fast velocity of obtaining data (i.e., big data), quality of big data is far from perfect. Therefore, many firms defer collecting and integrating big data as they have concerns regarding the impact of utilizing big data on data diagnosticity (i.e., retrieval of valuable information from data) and firm decision making quality. In this study, we use the Organizational Learning Theory and Wang and Strong's data quality framework to explore the impact of processing big data on firm decision quality and the mediating role of data quality (DQ) and data diagnosticity on this relationship. We validate the proposed research model using survey data from 130 firms, obtained from data analysts and IT managers. Results confirm the critical role of DQ in increasing data diagnosticity and improving firm decision quality when processing big data; suggesting important implications for practice and theory. Findings also reveal that while big data utilization positively impacts contextual DQ, accessibility DQ, and representational DQ, interestingly, it negatively impacts intrinsic DQ. Furthermore, findings show that while intrinsic DQ, contextual DQ, and representational DQ significantly increase data diagnosticity, accessibility DQ does not influence it. Most importantly, the findings show that big data utilization does not significantly impact the quality of firm decisions and it is fully mediated through DQ and data diagnosticity. The results of this study contribute to practice by providing important guidelines for managers to improve firm decision quality through the use of big data.",https://doi.org/10.1016/j.dss.2019.03.008,2019,Maryam Ghasemaghaei and Goran Calic,CAN BIG DATA IMPROVE FIRM DECISION QUALITY? THE ROLE OF DATA QUALITY AND DATA DIAGNOSTICITY,article
252,21933,DECISION SUPPORT SYSTEMS,journal,01679236,"1,564",Q1,151,115,342,7152,2672,338,"7,04","62,19",Netherlands,Western Europe,Elsevier,1985-2020,Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1),DECISION SUPPORT SYSTEMS,DECISION SUPPORT SYSTEMS,"13,580",5.795,0.0081,"An IS researcher may obtain Big Data from primary or secondary data sources. Sometimes, acquiring primary Big Data is infeasible due to availability, accessibility, cost, time, and/or complexity considerations. In this paper, we focus on Big Data-based IS research and discuss ways in which one may, post hoc, establish quality thresholds for numerical Big Data obtained from secondary sources. We also present guidelines for developing journal policies aimed at ensuring the veracity and verifiability of such data when used for research purposes.",https://doi.org/10.1016/j.dss.2019.113135,2019,Anita Lee-Post and Ram Pakath,"NUMERICAL, SECONDARY BIG DATA QUALITY ISSUES, QUALITY THRESHOLD ESTABLISHMENT, & GUIDELINES FOR JOURNAL POLICY DEVELOPMENT",article
253,21933,DECISION SUPPORT SYSTEMS,journal,01679236,"1,564",Q1,151,115,342,7152,2672,338,"7,04","62,19",Netherlands,Western Europe,Elsevier,1985-2020,Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1),DECISION SUPPORT SYSTEMS,DECISION SUPPORT SYSTEMS,"13,580",5.795,0.0081,"In recent years an increasing number of academic disciplines, including IS, have sourced digital trace data for their research. Notwithstanding the potential of such data in (re)investigations of various phenomena of interest that would otherwise be difficult or impossible to study using other sources of data, we view the quality of digital trace data as an underappreciated issue in IS research. To initiate a discussion of how to evaluate and report on the quality of digital trace data in IS research, we couch our arguments within the broader tradition of research on data quality. We explain how the uncontrolled nature of digital trace data creates unique challenges for IS researchers, who need to collect, store, retrieve, and transform those data for the purpose of numerical analysis. We then draw parallels with concepts and patterns commonly used in data analysis projects and argue that, although IS researchers probably apply such concepts and patterns, this is not reported in publications, undermining the reader's ability to assess the reliability, statistical power and replicability of the findings. Using the case of GitHub to illustrate such challenges, we develop a preliminary set of guidelines to help researchers consider and report on the quality of the digital trace data they use in their research. Our work contributes to the debate on data quality and provides relevant recommendations for scholars and IS journals at a time when a growing number of publications are relying on digital trace data.",https://doi.org/10.1016/j.dss.2019.113133,2019,Gregory Vial,REFLECTIONS ON QUALITY REQUIREMENTS FOR DIGITAL TRACE DATA IN IS RESEARCH,article
254,21933,DECISION SUPPORT SYSTEMS,journal,01679236,"1,564",Q1,151,115,342,7152,2672,338,"7,04","62,19",Netherlands,Western Europe,Elsevier,1985-2020,Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1),DECISION SUPPORT SYSTEMS,DECISION SUPPORT SYSTEMS,"13,580",5.795,0.0081,"Big data analytics (BDA) are gaining importance in all aspects of business management. This is driven by both the presence of large-scale data and management's desire to root decisions in data. Extant research demonstrates that supply chain and operations management functions are among the biggest sources and users of data in the company. Therefore, their decision-making processes would benefit from increased use of BDA technologies. However, there is still a lack of understanding of what determines a company's ability to build BDA capability to gain a competitive advantage. In this study, we attempt to answer this fundamental question by identifying the factors that assist a company in or inhibit it from building its BDA capability and maximizing its gains through BDA technologies. We base our findings on a qualitative analysis of data collected from field visits, interviews with senior management, and secondary resources. We find that, in addition to technical capacity, competitive landscape and intra-firm power dynamics play an important role in building BDA capability and using BDA technologies.",https://doi.org/10.1016/j.dss.2020.113382,2020,Ashish Kumar Jha and Maher A.N. Agi and Eric W.T. Ngai,A NOTE ON BIG DATA ANALYTICS CAPABILITY DEVELOPMENT IN SUPPLY CHAIN,article
255,21933,DECISION SUPPORT SYSTEMS,journal,01679236,"1,564",Q1,151,115,342,7152,2672,338,"7,04","62,19",Netherlands,Western Europe,Elsevier,1985-2020,Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1),DECISION SUPPORT SYSTEMS,DECISION SUPPORT SYSTEMS,"13,580",5.795,0.0081,"The low quality of data in information systems poses enormous risks to business operations and decision making. In this paper, a single-period resource allocation problem for controlling the information system's data quality problem is considered. We develop a Data-Quality-Petri net to capture the process through which data quality problem generates, propagates, and accumulates in the information system. The net considers not only the factors leading to the production of the data quality problem by the data operation nodes and the data flow structure, but also the data transfer ratio of the nodes. Then, we propose a nonlinear programming optimization model with control resource constraints. The result of the model provides an optimal strategy to allocate resources for minimizing the expected data quality problem of an information system. Further, we examine the impact of the data flow structure on optimal resource allocation. The result shows that the optimal resource input level for a data operation node is proportional to its potential for downstream propagation. A warehouse management system of an e-commerce company is utilized to illustrate the model. Our study provides a method for data managers to control the information system's data quality problem by employing a process perspective.",https://doi.org/10.1016/j.dss.2020.113381,2020,Qi Liu and Gengzhong Feng and Xi Zhao and Wenlong Wang,MINIMIZING THE DATA QUALITY PROBLEM OF INFORMATION SYSTEMS: A PROCESS-BASED METHOD,article
256,21933,DECISION SUPPORT SYSTEMS,journal,01679236,"1,564",Q1,151,115,342,7152,2672,338,"7,04","62,19",Netherlands,Western Europe,Elsevier,1985-2020,Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1),DECISION SUPPORT SYSTEMS,DECISION SUPPORT SYSTEMS,"13,580",5.795,0.0081,"Using service-oriented decision support systems (DSS in cloud) is one of the major trends for many organizations in hopes of becoming more agile. In this paper, after defining a list of requirements for service-oriented DSS, we propose a conceptual framework for DSS in cloud, and discus about research directions. A unique contribution of this paper is its perspective on how to servitize the product oriented DSS environment, and demonstrate the opportunities and challenges of engineering service oriented DSS in cloud. When we define data, information and analytics as services, we see that traditional measurement mechanisms, which are mainly time and cost driven, do not work well. Organizations need to consider value of service level and quality in addition to the cost and duration of delivered services. DSS in CLOUD enables scale, scope and speed economies. This article contributes new knowledge in service science by tying the information technology strategy perspectives to the database and design science perspectives for a broader audience.",https://doi.org/10.1016/j.dss.2012.05.048,2013,Haluk Demirkan and Dursun Delen,LEVERAGING THE CAPABILITIES OF SERVICE-ORIENTED DECISION SUPPORT SYSTEMS: PUTTING ANALYTICS AND BIG DATA IN CLOUD,article
257,21933,DECISION SUPPORT SYSTEMS,journal,01679236,"1,564",Q1,151,115,342,7152,2672,338,"7,04","62,19",Netherlands,Western Europe,Elsevier,1985-2020,Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1),DECISION SUPPORT SYSTEMS,DECISION SUPPORT SYSTEMS,"13,580",5.795,0.0081,"To succeed in their business processes, organizations need data that not only attains suitable levels of quality for the task at hand, but that can also be considered as usable for the business. However, many researchers ground the potential usability of the data on its quality. Organizations would benefit from receiving recommendations on the usability of the data before its use. We propose that the recommendation on the usability of the data be supported by a decision process, which includes a context-dependent data-quality assessment based on business rules. Ideally, this recommendation would be generated automatically. Decision Model and Notation (DMN) enables the assessment of data quality based on the evaluation of business rules, and also, provides stakeholders (e.g., data stewards) with sound support for the automation of the whole process of generation of a recommendation regarding usability based on data quality. The main contribution of the proposal involves designing and enabling both DMN-driven mechanisms and a guiding methodology (DMN4DQ) to support the automatic generation of a decision-based recommendation on the potential usability of a data record in terms of its level of data quality. Furthermore, the validation of the proposal is performed through the application of a real dataset.",https://doi.org/10.1016/j.dss.2020.113450,2021,Álvaro Valencia-Parra and Luisa Parody and Ángel Jesús Varela-Vaca and Ismael Caballero and María Teresa Gómez-López,DMN4DQ: WHEN DATA QUALITY MEETS DMN,article
258,21933,DECISION SUPPORT SYSTEMS,journal,01679236,"1,564",Q1,151,115,342,7152,2672,338,"7,04","62,19",Netherlands,Western Europe,Elsevier,1985-2020,Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1),DECISION SUPPORT SYSTEMS,DECISION SUPPORT SYSTEMS,"13,580",5.795,0.0081,"Forecasting tourism demand has important implications for both policy makers and companies operating in the tourism industry. In this research, we applied methods and tools of social network and semantic analysis to study user-generated content retrieved from online communities which interacted on the TripAdvisor travel forum. We analyzed the forums of 7 major European capital cities, over a period of 10 years, collecting more than 2,660,000 posts, written by about 147,000 users. We present a new methodology of analysis of tourism-related big data and a set of variables which could be integrated into traditional forecasting models. We implemented Factor Augmented Autoregressive and Bridge models with social network and semantic variables which often led to a better forecasting performance than univariate models and models based on Google Trend data. Forum language complexity and the centralization of the communication network – i.e. the presence of eminent contributors – were the variables that contributed more to the forecasting of international airport arrivals.",https://doi.org/10.1016/j.dss.2019.113075,2019,Andrea {Fronzetti Colladon} and Barbara Guardabascio and Rosy Innarella,USING SOCIAL NETWORK AND SEMANTIC ANALYSIS TO ANALYZE ONLINE TRAVEL FORUMS AND FORECAST TOURISM DEMAND,article
259,21933,DECISION SUPPORT SYSTEMS,journal,01679236,"1,564",Q1,151,115,342,7152,2672,338,"7,04","62,19",Netherlands,Western Europe,Elsevier,1985-2020,Arts and Humanities (miscellaneous) (Q1); Developmental and Educational Psychology (Q1); Information Systems (Q1); Information Systems and Management (Q1); Management Information Systems (Q1),DECISION SUPPORT SYSTEMS,DECISION SUPPORT SYSTEMS,"13,580",5.795,0.0081,"The rapid growth of big data environment imposes new challenges that traditional knowledge discovery and data mining process (KDDM) models are not adequately suited to address. We propose a snail shell process model for knowledge discovery via data analytics (KDDA) to address these challenges. We evaluate the utility of the KDDA process model using real-world analytic case studies at a global multi-media company. By comparing against traditional KDDM models, we demonstrate the need and relevance of the snail shell model, particularly in addressing faster turnaround and frequent model updates that characterize knowledge discovery in the big data environment.",https://doi.org/10.1016/j.dss.2016.07.003,2016,Yan Li and Manoj A. Thomas and Kweku-Muata Osei-Bryson,A SNAIL SHELL PROCESS MODEL FOR KNOWLEDGE DISCOVERY VIA DATA ANALYTICS,article
260,18080,ROBOTICS AND COMPUTER-INTEGRATED MANUFACTURING,journal,07365845,"1,561",Q1,93,139,404,6448,2949,400,"7,35","46,39",United Kingdom,Western Europe,Elsevier Ltd.,"1984-1994, 1996-2021",Computer Science Applications (Q1); Control and Systems Engineering (Q1); Industrial and Manufacturing Engineering (Q1); Mathematics (miscellaneous) (Q1); Software (Q1),ROBOTICS AND COMPUTER-INTEGRATED MANUFACTURING,ROBOTICS AND COMPUTER-INTEGRATED MANUFACTURING,"6,215",5.666,0.0057,"Advanced manufacturing is one of the core national strategies in the US (AMP), Germany (Industry 4.0) and China (Made-in China 2025). The emergence of the concept of Cyber Physical System (CPS) and big data imperatively enable manufacturing to become smarter and more competitive among nations. Many researchers have proposed new solutions with big data enabling tools for manufacturing applications in three directions: product, production and business. Big data has been a fast-changing research area with many new opportunities for applications in manufacturing. This paper presents a systematic literature review of the state-of-the-art of big data in manufacturing. Six key drivers of big data applications in manufacturing have been identified. The key drivers are system integration, data, prediction, sustainability, resource sharing and hardware. Based on the requirements of manufacturing, nine essential components of big data ecosystem are captured. They are data ingestion, storage, computing, analytics, visualization, management, workflow, infrastructure and security. Several research domains are identified that are driven by available capabilities of big data ecosystem. Five future directions of big data applications in manufacturing are presented from modelling and simulation to real-time big data analytics and cybersecurity.",https://doi.org/10.1016/j.rcim.2019.101861,2020,Yesheng Cui and Sami Kara and Ka C. Chan,MANUFACTURING BIG DATA ECOSYSTEM: A SYSTEMATIC LITERATURE REVIEW,article
261,18080,ROBOTICS AND COMPUTER-INTEGRATED MANUFACTURING,journal,07365845,"1,561",Q1,93,139,404,6448,2949,400,"7,35","46,39",United Kingdom,Western Europe,Elsevier Ltd.,"1984-1994, 1996-2021",Computer Science Applications (Q1); Control and Systems Engineering (Q1); Industrial and Manufacturing Engineering (Q1); Mathematics (miscellaneous) (Q1); Software (Q1),ROBOTICS AND COMPUTER-INTEGRATED MANUFACTURING,ROBOTICS AND COMPUTER-INTEGRATED MANUFACTURING,"6,215",5.666,0.0057,"Today, in a smart manufacturing environment based on the Industry 4.0 paradigm, people, technological infrastructure and machinery equipped with sensors can constantly generate and communicate a huge volume of data, also known as Big Data. The manufacturing industry takes advantage of Big Data and analytics evolution by improving its capability to bring out valuable information and knowledge from industrial processes, production systems and sensors. The adoption of model-based frameworks in the Big Data Analytics pipeline can better address user configuration requirements (e.g. type of analysis to perform, type of algorithm to be applied) and also provide more transparency and clearness on the execution of workflows and data processing. In the current state of art, an application of a model-based framework in a manufacturing scenario is missing. Therefore, in this study, by means of a case study research focused on data from sensors associated with Computer Numerical Control machines, the configuration and execution of a Big Data Analytics pipeline with a Model-based Big Data Analytics-as-a-Service framework is described. The case study provides to theoreticians and managerial experts useful evidence for managing real-time data analytics and deploying a workflow that addresses specific analytical goals, driven by user requirements and developer models, in a complex manufacturing domain.",https://doi.org/10.1016/j.rcim.2022.102331,2022,Angelo Corallo and Anna Maria Crespino and Mariangela Lazoi and Marianna Lezzi,MODEL-BASED BIG DATA ANALYTICS-AS-A-SERVICE FRAMEWORK IN SMART MANUFACTURING: A CASE STUDY,article
262,18080,ROBOTICS AND COMPUTER-INTEGRATED MANUFACTURING,journal,07365845,"1,561",Q1,93,139,404,6448,2949,400,"7,35","46,39",United Kingdom,Western Europe,Elsevier Ltd.,"1984-1994, 1996-2021",Computer Science Applications (Q1); Control and Systems Engineering (Q1); Industrial and Manufacturing Engineering (Q1); Mathematics (miscellaneous) (Q1); Software (Q1),ROBOTICS AND COMPUTER-INTEGRATED MANUFACTURING,ROBOTICS AND COMPUTER-INTEGRATED MANUFACTURING,"6,215",5.666,0.0057,"From the last decade, additive manufacturing (AM) has been evolving speedily and has revealed the great potential for energy-saving and cleaner environmental production due to a reduction in material and resource consumption and other tooling requirements. In this modern era, with the advancements in manufacturing technologies, academia and industry have been given more interest in smart manufacturing for taking benefits for making their production more sustainable and effective. In the present study, the significant techniques of smart manufacturing, sustainable manufacturing, and additive manufacturing are combined to make a unified term of sustainable and smart additive manufacturing (SSAM). The paper aims to develop framework by combining big data analytics, additive manufacturing, and sustainable smart manufacturing technologies which is beneficial to the additive manufacturing enterprises. So, a framework of big data-driven sustainable and smart additive manufacturing (BD-SSAM) is proposed which helped AM industry leaders to make better decisions for the beginning of life (BOL) stage of product life cycle. Finally, an application scenario of the additive manufacturing industry was presented to demonstrate the proposed framework. The proposed framework is implemented on the BOL stage of product lifecycle due to limitation of available resources and for fabrication of AlSi10Mg alloy components by using selective laser melting (SLM) technique of AM. The results indicate that energy consumption and quality of the product are adequately controlled which is helpful for smart sustainable manufacturing, emission reduction, and cleaner production.",https://doi.org/10.1016/j.rcim.2020.102026,2021,Arfan Majeed and Yingfeng Zhang and Shan Ren and Jingxiang Lv and Tao Peng and Saad Waqar and Enhuai Yin,A BIG DATA-DRIVEN FRAMEWORK FOR SUSTAINABLE AND SMART ADDITIVE MANUFACTURING,article
263,16313,FUEL,journal,"00162361, 18737153","1,560",Q1,213,2396,5152,119785,35895,5139,"6,88","49,99",Netherlands,Western Europe,Elsevier BV,"1922, 1970-2021",Chemical Engineering (miscellaneous) (Q1); Energy Engineering and Power Technology (Q1); Fuel Technology (Q1); Organic Chemistry (Q1),FUEL,FUEL,"98,202",6.609,0.08033,"In this paper, the droplet size distributions of high-velocity airblast atomization were analyzed. The spray measurement was performed by a Phase-Doppler anemometer at several points and different diameters across the spray for diesel oil, light heating oil, crude rapeseed oil, and water. The atomizing gauge pressure and the liquid preheating temperature varied from 0.3 to 2.4 bar and 25 to 100 °C, respectively. Approximately 400 million individual droplets were recorded; therefore, a big data evaluation technique was applied. 18 of the most commonly used probability density functions (PDF) were fitted to the histogram of each measuring point and evaluated by their relative log-likelihood. Among the three-parameter PDFs, Generalized Extreme Value and Burr PDFs provided the most desirable result to describe a complete drop size distribution. With restriction to two-parameter PDFs, the Nakagami PDF unexpectedly outperformed all the others, including Weibull (Rosin-Rammler) PDF, which is commonly used in atomization. However, if the spray is characterized by a single value, such as the Sauter Mean Diameter, i.e. an expected value-like parameter is of primary importance over the distribution, Gamma PDF is the best option, used in several papers of the atomization literature.",https://doi.org/10.1016/j.fuel.2020.117792,2020,András Urbán and Axel Groniewsky and Milan Malý and Viktor Józsa and Jan Jedelský,APPLICATION OF BIG DATA ANALYSIS TECHNIQUE ON HIGH-VELOCITY AIRBLAST ATOMIZATION: SEARCHING FOR OPTIMUM PROBABILITY DENSITY FUNCTION,article
264,13760,HABITAT INTERNATIONAL,journal,01973975,"1,542",Q1,78,117,365,8006,2114,361,"5,46","68,43",United Kingdom,Western Europe,Elsevier Ltd.,"1970, 1976-2020",Nature and Landscape Conservation (Q1); Urban Studies (Q1),HABITAT INTERNATIONAL,HABITAT INTERNATIONAL,"8,872",5.369,0.00827,"Urban studies attempt to identify the geographic areas with restricted access to healthy and affordable foods (defined as food deserts in the literature). While prior publications have reported the socioeconomic disparities in healthy food accessibility, little evidence has been released from developing countries, especially in China. This paper proposes a geo-big data approach to measuring transit-varying healthy food accessibility and applies it to identify the food deserts within Shenzhen, China. In particular, we develop a crawling tool to harvest the daily travel time from each community (8117) to each healthy food store (102) from the Baidu Map under four transport modes (walking, public transit, private car, and bicycle) during 17:30–20:30 in June 2016. Based on the travel time calculations, we develop four travel time indicators to measure the healthy food accessibility: the minimum, the maximum, the weighted average, and the standard deviation. Results show that the four accessibility indicators generate different estimations and the nearest service (minimum time) alone fails to reflect the multidimensional nature of healthy food accessibility. The communities within Shenzhen present quite different typology with respect to healthy food accessibility under different transport modes. Multilevel additive regression is further applied to examine the associations between healthy food accessibility and nested socioeconomic characteristics at two geographic levels (community and district). We discover that the associations are divergent with transport modes and with geographic levels. More specifically, significant social equalities in healthy food accessibility are identified via walking, public transit, and bicycle in Shenzhen. Based on the associations, we finally map the food deserts and propose corresponding planning strategies. The methods demonstrated in this study should offer deeper spatial insights into intra-urban foodscapes and provide more nuanced understanding of food deserts in urban settings of developing countries.",https://doi.org/10.1016/j.habitatint.2017.04.007,2017,Shiliang Su and Zekun Li and Mengya Xu and Zhongliang Cai and Min Weng,"A GEO-BIG DATA APPROACH TO INTRA-URBAN FOOD DESERTS: TRANSIT-VARYING ACCESSIBILITY, SOCIAL INEQUALITIES, AND IMPLICATIONS FOR URBAN PLANNING",article
265,22305,SURGERY,journal,"15327361, 00396060","1,532",Q1,162,487,1520,10740,4267,1108,"2,40","22,05",United States,Northern America,Mosby Inc.,1937-2020,Surgery (Q1),SURGERY,SURGERY,"25,223",3.982,0.02493,"The term big data has been popularized over the past decade and is often used to refer to data sets that are too large or complex to be analyzed by traditional means. Although the term has been utilized for some time in business and engineering, the concept of big data is relatively new to medicine. The reception from the medical community has been mixed; however, the widespread utilization of electronic health records in the United States, the creation of large clinical data sets and national registries that capture information on numerous vectors affecting healthcare delivery and patient outcomes, and the sequencing of the human genome are all opportunities to leverage big data. This review was inspired by a lively panel discussion on big data that took place at the 75th Central Surgical Association Annual Meeting. The authors’ aim was to describe big data, the methodologies used to analyze big data, and their practical clinical application.",https://doi.org/10.1016/j.surg.2018.06.022,2018,Adrienne N. Cobb and Andrew J. Benjamin and Erich S. Huang and Paul C. Kuo,BIG DATA: MORE THAN BIG DATA SETS,article
266,9600153103,SURGERY,journal,"18781764, 02639319","0,195",Q4,19,118,321,1660,207,320,"0,45","14,07",United Kingdom,Western Europe,Elsevier BV,2006-2020,Surgery (Q4),SURGERY,SURGERY,"25,223",3.982,0.02493,"The term big data has been popularized over the past decade and is often used to refer to data sets that are too large or complex to be analyzed by traditional means. Although the term has been utilized for some time in business and engineering, the concept of big data is relatively new to medicine. The reception from the medical community has been mixed; however, the widespread utilization of electronic health records in the United States, the creation of large clinical data sets and national registries that capture information on numerous vectors affecting healthcare delivery and patient outcomes, and the sequencing of the human genome are all opportunities to leverage big data. This review was inspired by a lively panel discussion on big data that took place at the 75th Central Surgical Association Annual Meeting. The authors’ aim was to describe big data, the methodologies used to analyze big data, and their practical clinical application.",https://doi.org/10.1016/j.surg.2018.06.022,2018,Adrienne N. Cobb and Andrew J. Benjamin and Erich S. Huang and Paul C. Kuo,BIG DATA: MORE THAN BIG DATA SETS,article
267,15134,INFORMATION SCIENCES,journal,00200255,"1,524",Q1,184,928,2184,40007,17554,2168,"7,89","43,11",United States,Northern America,Elsevier Inc.,1968-2021,Artificial Intelligence (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Information Systems and Management (Q1); Software (Q1); Theoretical Computer Science (Q1),INFORMATION SCIENCES,INFORMATION SCIENCES,"44,038",6.795,0.04908,"In any knowledge discovery process the value of extracted knowledge is directly related to the quality of the data used. Big Data problems, generated by massive growth in the scale of data observed in recent years, also follow the same dictate. A common problem affecting data quality is the presence of noise, particularly in classification problems, where label noise refers to the incorrect labeling of training instances, and is known to be a very disruptive feature of data. However, in this Big Data era, the massive growth in the scale of the data poses a challenge to traditional proposals created to tackle noise, as they have difficulties coping with such a large amount of data. New algorithms need to be proposed to treat the noise in Big Data problems, providing high quality and clean data, also known as Smart Data. In this paper, two Big Data preprocessing approaches to remove noisy examples are proposed: an homogeneous ensemble and an heterogeneous ensemble filter, with special emphasis in their scalability and performance traits. The obtained results show that these proposals enable the practitioner to efficiently obtain a Smart Dataset from any Big Data classification problem.",https://doi.org/10.1016/j.ins.2018.12.002,2019,Diego García-Gil and Julián Luengo and Salvador García and Francisco Herrera,ENABLING SMART DATA: NOISE FILTERING IN BIG DATA CLASSIFICATION,article
268,15134,INFORMATION SCIENCES,journal,00200255,"1,524",Q1,184,928,2184,40007,17554,2168,"7,89","43,11",United States,Northern America,Elsevier Inc.,1968-2021,Artificial Intelligence (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Information Systems and Management (Q1); Software (Q1); Theoretical Computer Science (Q1),INFORMATION SCIENCES,INFORMATION SCIENCES,"44,038",6.795,0.04908,"In online review systems, a participant's level of knowledge impacts his/her posting behaviors, and an increase in knowledge occurs when the participant reads the reviews posted on the systems. To capture the collective dynamics of posting reviews, we used real-world big data collected over 153 months to drive an agent-based model for replicating the operation process of online review systems. The model explains the effects of clicking position (e.g., on a review webpage's serial list) and the number of items per webpage on posting contributions. Reading reviews from the last webpage only, or from the first webpage and last webpage simultaneously, can promote a greater review volume than reading reviews in other positions. This illustrates that representing primacy (first items) and recency (recent items) within one page simultaneously, or displaying recent items in reverse chronological order, are relatively better strategies for the webpage display of online reviews. The number of items plays a nonlinear moderating role in bridging the clicking position and posting behavior, and we determine the optimal number of items. To effectively establish strategies for webpage design in online review systems, business managers must switch from reliance on experience to reliance on an agent-based model as a decision support system for the formalized webpage design of online review systems.",https://doi.org/10.1016/j.ins.2019.09.053,2020,Guoyin Jiang and Xiaodong Feng and Wenping Liu and Xingjun Liu,CLICKING POSITION AND USER POSTING BEHAVIOR IN ONLINE REVIEW SYSTEMS: A DATA-DRIVEN AGENT-BASED MODELING APPROACH,article
269,15134,INFORMATION SCIENCES,journal,00200255,"1,524",Q1,184,928,2184,40007,17554,2168,"7,89","43,11",United States,Northern America,Elsevier Inc.,1968-2021,Artificial Intelligence (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Information Systems and Management (Q1); Software (Q1); Theoretical Computer Science (Q1),INFORMATION SCIENCES,INFORMATION SCIENCES,"44,038",6.795,0.04908,"In state-of-the-art big-data applications, the process of building machine learning models can be very challenging due to continuous changes in data structures and the need for human interaction to tune the variables and models over time. Hence, expedited learning in rapidly changing environments is required. In this work, we address this challenge by implementing concepts from the field of intrinsically motivated computational learning, also known as artificial curiosity (AC). In AC, an autonomous agent acts to optimize its learning about itself and its environment by receiving internal rewards based on prediction errors. We present a novel method of intrinsically motivated learning, based on the curiosity loop, to learn the data structures in large and varied datasets. An autonomous agent learns to select a subset of relevant features in the data, i.e., feature selection, to be used later for model construction. The agent optimizes its learning about the data structure over time without requiring external supervision. We show that our method, called the Curious Feature Selection (CFS) algorithm, positively impacts the accuracy of learning models on three public datasets.",https://doi.org/10.1016/j.ins.2019.02.009,2019,Michal Moran and Goren Gordon,CURIOUS FEATURE SELECTION,article
270,15134,INFORMATION SCIENCES,journal,00200255,"1,524",Q1,184,928,2184,40007,17554,2168,"7,89","43,11",United States,Northern America,Elsevier Inc.,1968-2021,Artificial Intelligence (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Information Systems and Management (Q1); Software (Q1); Theoretical Computer Science (Q1),INFORMATION SCIENCES,INFORMATION SCIENCES,"44,038",6.795,0.04908,"In big data era, information integration often requires abundant data extracted from massive data sources. Due to a large number of data sources, data source selection plays a crucial role in information integration, since it is costly and even impossible to access all data sources. Data Source selection should consider both efficiency and effectiveness issues. For efficiency, the approach should scale to large data source amount. From effectiveness aspect, data quality and overlapping of sources are to be considered. In this paper, we study source selection problem in Big Data and propose methods which can scale to datasets with up to millions of data sources and guarantee the quality of results. Motivated by this, we propose a new metric taking the expected number of true values a source can provide as a criteria to evaluate the contribution of a data source. Based on our proposed index, we present a scalable algorithm and two pruning strategies to improve the efficiency without sacrificing precision. Experimental results on both real world and synthetic data sets show that our methods can select sources providing a large proportion of true values efficiently and can scale to massive data sources.",https://doi.org/10.1016/j.ins.2018.11.029,2019,Yiming Lin and Hongzhi Wang and Jianzhong Li and Hong Gao,DATA SOURCE SELECTION FOR INFORMATION INTEGRATION IN BIG DATA ERA,article
271,15134,INFORMATION SCIENCES,journal,00200255,"1,524",Q1,184,928,2184,40007,17554,2168,"7,89","43,11",United States,Northern America,Elsevier Inc.,1968-2021,Artificial Intelligence (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Information Systems and Management (Q1); Software (Q1); Theoretical Computer Science (Q1),INFORMATION SCIENCES,INFORMATION SCIENCES,"44,038",6.795,0.04908,"Clustering technique plays a critical role in data mining, and has received great success to solve application problems like community analysis, image retrieval, personalized recommendation, activity prediction, etc. This paper first reviews the traditional clustering and the emerging multiple clustering methods, respectively. Although the existing methods have superior performance on some small or certain datasets, they fall short when clustering is performed on CPSS big data because of the high cost of computation and storage. With the powerful cloud computing, this challenge can be effectively addressed, but it brings enormous threat to individual or company’s privacy. Currently, privacy preserving data mining has attracted widespread attention in academia. Compared to other reviews, this paper focuses on privacy preserving clustering technique, guiding a detailed overview and discussion. Specifically, we introduce a novel privacy-preserving tensor-based multiple clustering, propose a privacy-preserving tensor-based multiple clustering analytic and service framework, and give an illustrated case study on the public transportation dataset. Furthermore, we indicate the remaining challenges of privacy preserving clustering and discuss the future significant research in this area.",https://doi.org/10.1016/j.ins.2019.10.019,2020,Yaliang Zhao and Samwel K. Tarus and Laurence T. Yang and Jiayu Sun and Yunfei Ge and Jinke Wang,PRIVACY-PRESERVING CLUSTERING FOR BIG DATA IN CYBER-PHYSICAL-SOCIAL SYSTEMS: SURVEY AND PERSPECTIVES,article
272,15134,INFORMATION SCIENCES,journal,00200255,"1,524",Q1,184,928,2184,40007,17554,2168,"7,89","43,11",United States,Northern America,Elsevier Inc.,1968-2021,Artificial Intelligence (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Information Systems and Management (Q1); Software (Q1); Theoretical Computer Science (Q1),INFORMATION SCIENCES,INFORMATION SCIENCES,"44,038",6.795,0.04908,"The era of Big Data has arrived along with large volume, complex and growing data generated by many distinct sources. Nowadays, nearly every aspect of the modern society is impacted by Big Data, involving medical, health care, business, management and government. It has been receiving growing attention of researches from many disciplines including natural sciences, life sciences, engineering and even art & humanities. It also leads to new research paradigms and ways of thinking on the path of development. Lots of developed and under-developing tools improve our ability to make more felicitous decisions than what we have made ever before. This paper presents an overview on Big Data including four issues, namely: (i) concepts, characteristics and processing paradigms of Big Data; (ii) the state-of-the-art techniques for decision making in Big Data; (iii) felicitous decision making applications of Big Data in social science; and (iv) the current challenges of Big Data as well as possible future directions.",https://doi.org/10.1016/j.ins.2016.07.007,2016,Hai Wang and Zeshui Xu and Hamido Fujita and Shousheng Liu,TOWARDS FELICITOUS DECISION MAKING: AN OVERVIEW ON CHALLENGES AND TRENDS OF BIG DATA,article
273,15134,INFORMATION SCIENCES,journal,00200255,"1,524",Q1,184,928,2184,40007,17554,2168,"7,89","43,11",United States,Northern America,Elsevier Inc.,1968-2021,Artificial Intelligence (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Information Systems and Management (Q1); Software (Q1); Theoretical Computer Science (Q1),INFORMATION SCIENCES,INFORMATION SCIENCES,"44,038",6.795,0.04908,"It is already true that Big Data has drawn huge attention from researchers in information sciences, policy and decision makers in governments and enterprises. As the speed of information growth exceeds Moore’s Law at the beginning of this new century, excessive data is making great troubles to human beings. However, there are so much potential and highly useful values hidden in the huge volume of data. A new scientific paradigm is born as data-intensive scientific discovery (DISD), also known as Big Data problems. A large number of fields and sectors, ranging from economic and business activities to public administration, from national security to scientific researches in many areas, involve with Big Data problems. On the one hand, Big Data is extremely valuable to produce productivity in businesses and evolutionary breakthroughs in scientific disciplines, which give us a lot of opportunities to make great progresses in many fields. There is no doubt that the future competitions in business productivity and technologies will surely converge into the Big Data explorations. On the other hand, Big Data also arises with many challenges, such as difficulties in data capture, data storage, data analysis and data visualization. This paper is aimed to demonstrate a close-up view about Big Data, including Big Data applications, Big Data opportunities and challenges, as well as the state-of-the-art techniques and technologies we currently adopt to deal with the Big Data problems. We also discuss several underlying methodologies to handle the data deluge, for example, granular computing, cloud computing, bio-inspired computing, and quantum computing.",https://doi.org/10.1016/j.ins.2014.01.015,2014,C.L. {Philip Chen} and Chun-Yang Zhang,"DATA-INTENSIVE APPLICATIONS, CHALLENGES, TECHNIQUES AND TECHNOLOGIES: A SURVEY ON BIG DATA",article
274,15134,INFORMATION SCIENCES,journal,00200255,"1,524",Q1,184,928,2184,40007,17554,2168,"7,89","43,11",United States,Northern America,Elsevier Inc.,1968-2021,Artificial Intelligence (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Information Systems and Management (Q1); Software (Q1); Theoretical Computer Science (Q1),INFORMATION SCIENCES,INFORMATION SCIENCES,"44,038",6.795,0.04908,"Decision support systems aim to help a decision maker with selecting the option from a set of available options that best meets her or his needs. In multi-criteria based decision support approaches, a suitability degree is computed for each option, reflecting how suitable that option is considering the preferences of the decision maker. Nowadays, it becomes more and more common that data of different quality, originating from different data sets and different data providers have to be integrated and processed in order to compute the suitability degrees. Also, data sets can be very large such that their data become commonly prone to incompleteness, imprecision and uncertainty. Hence, not all data used for decision making can be trusted to the same extent and consequently, neither the results of computations with such data can be trusted to the same extent. For this reason, data quality assessment becomes an important aspect of a decision making process. To correctly inform the users, it is essential to communicate not only the computed suitability degrees of the available options, but also the confidence about these suitability degrees as can be derived from data quality assessment. In this paper a novel multi-dimensional approach for data quality assessment in multi-criteria decision making, supporting the computation of associated confidence degrees, is presented. Providing confidence information adds an extra dimension to the decision making process and leads to more soundly decisions. The added value of the approach is illustrated with aspects of a geographic decision making process.",https://doi.org/10.1016/j.ins.2017.09.008,2018,Guy {De Tré} and Robin {De Mol} and Antoon Bronselaer,HANDLING VERACITY IN MULTI-CRITERIA DECISION-MAKING: A MULTI-DIMENSIONAL APPROACH,article
275,15134,INFORMATION SCIENCES,journal,00200255,"1,524",Q1,184,928,2184,40007,17554,2168,"7,89","43,11",United States,Northern America,Elsevier Inc.,1968-2021,Artificial Intelligence (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Information Systems and Management (Q1); Software (Q1); Theoretical Computer Science (Q1),INFORMATION SCIENCES,INFORMATION SCIENCES,"44,038",6.795,0.04908,"Due to the uncertainty of the value of big data, it is difficult to directly give a reasonable price for big data. Auction is an effective method of distributing goods to the bidder with the highest valuation. Hence, the use of auction strategy can not only guarantee the interests of data sellers, but also conform to market principles. However, existing data auction mechanisms are centralized. It is hard to build trust among sellers, buyers and auctioneers. An open and anonymous online environment may cause entities involved in data auctions to collude to manipulate the results of data auctions. This will cause the price of auction data to fail to reach a fair and truthful level. Therefore, the first anti-collusion data auction mechanism based on smart contract is proposed. Through a well-designed anti-collusion data auction algorithm, mutual distrust and rational buyers and sellers safely participate in the data auction without a trusted third party. The data auction mechanism designed in the smart contract can effectively prevent collusion and realize the fairness and truthfulness of data auction. The webpack in the Truffle Boxes is used to implement the data auction mechanism, and the anti-collusion property of the mechanism has been verified. The source code of the smart contract has been uploaded to GitHub.",https://doi.org/10.1016/j.ins.2020.10.053,2021,Wei Xiong and Li Xiong,ANTI-COLLUSION DATA AUCTION MECHANISM BASED ON SMART CONTRACT,article
276,15673,JOURNAL OF RURAL STUDIES,journal,07430167,"1,497",Q1,104,280,573,20744,2862,557,"4,43","74,09",United Kingdom,Western Europe,Elsevier Ltd.,1985-2020,"Development (Q1); Forestry (Q1); Geography, Planning and Development (Q1); Sociology and Political Science (Q1)",JOURNAL OF RURAL STUDIES,JOURNAL OF RURAL STUDIES,"9,142",4.849,0.01005,"The myriad potential benefits of digital farming hinge on the promise of increased accuracy, which allows ‘doing more with less’ through precise, data-driven operations. Yet, precision farming's foundational claim of increased accuracy has hardly been the subject of comprehensive examination. Drawing on social science studies of big data, this article examines digital agriculture's (in)accuracies and their repercussions. Based on an examination of the daily functioning of the various components of yield mapping, it finds that digital farming is often ‘precisely inaccurate’, with the high volume and granularity of big data erroneously equated with high accuracy. The prevailing discourse of ‘ultra-precise’ digital technologies ignores farmers' essential efforts in making these technologies more accurate, via calibration, corroboration and interpretation. We suggest that there is the danger of a ‘precision trap’. Namely, an exaggerated belief in the precision of big data that over time leads to an erosion of checks and balances (analogue data, farmer observation et cetera) on farms. The danger of ‘precision traps’ increases with the opacity of algorithms, with shifts from real-time measurement and advice towards forecasting, and with farmers' increased remoteness from field operations. Furthermore, we identify an emerging ‘precision divide’: unequally distributed precision benefits resulting from the growing algorithmic divide between farmers focusing on staple crops, catered well by technological innovation on the one hand, and farmers cultivating other crops, who have to make do with much less advanced or applicable algorithms on the other. Consequently, for the latter farms digital farming may feel more like ‘imprecision farming’.",https://doi.org/10.1016/j.jrurstud.2021.07.024,2021,Oane Visser and Sarah Ruth Sippel and Louis Thiemann,IMPRECISION FARMING? EXAMINING THE (IN)ACCURACY AND RISKS OF DIGITAL AGRICULTURE,article
277,21524,ENVIRONMENTAL RESEARCH,journal,"10960953, 00139351","1,460",Q1,136,1355,1688,80881,11076,1650,"6,28","59,69",United States,Northern America,Academic Press Inc.,1967-2020,Biochemistry (Q1); Environmental Science (miscellaneous) (Q1),ENVIRONMENTAL RESEARCH,ENVIRONMENTAL RESEARCH,"28,576",6.498,0.03426,"Recent rapid technological advances are producing exposure data sets for which there are no available data quality assessment tools. At the same time, regulatory agencies are moving in the direction of data quality assessment for environmental risk assessment and decision-making. A transparent and systematic approach to evaluating exposure data will aid in those efforts. Any approach to assessing data quality must consider the level of quality needed for the ultimate use of the data. While various fields have developed approaches to assess data quality, there is as yet no general, user-friendly approach to assess both measured and modeled data in the context of a fit-for-purpose risk assessment. Here we describe ExpoQual, an instrument developed for this purpose which applies recognized parameters and exposure data quality elements from existing approaches for assessing exposure data quality. Broad data streams such as quantitative measured and modeled human exposure data as well as newer and developing approaches can be evaluated. The key strength of ExpoQual is that it facilitates a structured, reproducible and transparent approach to exposure data quality evaluation and provides for an explicit fit-for-purpose determination. ExpoQual was designed to minimize subjectivity and to include transparency in aspects based on professional judgment. ExpoQual is freely available on-line for testing and user feedback (exposurequality.com).",https://doi.org/10.1016/j.envres.2019.01.039,2019,Judy S. LaKind and Cian O’Mahony and Thomas Armstrong and Rosalie Tibaldi and Benjamin C. Blount and Daniel Q. Naiman,EXPOQUAL: EVALUATING MEASURED AND MODELED HUMAN EXPOSURE DATA,article
278,21524,ENVIRONMENTAL RESEARCH,journal,"10960953, 00139351","1,460",Q1,136,1355,1688,80881,11076,1650,"6,28","59,69",United States,Northern America,Academic Press Inc.,1967-2020,Biochemistry (Q1); Environmental Science (miscellaneous) (Q1),ENVIRONMENTAL RESEARCH,ENVIRONMENTAL RESEARCH,"28,576",6.498,0.03426,"The Korean CHildren's ENvironmental health Study (Ko-CHENS) is a nationwide prospective birth cohort showing the correlation between the environmental exposures and the health effects to prevent the environmental diseases in children, and it provides the guidelines for the environmental hazardous factors, applying the life-course approach to the environmental-health management system. The Ko-CHENS consists of 5000 Core and 65,000 Main Cohorts. The children in the Core Cohort are followed up at 6 months, every year before their admission into the elementary school, and every 3 years from the first year after this admission. The children in the Cohort will be followed up through the data links (Statistics Korea, National Health Insurance Service [NHIS], and Ministry of Education). The individual biospecimens will be analyzed for 19 substances. The long-term-storage biological samples will be used for the further substance analysis. The Ko-CHENS will investigate whether the environmental variables including the perinatal outdoor and indoor factors and the greenness contribute causally to the health outcomes in the children and adolescents. In addition to the individual surveys, the assessments of the outdoor exposures and health outcomes will use the national air-quality monitoring data and claim data of the NHIS, respectively. The two big-data forms of the Ko-CHENS are as follows: The Ko-CHENS data that can be linked with the nationally registered NHIS health-related database, including the medical utilization and the periodic health screening, and the birth/mortality database in the Statistics; the other is the Big-CHENS dataset that is based on the NHIS mother delivery code, for which the follow-up of almost 97% of the total birth population is expected. The Ko-CHENS is a very cost-effective study that fully exploits the existing national big-data systems with the data linkage.",https://doi.org/10.1016/j.envres.2018.12.009,2019,Kyoung Sook Jeong and Suejin Kim and Woo Jin Kim and Hwan-Cheol Kim and Jisuk Bae and Yun-Chul Hong and Mina Ha and Kangmo Ahn and Ji-Young Lee and Yangho Kim and Eunhee Ha,COHORT PROFILE: BEYOND BIRTH COHORT STUDY – THE KOREAN CHILDREN'S ENVIRONMENTAL HEALTH STUDY (KO-CHENS),article
279,21524,ENVIRONMENTAL RESEARCH,journal,"10960953, 00139351","1,460",Q1,136,1355,1688,80881,11076,1650,"6,28","59,69",United States,Northern America,Academic Press Inc.,1967-2020,Biochemistry (Q1); Environmental Science (miscellaneous) (Q1),ENVIRONMENTAL RESEARCH,ENVIRONMENTAL RESEARCH,"28,576",6.498,0.03426,"The effect of height on pollen concentration is not well documented and little is known about the near-ground vertical profile of airborne pollen. This is important as most measuring stations are on roofs, but patient exposure is at ground level. Our study used a big data approach to estimate the near-ground vertical profile of pollen concentrations based on a global study of paired stations located at different heights. We analyzed paired sampling stations located at different heights between 1.5 and 50 m above ground level (AGL). This provided pollen data from 59 Hirst-type volumetric traps from 25 different areas, mainly in Europe, but also covering North America and Australia, resulting in about 2,000,000 daily pollen concentrations analyzed. The daily ratio of the amounts of pollen from different heights per location was used, and the values of the lower station were divided by the higher station. The lower station of paired traps recorded more pollen than the higher trap. However, while the effect of height on pollen concentration was clear, it was also limited (average ratio 1.3, range 0.7–2.2). The standard deviation of the pollen ratio was highly variable when the lower station was located close to the ground level (below 10 m AGL). We show that pollen concentrations measured at >10 m are representative for background near-ground levels.",https://doi.org/10.1016/j.envres.2019.04.027,2019,Jesús Rojo and Jose Oteros and Rosa Pérez-Badia and Patricia Cervigón and Zuzana Ferencova and A. Monserrat Gutiérrez-Bustillo and Karl-Christian Bergmann and Gilles Oliver and Michel Thibaudon and Roberto Albertini and David {Rodríguez-De la Cruz} and Estefanía Sánchez-Reyes and José Sánchez-Sánchez and Anna-Mari Pessi and Jukka Reiniharju and Annika Saarto and M. Carmen Calderón and César Guerrero and Daniele Berra and Maira Bonini and Elena Chiodini and Delia Fernández-González and José García and M. Mar Trigo and Dorota Myszkowska and Santiago Fernández-Rodríguez and Rafael Tormo-Molina and Athanasios Damialis and Franziska Kolek and Claudia Traidl-Hoffmann and Elena Severova and Elsa Caeiro and Helena Ribeiro and Donát Magyar and László Makra and Orsolya Udvardy and Purificación Alcázar and Carmen Galán and Katarzyna Borycka and Idalia Kasprzyk and Ed Newbigin and Beverley Adams-Groom and Godfrey P. Apangu and Carl A. Frisk and Carsten A. Skjøth and Predrag Radišić and Branko Šikoparija and Sevcan Celenk and Carsten B. Schmidt-Weber and Jeroen Buters,NEAR-GROUND EFFECT OF HEIGHT ON POLLEN EXPOSURE,article
280,21100202157,TOURISM MANAGEMENT PERSPECTIVES,journal,22119736,"1,454",Q1,43,140,277,12102,1824,274,"6,77","86,44",United States,Northern America,Elsevier USA,2012-2020,"Tourism, Leisure and Hospitality Management (Q1)",TOURISM MANAGEMENT PERSPECTIVES,TOURISM MANAGEMENT PERSPECTIVES,"3,902",6.586,0.00445,"This study aims to provide a comprehensive network analysis to understand the current state of big data research in tourism by investigating multi-disciplinary contributions relevant to big data. A comprehensive network analytical method, which includes co-citation, clustering and trend analysis, is applied to systematically analyse publications from 2008 to 2017. Two unique data sets from Web of Science are collected. The first data set focuses on big data research in tourism and hospitality. The second data set involves other disciplines, such as computer science, for a comparison with tourism. Results suggest that applications of social media and user-generated content are gaining momentum, whereas theory-based studies on big data in tourism remain limited. Tourism and other relevant domains have similar concerns with the challenges involved in big data, such as privacy, data quality and appropriate data use. This comparative network analysis has implications for future big data research in tourism.",https://doi.org/10.1016/j.tmp.2019.100608,2020,Xin Li and Rob Law,NETWORK ANALYSIS OF BIG DATA RESEARCH IN TOURISM,article
281,21100202157,TOURISM MANAGEMENT PERSPECTIVES,journal,22119736,"1,454",Q1,43,140,277,12102,1824,274,"6,77","86,44",United States,Northern America,Elsevier USA,2012-2020,"Tourism, Leisure and Hospitality Management (Q1)",TOURISM MANAGEMENT PERSPECTIVES,TOURISM MANAGEMENT PERSPECTIVES,"3,902",6.586,0.00445,"Utilizing a scientometric review of global trends and structure from 388 bibliographic records over two decades (1999–2018), this study seeks to advance the building of comprehensive knowledge maps that draw upon global travel demand studies. The study, using the techniques of co-citation analysis, collaboration network and emerging trends analysis, identified major disciplines that provide knowledge and theories for tourism demand forecasting, many trending research topics, the most critical countries, institutions, publications, and articles, and the most influential researchers. The increasing interest and output for big data and machine learning techniques in the field were visualized via comprehensive knowledge maps. This research provides meaningful guidance for researchers, operators and decision makers who wish to improve the accuracy of tourism demand forecasting.",https://doi.org/10.1016/j.tmp.2020.100715,2020,Chengyuan Zhang and Shouyang Wang and Shaolong Sun and Yunjie Wei,KNOWLEDGE MAPPING OF TOURISM DEMAND FORECASTING RESEARCH,article
282,21100202157,TOURISM MANAGEMENT PERSPECTIVES,journal,22119736,"1,454",Q1,43,140,277,12102,1824,274,"6,77","86,44",United States,Northern America,Elsevier USA,2012-2020,"Tourism, Leisure and Hospitality Management (Q1)",TOURISM MANAGEMENT PERSPECTIVES,TOURISM MANAGEMENT PERSPECTIVES,"3,902",6.586,0.00445,"Previous research studied the spatiotemporal patterns in different visitor segments but lacks evidence of the segmentation of resident tourists and non-resident tourists in multi-city travel. To fill this gap, this study conducts a big data study using hotel check-in registers. The exploratory data analysis visualizes the spatiotemporal patterns and the differences between resident tourists and non-resident tourists. Then, the spatiotemporal patterns are measured by the length of stay and the number of visited cities. The regression shows that both the length of stay and the number of visited cities of non-resident tourists are higher than those of resident tourists. Moreover, non-resident tourists reduce their length of stay and their number of visited cities more than resident tourists on three-day holidays, while they increase their number of visited cities less than resident tourists on seven-day holidays. This study has significant implications for understanding spatiotemporal patterns and visitors' segmentations.",https://doi.org/10.1016/j.tmp.2021.100860,2021,Yuquan Xu and Xiaobin Ran and Yuewen Liu and Wei Huang,COMPARING DIFFERENCES IN THE SPATIOTEMPORAL PATTERNS BETWEEN RESIDENT TOURISTS AND NON-RESIDENT TOURISTS USING HOTEL CHECK-IN REGISTERS,article
283,19080,COMPUTERS IN INDUSTRY,journal,01663615,"1,432",Q1,100,115,323,6926,3165,319,"9,96","60,23",Netherlands,Western Europe,Elsevier,1979-2020,Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1),COMPUTERS IN INDUSTRY,COMPUTERS IN INDUSTRY,"6,018",7.635,0.00584,"The Internet of Things (IoT) and the relevant technologies have had a significant impact on smart farming as a major sub-domain within the field of agriculture. Modern technology supports data collection from IoT devices through several farming processes. The extensive amount of collected smart farming data can be utilized for daily decision making and analysis such as yield prediction, growth analysis, quality maintenance, animal and aquaculture, as well as farm management. This survey focuses on three major aspects of contemporary smart farming. First, it highlights various types of big data generated through smart farming and makes a broad categorization of such data. Second, this paper discusses a comprehensive set of typical applications of big data in smart farming. Third, it identifies and introduces the principal big data and machine learning techniques that are utilized in smart farming data analysis. In doing so, this survey also identifies some of the major, current challenges in smart farming big data analysis.This paper provides a discussion on potential pathways toward more effective smart farming through relevant analytics-guided decision making.",https://doi.org/10.1016/j.compind.2022.103624,2022,Sandya De Alwis and Ziwei Hou and Yishuo Zhang and Myung Hwan Na and Bahadorreza Ofoghi and Atul Sajjanhar,"A SURVEY ON SMART FARMING DATA, APPLICATIONS AND TECHNIQUES",article
284,19080,COMPUTERS IN INDUSTRY,journal,01663615,"1,432",Q1,100,115,323,6926,3165,319,"9,96","60,23",Netherlands,Western Europe,Elsevier,1979-2020,Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1),COMPUTERS IN INDUSTRY,COMPUTERS IN INDUSTRY,"6,018",7.635,0.00584,"With the increasing amount of available data, computing power and network speed for a decreasing cost, the manufacturing industry is facing an unprecedented amount of data to process, understand and exploit. Phenomena such as Big Data, the Internet-of-Things, Closed-Loop Product Lifecycle Management, and the advances of Smart Factories tend to produce humanly unmanageable quantities of data. The paper approaches the aforesaid context by assuming that any data processing automation is not only desirable but rather necessary in order to prevent prohibitive data analytics costs. This study focuses on highlighting the major specificities of engineering data and the data-processing difficulties which are inherent to data coming from the manufacturing industry. The artificial intelligence field of research is able to provide methods and tools to address some of the identified issues. A special attention was paid to provide a literature review of the most recent (in 2017) applications, that could present a high potential for the manufacturing industry, in the fields of machine learning and deep learning. In order to illustrate the proposed work, a case study was conducted on the challenging research question of object recognition in heterogeneous formats (3D models, photos and videos) with deep learning techniques. The DICE project – DMU Imagery Comparison Engine – is presented and has been completely open-sourced in order to encourage reuse and improvements of the proposed case-study. This project also leads to the development of an open-source research dataset of 2000 CAD Models, called DMU-Net available at: https://www.dmu-net.org.",https://doi.org/10.1016/j.compind.2018.04.005,2018,Jonathan Dekhtiar and Alexandre Durupt and Matthieu Bricogne and Benoit Eynard and Harvey Rowson and Dimitris Kiritsis,"DEEP LEARNING FOR BIG DATA APPLICATIONS IN CAD AND PLM – RESEARCH REVIEW, OPPORTUNITIES AND CASE STUDY",article
285,19080,COMPUTERS IN INDUSTRY,journal,01663615,"1,432",Q1,100,115,323,6926,3165,319,"9,96","60,23",Netherlands,Western Europe,Elsevier,1979-2020,Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1),COMPUTERS IN INDUSTRY,COMPUTERS IN INDUSTRY,"6,018",7.635,0.00584,"Big Data Analytics (BDA) has attracted significant attention from both academicians and practitioners alike as it provides several ways to improve strategic, tactical and operational capabilities to eventually create a positive impact on the economic performance of organizations. In the present study, twelve significant barriers against BDA implementation are identified and assessed in the context of Indian manufacturing Supply Chains (SC). These barriers are modeled using an integrated two-stage approach, consisting of Interpretive Structural Modeling (ISM) in the first stage and Decision-Making Trial and Evaluation Laboratory (DEMATEL) in the second stage. The approach developed provides the interrelationships between the identified constructs and their intensities. Moreover, Fuzzy MICMAC technique is applied to analyze the high impact (i.e., high driving power) barriers. Results show that four constructs, namely lack of top management support, lack of financial support, lack of skills, and lack of techniques or procedures, are the most significant barriers. This study aids policy-makers in conceptualizing the mutual interaction of the barriers for developing policies and strategies to improve the penetration of BDA in manufacturing SC.",https://doi.org/10.1016/j.compind.2020.103368,2021,Rakesh D. Raut and Vinay Surendra Yadav and Naoufel Cheikhrouhou and Vaibhav S. Narwane and Balkrishna E. Narkhede,BIG DATA ANALYTICS: IMPLEMENTATION CHALLENGES IN INDIAN MANUFACTURING SUPPLY CHAINS,article
286,19080,COMPUTERS IN INDUSTRY,journal,01663615,"1,432",Q1,100,115,323,6926,3165,319,"9,96","60,23",Netherlands,Western Europe,Elsevier,1979-2020,Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1),COMPUTERS IN INDUSTRY,COMPUTERS IN INDUSTRY,"6,018",7.635,0.00584,"For the past few years, we have been hearing about Industry 4.0 (or the fourth industrial revolution), which promises to improve productivity, flexibility, quality, customer satisfaction and employee well-being. To assess whether these goals are achieved, it is necessary to implement a performance management system (PMS). However, a PMS must take into account the various challenges associated with Industry 4.0, including the availability of large amounts of data. While it represents an opportunity for companies to improve performance, big data does not necessarily mean good data. It can be uncertain, imprecise, ambiguous, etc. Uncertainty is one of the major challenges and it is essential to take it into account when computing performance indicators to increase confidence in decision making. To address this issue, we propose a method to model uncertainty in key performance indicators (KPIs). Our work allows associating with each indicator an uncertainty noted m, computed on the basis of the theory of belief functions. The KPI and its associated uncertainty form a pair (KP I, m). The method developed allows calculating this uncertainty m for the input data of the performance management system. We show how these modeled uncertainties should be propagated to the KPIs. For these KPI uncertainties, we have defined rules to support decision-making. The method developed, based on the theory of belief functions, is part of a methodology we propose to define and extract smart data from massive data. To our knowledge, this is the first attempt to use this theory to model uncertain performance indicators. Our work has shown its effectiveness and its applicability to a case of bottle filling line simulation. In addition to these results, this work opens up new perspectives, particularly for taking uncertainty into account in expert opinions and in industrial risk assessment.",https://doi.org/10.1016/j.compind.2022.103666,2022,Amel Souifi and Zohra Cherfi Boulanger and Marc Zolghadri and Maher Barkallah and Mohamed Haddar,UNCERTAINTY OF KEY PERFORMANCE INDICATORS FOR INDUSTRY 4.0: A METHODOLOGY BASED ON THE THEORY OF BELIEF FUNCTIONS,article
287,19080,COMPUTERS IN INDUSTRY,journal,01663615,"1,432",Q1,100,115,323,6926,3165,319,"9,96","60,23",Netherlands,Western Europe,Elsevier,1979-2020,Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1),COMPUTERS IN INDUSTRY,COMPUTERS IN INDUSTRY,"6,018",7.635,0.00584,"Recently, with the development of “Industry 4.0”, “Oil and Gas 4.0” has also been put on the agenda in the past two years. Some companies and experts believe that “Oil and Gas 4.0” can completely change the status quo of the oil and gas industry, which can bring huge benefits because it accelerates the digitization and intelligentization of the oil and gas industry. However, the “Oil and Gas 4.0” is still in its infancy. Therefore, this paper systematically introduces the concept and core technologies of “Oil and Gas 4.0”, such as big data and the industrial Internet of Things (IIoT). Moreover, this paper analyzes typical application scenarios of the oil and gas industry chain (upstream, midstream and downstream) through examples, such as intelligent oilfield, intelligent pipeline, and intelligent refinery. It is concluded that the essence of “Oil and Gas 4.0” is a data-driven intelligence system based on the highly digitization. To the best of our knowledge, this is the first academic peer-reviewed paper on the “Oil and Gas 4.0” era, aiming to let more oil and gas industry personnel understand its benefits and application scenarios, so as to better apply it to practical engineering in the future. In the discussion section, this paper also analyzes the opportunities and difficulties that may be brought about by the “Oil and Gas 4.0” era. Finally, relevant policy recommendations are proposed.",https://doi.org/10.1016/j.compind.2019.06.007,2019,Hongfang Lu and Lijun Guo and Mohammadamin Azimi and Kun Huang,OIL AND GAS 4.0 ERA: A SYSTEMATIC REVIEW AND OUTLOOK,article
288,19080,COMPUTERS IN INDUSTRY,journal,01663615,"1,432",Q1,100,115,323,6926,3165,319,"9,96","60,23",Netherlands,Western Europe,Elsevier,1979-2020,Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1),COMPUTERS IN INDUSTRY,COMPUTERS IN INDUSTRY,"6,018",7.635,0.00584,"Big Data is dominating the landscape as data originated in many sources keeps piling up. Information Technology (IT) business companies are making tremendous efforts to keep the pace with this wave of innovative technologies. This study aims to identify how the different IT companies are aligned with emerging Big Data technologies. The approach consisted in analyzing 11,505 news published between 2013 and 2016 and aggregated through Google News. The companies were categorized according to their position in the 2017 Gartner Magic Quadrant for advanced analytics. A text mining and topic modeling procedure assisted in summarizing the main findings. Leaders dominated a large fraction of the published news. Challengers are making a significant effort in investing in predictive analytics, overlooking other technologies such as those related to data preparation and integration. The results helped to shed light on the emerging field of Big Data from a corporate perspective.",https://doi.org/10.1016/j.compind.2018.03.018,2018,João Canito and Pedro Ramos and Sérgio Moro and Paulo Rita,UNFOLDING THE RELATIONS BETWEEN COMPANIES AND TECHNOLOGIES UNDER THE BIG DATA UMBRELLA,article
289,19080,COMPUTERS IN INDUSTRY,journal,01663615,"1,432",Q1,100,115,323,6926,3165,319,"9,96","60,23",Netherlands,Western Europe,Elsevier,1979-2020,Computer Science (miscellaneous) (Q1); Engineering (miscellaneous) (Q1),COMPUTERS IN INDUSTRY,COMPUTERS IN INDUSTRY,"6,018",7.635,0.00584,"Data-intensive environments enable us to capture information and knowledge about the physical surroundings, to optimise our resources, enjoy personalised services and gain unprecedented insights into our lives. However, to obtain these endeavours extracted from the data, this data should be generated, collected and the insight should be exploited. Following an argumentation reasoning approach for data processing and building on the theoretical background of data management, we highlight the importance of data sharing agreements (DSAs) and quality attributes for the proposed data processing mechanism. The proposed approach is taking into account the DSAs and usage policies as well as the quality attributes of the data, which were previously neglected compared to existing methods in the data processing and management field. Previous research provided techniques towards this direction; however, a more intensive research approach for processing techniques should be introduced for the future to enhance the value creation from the data and new strategies should be formed around this data generated daily from various devices and sources.",https://doi.org/10.1016/j.compind.2017.09.002,2018,Erisa Karafili and Konstantina Spanaki and Emil C. Lupu,AN ARGUMENTATION REASONING APPROACH FOR DATA PROCESSING,article
290,23911,ANALYTICA CHIMICA ACTA,journal,"00032670, 18734324","1,403",Q1,203,897,2235,45034,14014,2228,"6,27","50,21",Netherlands,Western Europe,Elsevier,1947-2020,Analytical Chemistry (Q1); Biochemistry (Q1); Environmental Chemistry (Q1); Spectroscopy (Q1),ANALYTICA CHIMICA ACTA,ANALYTICA CHIMICA ACTA,"58,170",6.558,0.03972,"Efficient and reliable analysis of chemical analytical data is a great challenge due to the increase in data size, variety and velocity. New methodologies, approaches and methods are being proposed not only by chemometrics but also by other data scientific communities to extract relevant information from big datasets and provide their value to different applications. Besides common goal of big data analysis, different perspectives and terms on big data are being discussed in scientific literature and public media. The aim of this comprehensive review is to present common trends in the analysis of chemical analytical data across different data scientific fields together with their data type-specific and generic challenges. Firstly, common data science terms used in different data scientific fields are summarized and discussed. Secondly, systematic methodologies to plan and run big data analysis projects are presented together with their steps. Moreover, different analysis aspects like assessing data quality, selecting data pre-processing strategies, data visualization and model validation are considered in more detail. Finally, an overview of standard and new data analysis methods is provided and their suitability for big analytical chemical datasets shortly discussed.",https://doi.org/10.1016/j.aca.2018.05.038,2018,Ewa Szymańska,MODERN DATA SCIENCE FOR ANALYTICAL CHEMICAL DATA – A COMPREHENSIVE REVIEW,article
291,22504,CANADIAN JOURNAL OF CARDIOLOGY,journal,"19167075, 0828282X","1,395",Q1,90,342,921,9052,2482,691,"2,49","26,47",Canada,Northern America,Elsevier Inc.,1985-2020,Cardiology and Cardiovascular Medicine (Q1),CANADIAN JOURNAL OF CARDIOLOGY,CANADIAN JOURNAL OF CARDIOLOGY,"8,782",5.223,0.01509,,https://doi.org/10.1016/j.cjca.2019.09.018,2020,George A. Heckman and John P. Hirdes and Robert S. McKelvie,THE ROLE OF PHYSICIANS IN THE ERA OF BIG DATA,article
292,21100780794,ENGINEERING,journal,20958099,"1,376",Q1,45,186,372,9769,3152,324,"6,73","52,52",United Kingdom,Western Europe,Elsevier Ltd.,2015-2020,Chemical Engineering (miscellaneous) (Q1); Computer Science (miscellaneous) (Q1); Energy Engineering and Power Technology (Q1); Engineering (miscellaneous) (Q1); Environmental Engineering (Q1); Materials Science (miscellaneous) (Q1),ENGINEERING,ENGINEERING,"4,023",7.553,0.00715,"Intelligent sensing, mechanism understanding, and the deterioration forecasting based on spatio–temporal big data not only promote the safety of the infrastructure but also indicate the basic theory and key technology for the infrastructure construction to turn to intelligentization. The advancement of underground space utilization has led to the development of three characteristics (deep, big, and clustered) that help shape a tridimensional urban layout. However, compared to buildings and bridges overground, the diseases and degradation that occur underground are more insidious and difficult to identify. Numerous challenges during the construction and service periods remain. To address this gap, this paper summarizes the existing methods and evaluates their strong points and weak points based on real-world space safety management. The key scientific issues, as well as solutions, are discussed in a unified intelligent monitoring system.",https://doi.org/10.1016/j.eng.2022.07.016,2022,Bowen Du and Junchen Ye and Hehua Zhu and Leilei Sun and Yanliang Du,INTELLIGENT MONITORING SYSTEM BASED ON SPATIO–TEMPORAL DATA FOR UNDERGROUND SPACE INFRASTRUCTURE,article
293,29049,ENGINEERING,journal,00137782,"0,100",Q4,2,0,87,0,0,76,"0,00","0,00",United Kingdom,Western Europe,Gillard Welch Ltd,"1968, 1970-1990, 1994-2010, 2012, 2014-2019",Electrical and Electronic Engineering (Q4); Industrial and Manufacturing Engineering (Q4); Mechanical Engineering (Q4),ENGINEERING,ENGINEERING,"4,023",7.553,0.00715,"Intelligent sensing, mechanism understanding, and the deterioration forecasting based on spatio–temporal big data not only promote the safety of the infrastructure but also indicate the basic theory and key technology for the infrastructure construction to turn to intelligentization. The advancement of underground space utilization has led to the development of three characteristics (deep, big, and clustered) that help shape a tridimensional urban layout. However, compared to buildings and bridges overground, the diseases and degradation that occur underground are more insidious and difficult to identify. Numerous challenges during the construction and service periods remain. To address this gap, this paper summarizes the existing methods and evaluates their strong points and weak points based on real-world space safety management. The key scientific issues, as well as solutions, are discussed in a unified intelligent monitoring system.",https://doi.org/10.1016/j.eng.2022.07.016,2022,Bowen Du and Junchen Ye and Hehua Zhu and Leilei Sun and Yanliang Du,INTELLIGENT MONITORING SYSTEM BASED ON SPATIO–TEMPORAL DATA FOR UNDERGROUND SPACE INFRASTRUCTURE,article
294,24201,EXPERT SYSTEMS WITH APPLICATIONS,journal,09574174,"1,368",Q1,207,770,1945,42314,17345,1943,"8,67","54,95",United Kingdom,Western Europe,Elsevier Ltd.,1990-2021,Artificial Intelligence (Q1); Computer Science Applications (Q1); Engineering (miscellaneous) (Q1),EXPERT SYSTEMS WITH APPLICATIONS,EXPERT SYSTEMS WITH APPLICATIONS,"55,444",6.954,0.04053,"Today’s largest and fastest growing companies’ assets are no longer physical, but rather digital (software, algorithms...). This is all the more true in the manufacturing, and particularly in the maintenance sector where quality of enterprise maintenance services are closely linked to the quality of maintenance data reporting procedures. If quality of the reported data is too low, it can results in wrong decision-making and loss of money. Furthermore, various maintenance experts are involved and directly concerned about the quality of enterprises’ daily maintenance data reporting (e.g., maintenance planners, plant managers...), each one having specific needs and responsibilities. To address this Multi-Criteria Decision Making (MCDM) problem, and since data quality is hardly considered in existing expert maintenance systems, this paper develops a maintenance reporting quality assessment (MRQA) dashboard that enables any company stakeholder to easily – and in real-time – assess/rank company branch offices in terms of maintenance reporting quality. From a theoretical standpoint, AHP is used to integrate various data quality dimensions as well as expert preferences. A use case describes how the proposed MRQA dashboard is being used by a Finnish multinational equipment manufacturer to assess and enhance reporting practices in a specific or a group of branch offices.",https://doi.org/10.1016/j.eswa.2016.06.043,2016,Manik Madhikermi and Sylvain Kubler and Jérémy Robert and Andrea Buda and Kary Främling,DATA QUALITY ASSESSMENT OF MAINTENANCE REPORTING PROCEDURES,article
295,24201,EXPERT SYSTEMS WITH APPLICATIONS,journal,09574174,"1,368",Q1,207,770,1945,42314,17345,1943,"8,67","54,95",United Kingdom,Western Europe,Elsevier Ltd.,1990-2021,Artificial Intelligence (Q1); Computer Science Applications (Q1); Engineering (miscellaneous) (Q1),EXPERT SYSTEMS WITH APPLICATIONS,EXPERT SYSTEMS WITH APPLICATIONS,"55,444",6.954,0.04053,"With the unprecedented increase in data all over the world, financial sector such as companies and industries try to remain competitive by transforming themselves into data-driven organizations. By analyzing a huge amount of financial data, companies are able to obtain valuable information to determine their strategic plans such as risk control, crisis management, or growth management. However, as the amount of data increase dramatically, traditional data analytic platforms confront with storing, managing, and analyzing difficulties. Emerging Big Data Analytics (BDA) overcome these problems by providing decentralized and distributed processing. In this study, we propose two new models for default prediction. In the first model, called DPModel-1, statistical (logistic regression), and machine learning methods (decision tree, random forest, gradient boosting) are employed to predict company default. Derived from the first model, we propose DPModel-2 based on graph theory. DPModel-2 also comprises new variables obtained from the trading interactions of companies. In both models, grid search optimization and SHapley Additive exPlanations (SHAP) value are utilized in order to determine the best hyperparameters and make the models interpretable, respectively. By leveraging balance sheet, credit, and invoice datasets, default prediction is realized for about one million companies in Turkey between the years 2010–2018. The default rates of companies range between 3%-6% by year. The experimental results are conducted on a BDA platform. According to the DPModel-1 results, the highest AUC score is ensured by random forest with 0.87. In addition, the results are improved for each technique separately by adjusting new variables with graph theory. According to DPModel-2 results, the best AUC score is achieved by random forest with 0.89.",https://doi.org/10.1016/j.eswa.2021.114840,2021,Mustafa Yıldırım and Feyza Yıldırım Okay and Suat Özdemir,BIG DATA ANALYTICS FOR DEFAULT PREDICTION USING GRAPH THEORY,article
296,24201,EXPERT SYSTEMS WITH APPLICATIONS,journal,09574174,"1,368",Q1,207,770,1945,42314,17345,1943,"8,67","54,95",United Kingdom,Western Europe,Elsevier Ltd.,1990-2021,Artificial Intelligence (Q1); Computer Science Applications (Q1); Engineering (miscellaneous) (Q1),EXPERT SYSTEMS WITH APPLICATIONS,EXPERT SYSTEMS WITH APPLICATIONS,"55,444",6.954,0.04053,"Combining query processing techniques with data quality management approaches enables enforcement of quality constraints, such as timeliness, accuracy and completeness, as part of ad-hoc query specification and execution, improving the quality of query results. Despite the emergence of novel data quality processing tools, there is a dearth of studies assessing performance and scalability in the execution of data quality assessment tasks during query processing. This paper reports on an empirical study aiming to investigate the extent to which a big data computing framework (Spark) can offer significant gains in performance and scalability when executing data quality querying tasks over a range of computational platforms including a single commodity multi-core machine and a cluster-based platform for a wide range of workloads. Our results show that substantial performance and scalability gains can be obtained by using optimized data science libraries combined with the parallel and distributed capabilities of big data computing. We also provide guidelines on choosing the appropriate computational infrastructure for executing DQ-aware queries.",https://doi.org/10.1016/j.eswa.2021.114858,2021,Sonia Cisneros-Cabrera and Anna-Valentini Michailidou and Sandra Sampaio and Pedro Sampaio and Anastasios Gounaris,EXPERIMENTING WITH BIG DATA COMPUTING FOR SCALING DATA QUALITY-AWARE QUERY PROCESSING,article
297,24201,EXPERT SYSTEMS WITH APPLICATIONS,journal,09574174,"1,368",Q1,207,770,1945,42314,17345,1943,"8,67","54,95",United Kingdom,Western Europe,Elsevier Ltd.,1990-2021,Artificial Intelligence (Q1); Computer Science Applications (Q1); Engineering (miscellaneous) (Q1),EXPERT SYSTEMS WITH APPLICATIONS,EXPERT SYSTEMS WITH APPLICATIONS,"55,444",6.954,0.04053,"In this study, a data driven predictive maintenance system was developed for production lines in manufacturing. By utilizing the data generated from IoT sensors in real-time, the system aims to detect signals for potential failures before they occur by using machine learning methods. Consequently, it helps address the issues by notifying operators early such that preventive actions can be taken prior to a production stop. In current study, the effectiveness of the system was also assessed using real-world manufacturing system IoT data. The evaluation results indicated that the predictive maintenance system was successful in identifying the indicators of potential failures and it can help prevent some production stops from happening. The findings of comparative evaluations of machine learning algorithms indicated that models of Random Forest, a bagging ensemble algorithm, and XGBoost, a boosting method, appeared to outperform the individual algorithms in the assessment. The best performing machine learning models in this study have been integrated into the production system in the factory.",https://doi.org/10.1016/j.eswa.2021.114598,2021,Serkan Ayvaz and Koray Alpay,PREDICTIVE MAINTENANCE SYSTEM FOR PRODUCTION LINES IN MANUFACTURING: A MACHINE LEARNING APPROACH USING IOT DATA IN REAL-TIME,article
298,24201,EXPERT SYSTEMS WITH APPLICATIONS,journal,09574174,"1,368",Q1,207,770,1945,42314,17345,1943,"8,67","54,95",United Kingdom,Western Europe,Elsevier Ltd.,1990-2021,Artificial Intelligence (Q1); Computer Science Applications (Q1); Engineering (miscellaneous) (Q1),EXPERT SYSTEMS WITH APPLICATIONS,EXPERT SYSTEMS WITH APPLICATIONS,"55,444",6.954,0.04053,"The entity reconciliation (ER) problem aroused much interest as a research topic in today's Big Data era, full of big and open heterogeneous data sources. This problem poses when relevant information on a topic needs to be obtained using methods based on: (i) identifying records that represent the same real world entity, and (ii) identifying those records that are similar but do not correspond to the same real-world entity. ER is an operational intelligence process, whereby organizations can unify different and heterogeneous data sources in order to relate possible matches of non-obvious entities. Besides, the complexity that the heterogeneity of data sources involves, the large number of records and differences among languages, for instance, must be added. This paper describes a Systematic Mapping Study (SMS) of journal articles, conferences and workshops published from 2010 to 2017 to solve the problem described before, first trying to understand the state-of-the-art, and then identifying any gaps in current research. Eleven digital libraries were analyzed following a systematic, semiautomatic and rigorous process that has resulted in 61 primary studies. They represent a great variety of intelligent proposals that aim to solve ER. The conclusion obtained is that most of the research is based on the operational phase as opposed to the design phase, and most studies have been tested on real-world data sources, where a lot of them are heterogeneous, but just a few apply to industry. There is a clear trend in research techniques based on clustering/blocking and graphs, although the level of automation of the proposals is hardly ever mentioned in the research work.",https://doi.org/10.1016/j.eswa.2017.03.010,2017,J.G. Enríquez and F.J. Domínguez-Mayo and M.J. Escalona and M. Ross and G. Staples,ENTITY RECONCILIATION IN BIG DATA SOURCES: A SYSTEMATIC MAPPING STUDY,article
299,24201,EXPERT SYSTEMS WITH APPLICATIONS,journal,09574174,"1,368",Q1,207,770,1945,42314,17345,1943,"8,67","54,95",United Kingdom,Western Europe,Elsevier Ltd.,1990-2021,Artificial Intelligence (Q1); Computer Science Applications (Q1); Engineering (miscellaneous) (Q1),EXPERT SYSTEMS WITH APPLICATIONS,EXPERT SYSTEMS WITH APPLICATIONS,"55,444",6.954,0.04053,"This paper describes the design and implementation of the Data Quality Query System (DQ2S), a query processing framework and tool incorporating data quality profiling functionality in the processing of queries involving quality-aware query language extensions. DQ2S supports the combination of performance and quality-oriented query optimizations, and a query processing platform that enables advanced data profiling queries to be formulated based on well established query language constructs, often used to interact with relational database management systems. DQ2S encompasses a declarative query language and a data model that provides users with the capability to express constraints on the quality of query results as well as query quality-related information; a set of algebraic operators for manipulating data quality-related information, and optimization heuristics. The proposed query language and algebra represent seamless extensions to SQL and relational database engines, respectively. The constructs of the proposed data model are implemented at the user’s view level and are internally mapped into relational model constructs. The quality-aware extensions and features are extremely useful when users need to assess the quality of relational data sets and define quality constraints for acceptable data prior to using candidate data sources in decision support systems and conducting big data analytical tasks.",https://doi.org/10.1016/j.eswa.2015.06.050,2015,Sandra de F. {Mendes Sampaio} and Chao Dong and Pedro Sampaio,DQ2S – A FRAMEWORK FOR DATA QUALITY-AWARE INFORMATION MANAGEMENT,article
300,24201,EXPERT SYSTEMS WITH APPLICATIONS,journal,09574174,"1,368",Q1,207,770,1945,42314,17345,1943,"8,67","54,95",United Kingdom,Western Europe,Elsevier Ltd.,1990-2021,Artificial Intelligence (Q1); Computer Science Applications (Q1); Engineering (miscellaneous) (Q1),EXPERT SYSTEMS WITH APPLICATIONS,EXPERT SYSTEMS WITH APPLICATIONS,"55,444",6.954,0.04053,"In recent years, the growing availability of huge amounts of information, generated in every sector at high speed and in a wide variety of forms and formats, is unprecedented. The ability to harness big data is an opportunity to obtain more accurate analyses and to improve decision-making in industry, government and many other organizations. However, handling big data may be challenging and proper data integration is a key dimension in achieving high information quality. In this paper, we propose a novel approach to data integration that calibrates online generated big data with interview based customer survey data. A common issue of customer surveys is that responses are often overly positive, making it difficult to identify areas of weaknesses in organizations. On the other hand, online reviews are often overly negative, hampering an accurate evaluation of areas of excellence. The proposed methodology calibrates the levels of unbalanced responses in different data sources via resampling and performs data integration using Bayesian Networks to propagate the new re-balanced information. In this paper we show, with a case study example, how the novel data integration approach allows businesses and organizations to get a bias corrected appraisal of the level of satisfaction of their customers. The application is based on the integration of online data of review blogs and customer satisfaction surveys from the San Francisco airport. We illustrate how this integration enhances the information quality of the data analytic work in four of InfoQ dimensions, namely, Data Structure, Data Integration, Temporal Relevance and Chronology of Data and Goal.",https://doi.org/10.1016/j.eswa.2017.12.044,2018,Luciana {Dalla Valle} and Ron Kenett,SOCIAL MEDIA BIG DATA INTEGRATION: A NEW APPROACH BASED ON CALIBRATION,article
301,22491,EUROPEAN MANAGEMENT JOURNAL,journal,02632373,"1,365",Q1,102,103,215,9317,1147,208,"5,72","90,46",United Kingdom,Western Europe,Elsevier Ltd.,1982-2020,Strategy and Management (Q1),EUROPEAN MANAGEMENT JOURNAL,EUROPEAN MANAGEMENT JOURNAL,"5,790",5.075,0.00371,"Focus
The transformative power of today's big data (BD) has allowed many companies, i.e., decision-makers, to evolve at an unprecedented pace. With regard to decision-making, artificial intelligence (AI) takes task delegation to a new level, and by employing AI-assisted tools, companies can provide their HR departments with the means to manage the existing data and HR altogether.
Objectives
To determine how HR managers assess whether BD management is facilitated by AI, and how they frame the changes necessary to meet the trends related to AI and its implementation, namely their willingness to master its implementation and to meet the possible challenges.
Methodology
Content analysis was conducted on interviews held with a sample of 16 HR practitioners from a spectrum of areas, and the findings were analysed using the big data maturity model (BDMM) framework. Domains covered by this model allow the study of decision-making trends, in terms of preparedness and willingness to tackle disruptive technology with the aim of improving and gaining the competitive edge in decision-making.
Findings
The central potential of AI lies in faster data storage and processing power, thereby leading to more insightful and effective decision-making. This article contains closer insights into the challenges underlying the implementation of AI in decision-making processes, specifically in terms of strategic alignment, governance, and implementation. The results reflect the notions regarding the nature of AI – in assisting HR – and lay out the path that precedes the extraction of BD, through the delivery of advantageous intelligence, to augment decision-making in HR.",https://doi.org/10.1016/j.emj.2022.07.001,2022,Aleksandar Radonjić and Henrique Duarte and Nádia Pereira,ARTIFICIAL INTELLIGENCE AND HRM: HR MANAGERS’ PERSPECTIVE ON DECISIVENESS AND CHALLENGES,article
302,20566,CHINA ECONOMIC REVIEW,journal,1043951X,"1,361",Q1,76,184,319,8316,1318,311,"3,96","45,20",Netherlands,Western Europe,Elsevier,1989-2020,Economics and Econometrics (Q1); Finance (Q1),CHINA ECONOMIC REVIEW,CHINA ECONOMIC REVIEW,"5,722",4.227,0.00605,"Based on mobile internet user data, we construct an “Internet population” measure and reexamine spatial population distribution in China. The location based service (LBS) data of mobile internet uses is able to capture the accurate location of users' residence and solve the underestimation problem of missing migrants. We have three main findings. First, contrary to previous studies based on traditional population statistics, city size distribution of Internet population fits well into Zipf's law with a R2 of 90.7%. Second, the Internet population indicator is superior to traditional population statistics in explaining inelastic household consumption such as water consumption, electricity consumption, and garbage disposal. It suggests that the “Internet population” is a better proxy of actual city population. Third, the traditional population statistics systematically overestimate population in small cities and underestimate population in large cities. It indicates that the public resource distortions will continue to exist or even worsen off in China if the allocation process relies greatly on traditional population statistics. Although no measures are perfect, our new population measure provides important incremental information for future discussion.",https://doi.org/10.1016/j.chieco.2022.101808,2022,Huixuan Li and Jing Chen and Zihao Chen and Jianguo Xu,URBAN POPULATION DISTRIBUTION IN CHINA: EVIDENCE FROM INTERNET POPULATION,article
303,27660,MATURITAS,journal,03785122,"1,346",Q1,105,146,527,5638,2319,475,"3,97","38,62",Ireland,Western Europe,Elsevier Ireland Ltd,1978-2021,"Biochemistry, Genetics and Molecular Biology (miscellaneous) (Q1); Obstetrics and Gynecology (Q1)",MATURITAS,MATURITAS,"9,715",4.342,0.01085,"The increasingly aging population in Europe and worldwide brings up the need for the restructuring of healthcare. Technological advancements in electronic health can be a driving force for new health management models, especially in chronic care. In a patient-centered e-health management model, communication and coordination between patient, healthcare professionals in primary care and hospitals can be facilitated, and medical decisions can be made timely and easily communicated. Bringing the right information to the right person at the right time is what connected health aims at, and this may set the basis for the investigation and deployment of the integrated care models. In this framework, an overview of the main technological axes and challenges around connected health technologies in chronic disease management are presented and discussed. A central concept is personal health system for the patient/citizen and three main application areas are identified. The connected health ecosystem is making progress, already shows benefits in (a) new biosensors, (b) data management, (c) data analytics, integration and feedback. Examples are illustrated in each case, while open issues and challenges for further research and development are pinpointed.",https://doi.org/10.1016/j.maturitas.2015.03.015,2015,Ioanna G. Chouvarda and Dimitrios G. Goulis and Irene Lambrinoudaki and Nicos Maglaveras,CONNECTED HEALTH AND INTEGRATED CARE: TOWARD NEW MODELS FOR CHRONIC DISEASE MANAGEMENT,article
304,15139,REGIONAL SCIENCE AND URBAN ECONOMICS,journal,"18792308, 01660462","1,343",Q1,79,84,236,3483,606,235,"2,36","41,46",Netherlands,Western Europe,Elsevier,1973-2020,Economics and Econometrics (Q1); Urban Studies (Q1),REGIONAL SCIENCE AND URBAN ECONOMICS,REGIONAL SCIENCE AND URBAN ECONOMICS,"4,552",2.613,0.00472,"We use a new property-level data set and an innovative methodology to estimate the price of land from 2000 to 2013 for nearly the universe of detached single-family homes in the Washington, DC metro area and to characterize the boom-bust cycle in land and house prices at a fine geography. The results show that land prices were more volatile than house prices everywhere, but especially so in the areas where land was inexpensive in 2000. We demonstrate that the change in the land share of house value during the boom was a significant predictor of the decline in house prices during the bust, highlighting the value of focusing on land in assessing house-price risk.",https://doi.org/10.1016/j.regsciurbeco.2017.06.006,2017,Morris A. Davis and Stephen D. Oliner and Edward J. Pinto and Sankar Bokka,"RESIDENTIAL LAND VALUES IN THE WASHINGTON, DC METRO AREA: NEW INSIGHTS FROM BIG DATA",article
305,28339,OCEAN ENGINEERING,journal,00298018,"1,321",Q1,100,1216,2475,51177,10979,2469,"4,31","42,09",United Kingdom,Western Europe,Elsevier BV,1968-2020,Environmental Engineering (Q1); Ocean Engineering (Q1),OCEAN ENGINEERING,OCEAN ENGINEERING,"23,463",3.795,0.0247,"Improving the operational energy efficiency of existing ships is attracting considerable interests to reduce the environmental footprint due to air emissions. As the shipping industry is entering into Shipping 4.0 with digitalization as a disruptive force, an intriguing area in the field of ship’s operational energy efficiency is big data analytics. This paper proposes a big data analytics framework for ship performance monitoring under localized operational conditions with the help of appropriate data analytics together with domain knowledge. The proposed framework is showcased through a data set obtained from a bulk carrier pertaining the detection of data anomalies, the investigation of the ship’s localized operational conditions, the identification of the relative correlations among parameters and the quantification of the ship’s performance in each of the respective conditions. The novelty of this study is to provide a KPI (i.e. key performance indicator) for ship performance quantification in order to identify the best performance trim-draft mode under the engine modes of the case study ship. The proposed framework has the features to serve as an operational energy efficiency measure to provide data quality evaluation and decision support for ship performance monitoring that is of value for both ship operators and decision-makers.",https://doi.org/10.1016/j.oceaneng.2021.109392,2021,Khanh Q. Bui and Lokukaluge P. Perera,ADVANCED DATA ANALYTICS FOR SHIP PERFORMANCE MONITORING UNDER LOCALIZED OPERATIONAL CONDITIONS,article
306,28339,OCEAN ENGINEERING,journal,00298018,"1,321",Q1,100,1216,2475,51177,10979,2469,"4,31","42,09",United Kingdom,Western Europe,Elsevier BV,1968-2020,Environmental Engineering (Q1); Ocean Engineering (Q1),OCEAN ENGINEERING,OCEAN ENGINEERING,"23,463",3.795,0.0247,"Energy efficiency of inland ships is significantly influenced by navigational environment, including wind speed and direction as well as water depth and speed. The complexity of the inland navigational environment makes it rather difficult to determine the optimal speeds under different environmental conditions to achieve the best energy efficiency. Route division according to the characteristics of these environmental factors could provide a good solution for the optimization of ship engine speed under different navigational environments. In this paper, the distributed parallel k-means clustering algorithm is adopted to achieve an elaborate route division by analyzing the corresponding environmental factors based on a self-developed big data analytics platform. Subsequently, a ship energy efficiency optimization model considering multiple environmental factors is established through analyzing the energy transfer among hull, propeller and main engine. Then, decisions are made concerning the optimal engine speeds in different segments along the path. Finally, a case study on the Yangtze River is performed to validate the present optimization method. The results show that the proposed method can effectively reduce energy consumption and CO2 emissions of ships.",https://doi.org/10.1016/j.oceaneng.2018.08.050,2018,Xinping Yan and Kai Wang and Yupeng Yuan and Xiaoli Jiang and Rudy R. Negenborn,ENERGY-EFFICIENT SHIPPING: AN APPLICATION OF BIG DATA ANALYSIS FOR OPTIMIZING ENGINE SPEED OF INLAND SHIPS CONSIDERING MULTIPLE ENVIRONMENTAL FACTORS,article
307,28339,OCEAN ENGINEERING,journal,00298018,"1,321",Q1,100,1216,2475,51177,10979,2469,"4,31","42,09",United Kingdom,Western Europe,Elsevier BV,1968-2020,Environmental Engineering (Q1); Ocean Engineering (Q1),OCEAN ENGINEERING,OCEAN ENGINEERING,"23,463",3.795,0.0247,"This study analyzes the energy efficiency of ships based on ISO 19030, which is a standard for the measurement of changes in hull and propeller performance. The goal is to provide energy efficiency management with digital indicators that have not been easily provided. The ship navigation information platform (SNIP) is developed to determine the dynamic information of each ship, including the fuel consumption, ship speed, horsepower of the engine, rotation speed of the engine, wind direction, and wind speed. In addition, model test data and computational fluid dynamics (CFD) calculation data are applied to calculate the energy efficiency performance indicators. Finally, the relationship of the speed through water and the speed over ground enables us to modify the effects of the ocean currents. The results verify that these indicators can be used as a reference for performance monitoring and maintenance prediction of international maritime affairs.",https://doi.org/10.1016/j.oceaneng.2021.108953,2021,Heiu-Jou Shaw and Cheng-Kuan Lin,MARINE BIG DATA ANALYSIS OF SHIPS FOR THE ENERGY EFFICIENCY CHANGES OF THE HULL AND MAINTENANCE EVALUATION BASED ON THE ISO 19030 STANDARD,article
308,23393,JOURNAL OF ENVIRONMENTAL SCIENCES,journal,"10010742, 18787320","1,316",Q1,99,329,1018,17571,5975,1001,"5,83","53,41",China,Asiatic Region,Chinese Academy of Sciences,"1970, 1972-1973, 1978-1985, 1993, 1995-2021",Environmental Chemistry (Q1); Environmental Engineering (Q1); Environmental Science (miscellaneous) (Q1); Medicine (miscellaneous) (Q1),JOURNAL OF ENVIRONMENTAL SCIENCES,JOURNAL OF ENVIRONMENTAL SCIENCES,"17,274",5.565,0.01334,"Traditional air quality data have a spatial resolution of 1 km or above, making it challenging to resolve detailed air pollution exposure in complex urban areas. Combining urban morphology, dynamic traffic emission, regional and local meteorology, physicochemical transformations in air quality models using big data fusion technology, an ultra-fine resolution modeling system was developed to provide air quality data down to street level. Based on one-year ultra-fine resolution data, this study investigated the effects of pollution heterogeneity on the individual and population exposure to particulate matter (PM2.5 and PM10), nitrogen dioxide (NO2), and ozone (O3) in Hong Kong, one of the most densely populated and urbanized cities. Sharp fine-scale variabilities in air pollution were revealed within individual city blocks. Using traditional 1 km average to represent individual exposure resulted in a positively skewed deviation of up to 200% for high-end exposure individuals. Citizens were disproportionally affected by air pollution, with annual pollutant concentrations varied by factors of 2 to 5 among 452 District Council Constituency Areas (DCCAs) in Hong Kong, indicating great environmental inequities among the population. Unfavorable city planning resulted in a positive spatial coincidence between pollution and population, which increased public exposure to air pollutants by as large as 46% among districts in Hong Kong. Our results highlight the importance of ultra-fine pollutant data in quantifying the heterogeneity in pollution exposure in the dense urban area and the critical role of smart urban planning in reducing exposure inequities.",https://doi.org/10.1016/j.jes.2022.02.041,2023,Wenwei Che and Yumiao Zhang and Changqing Lin and Yik Him Fung and Jimmy C.H. Fung and Alexis K.H. Lau,IMPACTS OF POLLUTION HETEROGENEITY ON POPULATION EXPOSURE IN DENSE URBAN AREAS USING ULTRA-FINE RESOLUTION AIR QUALITY DATA,article
309,130156,INTERNATIONAL JOURNAL OF SURGERY,journal,"17439191, 17439159","1,315",Q1,61,698,1214,12301,4685,959,"4,38","17,62",Netherlands,Western Europe,Elsevier BV,2003-2020,Medicine (miscellaneous) (Q1); Surgery (Q1),INTERNATIONAL JOURNAL OF SURGERY,INTERNATIONAL JOURNAL OF SURGERY,"16,011",6.071,0.01876,"Staggering statistics regarding the global burden of disease due to lack of surgical care worldwide has been gaining attention in the global health literature over the last 10 years. The Lancet Commission on Global Surgery reported that 16.9 million lives were lost due to an absence of surgical care in 2010, equivalent to 33% of all deaths worldwide. Although data from low- and middle-income countries (LMICs) are limited, recent investigations, such as the African Surgical Outcomes Study, highlight that despite operating on low risk patients, there is increased postoperative mortality in LMICs versus higher-resource settings, a majority of which occur secondary to seemingly preventable complications like surgical site infections. We propose that implementing creative, low-cost surgical outcomes monitoring and select quality improvement systems proven effective in high-income countries, such as surgical infection prevention programs and safety checklists, can enhance the delivery of safe surgical care in existing LMIC surgical systems. While efforts to initiate and expand surgical access and capacity continues to deserve attention in the global health community, here we advocate for creative modifications to current service structures, such as promoting a culture of safety, employing technology and mobile health (mHealth) for patient data collection and follow-up, and harnessing partnerships for information sharing, to create a framework for improving morbidity and mortality in responsible, scalable, and sustainable ways.",https://doi.org/10.1016/j.ijsu.2019.07.036,2019,Belain Eyob and Marissa A. Boeck and Patrick FaSiOen and Shamir Cawich and Michael D. Kluger,ENSURING SAFE SURGICAL CARE ACROSS RESOURCE SETTINGS VIA SURGICAL OUTCOMES DATA & QUALITY IMPROVEMENT INITIATIVES,article
310,21100255484,JOURNAL OF HOSPITALITY AND TOURISM MANAGEMENT,journal,"18395260, 14476770","1,310",Q1,34,153,194,12127,1119,190,"5,44","79,26",United Kingdom,Western Europe,Elsevier BV,2006-2020,"Tourism, Leisure and Hospitality Management (Q1)",JOURNAL OF HOSPITALITY AND TOURISM MANAGEMENT,JOURNAL OF HOSPITALITY AND TOURISM MANAGEMENT,"2,467",5.959,0.00232,"To address the unsolved problem of the mechanism underlying the effect of big data analytics capabilities on competitive advantage and performance, this study combined quantitative and qualitative methods to test the examined framework. The results of 257 questionnaires from hotel marketing managers and 19 semistructured interviews, confirm that big data analytics capabilities develop from big data strategies and knowledge management and enhance competitive advantage and performance through sustainability marketing. Moreover, social media enhance sustainability marketing and competitive advantage and performance. The original findings of the current research contribute to the development of big data, sustainability marketing, and social media.",https://doi.org/10.1016/j.jhtm.2022.02.026,2022,Jeou-Shyan Horng and Chih-Hsing Liu and Sheng-Fang Chou and Tai-Yi Yu and Da-Chian Hu,ROLE OF BIG DATA CAPABILITIES IN ENHANCING COMPETITIVE ADVANTAGE AND PERFORMANCE IN THE HOSPITALITY SECTOR: KNOWLEDGE-BASED DYNAMIC CAPABILITIES VIEW,article
311,21100370190,CURRENT OPINION IN FOOD SCIENCE,journal,22147993,"1,297",Q1,38,89,318,5095,1714,286,"5,16","57,25",Netherlands,Western Europe,Elsevier BV,2015-2021,Applied Microbiology and Biotechnology (Q1); Food Science (Q1),CURRENT OPINION IN FOOD SCIENCE,CURRENT OPINION IN FOOD SCIENCE,"3,298",6.031,0.00491,"The massive rise of Big Data generated from smartphones, social media, Internet of Things (IoT), and multimedia, has produced an overwhelming flow of data in either structured or unstructured format. Big Data technologies are being developed and implemented in the food supply chain that gather and analyse these data. Such technologies demand new approaches in data collection, storage, processing and knowledge extraction. In this article, an overview of the recent developments in Big Data applications in food safety are presented. This review shows that the use of Big Data in food safety remains in its infancy but it is influencing the entire food supply chain. Big Data analysis is used to provide predictive insights in several steps in the food supply chain, support supply chain actors in taking real time decisions, and design the monitoring and sampling strategies. Lastly, the main research challenges that require research efforts are introduced.",https://doi.org/10.1016/j.cofs.2020.11.006,2020,Cangyu Jin and Yamine Bouzembrak and Jiehong Zhou and Qiao Liang and Leonieke M. {van den Bulk} and Anand Gavai and Ningjing Liu and Lukas J. {van den Heuvel} and Wouter Hoenderdaal and Hans J.P. Marvin,BIG DATA IN FOOD SAFETY- A REVIEW,article
312,21100902965,MEASUREMENT,journal,"15366367, 15366359","1,293",Q1,15,16,68,535,67,43,"1,28","33,44",United Kingdom,Western Europe,Taylor and Francis Ltd.,2010-2020,Applied Mathematics (Q1); Education (Q1); Statistics and Probability (Q1),MEASUREMENT,MEASUREMENT,"21,942",3.927,0.02432,"Vibration serviceability issue has attracted increasing attentions recently. Many studies on vibration serviceability limitations have been performed in labs using simulation. The proposed limits were incompatible and lacked details about the physiological and environmental factors because of small sample sizes and unrealistic environments. This study proposes a novel online big data approach for investigating vibration serviceability limits in real environment. A smartphone-based application (App) was designed and spread to volunteers to collect multi-source heterogeneous data including questionnaires of personal judgement on vibration level, vibration signals, environmental and biological factors in their daily life. So far, 8521 records have been received. Data cleaning was performed and a qualified database with large volume and various types of factor information was produced. Analysis of the database showed that vibration limits given by the new method were compatible with previous results, but with more abundant details that were ignored in previous studies.",https://doi.org/10.1016/j.measurement.2020.107850,2020,Lei Cao and Jun Chen,ONLINE INVESTIGATION OF VIBRATION SERVICEABILITY LIMITATIONS USING SMARTPHONES,article
313,22706,INTERNATIONAL JOURNAL OF FORECASTING,journal,01692070,"1,268",Q1,96,142,277,5578,1254,261,"4,50","39,28",Netherlands,Western Europe,Elsevier,1985-2020,Business and International Management (Q1),INTERNATIONAL JOURNAL OF FORECASTING,INTERNATIONAL JOURNAL OF FORECASTING,"7,207",3.779,0.00612,,https://doi.org/10.1016/j.ijforecast.2019.05.004,2019,Tao Hong and Pierre Pinson,ENERGY FORECASTING IN THE BIG DATA WORLD,article
314,15220,ANNALS OF EMERGENCY MEDICINE,journal,"01960644, 10976760","1,241",Q1,153,392,1312,6283,2154,703,"1,47","16,03",United States,Northern America,Mosby Inc.,1980-2020,Emergency Medicine (Q1),ANNALS OF EMERGENCY MEDICINE,ANNALS OF EMERGENCY MEDICINE,"14,808",5.721,0.01573,"Clinical research often focuses on resource-intensive causal inference, whereas the potential of predictive analytics with constantly increasing big data sources remains largely unexplored. Basic prediction, divorced from causal inference, is much easier with big data. Emergency care may benefit from this simpler application of big data. Historically, predictive analytics have played an important role in emergency care as simple heuristics for risk stratification. These tools generally follow a standard approach: parsimonious criteria, easy computability, and independent validation with distinct populations. Simplicity in a prediction tool is valuable, but technological advances make it no longer a necessity. Emergency care could benefit from clinical predictions built using data science tools with abundant potential input variables available in electronic medical records. Patients’ risks could be stratified more precisely with large pools of data and lower resource requirements for comparing each clinical encounter to those that came before it, benefiting clinical decisionmaking and health systems operations. The largest value of predictive analytics comes early in the clinical encounter, in which diagnostic and prognostic uncertainty are high and resource-committing decisions need to be made. We propose an agenda for widening the application of predictive analytics in emergency care. Throughout, we express cautious optimism because there are myriad challenges related to database infrastructure, practitioner uptake, and patient acceptance. The quality of routinely compiled clinical data will remain an important limitation. Complementing big data sources with prospective data may be necessary if predictive analytics are to achieve their full potential to improve care quality in the emergency department.",https://doi.org/10.1016/j.annemergmed.2015.06.024,2016,Alexander T. Janke and Daniel L. Overbeek and Keith E. Kocher and Phillip D. Levy,EXPLORING THE POTENTIAL OF PREDICTIVE ANALYTICS AND BIG DATA IN EMERGENCY CARE,article
315,20532,JOURNAL OF AIR TRANSPORT MANAGEMENT,journal,09696997,"1,220",Q1,75,142,356,7377,1612,349,"4,68","51,95",United Kingdom,Western Europe,Elsevier Ltd.,"1994-1995, 1997-2020","Law (Q1); Management, Monitoring, Policy and Law (Q1); Strategy and Management (Q1); Transportation (Q1)",JOURNAL OF AIR TRANSPORT MANAGEMENT,JOURNAL OF AIR TRANSPORT MANAGEMENT,"4,929",4.134,0.00394,"The evaluation, acquisition and use of newly available big data sources has become a major strategic and organizational challenge for airline network planners. We address this challenge by developing a maturity model for big data readiness for airline network planning. The development of the maturity model is grounded in literature, expert interviews and case study research involving nine airlines. Four airline business models are represented, namely full-service carriers, low-cost airlines, scheduled charter airlines and cargo airlines. The maturity model has been well received with seven change requests in the model development phase. The revised version has been evaluated as exhaustive and useful by airline network planners. The self-assessment of airlines revealed low to medium maturity for most domains. Organizational factors show the lowest average maturity, IT architecture the highest. Full-service carriers seem to be more mature than airlines with different business models.",https://doi.org/10.1016/j.jairtraman.2019.101721,2020,Iris Hausladen and Maximilian Schosser,TOWARDS A MATURITY MODEL FOR BIG DATA ANALYTICS IN AIRLINE NETWORK PLANNING,article
316,30441,COMPUTERS AND ELECTRONICS IN AGRICULTURE,journal,01681699,"1,208",Q1,115,648,1300,28725,9479,1298,"7,27","44,33",Netherlands,Western Europe,Elsevier,1985-2020,Agronomy and Crop Science (Q1); Animal Science and Zoology (Q1); Computer Science Applications (Q1); Forestry (Q1); Horticulture (Q1),COMPUTERS AND ELECTRONICS IN AGRICULTURE,COMPUTERS AND ELECTRONICS IN AGRICULTURE,"17,657",5.565,0.01646,"With the rapid developments in ICT, the current agriculture businesses have become increasingly data-driven and are supported by advanced data analytics techniques. In this context, several studies have investigated the adopted data analytics platforms in the agricultural sector. However, the main characteristics and overall findings on these platforms are scattered over the various studies, and to the best of our knowledge, there has been no attempt yet to systematically synthesize the features and obstacles of the adopted data analytics platforms. This article presents the results of an in-depth systematic literature review (SLR) that has explicitly focused on the domains of the platforms, the stakeholders, the objectives, the adopted technologies, the data properties and the obstacles. According to the year-wise analysis, it is found that no relevant primary study between 2010 and 2013 was found. This implies that the research of data analytics in agricultural sectors is a popular topic from recent years, so the results from before 2010 are likely less relevant. In total, 535 papers published from 2010 to 2020 were retrieved using both automatic and manual search strategies, among which 45 journal articles were selected for further analysis. From these primary studies, 33 features and 34 different obstacles were identified. The identified features and obstacles help characterize the different data analytics platforms and pave the way for further research.",https://doi.org/10.1016/j.compag.2022.106813,2022,Ngakan {Nyoman Kutha Krisnawijaya} and Bedir Tekinerdogan and Cagatay Catal and Rik van der Tol,DATA ANALYTICS PLATFORMS FOR AGRICULTURAL SYSTEMS: A SYSTEMATIC LITERATURE REVIEW,article
317,30441,COMPUTERS AND ELECTRONICS IN AGRICULTURE,journal,01681699,"1,208",Q1,115,648,1300,28725,9479,1298,"7,27","44,33",Netherlands,Western Europe,Elsevier,1985-2020,Agronomy and Crop Science (Q1); Animal Science and Zoology (Q1); Computer Science Applications (Q1); Forestry (Q1); Horticulture (Q1),COMPUTERS AND ELECTRONICS IN AGRICULTURE,COMPUTERS AND ELECTRONICS IN AGRICULTURE,"17,657",5.565,0.01646,"The world population is estimated to reach nine billion by 2050. Many challenges are adding pressure on the current agriculture supply chains that include shrinking land sizes, ever increasing demand for natural resources and environmental issues. The agriculture systems need a major transformation from the traditional practices to precision agriculture or smart farming practices to overcome these challenges. Geographic information system (GIS) is one such technology that pushes the current methods to precision agriculture. In this paper, we present a systematic literature review (SLR) of 120 research papers on various applications of big GIS analytics (BGA) in agriculture. The selected papers are classified into two broad categories; the level of analytics and GIS applications in agriculture. The GIS applications viz., land suitability, site search and selection, resource allocation, impact assessment, land allocation, and knowledge-based systems are considered in this study. The outcome of this study is a proposed BGA framework for agriculture supply chain. This framework identifies big data analytics to play a significant role in improving the quality of GIS application in agriculture and provides the researchers, practitioners, and policymakers with guidelines on the successful management of big GIS data for improved agricultural productivity.",https://doi.org/10.1016/j.compag.2018.10.001,2018,Rohit Sharma and Sachin S. Kamble and Angappa Gunasekaran,BIG GIS ANALYTICS FRAMEWORK FOR AGRICULTURE SUPPLY CHAINS: A LITERATURE REVIEW IDENTIFYING THE CURRENT TRENDS AND FUTURE PERSPECTIVES,article
318,30441,COMPUTERS AND ELECTRONICS IN AGRICULTURE,journal,01681699,"1,208",Q1,115,648,1300,28725,9479,1298,"7,27","44,33",Netherlands,Western Europe,Elsevier,1985-2020,Agronomy and Crop Science (Q1); Animal Science and Zoology (Q1); Computer Science Applications (Q1); Forestry (Q1); Horticulture (Q1),COMPUTERS AND ELECTRONICS IN AGRICULTURE,COMPUTERS AND ELECTRONICS IN AGRICULTURE,"17,657",5.565,0.01646,"With increasing population, the demand for agricultural productivity is rising to meet the goal of “Zero Hunger”. Consequently, farmers have optimized the agricultural activities in a sustainable way with the modern technologies. This integration has boosted the agriculture production due to high potentiality in assisting the farmers. The impulse towards the technological advancement has revived the traditional agriculture methods and resulted in eco-friendly, sustainable, and efficient farming. This has revolutionized the era of smart farming which primarily alliance with modern technologies like, big data, machine learning, deep learning, swarm intelligence, internet-of-things, block chain, robotics and autonomous system, cloud-fog-edge computing, cyber physical systems, and generative adversarial networks (GAN). To cater the same, a detailed survey on ten hot-spots of smart farming is presented in this paper. The survey covers the technology-wise state-of-the-art methods along with their application domains. Moreover, the publicly available data sets with existing research challenges are investigated. Lastly, the paper concludes with suggestions to the identified problems and possible future research directions.",https://doi.org/10.1016/j.compag.2022.107217,2022,Vivek Sharma and Ashish Kumar Tripathi and Himanshu Mittal,"TECHNOLOGICAL REVOLUTIONS IN SMART FARMING: CURRENT TRENDS, CHALLENGES & FUTURE DIRECTIONS",article
319,30441,COMPUTERS AND ELECTRONICS IN AGRICULTURE,journal,01681699,"1,208",Q1,115,648,1300,28725,9479,1298,"7,27","44,33",Netherlands,Western Europe,Elsevier,1985-2020,Agronomy and Crop Science (Q1); Animal Science and Zoology (Q1); Computer Science Applications (Q1); Forestry (Q1); Horticulture (Q1),COMPUTERS AND ELECTRONICS IN AGRICULTURE,COMPUTERS AND ELECTRONICS IN AGRICULTURE,"17,657",5.565,0.01646,"Crowdsourcing, understood as outsourcing tasks or data collection by a large group of non-professionals, is increasingly used in scientific research and operational applications. In this paper, we reviewed crowdsourcing initiatives in agricultural science and farming activities and further discussed the particular characteristics of this approach in the field of agriculture. On-going crowdsourcing initiatives in agriculture were analysed and categorised according to their crowdsourcing component. We identified eight types of agricultural data and information that can be generated from crowdsourcing initiatives. Subsequently we described existing methods of quality control of the crowdsourced data. We analysed the profiles of potential contributors in crowdsourcing initiatives in agriculture, suggested ways for increasing farmers’ participation, and discussed the on-going initiatives in the light of their target beneficiaries. While crowdsourcing is reported to be an efficient way of collecting observations relevant to environmental monitoring and contributing to science in general, we pointed out that crowdsourcing applications in agriculture may be hampered by privacy issues and other barriers to participation. Close connections with the farming sector, including extension services and farm advisory companies, could leverage the potential of crowdsourcing for both agricultural research and farming applications. This paper coins the term of farmsourcing asa professional crowdsourcing strategy in farming activities and provides a source of recommendations and inspirations for future collaborative actions in agricultural crowdsourcing.",https://doi.org/10.1016/j.compag.2017.08.026,2017,Julien Minet and Yannick Curnel and Anne Gobin and Jean-Pierre Goffart and François Mélard and Bernard Tychon and Joost Wellens and Pierre Defourny,CROWDSOURCING FOR AGRICULTURAL APPLICATIONS: A REVIEW OF USES AND OPPORTUNITIES FOR A FARMSOURCING APPROACH,article
320,30441,COMPUTERS AND ELECTRONICS IN AGRICULTURE,journal,01681699,"1,208",Q1,115,648,1300,28725,9479,1298,"7,27","44,33",Netherlands,Western Europe,Elsevier,1985-2020,Agronomy and Crop Science (Q1); Animal Science and Zoology (Q1); Computer Science Applications (Q1); Forestry (Q1); Horticulture (Q1),COMPUTERS AND ELECTRONICS IN AGRICULTURE,COMPUTERS AND ELECTRONICS IN AGRICULTURE,"17,657",5.565,0.01646,"As the volume of data collected by various IoT sensors used in smart farm applications increases, the storing and processing of big data for agricultural applications become a huge challenge. The insight of this paper is that lossy compression can unleash the power of compression to IoT because, as compared with its counterpart (a lossless one), it can significantly reduce the data volume when the spatiotemporal characteristics of IoT sensor data are properly exploited. However, lossy compression faces the challenge of compressing too much data thus losing data fidelity, which might affect the quality of the data and potential analytics outcomes. To understand the impact of lossy compression on IoT data management and analytics, we evaluated four classification algorithms with reconstructed agricultural sensor data based on various energy concentration. Specifically, we applied three transformation-based lossy compression mechanisms to five real-world weather datasets collected at different sampling granularities from IoT weather stations. Our experimental results indicate that there is a strong positive correlation between the concentrated energy of the transformed coefficients and the compression ratio as well as the data quality. While we observed a general trend where much higher compression ratios can be achieved at the cost of a decrease in quality, we also observed that the impact on the classification accuracy varies among the data sets and algorithms we evaluated. Lastly, we show that the sampling granularity also influences the data fidelity in terms of the prediction performance and compression ratio.",https://doi.org/10.1016/j.compag.2018.08.045,2018,Aekyeung Moon and Jaeyoung Kim and Jialing Zhang and Seung Woo Son,EVALUATING FIDELITY OF LOSSY COMPRESSION ON SPATIOTEMPORAL DATA FROM AN IOT ENABLED SMART FARM,article
321,21100389511,ENERGY REPORTS,journal,23524847,"1,199",Q1,33,983,239,28463,1788,239,"7,37","28,96",United Kingdom,Western Europe,Elsevier Ltd.,2015-2020,Energy (miscellaneous) (Q1),ENERGY REPORTS,ENERGY REPORTS,"2,964",6.870,0.00318,"With the development of big data technology, power system has entered the era of data analysis. With the help of the massive data provided by the wide area measurement system, the power system can be easily evaluated, and the abnormal operation status can be detected and positioned. As the increase of renewable energy permeability, more new abnormal operating status have appeared in the system. Aimed at the abnormal operation state in the development of new energy, this paper proposes an oscillation location scheme based on evidence theory and support vector machine, which makes up for the limitation of single oscillation location method. The result of location analysis of oscillation energy method, oscillation phase difference method and forced oscillation phase difference location method is fused by evidence theory.",https://doi.org/10.1016/j.egyr.2022.02.022,2022,Jian Wang,A NOVEL OSCILLATION IDENTIFICATION METHOD FOR GRID-CONNECTED RENEWABLE ENERGY BASED ON BIG DATA TECHNOLOGY,article
322,21100389511,ENERGY REPORTS,journal,23524847,"1,199",Q1,33,983,239,28463,1788,239,"7,37","28,96",United Kingdom,Western Europe,Elsevier Ltd.,2015-2020,Energy (miscellaneous) (Q1),ENERGY REPORTS,ENERGY REPORTS,"2,964",6.870,0.00318,"Electric power systems are taking drastic advances in deployment of information and communication technologies; numerous new measurement devices are installed in forms of advanced metering infrastructure, distributed energy resources (DER) monitoring systems, high frequency synchronized wide-area awareness systems that with great speed are generating immense volume of energy data. However, it is still questioned that whether the today’s power system data, the structures and the tools being developed are indeed aligned with the pillars of the big data science. Further, several requirements and especial features of power systems and energy big data call for customized methods and platforms. This paper provides an assessment of the distinguished aspects in big data analytics developments in the domain of power systems. We perform several taxonomy of the existing and the missing elements in the structures and methods associated with big data analytics in power systems. We also provide a holistic outline, classifications, and concise discussions on the technical approaches, research opportunities, and application areas for energy big data analytics.",https://doi.org/10.1016/j.egyr.2017.11.002,2018,Hossein Akhavan-Hejazi and Hamed Mohsenian-Rad,POWER SYSTEMS BIG DATA ANALYTICS: AN ASSESSMENT OF PARADIGM SHIFT BARRIERS AND PROSPECTS,article
323,21100389511,ENERGY REPORTS,journal,23524847,"1,199",Q1,33,983,239,28463,1788,239,"7,37","28,96",United Kingdom,Western Europe,Elsevier Ltd.,2015-2020,Energy (miscellaneous) (Q1),ENERGY REPORTS,ENERGY REPORTS,"2,964",6.870,0.00318,"The operation control of power units is usually carried out by the control personnel with the help of distributed control system. Although it can ensure the safety of unit operation and meet the requirements of power generation loads, the economy of unit operation and the accuracy of control process still need to be further improved. Therefore, by designing multiple view mapping and association, it provides interactive visualization support for relevant experts in the key links of model establishment and evaluation. In the exploration stage of estimating model parameter, the user can get the delay range by line chart and focus + context technology, while in the model screening stage, the user can provide the combination of screening views, selecting the model by its accuracy on different data sets, and finding the model anomalies by the model structure view. Besides, in the model evaluation stage, the user can get the delay range by predicting line chart and model accuracy radar chart. In addition, the method in this paper keeps between 4.2–7.2 in most distributions, and the maximum value is 18. The time series trend of the data segment is consistent, and the absolute value of the weight coefficient is basically 0 after being superimposed, which has great advantages compared with other methods, proving the effective results of the research content in this paper.",https://doi.org/10.1016/j.egyr.2021.09.205,2021,Jianguo Qian and Bingquan Zhu and Ying Li and Zhengchai Shi,VISUAL RECOGNITION PROCESSING OF POWER MONITORING DATA BASED ON BIG DATA COMPUTING,article
324,15057,ELECTRONIC COMMERCE RESEARCH AND APPLICATIONS,journal,15674223,"1,184",Q1,74,108,188,6016,1347,188,"7,27","55,70",Netherlands,Western Europe,Elsevier,2002-2020,Computer Networks and Communications (Q1); Computer Science Applications (Q1); Management of Technology and Innovation (Q1); Marketing (Q1),ELECTRONIC COMMERCE RESEARCH AND APPLICATIONS,ELECTRONIC COMMERCE RESEARCH AND APPLICATIONS,"4,026",6.014,0.00318,"Big data has increasingly appeared as a frontier of opportunity in enhancing firm performance. However, it still is in early stages of introduction and many enterprises are still un-decisive in its adoption. The aim of this study is to propose a theoretical model based on integration of Human-Organization-Technology fit and Technology-Organization-Environment frameworks to identify the key factors affecting big data adoption and its consequent impact on the firm performance. The significant factors are gained from the literature and the research model is developed. Data was collected from top managers and/or owners of SMEs hotels in Malaysia using online survey questionnaire. Structural Equation Modelling (SEM) is used to assess the developed model and Adaptive Neuro-Fuzzy Inference Systems (ANFIS) technique is used to prioritize adoption factors based on their importance levels. The results showed that relative advantage, management support, IT expertise, and external pressure are the most important factors in the technological, organizational, human, and environmental dimensions. The results further revealed that technology is the most important influential dimension. The outcomes of this study can assist the policy makers, businesses and governments to make well-informed decisions in adopting big data.",https://doi.org/10.1016/j.elerap.2019.100921,2020,Elaheh Yadegaridehkordi and Mehrbakhsh Nilashi and Liyana Shuib and Mohd {Hairul Nizam Bin Md Nasir} and Shahla Asadi and Sarminah Samad and Nor {Fatimah Awang},THE IMPACT OF BIG DATA ON FIRM PERFORMANCE IN HOTEL INDUSTRY,article
325,12332,SAFETY SCIENCE,journal,09257535,"1,178",Q1,111,451,1047,26276,5909,1020,"5,49","58,26",Netherlands,Western Europe,Elsevier,1991-2020,"Public Health, Environmental and Occupational Health (Q1); Safety Research (Q1); Safety, Risk, Reliability and Quality (Q1)",SAFETY SCIENCE,SAFETY SCIENCE,"17,184",4.877,0.01488,"Big data and analytics have shown promise in predicting safety incidents and identifying preventative measures directed towards specific risk variables. However, the safety industry is lagging in big data utilization due to various obstacles, which may include lack of data readiness (e.g., disparate databases, missing data, low validity) and personnel competencies. This paper provides a primer on the application of big data to safety. We then describe a safety analytics readiness assessment framework that highlights system requirements and the challenges that safety professionals may encounter in meeting these requirements. The proposed framework suggests that safety analytics readiness depends on (a) the quality of the data available, (b) organizational norms around data collection, scaling, and nomenclature, (c) foundational infrastructure, including technological platforms and skills required for data collection, storage, and analysis of health and safety metrics, and (d) measurement culture, or the emergent social patterns between employees, data acquisition, and analytic processes. A safety-analytics readiness assessment can assist organizations with understanding current capabilities so measurement systems can be matured to accommodate more advanced analytics for the ultimate purpose of improving decisions that mitigate injury and incidents.",https://doi.org/10.1016/j.ssci.2021.105569,2022,Maira E. Ezerins and Timothy D. Ludwig and Tara O'Neil and Anne M. Foreman and Yalçın Açıkgöz,ADVANCING SAFETY ANALYTICS: A DIAGNOSTIC FRAMEWORK FOR ASSESSING SYSTEM READINESS WITHIN OCCUPATIONAL SAFETY AND HEALTH,article
326,12332,SAFETY SCIENCE,journal,09257535,"1,178",Q1,111,451,1047,26276,5909,1020,"5,49","58,26",Netherlands,Western Europe,Elsevier,1991-2020,"Public Health, Environmental and Occupational Health (Q1); Safety Research (Q1); Safety, Risk, Reliability and Quality (Q1)",SAFETY SCIENCE,SAFETY SCIENCE,"17,184",4.877,0.01488,"The paper at hand motivates, proposes, demonstrates, and evaluates a novel systematic approach to discovering causal dependencies between events encoded in large arrays of data, called causality mining. The approach has emerged in the discussions with our project partner, an Australian public energy company. It was successfully evaluated in a case study with the project partner to extract valuable, and otherwise unknown, information on the causal dependencies between observations reported by the company’s employees as part of the organizational health and safety management practices and incidents that had occurred at the organization’s sites. The dependencies were derived based on the notion of proximity of the observations and incidents. The setup and results of the evaluation are reported in this paper. The new approach and the delivered insights aim at improving the overall health and safety culture of the project partner practices, as they can be applied to caution and, thus, prevent future incidents.",https://doi.org/10.1016/j.ssci.2019.04.045,2019,Artem Polyvyanyy and Anastasiia Pika and Moe T. Wynn and Arthur H.M. {ter Hofstede},A SYSTEMATIC APPROACH FOR DISCOVERING CAUSAL DEPENDENCIES BETWEEN OBSERVATIONS AND INCIDENTS IN THE HEALTH AND SAFETY DOMAIN,article
327,12332,SAFETY SCIENCE,journal,09257535,"1,178",Q1,111,451,1047,26276,5909,1020,"5,49","58,26",Netherlands,Western Europe,Elsevier,1991-2020,"Public Health, Environmental and Occupational Health (Q1); Safety Research (Q1); Safety, Risk, Reliability and Quality (Q1)",SAFETY SCIENCE,SAFETY SCIENCE,"17,184",4.877,0.01488,"It is clear that big data has numerous potential impacts in many fields. However, few papers discussed its applications in the field of safety science research. Additionally, there exist many problems that cannot be ignored when big data is applied to safety science, most outstanding of which is lack of universal supporting theory that guides how to apply big data to safety science research like methods, principles and approaches, etc. In other terms, it is not enough for big data to be viewed asa strong enabler for safety science applications mainly due to lack of universal and basic theory from the perspective of safety science. Considering the above analyzes, the two key objectives of this paper are: (1) to propose the connotation of safety big data (SBD) and its applying rules, methods and principles, and (2) to put forward some application prospects and challenges of big data to safety science research seen from theoretical research. First, by comparing SBD and traditional safety small data (SSD) from four aspects including theoretical research, typical research method, specific analysis method and processing mode, this paper puts forward the definition and connotation of SBD. Subsequently this paper further summarizes and extracts the application rules and methods of SBD. And then nine principles of SBD are explored and their relationship and application are addressed from the view of theory architecture and working framework in data processing flow. At last, this paper also discusses the potential applications and some hot issues of SBD. Overall, this paper will play an essential role in supporting the application of SBD. In addition, it will fill in the theory gaps in the field of SBD beyond traditional safety statistics, and further enriches the contents of safety science.",https://doi.org/10.1016/j.ssci.2017.08.012,2018,Qiumei Ouyang and Chao Wu and Lang Huang,"METHODOLOGIES, PRINCIPLES AND PROSPECTS OF APPLYING BIG DATA IN SAFETY SCIENCE RESEARCH",article
328,12332,SAFETY SCIENCE,journal,09257535,"1,178",Q1,111,451,1047,26276,5909,1020,"5,49","58,26",Netherlands,Western Europe,Elsevier,1991-2020,"Public Health, Environmental and Occupational Health (Q1); Safety Research (Q1); Safety, Risk, Reliability and Quality (Q1)",SAFETY SCIENCE,SAFETY SCIENCE,"17,184",4.877,0.01488,"Safety data and information are the most valuable assets for organizations’ safety decision-making (SDM), especially in the era of big data (BD). In this study, a conceptual framework for SDM based on BD, known as BD-driven SDM, was developed and its detailed structure and elements as well as strategies were presented. Other theoretical and practical contributions include: (a) the description of the meta-process and interdisciplinary research area of BD-driven SDM, (b) the design of six types of general analytics and five types of special analytics for SBD mining according to different requirements of safety management applications, (c) the analysis of influencing factors of BD-driven SDM, and (d) the discussion of advantages and limitations in this research as well as suggestions for future research. The results obtained from this study are of important implications for research and practice on BD-driven SDM.",https://doi.org/10.1016/j.ssci.2018.05.012,2018,Lang Huang and Chao Wu and Bing Wang and Qiumei Ouyang,BIG-DATA-DRIVEN SAFETY DECISION-MAKING: A CONCEPTUAL FRAMEWORK AND ITS INFLUENCING FACTORS,article
329,12469,ORTHOPEDIC CLINICS OF NORTH AMERICA,journal,"00305898, 15581373","1,177",Q1,86,61,186,2288,437,153,"2,01","37,51",United Kingdom,Western Europe,W.B. Saunders Ltd,1970-2020,Orthopedics and Sports Medicine (Q1),ORTHOPEDIC CLINICS OF NORTH AMERICA,ORTHOPEDIC CLINICS OF NORTH AMERICA,"3,693",2.472,0.00273,,https://doi.org/10.1016/j.ocl.2016.05.009,2016,Steven H. Shaha and Zain Sayeed and Afshin A. Anoushiravani and Mouhanad M. El-Othmani and Khaled J. Saleh,"BIG DATA, BIG PROBLEMS: INCORPORATING MISSION, VALUES, AND CULTURE IN PROVIDER AFFILIATIONS",article
330,18174,CONTROL ENGINEERING PRACTICE,journal,09670661,"1,175",Q1,119,196,615,8010,2779,611,"4,42","40,87",United Kingdom,Western Europe,Elsevier Ltd.,1993-2020,Applied Mathematics (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Electrical and Electronic Engineering (Q1),CONTROL ENGINEERING PRACTICE,CONTROL ENGINEERING PRACTICE,"8,368",3.475,0.00717,"In this paper, a novel robust Bayesian network is proposed for process modeling with low-quality data. Since unreliable data can cause model parameters to deviate from the real distributions and make network structures unable to characterize the true causalities, data quality feature is utilized to improve the process modeling and monitoring performance. With a predetermined trustworthy center, the data quality measurement results can be evaluated through an exponential function with Mahalanobis distances. The conventional Bayesian network learning algorithms including structure learning and parameter learning are modified by the quality feature in a weighting form, intending to extract useful information and make a reasonable model. The effectiveness of the proposed method is demonstrated through TE benchmark process and a real industrial process.",https://doi.org/10.1016/j.conengprac.2020.104344,2020,Guangjie Chen and Zhiqiang Ge,ROBUST BAYESIAN NETWORKS FOR LOW-QUALITY DATA MODELING AND PROCESS MONITORING APPLICATIONS,article
331,13754,PROCESS SAFETY AND ENVIRONMENTAL PROTECTION,journal,"17443598, 09575820","1,173",Q1,76,417,1079,20761,6888,1071,"6,41","49,79",United Kingdom,Western Europe,Institution of Chemical Engineers,1990-2021,"Chemical Engineering (miscellaneous) (Q1); Environmental Engineering (Q1); Safety, Risk, Reliability and Quality (Q1); Environmental Chemistry (Q2)",PROCESS SAFETY AND ENVIRONMENTAL PROTECTION,PROCESS SAFETY AND ENVIRONMENTAL PROTECTION,"12,452",6.158,0.01335,"In the age of big data, intelligence, and Industry 4.0, intelligence plays an increasingly significant role in management or, more specifically, decision making; thus, it becomes a popular topic and is recognised as an important discipline. Hence, safety intelligence (SI) as a new safety concept and term was proposed. SI aims to transform raw safety data and information into meaningful and actionable information for safety management; it is considered an essential perspective for safety management in the era of Safety 4.0 (computational safety science—a new paradigm for safety science in the age of big data, intelligence, and Industry 4.0). However, thus far, no existing research provides a framework that comprehensively describes SI and guides the implementation of SI practices in organisations. To address this research gap and to provide a framework for SI and its practice in the context of safety management, based on a systematic and comprehensive explanation on SI from different perspectives, this study attempts to propose a theoretical framework for SI from a safety management perspective and then presents an SI practice model aimed at supporting safety management in organisations.",https://doi.org/10.1016/j.psep.2020.10.008,2021,Bing Wang,SAFETY INTELLIGENCE AS AN ESSENTIAL PERSPECTIVE FOR SAFETY MANAGEMENT IN THE ERA OF SAFETY 4.0: FROM A THEORETICAL TO A PRACTICAL FRAMEWORK,article
332,21100228018,INTERNATIONAL JOURNAL OF DISASTER RISK REDUCTION,journal,22124209,"1,161",Q1,45,562,856,35838,4363,844,"4,78","63,77",United Kingdom,Western Europe,Elsevier Ltd.,2012-2020,Geology (Q1); Geotechnical Engineering and Engineering Geology (Q1); Safety Research (Q1),INTERNATIONAL JOURNAL OF DISASTER RISK REDUCTION,INTERNATIONAL JOURNAL OF DISASTER RISK REDUCTION,"6,931",4.320,0.01003,"The effects of data governance (as a means to maximize big data value creation in fire risk management) performance on fire risk was analyzed based on multi-source statistical data of 105 cities in China from 2016 to 2018. Specifically, data governance was first quantified with ten detailed indicators, which were then selected for explaining urban fire risk through correlation analysis. Next, the sample cities were clustered in terms of major socio-economic characteristics, and then the effects of data governance were examined by constructing multivariate regression models for each city cluster with ordinary least squares (OLS). The results showed that the constructed regression models produced good interpretation of fire risk in different types of cities, with coefficient of determination (R2) in each model exceeding 0.65. Among the indicators, the development of infrastructures (e.g. data collection devices and data analysis platforms), the level of data use, and the updating of fire risk related data were proved to produce significant effects on the reduction of fire frequency and fire consequence. Moreover, the organizational maturity of data governance was proved to be helpful in reducing fire frequency. For the cities with large population, the cross-department sharing of high-value data was found to be another important determinant of urban fire frequency. In comparison with existing statistical models which interpreted fire risk with general social factors (with the highest R2 = 0.60), these new regression models presented a better statistical performance (with the average R2 = 0.72). These findings are expected to provide decision support for the local governments of China and other jurisdictions to facilitate big data projects in improving fire risk management.",https://doi.org/10.1016/j.ijdrr.2022.103138,2022,Zhao-Ge Liu and Xiang-Yang Li and Grunde Jomaas,EFFECTS OF GOVERNMENTAL DATA GOVERNANCE ON URBAN FIRE RISK: A CITY-WIDE ANALYSIS IN CHINA,article
333,12212,JOURNAL OF CRITICAL CARE,journal,"15578615, 08839441","1,149",Q1,83,300,1095,9042,2889,847,"2,53","30,14",Netherlands,Western Europe,Elsevier BV,1986-2020,Critical Care and Intensive Care Medicine (Q1),JOURNAL OF CRITICAL CARE,JOURNAL OF CRITICAL CARE,"9,746",3.425,0.01662,"The digitalization of the Intensive Care Unit (ICU) led to an increasing amount of clinical data being collected at the bedside. The term “Big Data” can be used to refer to the analysis of these datasets that collect enormous amount of data of different origin and format. Complexity and variety define the value of Big Data. In fact, the retrospective analysis of these datasets allows to generate new knowledge, with consequent potential improvements in the clinical practice. Despite the promising start of Big Data analysis in medical research, which has seen a rising number of peer-reviewed articles, very limited applications have been used in ICU clinical practice. A close future effort should be done to validate the knowledge extracted from clinical Big Data and implement it in the clinic. In this article, we provide an introduction to Big Data in the ICU, from data collection and data analysis, to the main successful examples of prognostic, predictive and classification models based on ICU data. In addition, we focus on the main challenges that these models face to reach the bedside and effectively improve ICU care.",https://doi.org/10.1016/j.jcrc.2020.09.002,2020,Giorgia Carra and Jorge I.F. Salluh and Fernando José {da Silva Ramos} and Geert Meyfroidt,DATA-DRIVEN ICU MANAGEMENT: USING BIG DATA AND ALGORITHMS TO IMPROVE OUTCOMES,article
334,12212,JOURNAL OF CRITICAL CARE,journal,"15578615, 08839441","1,149",Q1,83,300,1095,9042,2889,847,"2,53","30,14",Netherlands,Western Europe,Elsevier BV,1986-2020,Critical Care and Intensive Care Medicine (Q1),JOURNAL OF CRITICAL CARE,JOURNAL OF CRITICAL CARE,"9,746",3.425,0.01662,,https://doi.org/10.1016/j.jcrc.2019.09.005,2019,Walter Verbrugghe and Kirsten Colpaert,"THE ELECTRONIC MEDICAL RECORD: BIG DATA, LITTLE INFORMATION?",article
335,29805,ISA TRANSACTIONS,journal,00190578,"1,147",Q1,79,559,892,22562,5669,884,"6,08","40,36",United States,Northern America,"ISA - Instrumentation, Systems, and Automation Society",1968-2020,Applied Mathematics (Q1); Computer Science Applications (Q1); Control and Systems Engineering (Q1); Electrical and Electronic Engineering (Q1); Instrumentation (Q1),ISA TRANSACTIONS,ISA TRANSACTIONS,"10,483",5.468,0.01283,"The data of the power Internet of Things (IOT) system is transferred from the IaaS layer to the SaaS layer. The general data preprocessing method mainly solves the problem of big data anomalies and missing at the PaaS layer, but it still lacks the ability to judge the high error data that meets the timing characteristics, making it difficult to deal with heterogeneous power inconsistent issues. This paper shows this phenomenon and its physical mechanism, showing the difficulty of building a quantitative model forward. A data-driven method is needed to form a hybrid model to correct the data. The research object is the electricity meter data on both sides of a commercial building transformer, which comes from different power IOT systems. The low-voltage side was revised based on the high-voltage side. Compared with the correction method based on purely using neural networks, the combined method, Linear Regression (LS) + Differential Evolution (DE) + Extreme Learning Machine (ELM), further reduces the deviation from approximately 4% to 1%.",https://doi.org/10.1016/j.isatra.2021.01.056,2021,Haoyu Jiang and Kai Chen and Quanbo Ge and Jinqiang Xu and Yingying Fu and Chunxi Li,DATA CONSISTENCY METHOD OF HETEROGENEOUS POWER IOT BASED ON HYBRID MODEL,article
336,27277,JOURNAL OF NETWORK AND COMPUTER APPLICATIONS,journal,"10958592, 10848045","1,145",Q1,105,187,713,13916,5998,701,"8,17","74,42",United States,Northern America,Academic Press Inc.,1996-2020,Computer Networks and Communications (Q1); Computer Science Applications (Q1); Hardware and Architecture (Q1),JOURNAL OF NETWORK AND COMPUTER APPLICATIONS,JOURNAL OF NETWORK AND COMPUTER APPLICATIONS,"9,700",6.281,0.0115,"In the Internet of Things (IoT), data gathered from a global-scale deployment of smart-things, are the base for making intelligent decisions and providing services. If data are of poor quality, decisions are likely to be unsound. Data quality (DQ) is crucial to gain user engagement and acceptance of the IoT paradigm and services. This paper aims at enhancing DQ in IoT by providing an overview of its state-of-the-art. Data properties and their new lifecycle in IoT are surveyed. The concept of DQ is defined and a set of generic and domain-specific DQ dimensions, fit for use in assessing IoT's DQ, are selected. IoT-related factors endangering the DQ and their impact on various DQ dimensions and on the overall DQ are exhaustively analyzed. DQ problems manifestations are discussed and their symptoms identified. Data outliers, as a major DQ problem manifestation, their underlying knowledge and their impact in the context of IoT and its applications are studied. Techniques for enhancing DQ are presented with a special focus on data cleaning techniques which are reviewed and compared using an extended taxonomy to outline their characteristics and their fitness for use for IoT. Finally, open challenges and possible future research directions are discussed.",https://doi.org/10.1016/j.jnca.2016.08.002,2016,Aimad Karkouch and Hajar Mousannif and Hassan {Al Moatassime} and Thomas Noel,DATA QUALITY IN INTERNET OF THINGS: A STATE-OF-THE-ART SURVEY,article
337,27277,JOURNAL OF NETWORK AND COMPUTER APPLICATIONS,journal,"10958592, 10848045","1,145",Q1,105,187,713,13916,5998,701,"8,17","74,42",United States,Northern America,Academic Press Inc.,1996-2020,Computer Networks and Communications (Q1); Computer Science Applications (Q1); Hardware and Architecture (Q1),JOURNAL OF NETWORK AND COMPUTER APPLICATIONS,JOURNAL OF NETWORK AND COMPUTER APPLICATIONS,"9,700",6.281,0.0115,"Over the last years, big data has emerged as a new paradigm for the processing and analysis of massive volumes of data. Big data processing has been combined with service and cloud computing, leading to a new class of services called “Big Services”. In this new model, services can be seen as an abstract layer that hides the complexity of the processed big data. To meet users' complex and heterogeneous needs in the era of big data, service reuse is a natural and efficient means that helps orchestrating available services' operations, to provide customer on-demand big services. However different from traditional Web service composition, composing big services refers to the reuse of, not only existing high-quality services, but also high-quality data sources, while taking into account their security constraints (e.g., data provenance, threat level and data leakage). Moreover, composing heterogeneous and large-scale data-centric services faces several challenges, apart from security risks, such as the big services' high execution time and the incompatibility between providers' policies across multiple domains and clouds. Aiming to solve the above issues, we propose a scalable approach for big service composition, which considers not only the quality of reused services (QoS), but also the quality of their consumed data sources (QoD). Since the correct representation of big services requirements is the first step towards an effective composition, we first propose a quality model for big services and we quantify the data breaches using L-Severity metrics. Then to facilitate processing and mining big services' related information during composition, we exploit the strong mathematical foundation of fuzzy Relational Concept Analysis (fuzzy RCA) to build the big services' repository as a lattice family. We also used fuzzy RCA to cluster services and data sources based on various criteria, including their quality levels, their domains, and the relationships between them. Finally, we define algorithms that parse the lattice family to select and compose high-quality and secure big services in a parallel fashion. The proposed method, which is implemented on top of Spark big data framework, is compared with two existing approaches, and experimental studies proved the effectiveness of our big service composition approach in terms of QoD-aware composition, scalability, and security breaches.",https://doi.org/10.1016/j.jnca.2020.102732,2020,Mokhtar Sellami and Haithem Mezni and Mohand Said Hacid,ON THE USE OF BIG DATA FRAMEWORKS FOR BIG SERVICE COMPOSITION,article
338,27277,JOURNAL OF NETWORK AND COMPUTER APPLICATIONS,journal,"10958592, 10848045","1,145",Q1,105,187,713,13916,5998,701,"8,17","74,42",United States,Northern America,Academic Press Inc.,1996-2020,Computer Networks and Communications (Q1); Computer Science Applications (Q1); Hardware and Architecture (Q1),JOURNAL OF NETWORK AND COMPUTER APPLICATIONS,JOURNAL OF NETWORK AND COMPUTER APPLICATIONS,"9,700",6.281,0.0115,"Big Data Cyber Security Analytics (BDCA) systems use big data technologies (e.g., Apache Spark) to collect, store, and analyse a large volume of security event data for detecting cyber-attacks. The volume of digital data in general and security event data in specific is increasing exponentially. The velocity with which security event data is generated and fed into a BDCA system is unpredictable. Therefore, a BDCA system should be highly scalable to deal with the unpredictable increase/decrease in the velocity of security event data. However, there has been little effort to investigate the scalability of BDCA systems to identify and exploit the sources of scalability improvement. In this paper, we first investigate the scalability of a Spark-based BDCA system with default Spark settings. We then identify Spark configuration parameters (e.g., execution memory) that can significantly impact the scalability of a BDCA system. Based on the identified parameters, we finally propose a parameter-driven adaptation approach, SCALER, for optimizing a system's scalability. We have conducted a set of experiments by implementing a Spark-based BDCA system on a large-scale OpenStack cluster. We ran our experiments with four security datasets. We have found that (i) a BDCA system with default settings of Spark configuration parameters deviates from ideal scalability by 59.5% (ii) 9 out of 11 studied Spark configuration parameters significantly impact scalability and (iii) SCALER improves the BDCA system's scalability by 20.8% compared to the scalability with default Spark parameter setting. The findings of our study highlight the importance of exploring the parameter space of the underlying big data framework (e.g., Apache Spark) for scalable cyber security analytics.",https://doi.org/10.1016/j.jnca.2021.103294,2022,Faheem Ullah and M. Ali Babar,ON THE SCALABILITY OF BIG DATA CYBER SECURITY ANALYTICS SYSTEMS,article
339,27277,JOURNAL OF NETWORK AND COMPUTER APPLICATIONS,journal,"10958592, 10848045","1,145",Q1,105,187,713,13916,5998,701,"8,17","74,42",United States,Northern America,Academic Press Inc.,1996-2020,Computer Networks and Communications (Q1); Computer Science Applications (Q1); Hardware and Architecture (Q1),JOURNAL OF NETWORK AND COMPUTER APPLICATIONS,JOURNAL OF NETWORK AND COMPUTER APPLICATIONS,"9,700",6.281,0.0115,"The crowd's power, combined with the sensing capabilities of smart mobile de-vices, has resulted in the emergence of a revolutionary data acquisition paradigm known as Mobile Crowdsensing. In exchange for rewards, mobile users collect and share location-specific data values. However, most existing crowdsensing systems built on traditional centralized architectures are highly prone to attacks, intrusions, single point of failure, manipulations, and low reliability. Recently, decentralized blockchain technologies are being applied in mobile crowdsensing systems to ensure workers' privacy, data privacy, and the quality of sensed data at a low service fee. By leveraging blockchain technology, this paper inherits the advantages of the public blockchain without the need for any trusted third-parties. We propose a smart contract-based privacy-preserving data aggregation and quality assessment protocol to infer reliable aggregated results and estimate data quality, wherein, we design a fair quality-driven incentive mechanism to distribute rewards based on the data quality. The protocol ensures a secure, cost-optimal, and reliable aggregation and estimation of the sensed data quality on the public blockchain without disclosing the sensed data's and participants' privacy. We adopt Interplanetary File Systems to offset the blockchain's expensive storage costs. Experiments were conducted using real-world datasets which were implemented on a full-stack on-chain and off-chain decentralized application on the Ethereum blockchain. The experimental results demonstrate our design is highly efficient in achieving privacy-preserving data aggregation and significantly reduces on-chain computation costs.",https://doi.org/10.1016/j.jnca.2022.103483,2022,Ruiyun Yu and Ann Move Oguti and Dennis Reagan Ochora and Shuchen Li,TOWARDS A PRIVACY-PRESERVING SMART CONTRACT-BASED DATA AGGREGATION AND QUALITY-DRIVEN INCENTIVE MECHANISM FOR MOBILE CROWDSENSING,article
340,27277,JOURNAL OF NETWORK AND COMPUTER APPLICATIONS,journal,"10958592, 10848045","1,145",Q1,105,187,713,13916,5998,701,"8,17","74,42",United States,Northern America,Academic Press Inc.,1996-2020,Computer Networks and Communications (Q1); Computer Science Applications (Q1); Hardware and Architecture (Q1),JOURNAL OF NETWORK AND COMPUTER APPLICATIONS,JOURNAL OF NETWORK AND COMPUTER APPLICATIONS,"9,700",6.281,0.0115,"There is a rapid increase in the adoption of emerging technologies like the Internet of Things (IoT), Unmanned Aerial Vehicles (UAV), Internet of Underground Things (IoUT), Data analytics in the agriculture domain to meet the increased food demand to cater to the increasing population. Agriculture 4.0 is set to revolutionize agriculture productivity by using Precision Agriculture (PA), IoT, UAVs, IoUT, and other technologies to increase agriculture produce for growing demographics while addressing various farm-related issues. This survey provides a comprehensive overview of how multiple technologies such as IoT, UAVs, IoUT, Big Data Analytics, Deep Learning Techniques, and Machine Learning methods can be used to manage various farm-related operations. For each of these technologies, a detailed review is done on how the technology is being used in Agriculture 4.0. These discussions include an overview of relevant technologies, their use cases, existing case studies, and research works that demonstrate the use of these technologies in Agriculture 4.0. This paper also highlights the various future research gaps in the adoption of these technologies in Agriculture 4.0.",https://doi.org/10.1016/j.jnca.2021.103107,2021,Meghna Raj and Shashank Gupta and Vinay Chamola and Anubhav Elhence and Tanya Garg and Mohammed Atiquzzaman and Dusit Niyato,A SURVEY ON THE ROLE OF INTERNET OF THINGS FOR ADOPTING AND PROMOTING AGRICULTURE 4.0,article
341,27277,JOURNAL OF NETWORK AND COMPUTER APPLICATIONS,journal,"10958592, 10848045","1,145",Q1,105,187,713,13916,5998,701,"8,17","74,42",United States,Northern America,Academic Press Inc.,1996-2020,Computer Networks and Communications (Q1); Computer Science Applications (Q1); Hardware and Architecture (Q1),JOURNAL OF NETWORK AND COMPUTER APPLICATIONS,JOURNAL OF NETWORK AND COMPUTER APPLICATIONS,"9,700",6.281,0.0115,"The rapid growth of emerging applications and the evolution of cloud computing technologies have significantly enhanced the capability to generate vast amounts of data. Thus, it has become a great challenge in this big data era to manage such voluminous amount of data. The recent advancements in big data techniques and technologies have enabled many enterprises to handle big data efficiently. However, these advances in techniques and technologies have not yet been studied in detail and a comprehensive survey of this domain is still lacking. With focus on big data management, this survey aims to investigate feasible techniques of managing big data by emphasizing on storage, pre-processing, processing and security. Moreover, the critical aspects of these techniques are analyzed by devising a taxonomy in order to identify the problems and proposals made to alleviate these problems. Furthermore, big data management techniques are also summarized. Finally, several future research directions are presented.",https://doi.org/10.1016/j.jnca.2016.04.008,2016,Aisha Siddiqa and Ibrahim Abaker Targio Hashem and Ibrar Yaqoob and Mohsen Marjani and Shahabuddin Shamshirband and Abdullah Gani and Fariza Nasaruddin,A SURVEY OF BIG DATA MANAGEMENT: TAXONOMY AND STATE-OF-THE-ART,article
342,27277,JOURNAL OF NETWORK AND COMPUTER APPLICATIONS,journal,"10958592, 10848045","1,145",Q1,105,187,713,13916,5998,701,"8,17","74,42",United States,Northern America,Academic Press Inc.,1996-2020,Computer Networks and Communications (Q1); Computer Science Applications (Q1); Hardware and Architecture (Q1),JOURNAL OF NETWORK AND COMPUTER APPLICATIONS,JOURNAL OF NETWORK AND COMPUTER APPLICATIONS,"9,700",6.281,0.0115,"Affective computing is an emerging multidisciplinary research field that is increasingly drawing the attention of researchers and practitioners in various fields, including artificial intelligence, natural language processing, cognitive and social sciences. Research in affective computing includes areas such as sentiment, emotion, and opinion modelling. The internet is an excellent source of data required for sentiment analysis, such as customer reviews of products, social media, forums, blogs, etc. Most of these data, called big data, are unstructured and unorganized. Hence there is a strong demand for developing suitable data processing techniques to process these rich and valuable data to produce useful information. Early surveys on sentiment and emotion recognition in the literature have been limited to discussions using text, audio, and visual modalities. So far, to the author's knowledge, a comprehensive survey combining physiological modalities with these other modalities for affective computing has yet to be reported. The objective of this paper is to fill the gap in this surveyed area. The usage of physiological modalities for affective computing brings several benefits in that the signals can be used in different environmental conditions, more robust systems can be constructed in combination with other modalities, and it has increased anti-spoofing characteristics. The paper includes extensive reviews on different frameworks and categories for state-of-the-art techniques, critical analysis of their performances, and discussions of their applications, trends and future directions to serve as guidelines for readers towards this emerging research area.",https://doi.org/10.1016/j.jnca.2019.102447,2020,Nusrat J. Shoumy and Li-Minn Ang and Kah Phooi Seng and D.M.Motiur Rahaman and Tanveer Zia,"MULTIMODAL BIG DATA AFFECTIVE ANALYTICS: A COMPREHENSIVE SURVEY USING TEXT, AUDIO, VISUAL AND PHYSIOLOGICAL SIGNALS",article
343,27277,JOURNAL OF NETWORK AND COMPUTER APPLICATIONS,journal,"10958592, 10848045","1,145",Q1,105,187,713,13916,5998,701,"8,17","74,42",United States,Northern America,Academic Press Inc.,1996-2020,Computer Networks and Communications (Q1); Computer Science Applications (Q1); Hardware and Architecture (Q1),JOURNAL OF NETWORK AND COMPUTER APPLICATIONS,JOURNAL OF NETWORK AND COMPUTER APPLICATIONS,"9,700",6.281,0.0115,"With an exponential increase in the provisioning of multimedia devices over the Internet of Things (IoT), a significant amount of multimedia data (also referred to as multimedia big data – MMBD) is being generated. Current research and development activities focus on scalar sensor data based IoT or general MMBD and overlook the complexity of facilitating MMBD over IoT. This paper examines the unique nature and complexity of MMBD computing for IoT applications and develops a comprehensive taxonomy for MMBD abstracted into a novel process model reflecting MMBD over IoT. This process model addresses a number of research challenges associated with MMBD, such as scalability, accessibility, reliability, heterogeneity, and Quality of Service (QoS) requirements. A case study is presented to demonstrate the process model.",https://doi.org/10.1016/j.jnca.2018.09.014,2018,Aparna Kumari and Sudeep Tanwar and Sudhanshu Tyagi and Neeraj Kumar and Michele Maasberg and Kim-Kwang Raymond Choo,MULTIMEDIA BIG DATA COMPUTING AND INTERNET OF THINGS APPLICATIONS: A TAXONOMY AND PROCESS MODEL,article
344,23161,FOOD QUALITY AND PREFERENCE,journal,09503293,"1,135",Q1,120,227,558,12290,3000,548,"5,43","54,14",United Kingdom,Western Europe,Elsevier Ltd.,"1988-1991, 1993-2021",Food Science (Q1); Nutrition and Dietetics (Q1),FOOD QUALITY AND PREFERENCE,FOOD QUALITY AND PREFERENCE,"13,058",5.565,0.00829,"As sensory evaluation relies upon humans accurately communicating their sensory experience, the diverse and overlapping vocabulary of flavor descriptors remains a major challenge. The lexicon generation protocols used in methods like Descriptive Analysis are expensive and time-consuming, while the post-facto analyses of natural vocabulary in “quick and dirty” methods like Free Choice or Flash Profiling require considerable subjective decision-making on the part of the analyst. A potential alternative for producing lexicons and analyzing the sensory attributes of products in nonstandardized text can be found in Natural Language Processing (NLP). NLP tools allow for the analysis of larger volumes of free text with fewer subjective decisions. This paper describes the steps necessary to automatically collect, clean, and analyze existing product descriptions from the web. As a case study, online reviews of international whiskies from two prominent websites (2309 reviews from WhiskyCast and 4289 reviews from WhiskyAdvocate) were collected, preprocessed to only retain potentially-descriptive nouns, adjectives, and verbs, and then the final term list was grouped into a flavor wheel using Correspondence Analysis and Agglomerative Hierarchical Clustering. The wheel is compared to an existing Scotch flavor wheel. The ease of collecting nonstandardized descriptions of products and the improved speed of automated methods can facilitate collection of descriptive sensory data for products where no lexicon exists. This has the potential to speed up and standardize many of the bottlenecks in rapid descriptive methods and facilitate the collection and use of very large datasets of product descriptions.",https://doi.org/10.1016/j.foodqual.2020.103926,2020,Leah M. Hamilton and Jacob Lahne,FAST AND AUTOMATED SENSORY ANALYSIS: USING NATURAL LANGUAGE PROCESSING FOR DESCRIPTIVE LEXICON DEVELOPMENT,article
345,23689,INTERNATIONAL JOURNAL OF MEDICAL INFORMATICS,journal,"18728243, 13865056","1,124",Q1,106,210,579,9073,3102,572,"4,82","43,20",Ireland,Western Europe,Elsevier Ireland Ltd,1996-2020,Health Informatics (Q1),INTERNATIONAL JOURNAL OF MEDICAL INFORMATICS,INTERNATIONAL JOURNAL OF MEDICAL INFORMATICS,"7,651",4.046,0.01044,"Background
Big data analytics promise insights into healthcare processes and management, improving outcomes while reducing costs. However, data quality is a major challenge for reliable results. Business process discovery techniques and an associated data model were used to develop data management tool, ICU-DaMa, for extracting variables essential for overseeing the quality of care in the intensive care unit (ICU).
Objective
To determine the feasibility of using ICU-DaMa to automatically extract variables for the minimum dataset and ICU quality indicators from the clinical information system (CIS).
Methods
The Wilcoxon signed-rank test and Fisher’s exact test were used to compare the values extracted from the CIS with ICU-DaMa for 25 variables from all patients attended in a polyvalent ICU during a two-month period against the gold standard of values manually extracted by two trained physicians. Discrepancies with the gold standard were classified into plausibility, conformance, and completeness errors.
Results
Data from 149 patients were included. Although there were no significant differences between the automatic method and the manual method, we detected differences in values for five variables, including one plausibility error and two conformance and completeness errors. Plausibility: 1) Sex, ICU-DaMa incorrectly classified one male patient as female (error generated by the Hospital’s Admissions Department). Conformance: 2) Reason for isolation, ICU-DaMa failed to detect a human error in which a professional misclassified a patient’s isolation. 3) Brain death, ICU-DaMa failed to detect another human error in which a professional likely entered two mutually exclusive values related to the death of the patient (brain death and controlled donation after circulatory death). Completeness: 4) Destination at ICU discharge, ICU-DaMa incorrectly classified two patients due to a professional failing to fill out the patient discharge form when thepatients died. 5) Length of continuous renal replacement therapy, data were missing for one patient because the CRRT device was not connected to the CIS.
Conclusions
Automatic generation of minimum dataset and ICU quality indicators using ICU-DaMa is feasible. The discrepancies were identified and can be corrected by improving CIS ergonomics, training healthcare professionals in the culture of the quality of information, and using tools for detecting and correcting data errors.",https://doi.org/10.1016/j.ijmedinf.2018.02.007,2018,Gonzalo Sirgo and Federico Esteban and Josep Gómez and Gerard Moreno and Alejandro Rodríguez and Lluis Blanch and Juan José Guardiola and Rafael Gracia and Lluis {De Haro} and María Bodí,VALIDATION OF THE ICU-DAMA TOOL FOR AUTOMATICALLY EXTRACTING VARIABLES FOR MINIMUM DATASET AND QUALITY INDICATORS: THE IMPORTANCE OF DATA QUALITY ASSESSMENT,article
346,23689,INTERNATIONAL JOURNAL OF MEDICAL INFORMATICS,journal,"18728243, 13865056","1,124",Q1,106,210,579,9073,3102,572,"4,82","43,20",Ireland,Western Europe,Elsevier Ireland Ltd,1996-2020,Health Informatics (Q1),INTERNATIONAL JOURNAL OF MEDICAL INFORMATICS,INTERNATIONAL JOURNAL OF MEDICAL INFORMATICS,"7,651",4.046,0.01044,"Background
The application of Big Data analytics in healthcare has immense potential for improving the quality of care, reducing waste and error, and reducing the cost of care.
Purpose
This systematic review of literature aims to determine the scope of Big Data analytics in healthcare including its applications and challenges in its adoption in healthcare. It also intends to identify the strategies to overcome the challenges.
Data sources
A systematic search of the articles was carried out on five major scientific databases: ScienceDirect, PubMed, Emerald, IEEE Xplore and Taylor & Francis. The articles on Big Data analytics in healthcare published in English language literature from January 2013 to January 2018 were considered.
Study selection
Descriptive articles and usability studies of Big Data analytics in healthcare and medicine were selected.
Data extraction
Two reviewers independently extracted information on definitions of Big Data analytics; sources and applications of Big Data analytics in healthcare; challenges and strategies to overcome the challenges in healthcare.
Results
A total of 58 articles were selected as per the inclusion criteria and analyzed. The analyses of these articles found that: (1) researchers lack consensus about the operational definition of Big Data in healthcare; (2) Big Data in healthcare comes from the internal sources within the hospitals or clinics as well external sources including government, laboratories, pharma companies, data aggregators, medical journals etc.; (3) natural language processing (NLP) is most widely used Big Data analytical technique for healthcare and most of the processing tools used for analytics are based on Hadoop; (4) Big Data analytics finds its application for clinical decision support; optimization of clinical operations and reduction of cost of care (5) major challenge in adoption of Big Data analytics is non-availability of evidence of its practical benefits in healthcare.
Conclusion
This review study unveils that there is a paucity of information on evidence of real-world use of Big Data analytics in healthcare. This is because, the usability studies have considered only qualitative approach which describes potential benefits but does not take into account the quantitative study. Also, majority of the studies were from developed countries which brings out the need for promotion of research on Healthcare Big Data analytics in developing countries.",https://doi.org/10.1016/j.ijmedinf.2018.03.013,2018,Nishita Mehta and Anil Pandit,CONCURRENCE OF BIG DATA ANALYTICS AND HEALTHCARE: A SYSTEMATIC REVIEW,article
347,23689,INTERNATIONAL JOURNAL OF MEDICAL INFORMATICS,journal,"18728243, 13865056","1,124",Q1,106,210,579,9073,3102,572,"4,82","43,20",Ireland,Western Europe,Elsevier Ireland Ltd,1996-2020,Health Informatics (Q1),INTERNATIONAL JOURNAL OF MEDICAL INFORMATICS,INTERNATIONAL JOURNAL OF MEDICAL INFORMATICS,"7,651",4.046,0.01044,"Background
In recent years, the literature associated with healthcare big data has grown rapidly, but few studies have used bibliometrics and a visualization approach to conduct deep mining and reveal a panorama of the healthcare big data field.
Methods
To explore the foundational knowledge and research hotspots of big data research in the field of healthcare informatics, this study conducted a series of bibliometric analyses on the related literature, including papers’ production trends in the field and the trend of each paper’s co-author number, the distribution of core institutions and countries, the core literature distribution, the related information of prolific authors and innovation paths in the field, a keyword co-occurrence analysis, and research hotspots and trends for the future.
Results
By conducting a literature content analysis and structure analysis, we found the following: (a) In the early stage, researchers from the United States, the People’s Republic of China, the United Kingdom, and Germany made the most contributions to the literature associated with healthcare big data research and the innovation path in this field. (b) The innovation path in healthcare big data consists of three stages: the disease early detection, diagnosis, treatment, and prognosis phase, the life and health promotion phase, and the nursing phase. (c) Research hotspots are mainly concentrated in three dimensions: the disease dimension (e.g., epidemiology, breast cancer, obesity, and diabetes), the technical dimension (e.g., data mining and machine learning), and the health service dimension (e.g., customized service and elderly nursing).
Conclusion
This study will provide scholars in the healthcare informatics community with panoramic knowledge of healthcare big data research, as well as research hotspots and future research directions.",https://doi.org/10.1016/j.ijmedinf.2016.11.006,2017,Dongxiao Gu and Jingjing Li and Xingguo Li and Changyong Liang,VISUALIZING THE KNOWLEDGE STRUCTURE AND EVOLUTION OF BIG DATA RESEARCH IN HEALTHCARE INFORMATICS,article
348,23640,ADVANCED ENGINEERING INFORMATICS,journal,14740346,"1,107",Q1,81,146,295,8184,1973,289,"6,41","56,05",United Kingdom,Western Europe,Elsevier Ltd.,2002-2020,Artificial Intelligence (Q1); Information Systems (Q1),ADVANCED ENGINEERING INFORMATICS,ADVANCED ENGINEERING INFORMATICS,"4,432",5.603,0.00428,"The prospering Big data era is emerging in the power grid. Multiple world-wide studies are emphasizing the big data applications in the microgrid due to the huge amount of produced data. Big data analytics can impact the design and applications towards safer, better, more profitable, and effective power grid. This paper presents the recognition and challenges of the big data and the microgrid. The construction of big data analytics is introduced. The data sources, big data opportunities, and enhancement areas in the microgrid like stability improvement, asset management, renewable energy prediction, and decision-making support are summarized. Diverse case studies are presented including different planning, operation control, decision making, load forecasting, data attacks detection, and maintenance aspects of the microgrid. Finally, the open challenges of big data in the microgrid are discussed.",https://doi.org/10.1016/j.aei.2019.100945,2019,Karim Moharm,STATE OF THE ART IN BIG DATA APPLICATIONS IN MICROGRID: A REVIEW,article
349,23640,ADVANCED ENGINEERING INFORMATICS,journal,14740346,"1,107",Q1,81,146,295,8184,1973,289,"6,41","56,05",United Kingdom,Western Europe,Elsevier Ltd.,2002-2020,Artificial Intelligence (Q1); Information Systems (Q1),ADVANCED ENGINEERING INFORMATICS,ADVANCED ENGINEERING INFORMATICS,"4,432",5.603,0.00428,"Pipelines carrying energy products play vital roles in economic wealth and public safety, but incidents continue occurring. Condition assessment of pipelines is essential to identify anomalies timely. Advanced sensing technologies obtain informative data for condition assessment, while data analysis by human has limited efficiency, accuracy, and reliability. Advances in machine learning offer exciting opportunities for automated condition assessment with minimum human intervention. This paper reviews machine learning approaches to detect, classify, locate, and quantify pipeline anomalies based on intelligent interpretation of routine operation data, nondestructive testing data, and computer vision data. Statistics and uncertainties of performance metrics of machine learning approaches are discussed. An analysis on strengths, weaknesses, opportunities, and threats (SWOT) is performed. Guides for practitioners to perform automated pipeline condition assessment are recommended. This review provide insights into the machine learning approaches for automated pipeline condition assessment. The SWOT analysis will support decision making in the pipeline industry.",https://doi.org/10.1016/j.aei.2022.101687,2022,Yiming Liu and Yi Bao,REVIEW ON AUTOMATED CONDITION ASSESSMENT OF PIPELINES WITH MACHINE LEARNING,article
350,23640,ADVANCED ENGINEERING INFORMATICS,journal,14740346,"1,107",Q1,81,146,295,8184,1973,289,"6,41","56,05",United Kingdom,Western Europe,Elsevier Ltd.,2002-2020,Artificial Intelligence (Q1); Information Systems (Q1),ADVANCED ENGINEERING INFORMATICS,ADVANCED ENGINEERING INFORMATICS,"4,432",5.603,0.00428,"The aerospace sector is one of the many sectors in which large amounts of data are generated. Thanks to the evolution of technology, these data can be exploited in several ways to improve the operation and management of industrial processes. However, to achieve this goal, it is necessary to define architectures and data models that allow to manage and homogenise the heterogeneous data collected. In this paper, we present an Airport Digital Twin Reference Conceptualisation’s and data model based on FIWARE Generic Enablers and the Next Generation Service Interfaces-Linked Data standard. Concretely, we particularise the Airport Digital Twin to improve the efficiency of flight turnaround events. The architecture proposed is validated in the Aberdeen International Airport with the aim of reducing delays in commercial flights. The implementation includes an application that shows the real state of the airport, combining two-dimensional and three-dimensional virtual reality representations of the stands, and a mobile application that helps ground operators to schedule departure and arrival flights.",https://doi.org/10.1016/j.aei.2022.101723,2022,Javier Conde and Andres Munoz-Arcentales and Mario Romero and Javier Rojo and Joaquín Salvachúa and Gabriel Huecas and Álvaro Alonso,APPLYING DIGITAL TWINS FOR THE MANAGEMENT OF INFORMATION IN TURNAROUND EVENT OPERATIONS IN COMMERCIAL AIRPORTS,article
351,23640,ADVANCED ENGINEERING INFORMATICS,journal,14740346,"1,107",Q1,81,146,295,8184,1973,289,"6,41","56,05",United Kingdom,Western Europe,Elsevier Ltd.,2002-2020,Artificial Intelligence (Q1); Information Systems (Q1),ADVANCED ENGINEERING INFORMATICS,ADVANCED ENGINEERING INFORMATICS,"4,432",5.603,0.00428,"The ability to process large amounts of data and to extract useful insights from data has revolutionised society. This phenomenon—dubbed as Big Data—has applications for a wide assortment of industries, including the construction industry. The construction industry already deals with large volumes of heterogeneous data; which is expected to increase exponentially as technologies such as sensor networks and the Internet of Things are commoditised. In this paper, we present a detailed survey of the literature, investigating the application of Big Data techniques in the construction industry. We reviewed related works published in the databases of American Association of Civil Engineers (ASCE), Institute of Electrical and Electronics Engineers (IEEE), Association of Computing Machinery (ACM), and Elsevier Science Direct Digital Library. While the application of data analytics in the construction industry is not new, the adoption of Big Data technologies in this industry remains at a nascent stage and lags the broad uptake of these technologies in other fields. To the best of our knowledge, there is currently no comprehensive survey of Big Data techniques in the context of the construction industry. This paper fills the void and presents a wide-ranging interdisciplinary review of literature of fields such as statistics, data mining and warehousing, machine learning, and Big Data Analytics in the context of the construction industry. We discuss the current state of adoption of Big Data in the construction industry and discuss the future potential of such technologies across the multiple domain-specific sub-areas of the construction industry. We also propose open issues and directions for future work along with potential pitfalls associated with Big Data adoption in the industry.",https://doi.org/10.1016/j.aei.2016.07.001,2016,Muhammad Bilal and Lukumon O. Oyedele and Junaid Qadir and Kamran Munir and Saheed O. Ajayi and Olugbenga O. Akinade and Hakeem A. Owolabi and Hafiz A. Alaka and Maruf Pasha,"BIG DATA IN THE CONSTRUCTION INDUSTRY: A REVIEW OF PRESENT STATUS, OPPORTUNITIES, AND FUTURE TRENDS",article
352,14414,JOURNAL OF PROCESS CONTROL,journal,09591524,"1,102",Q1,114,136,413,5639,1820,408,"4,39","41,46",United Kingdom,Western Europe,Elsevier Ltd.,1991-2020,Computer Science Applications (Q1); Control and Systems Engineering (Q1); Industrial and Manufacturing Engineering (Q1); Modeling and Simulation (Q1),JOURNAL OF PROCESS CONTROL,JOURNAL OF PROCESS CONTROL,"7,446",3.666,0.00525,"With the ever increasing data collected from the process, the era of big data has arrived in the process industry. Therefore, the computational effort for data modeling and analytics in standalone modes has become increasingly demanding, particularly for large-scale processes. In this paper, a distributed parallel process modeling approach is presented based on a MapReduce framework for big data quality prediction. Firstly, the architecture for distributed parallel data modeling is formulated under the MapReduce framework. Secondly, a big data quality prediction scheme is developed based on the distributed parallel data modeling approach. As an example, the basic Semi-Supervised Probabilistic Principal Component Regression (SSPPCR) model is deployed to concurrently train a set of local models with split datasets. Meanwhile, Bayesian rule is utilized in a MapReduce way to integrate local models based on their predictive abilities. Two case studies demonstrate the effectiveness of the proposed method for big data quality prediction.",https://doi.org/10.1016/j.jprocont.2018.04.004,2018,Le Yao and Zhiqiang Ge,BIG DATA QUALITY PREDICTION IN THE PROCESS INDUSTRY: A DISTRIBUTED PARALLEL MODELING FRAMEWORK,article
353,14414,JOURNAL OF PROCESS CONTROL,journal,09591524,"1,102",Q1,114,136,413,5639,1820,408,"4,39","41,46",United Kingdom,Western Europe,Elsevier Ltd.,1991-2020,Computer Science Applications (Q1); Control and Systems Engineering (Q1); Industrial and Manufacturing Engineering (Q1); Modeling and Simulation (Q1),JOURNAL OF PROCESS CONTROL,JOURNAL OF PROCESS CONTROL,"7,446",3.666,0.00525,"In the big data era, small data problems still exist in many industrial sectors. Taking the high-value process industries as an example, a large number of materials and processing methods are often tested at the design stage. However, only a small amount of data can be collected for each material-process combination, which poses a serious challenge to data-driven process modeling. There is a great necessity to integrate the small data measured in different tasks and build the process model by sharing the information. In this work, a deep embedding neural network is proposed to extract the qualitative task information for process modeling. Specifically, an autoencoder is used to learn embeddings which are combined with the quantitative process conditions as the inputs of a feed-forward neural network to produce the final predictions. The feasibility, including interpretability and prediction accuracy, of the developed method is illustrated with an extrusion process.",https://doi.org/10.1016/j.jprocont.2022.04.018,2022,Haibin Wu and Yu-Han Lo and Le Zhou and Yuan Yao,PROCESS MODELING BY INTEGRATING QUANTITATIVE AND QUALITATIVE INFORMATION USING A DEEP EMBEDDING NETWORK AND ITS APPLICATION TO AN EXTRUSION PROCESS,article
354,14414,JOURNAL OF PROCESS CONTROL,journal,09591524,"1,102",Q1,114,136,413,5639,1820,408,"4,39","41,46",United Kingdom,Western Europe,Elsevier Ltd.,1991-2020,Computer Science Applications (Q1); Control and Systems Engineering (Q1); Industrial and Manufacturing Engineering (Q1); Modeling and Simulation (Q1),JOURNAL OF PROCESS CONTROL,JOURNAL OF PROCESS CONTROL,"7,446",3.666,0.00525,"With ever-accelerating advancement of information, communication, sensing and characterization technologies, such as industrial Internet of Things (IoT) and high-throughput instruments, it is expected that the data generated from manufacturing will grow exponentially, generating so called ‘big data’. One of the focuses of smart manufacturing is to create manufacturing intelligence from real-time data to support accurate and timely decision-making. Therefore, big data analytics is expected to contribute significantly to the advancement of smart manufacturing. In this work, a roadmap of statistical process monitoring (SPM) is presented. Most recent developments in SPM are briefly reviewed and summarized. Specific challenges and potential solutions in handling manufacturing big data are discussed. We suggest that process characteristics or feature based SPM, instead of process variable based SPM, is a promising route for next generation SPM and could play a significant role in smart manufacturing. The advantages of feature based SPM are discussed to support the suggestion and future directions in SPM are discussed in the context of smart manufacturing.",https://doi.org/10.1016/j.jprocont.2017.06.012,2018,Q. Peter He and Jin Wang,STATISTICAL PROCESS MONITORING AS A BIG DATA ANALYTICS TOOL FOR SMART MANUFACTURING,article
355,13929,APPLIED ERGONOMICS,journal,"18729126, 00036870","1,093",Q1,98,232,682,11314,2995,675,"4,17","48,77",United Kingdom,Western Europe,Elsevier Ltd.,1969-2021,"Engineering (miscellaneous) (Q1); Human Factors and Ergonomics (Q1); Physical Therapy, Sports Therapy and Rehabilitation (Q1); Safety, Risk, Reliability and Quality (Q1)",APPLIED ERGONOMICS,APPLIED ERGONOMICS,"9,523",3.661,0.00851,"Human reliability analysis plays an important role in the safety assessment and management of rail operations. This paper discusses how the increasing availability of operational data can be used to develop an understanding of train driver reliability. The paper derives human reliability data for two driving tasks, stopping at red signals and controlling speed on approach to buffer stops. In the first of these cases, a tool has been developed that can estimate the number of times a signal is approached at red by trains on the Great Britain (GB) rail network. The tool has been developed using big data techniques and ideas, recording and analysing millions of pieces of data from live operational feeds to update and summarise statistics from thousands of signal locations in GB on a daily basis. The resulting driver reliability data are compared to similar analyses of other train driving tasks. This shows human reliability approaching the currently accepted limits of human performance. It also shows higher error rates amongst freight train drivers than passenger train drivers for these tasks. The paper highlights the importance of understanding the task specific performance limits if further improvements in human reliability are sought. It also provides a practical example of how big data could play an increasingly important role in system error management, whether from the perspective of understanding normal performance and the limits of performance for specific tasks or as the basis for dynamic safety indicators which, if not leading, could at least become closer to real time.",https://doi.org/10.1016/j.apergo.2022.103795,2022,Chris Harrison and Julian Stow and Xiaocheng Ge and Jonathan Gregory and Huw Gibson and Alice Monk,AT THE LIMIT? USING OPERATIONAL DATA TO ESTIMATE TRAIN DRIVER HUMAN RELIABILITY,article
356,21100400826,JOURNAL OF ENERGY STORAGE,journal,2352152X,"1,088",Q1,42,860,831,44642,5383,828,"6,87","51,91",Netherlands,Western Europe,Elsevier BV,2015-2020,"Electrical and Electronic Engineering (Q1); Energy Engineering and Power Technology (Q1); Renewable Energy, Sustainability and the Environment (Q2)",JOURNAL OF ENERGY STORAGE,JOURNAL OF ENERGY STORAGE,"7,765",6.583,0.00926,"The establishment of an accurate battery model is of great significance to improve the reliability of electric vehicles (EVs). However, the battery is a complex electrochemical system with hardly observable and simulatable internal chemical reactions, and it is challenging to estimate the state of battery accurately. This paper proposes a novel flexible and reliable battery management method based on the battery big data platform and Cyber-Physical System (CPS) technology. First of all, to integrate the battery big data resources in the cloud, a Cyber-physical battery management framework is defined and served as the basic data platform for battery modeling issues. And to improve the quality of the collected battery data in the database, this work reports the first attempt to develop an adaptive data cleaning method for the cloud battery management issue. Furthermore, a deep learning algorithm-based feature extraction model, as well as a feature-oriented battery modeling method, is developed to mitigate the under-fitting problem and improve the accuracy of the cloud-based battery model. The actual operation data of electric buses is used to validate the proposed methodologies. The maximum data restoring error can be limited within 1.3% in the experiments, which indicates that the proposed data cleaning method is able to improve the cloud battery data quality effectively. Meanwhile, the maximum SoC estimation error in the proposed feature-oriented battery modeling method is within 2.47%, which highlights the effectiveness of the proposed method.",https://doi.org/10.1016/j.est.2020.102064,2021,Shuangqi Li and Pengfei Zhao,BIG DATA DRIVEN VEHICLE BATTERY MANAGEMENT METHOD: A NOVEL CYBER-PHYSICAL SYSTEM PERSPECTIVE,article
357,21100400826,JOURNAL OF ENERGY STORAGE,journal,2352152X,"1,088",Q1,42,860,831,44642,5383,828,"6,87","51,91",Netherlands,Western Europe,Elsevier BV,2015-2020,"Electrical and Electronic Engineering (Q1); Energy Engineering and Power Technology (Q1); Renewable Energy, Sustainability and the Environment (Q2)",JOURNAL OF ENERGY STORAGE,JOURNAL OF ENERGY STORAGE,"7,765",6.583,0.00926,"State of health (SOH) of lithium-ion battery pack directly determines the driving mileage and output power of the electric vehicle. With the development of big data storage and analysis technology, using big data to off-line estimate battery pack SOH is more feasible than before. This paper proposes a SOH estimation method based on real data of electric vehicles concerning user behavior. The charging capacity is calculated by historical charging data, and locally weighted linear regression (LWLR) algorithm is used to qualitatively characterize the capacity decline trend. The health features are extracted from historical operating data, maximal information coefficient (MIC) algorithm is used to measure the correlation between health features and capacity. Then, long and short-term memory (LSTM)-based neural network will further learn the nonlinear degradation relationship between capacity and health features. Bayesian optimization algorithm is used to ensure the generalization of the model when different electric vehicles produce different user behaviors. The estimation method is validated by the 300 days historical dataset from 100 vehicles with different driving behavior. The results indicates that the maximum relative error of estimating SOH is 0.2%.",https://doi.org/10.1016/j.est.2021.102867,2021,Zhigang He and Xiaoyu Shen and Yanyan Sun and Shichao Zhao and Bin Fan and Chaofeng Pan,STATE-OF-HEALTH ESTIMATION BASED ON REAL DATA OF ELECTRIC VEHICLES CONCERNING USER BEHAVIOR,article
358,21100400826,JOURNAL OF ENERGY STORAGE,journal,2352152X,"1,088",Q1,42,860,831,44642,5383,828,"6,87","51,91",Netherlands,Western Europe,Elsevier BV,2015-2020,"Electrical and Electronic Engineering (Q1); Energy Engineering and Power Technology (Q1); Renewable Energy, Sustainability and the Environment (Q2)",JOURNAL OF ENERGY STORAGE,JOURNAL OF ENERGY STORAGE,"7,765",6.583,0.00926,"This paper proposes the architecture of the combination of the battery management system (BMS) and the cloud big data platform. Firstly, BMS measures and extracts the mean voltage falloff (MVF). A regression model of capacity and MVF based on historical data is established with generalized Box-Cox Transformation and least squares. The capacity and MVF are uploaded to the cloud big data platform, and then the mean and variance of the MVF is predicted based on the relevance vector machine, thereby realizing the 2σ range prediction of the lithium battery's state of health and the probability density function prediction of the remaining useful life. This paper makes two contributions to the data-driven prediction method. First, the edge-cloud collaborative computing architecture combining BMS and cloud is proposed, which effectively utilizes the advantages of BMS data quality and cloud computing power. Second, through the combination of relevance vector machine with particle swarm optimization and horizontal parameter transfer, the number of samples required for model learning is reduced to 30% and has better accuracy and robustness. Through the verification of NASA data, the results show that the average error is less than 2.18%.",https://doi.org/10.1016/j.est.2021.103342,2021,Yong Zhou and Huanghui Gu and Teng Su and Xuebing Han and Languang Lu and Yuejiu Zheng,REMAINING USEFUL LIFE PREDICTION WITH PROBABILITY DISTRIBUTION FOR LITHIUM-ION BATTERIES BASED ON EDGE AND CLOUD COLLABORATIVE COMPUTATION,article
359,24807,NEUROCOMPUTING,journal,09252312,"1,085",Q1,143,1653,3586,78823,25554,3535,"7,08","47,68",Netherlands,Western Europe,Elsevier,1989-2020,Artificial Intelligence (Q1); Computer Science Applications (Q1); Cognitive Neuroscience (Q2),NEUROCOMPUTING,NEUROCOMPUTING,"46,751",5.719,0.06669,"The task of choosing the appropriate classifier for a given scenario is not an easy-to-solve question. First, there is an increasingly high number of algorithms available belonging to different families. And also there is a lack of methodologies that can help on recommending in advance a given family of algorithms for a certain type of datasets. Besides, most of these classification algorithms exhibit a degradation in the performance when faced with datasets containing irrelevant and/or redundant features. In this work we analyze the impact of feature selection in classification over several synthetic and real datasets. The experimental results obtained show that the significance of selecting a classifier decreases after applying an appropriate preprocessing step and, not only this alleviates the choice, but it also improves the results in almost all the datasets tested.",https://doi.org/10.1016/j.neucom.2021.05.107,2022,Laura Morán-Fernández and Verónica Bólon-Canedo and Amparo Alonso-Betanzos,HOW IMPORTANT IS DATA QUALITY? BEST CLASSIFIERS VS BEST FEATURES,article
360,24807,NEUROCOMPUTING,journal,09252312,"1,085",Q1,143,1653,3586,78823,25554,3535,"7,08","47,68",Netherlands,Western Europe,Elsevier,1989-2020,Artificial Intelligence (Q1); Computer Science Applications (Q1); Cognitive Neuroscience (Q2),NEUROCOMPUTING,NEUROCOMPUTING,"46,751",5.719,0.06669,"Machine-to-Machine (M2M) communication relies on the physical objects (e.g., satellites, sensors, and so forth) interconnected with each other, creating mesh of machines producing massive volume of data about large geographical area (e.g., living and non-living environment). Thus, the M2M is an ideal example of Big Data. On the contrary, the M2M platforms that handle Big Data might perform poorly or not according to the goals of their operator (in term of cost, database utilization, data quality, processing and computational efficiency, analysis and feature extraction applications). Therefore, to address the aforementioned needs, we propose a new effective, memory and processing efficient system architecture for Big Data in M2M, which, unlike other previous proposals, does not require whole set of data to be processed (including raw data sets), and to be kept in the main memory. Our designed system architecture exploits divide-and-conquer approach and data block-wise vertical representation of the database follows a particular petitionary strategy, which formalizes the problem of feature extraction applications. The architecture goes from physical objects to the processing servers, where Big Data set is first transformed into a several data blocks that can be quickly processed, then it classifies and reorganizes these data blocks from the same source. In addition, the data blocks are aggregated in a sequential manner based on a machine ID, and equally partitions the data using fusion algorithm. Finally, the results are stored in a server that helps the users in making decision. The feasibility and efficiency of the proposed system architecture are implemented on Hadoop single node setup on UBUNTU 14.04 LTS core™i5 machine with 3.2GHz processor and 4GB memory. The results show that the proposed system architecture efficiently extract various features (such as River) from the massive volume of satellite data.",https://doi.org/10.1016/j.neucom.2015.04.109,2016,Awais Ahmad and Anand Paul and M. Mazhar Rathore,AN EFFICIENT DIVIDE-AND-CONQUER APPROACH FOR BIG DATA ANALYTICS IN MACHINE-TO-MACHINE COMMUNICATION,article
361,21100823476,DIGITAL COMMUNICATIONS AND NETWORKS,journal,"23528648, 24685925","1,082",Q1,26,77,105,3226,881,96,"8,81","41,90",China,Asiatic Region,Chongqing University of Posts and Telecommunications,2015-2020,Communication (Q1); Computer Networks and Communications (Q1); Hardware and Architecture (Q1),DIGITAL COMMUNICATIONS AND NETWORKS,DIGITAL COMMUNICATIONS AND NETWORKS,823,6.797,0.00138,"Rapid developments in hardware, software, and communication technologies have facilitated the emergence of Internet-connected sensory devices that provide observations and data measurements from the physical world. By 2020, it is estimated that the total number of Internet-connected devices being used will be between 25 and 50 billion. As these numbers grow and technologies become more mature, the volume of data being published will increase. The technology of Internet-connected devices, referred to as Internet of Things (IoT), continues to extend the current Internet by providing connectivity and interactions between the physical and cyber worlds. In addition to an increased volume, the IoT generates big data characterized by its velocity in terms of time and location dependency, with a variety of multiple modalities and varying data quality. Intelligent processing and analysis of this big data are the key to developing smart IoT applications. This article assesses the various machine learning methods that deal with the challenges presented by IoT data by considering smart cities as the main use case. The key contribution of this study is the presentation of a taxonomy of machine learning algorithms explaining how different techniques are applied to the data in order to extract higher level information. The potential and challenges of machine learning for IoT data analytics will also be discussed. A use case of applying a Support Vector Machine (SVM) to Aarhus smart city traffic data is presented for a more detailed exploration.",https://doi.org/10.1016/j.dcan.2017.10.002,2018,Mohammad Saeid Mahdavinejad and Mohammadreza Rezvan and Mohammadamin Barekatain and Peyman Adibi and Payam Barnaghi and Amit P. Sheth,MACHINE LEARNING FOR INTERNET OF THINGS DATA ANALYSIS: A SURVEY,article
362,21100823476,DIGITAL COMMUNICATIONS AND NETWORKS,journal,"23528648, 24685925","1,082",Q1,26,77,105,3226,881,96,"8,81","41,90",China,Asiatic Region,Chongqing University of Posts and Telecommunications,2015-2020,Communication (Q1); Computer Networks and Communications (Q1); Hardware and Architecture (Q1),DIGITAL COMMUNICATIONS AND NETWORKS,DIGITAL COMMUNICATIONS AND NETWORKS,823,6.797,0.00138,"The development of data-driven artificial intelligence technology has given birth to a variety of big data applications. Data has become an essential factor to improve these applications. Federated learning, a privacy-preserving machine learning method, is proposed to leverage data from different data owners. It is typically used in conjunction with cryptographic methods, in which data owners train the global model by sharing encrypted model updates. However, data encryption makes it difficult to identify the quality of these model updates. Malicious data owners may launch attacks such as data poisoning and free-riding. To defend against such attacks, it is necessary to find an approach to audit encrypted model updates. In this paper, we propose a blockchain-based audit approach for encrypted gradients. It uses a behavior chain to record the encrypted gradients from data owners, and an audit chain to evaluate the gradients’ quality. Specifically, we propose a privacy-preserving homomorphic noise mechanism in which the noise of each gradient sums to zero after aggregation, ensuring the availability of aggregated gradient. In addition, we design a joint audit algorithm that can locate malicious data owners without decrypting individual gradients. Through security analysis and experimental evaluation, we demonstrate that our approach can defend against malicious gradient attacks in federated learning.",https://doi.org/10.1016/j.dcan.2022.05.006,2022,Zhe Sun and Junping Wan and Lihua Yin and Zhiqiang Cao and Tianjie Luo and Bin Wang,A BLOCKCHAIN-BASED AUDIT APPROACH FOR ENCRYPTED DATA IN FEDERATED LEARNING,article
363,21100823476,DIGITAL COMMUNICATIONS AND NETWORKS,journal,"23528648, 24685925","1,082",Q1,26,77,105,3226,881,96,"8,81","41,90",China,Asiatic Region,Chongqing University of Posts and Telecommunications,2015-2020,Communication (Q1); Computer Networks and Communications (Q1); Hardware and Architecture (Q1),DIGITAL COMMUNICATIONS AND NETWORKS,DIGITAL COMMUNICATIONS AND NETWORKS,823,6.797,0.00138,"Advanced technologies are required in future mobile wireless networks to support services with highly diverse requirements in terms of high data rate and reliability, low latency, and massive access. Deep Learning (DL), one of the most exciting developments in machine learning and big data, has recently shown great potential in the study of wireless communications. In this article, we provide a literature review on the applications of DL in the physical layer. First, we analyze the limitations of existing signal processing techniques in terms of model accuracy, global optimality, and computational scalability. Next, we provide a brief review of classical DL frameworks. Subsequently, we discuss recent DL-based physical layer technologies, including both DL-based signal processing modules and end-to-end systems. Deep neural networks are used to replace a single or several conventional functional modules, whereas the objective of the latter is to replace the entire transceiver structure. Lastly, we discuss the open issues and research directions of the DL-based physical layer in terms of model complexity, data quality, data representation, and algorithm reliability.",https://doi.org/10.1016/j.dcan.2021.09.014,2021,Siqi Liu and Tianyu Wang and Shaowei Wang,TOWARD INTELLIGENT WIRELESS COMMUNICATIONS: DEEP LEARNING - BASED PHYSICAL LAYER TECHNOLOGIES,article
364,23706,JOURNAL OF BIOMEDICAL INFORMATICS,journal,"15320480, 15320464","1,057",Q1,103,203,591,9904,3884,527,"7,05","48,79",United States,Northern America,Academic Press Inc.,2001-2020,Computer Science Applications (Q1); Health Informatics (Q1),JOURNAL OF BIOMEDICAL INFORMATICS,JOURNAL OF BIOMEDICAL INFORMATICS,"12,255",6.317,0.01469,"The construction of medical big data includes several problems that need to be solved, such as integration and data sharing of many heterogeneous information systems, efficient processing and analysis of large-scale medical data with complex structure or low degree of structure, and narrow application range of medical data. Therefore, medical big data construction is not only a simple collection and application of medical data but also a complex systematic project. This paper introduces China's experience in the construction of a regional medical big data ecosystem, including the overall goal of the project; establishment of policies to encourage data sharing; handling the relationship between personal privacy, information security, and information availability; establishing a cooperation mechanism between agencies; designing a polycentric medical data acquisition system; and establishing a large data centre. From the experience gained from one of China's earliest established medical big data projects, we outline the challenges encountered during its development and recommend approaches to overcome these challenges to design medical big data projects in China more rationally. Clear and complete top-level design of a project requires to be planned in advance and considered carefully. It is essential to provide a culture of information sharing and to facilitate the opening of data, and changes in ideas and policies need the guidance of the government. The contradiction between data sharing and data security must be handled carefully, that is not to say data openness could be abandoned. The construction of medical big data involves many institutions, and high-level management and cooperation can significantly improve efficiency and promote innovation. Compared with infrastructure construction, it is more challenging and time-consuming to develop appropriate data standards, data integration tools and data mining tools.",https://doi.org/10.1016/j.jbi.2019.103149,2019,Bei Li and Jianbin Li and Yuqiao Jiang and Xiaoyun Lan,EXPERIENCE AND REFLECTION FROM CHINA’S XIANGYA MEDICAL BIG DATA PROJECT,article
365,23706,JOURNAL OF BIOMEDICAL INFORMATICS,journal,"15320480, 15320464","1,057",Q1,103,203,591,9904,3884,527,"7,05","48,79",United States,Northern America,Academic Press Inc.,2001-2020,Computer Science Applications (Q1); Health Informatics (Q1),JOURNAL OF BIOMEDICAL INFORMATICS,JOURNAL OF BIOMEDICAL INFORMATICS,"12,255",6.317,0.01469,"Significant technological advances made in recent years have shepherded a dramatic increase in utilization of digital technologies for biomedicine– everything from the widespread use of electronic health records to improved medical imaging capabilities and the rising ubiquity of genomic sequencing contribute to a “digitization” of biomedical research and clinical care. With this shift toward computerized tools comes a dramatic increase in the amount of available data, and current tools for data analysis capable of extracting meaningful knowledge from this wealth of information have yet to catch up. This article seeks to provide an overview of emerging mathematical methods with the potential to improve the abilities of clinicians and researchers to analyze biomedical data, but may be hindered from doing so by a lack of conceptual accessibility and awareness in the life sciences research community. In particular, we focus on topological data analysis (TDA), a set of methods grounded in the mathematical field of algebraic topology that seeks to describe and harness features related to the “shape” of data. We aim to make such techniques more approachable to non-mathematicians by providing a conceptual discussion of their theoretical foundations followed by a survey of their published applications to scientific research. Finally, we discuss the limitations of these methods and suggest potential avenues for future work integrating mathematical tools into clinical care and biomedical informatics.",https://doi.org/10.1016/j.jbi.2022.104082,2022,Yara Skaf and Reinhard Laubenbacher,TOPOLOGICAL DATA ANALYSIS IN BIOMEDICINE: A REVIEW,article
366,26441,SOCIAL SCIENCE RESEARCH,journal,"10960317, 0049089X","1,042",Q1,89,80,398,5842,1123,398,"2,28","73,03",United States,Northern America,Academic Press Inc.,1972-2020,Education (Q1); Sociology and Political Science (Q1),SOCIAL SCIENCE RESEARCH,SOCIAL SCIENCE RESEARCH,"6,976",2.322,0.00899,"The term big data is currently a buzzword in social science, however its precise meaning is ambiguous. In this paper we focus on administrative data which is a distinctive form of big data. Exciting new opportunities for social science research will be afforded by new administrative data resources, but these are currently under appreciated by the research community. The central aim of this paper is to discuss the challenges associated with administrative data. We emphasise that it is critical for researchers to carefully consider how administrative data has been produced. We conclude that administrative datasets have the potential to contribute to the development of high-quality and impactful social science research, and should not be overlooked in the emerging field of big data.",https://doi.org/10.1016/j.ssresearch.2016.04.015,2016,Roxanne Connelly and Christopher J. Playford and Vernon Gayle and Chris Dibben,THE ROLE OF ADMINISTRATIVE DATA IN THE BIG DATA REVOLUTION IN SOCIAL SCIENCE RESEARCH,article
367,21100239262,SUSTAINABLE ENERGY TECHNOLOGIES AND ASSESSMENTS,journal,22131388,"1,040",Q1,39,270,333,13311,1833,331,"5,73","49,30",United Kingdom,Western Europe,Elsevier Ltd.,2013-2020,"Energy Engineering and Power Technology (Q1); Renewable Energy, Sustainability and the Environment (Q2)",SUSTAINABLE ENERGY TECHNOLOGIES AND ASSESSMENTS,SUSTAINABLE ENERGY TECHNOLOGIES AND ASSESSMENTS,"3,234",5.353,0.00348,"Angelica sinensis is a kind of traditional Chinese medicine with very good blood nourishing effect, and it is cultivated in many regions of China. But with the increasingly severe climate, angelica cultivation has become a big problem. Therefore, this paper starts from the soil microorganisms of angelica planting, and studies the influence of soil biodiversity and angelica planting in the context of big data. This paper proposes a Hadoop system for big data analysis, combining the biocommunity characteristics and metagenomes of soil microorganisms. It then calculates the distance between samples and generates a dissimilarity matrix. Finally, this paper proposes a soil optimization method for angelica planting based on big data analysis of soil microorganisms. In order to optimize the Angelica planting soil designed in this paper, a soil microbial genome comparison experiment and a big data concurrent control test experiment were designed in this paper. It then analyzes the data obtained from the experiment, and the results of the analysis are used to optimize the soil for angelica planting. It finally compares the soil method of Angelica planting designed in this paper with the traditional Angelica planting method. The experimental results show that the survival rate of Angelica sinensis planted by the soil optimization method based on big data analysis of soil microorganisms has increased by 16.09% compared with the traditional Angelica sinensis planting site. The growth rate of Angelica sinensis planted by the soil optimization method based on big data analysis of soil microorganisms increased by 9.64% compared with the traditional Angelica sinensis planting area.",https://doi.org/10.1016/j.seta.2022.102674,2022,Yinan Peng and Ze Ye and Peng Xi and Hongshan Qi and Bin Ji and Zhiye Wang,THE RELATIONSHIP BETWEEN SOIL MICROBIAL DIVERSITY AND ANGELICA PLANTING BASED ON NETWORK BIG DATA,article
368,21100239262,SUSTAINABLE ENERGY TECHNOLOGIES AND ASSESSMENTS,journal,22131388,"1,040",Q1,39,270,333,13311,1833,331,"5,73","49,30",United Kingdom,Western Europe,Elsevier Ltd.,2013-2020,"Energy Engineering and Power Technology (Q1); Renewable Energy, Sustainability and the Environment (Q2)",SUSTAINABLE ENERGY TECHNOLOGIES AND ASSESSMENTS,SUSTAINABLE ENERGY TECHNOLOGIES AND ASSESSMENTS,"3,234",5.353,0.00348,"The use of Internet of Things (IoT) networks offers great advantages over wired networks, especially due to their simple installation, low maintenance costs, and automatic configuration. IoT facilitates the integration of sensing and communication for various industries, including smart farming and precision agriculture. For several years, many researchers have strived to find new sources of energy that are always “cleaner” and more environmentally friendly. Energy harvesting technology is one of the most promising environment-friendly solutions that extend the lifetime of these IoT devices. In this paper, the state-of-art of IoT energy harvesting capabilities and communication technologies in smart agriculture is presented. In addition, this work proposes a comprehensive architecture that includes big data technologies, IoT components, and knowledge-based systems for innovative farm architecture. The solution answers some of the biggest challenges the agriculture industry faces, especially when handling small files in a big data environment without impacting the computation performance. The solution is built on top of a pre-defined big data architecture that includes an abstraction layer of the data lake that handles data quality following a data migration strategy to ensure the data's insights. Furthermore, in this paper, we compared several machine learning algorithms to find the most suitable smart farming analytics tools in terms of forecasting and predictions.",https://doi.org/10.1016/j.seta.2022.102093,2022,El Mehdi Ouafiq and Rachid Saadane and Abdellah Chehri and Seunggil Jeon,AI-BASED MODELING AND DATA-DRIVEN EVALUATION FOR SMART FARMING-ORIENTED BIG DATA ARCHITECTURE USING IOT WITH ENERGY HARVESTING CAPABILITIES,article
369,21100239262,SUSTAINABLE ENERGY TECHNOLOGIES AND ASSESSMENTS,journal,22131388,"1,040",Q1,39,270,333,13311,1833,331,"5,73","49,30",United Kingdom,Western Europe,Elsevier Ltd.,2013-2020,"Energy Engineering and Power Technology (Q1); Renewable Energy, Sustainability and the Environment (Q2)",SUSTAINABLE ENERGY TECHNOLOGIES AND ASSESSMENTS,SUSTAINABLE ENERGY TECHNOLOGIES AND ASSESSMENTS,"3,234",5.353,0.00348,"Solar power is expected to play a substantial role globally, due to it being one of the leading renewable electricity sources for future use. Even though the use of solar irradiation to generate electricity is currently at a fast deployment pace and technological evolution, its natural variability still presents an important barrier to overcome. Machine learning and data mining techniques arise as alternatives to aid solar electricity generation forecast reducing the impacts of its natural inconstant power supply. This paper presents a literature review on big data models for solar photovoltaic electricity generation forecasts, aiming to evaluate the most applicable and accurate state-of-art techniques to the problem, including the motivation behind each project proposal, the characteristics and quality of data used to address the problem, among other issues. A Systematic Literature Review (SLR) method was used, in which research questions were defined and translated into search strings. The search returned 38 papers for final evaluation, affirming that the use of these models to predict solar electricity generation is currently an ongoing academic research question. Machine learning is widely used, and neural networks is considered the most accurate algorithm. Extreme learning machine learning has reduced time and raised precision.",https://doi.org/10.1016/j.seta.2018.11.008,2019,Gabriel {de Freitas Viscondi} and Solange N. Alves-Souza,A SYSTEMATIC LITERATURE REVIEW ON BIG DATA FOR SOLAR PHOTOVOLTAIC ELECTRICITY GENERATION FORECASTING,article
370,21100239262,SUSTAINABLE ENERGY TECHNOLOGIES AND ASSESSMENTS,journal,22131388,"1,040",Q1,39,270,333,13311,1833,331,"5,73","49,30",United Kingdom,Western Europe,Elsevier Ltd.,2013-2020,"Energy Engineering and Power Technology (Q1); Renewable Energy, Sustainability and the Environment (Q2)",SUSTAINABLE ENERGY TECHNOLOGIES AND ASSESSMENTS,SUSTAINABLE ENERGY TECHNOLOGIES AND ASSESSMENTS,"3,234",5.353,0.00348,"Because of big data on energy consumption, there is a lack of research on the discrete manufacturing system. The discrete manufacturing system has plenty of multi-source and heterogeneous data; it was challenging to collect real-time data. Recently, low carbon and green manufacturing is a hot field; especially, it can save electrical energy. This paper proposes a significant energy consumption data of a data-driven analysis framework, which promoting the energy efficiency of discrete manufacturing plant, equipment, and workshop production process. Firstly, put forward the evaluation standards of energy efficiency for discrete manufacturing shops. Then make energy-consumption data preprocessing. Efficiency optimization of big data mining method is put forward based on grid computing function. Design the discrete manufacturing system energy-consumption parameter values, then summarizes prediction algorithms and models in order to predict the results and the trends. Finally, introduce the application of a mobile phone shell manufacturing shop to verify the proposed framework. Further research will focus on energy-consumption data mining processing.",https://doi.org/10.1016/j.seta.2021.101336,2021,Tao Zhang and Weixi Ji and Yongtao Qiu,A FRAMEWORK OF ENERGY-CONSUMPTION DRIVEN DISCRETE MANUFACTURING SYSTEM,article
371,24286,INTERNATIONAL JOURNAL OF APPROXIMATE REASONING,journal,"0888613X, 18734731","1,039",Q1,97,129,496,6026,2071,475,"4,55","46,71",United States,Northern America,Elsevier Inc.,"1974, 1987-2020",Applied Mathematics (Q1); Artificial Intelligence (Q1); Software (Q1); Theoretical Computer Science (Q1),INTERNATIONAL JOURNAL OF APPROXIMATE REASONING,INTERNATIONAL JOURNAL OF APPROXIMATE REASONING,"4,819",3.816,0.00487,"Recently, a fundamental study on measurement of data quality introduced an ordinal-scaled procedure of measurement. Besides the pure ordinal information about the level of quality, numerical information is induced when considering uncertainty involved during measurement. In the case where uncertainty is modelled as probability, this numerical information is ratio-scaled. An essential property of the mentioned approach is that the application of a measure on a large collection of data can be represented efficiently in the sense that (i) the representation has a low storage complexity and (ii) it can be updated incrementally when new data are observed. However, this property only holds when the evaluation of predicates is clear and does not deal with uncertainty. For some dimensions of quality, this assumption is far too strong and uncertainty comes into play almost naturally. In this paper, we investigate how the presence of uncertainty influences the efficiency of a measurement procedure. Hereby, we focus specifically on the case where uncertainty is caused by insufficient information and is thus modelled by means of possibility theory. It is shown that the amount of data that reaches a certain level of quality, can be summarized as a possibility distribution over the set of natural numbers. We investigate an approximation of this distribution that has a controllable loss of information, allows for incremental updates and exhibits a low space complexity.",https://doi.org/10.1016/j.ijar.2018.03.007,2018,A. Bronselaer and J. Nielandt and G. {De Tré},AN INCREMENTAL APPROACH FOR DATA QUALITY MEASUREMENT WITH INSUFFICIENT INFORMATION,article
372,29271,CLINICAL ONCOLOGY,journal,"14332981, 09366555","1,037",Q1,76,180,455,5806,1127,304,"2,47","32,26",United Kingdom,Western Europe,W.B. Saunders Ltd,"1975, 1984-1985, 1989-2020","Radiology, Nuclear Medicine and Imaging (Q1); Oncology (Q2)",CLINICAL ONCOLOGY,CLINICAL ONCOLOGY,"4,553",4.126,0.00538,"Modern artificial intelligence techniques have solved some previously intractable problems and produced impressive results in selected medical domains. One of their drawbacks is that they often need very large amounts of data. Pre-existing datasets in the form of national cancer registries, image/genetic depositories and clinical datasets already exist and have been used for research. In theory, the combination of healthcare Big Data with modern, data-hungry artificial intelligence techniques should offer significant opportunities for artificial intelligence development, but this has not yet happened. Here we discuss some of the structural reasons for this, barriers preventing artificial intelligence from making full use of existing datasets, and make suggestions as to enable progress. To do this, we use the framework of the 6Vs of Big Data and the FAIR criteria for data sharing and availability (Findability, Accessibility, Interoperability, and Reuse). We share our experience in navigating these barriers through The Brain Tumour Data Accelerator, a Brain Tumour Charity-supported initiative to integrate fragmented patient data into an enriched dataset. We conclude with some comments as to the limits of such approaches.",https://doi.org/10.1016/j.clon.2021.11.040,2022,J.W. Wang and M. Williams,"REGISTRIES, DATABASES AND REPOSITORIES FOR DEVELOPING ARTIFICIAL INTELLIGENCE IN CANCER CARE",article
373,19678,DIAGNOSTIC MICROBIOLOGY AND INFECTIOUS DISEASE,journal,"18790070, 07328893","1,027",Q1,95,192,693,4910,1788,677,"2,63","25,57",United States,Northern America,Elsevier Inc.,1983-2020,Medicine (miscellaneous) (Q1); Infectious Diseases (Q2); Microbiology (medical) (Q2),DIAGNOSTIC MICROBIOLOGY AND INFECTIOUS DISEASE,DIAGNOSTIC MICROBIOLOGY AND INFECTIOUS DISEASE,"8,091",2.803,0.00954,,https://doi.org/10.1016/j.diagmicrobio.2018.12.006,2019,Susanna K.P. Lau and Patrick C.Y. Woo,"PITFALLS IN BIG DATA ANALYSIS: NEXT-GENERATION TECHNOLOGIES, LAST-GENERATION DATA",article
374,5800175970,LEADERSHIP,journal,"17427150, 17427169","1,021",Q1,44,56,115,3340,252,110,"2,13","59,64",United Kingdom,Western Europe,SAGE Publications Ltd,2005-2020,Sociology and Political Science (Q1); Strategy and Management (Q1),LEADERSHIP,LEADERSHIP,"1,303",2.403,0.00103,,10.1109/EMR.2016.2568698,2016,"Gaynor, Gerard H.",LEADERSHIP,article
375,4100151536,RESEARCH IN TRANSPORTATION ECONOMICS,journal,07398859,"1,019",Q1,46,160,220,7437,694,205,"2,79","46,48",United States,Northern America,JAI Press,"1994, 1996, 1999, 2001, 2004-2020","Economics, Econometrics and Finance (miscellaneous) (Q1); Transportation (Q1)",RESEARCH IN TRANSPORTATION ECONOMICS,RESEARCH IN TRANSPORTATION ECONOMICS,"2,095",2.627,0.00218,"A group of researchers, consultants, software developers, and transit agencies convened in Santiago, Chile over 3 days as part of the Thredbo workshop titled “Harnessing Big Data”, to present their recent research and discuss the state of practice, state of the art, and future directions of big data in public transportation. This report documents their discussion. The key conclusion of the workshop is that, although much progress has been made in utilizing big data to improve transportation planning and operations, much remains to be done, both in terms of developing further analysis tools and use cases of big data, and of disseminating best practices so that they are adopted across the industry.",https://doi.org/10.1016/j.retrec.2016.10.008,2016,Gabriel E. Sánchez-Martínez and Marcela Munizaga,WORKSHOP 5 REPORT: HARNESSING BIG DATA,article
376,21100416081,SUSTAINABLE PRODUCTION AND CONSUMPTION,journal,23525509,"1,019",Q1,26,91,198,6098,1058,194,"5,34","67,01",Netherlands,Western Europe,Elsevier BV,2015-2021,"Environmental Engineering (Q1); Industrial and Manufacturing Engineering (Q1); Environmental Chemistry (Q2); Renewable Energy, Sustainability and the Environment (Q2)",SUSTAINABLE PRODUCTION AND CONSUMPTION,SUSTAINABLE PRODUCTION AND CONSUMPTION,"1,403",5.032,0.00196,"The generation of reliable life cycle inventories is essential towards Life Cycle Assessment (LCA) development, and the use of literature inventories as data sources can serve as a driving force for emerging LCA databases. The aim of this paper was to propose a method to select and rank scientific publications to be used as possible data sources for supplying LCA databases with new datasets. A case study was designed to identify eligible datasets to compose the emergent Brazilian Life Cycle Inventory Database System – the “SICV Brasil” launched in 2016. The methodology used was based on an exploratory research composed of three steps: i) a bibliographic survey on the scientific productions of Life Cycle Inventories (LCI) in Brazil from 2000 to 2017; ii) a cross-check of LCI data and information based on the 40 selected requirements used in order to analyze the quality of LCI datasets in terms of mandatory, recommended and optional requirements; and iii) an analysis of the data quality requirements for those datasets with support of principles of Analytical Hierarchy Process (AHP) to elect possible datasets to be included in the SICV Brasil database. In total, 57 publications were analyzed and the results indicated that mandatory requirements had under 50% acceptance and only 10 requirements (less than 25%) were fully met. The best LCI dataset received 73 points (90%) with the scoring method, while 16 datasets were given less than 40 points (50%). Therefore, it is necessary to improve data quality of LCI datasets found in literature before using them to integrate LCA databases. In this regard, this study proposed a guide with short, medium, and long-term measures to mitigate this problem. The idea is to put an action plan into practice to gather more LCI datasets from literature which may be eligible for publication to SICV Brasil to improve this national database with more and relevant high-quality datasets.",https://doi.org/10.1016/j.spc.2020.09.021,2021,Luri {Shirosaki Marçal de Souza} and Andréa Oliveira Nunes and Gabriela Giusti and Yovana M.B. Saavedra and Thiago Oliveira Rodrigues and Tiago E. {Nunes Braga} and Diogo A. {Lopes Silva},EVALUATING AND RANKING SECONDARY DATA SOURCES TO BE USED IN THE BRAZILIAN LCA DATABASE – “SICV BRASIL”,article
377,25240,TOXICOLOGY LETTERS,journal,"03784274, 18793169","1,007",Q1,145,303,840,14151,3425,821,"3,93","46,70",Netherlands,Western Europe,Elsevier BV,1977-2020,Medicine (miscellaneous) (Q1); Toxicology (Q1),TOXICOLOGY LETTERS,TOXICOLOGY LETTERS,"18,729",4.372,0.01073,,https://doi.org/10.1016/j.toxlet.2015.08.097,2015,T. Gant,THE IMPORTANCE OF DATA QUALITY TO ENHANCE THE IMPACT OF OMICS SCIENCES,article
378,24140,ARTIFICIAL INTELLIGENCE IN MEDICINE,journal,"09333657, 18732860","0,980",Q1,87,165,245,9065,1746,237,"6,69","54,94",Netherlands,Western Europe,Elsevier,1989-2020,Artificial Intelligence (Q1); Medicine (miscellaneous) (Q1),ARTIFICIAL INTELLIGENCE IN MEDICINE,ARTIFICIAL INTELLIGENCE IN MEDICINE,"4,245",5.326,0.00422,"Introduction
Machine learning capability holds promise to inform disease models, the discovery and development of novel disease modifying therapeutics and prevention strategies in psychiatry. Herein, we provide an introduction on how machine learning/Artificial Intelligence (AI) may instantiate such capabilities, as well as provide rationale for its application to psychiatry in both research and clinical ecosystems.
Methods
Databases PubMed and PsycINFO were searched from 1966 to June 2016 for keywords:Big Data, Machine Learning, Precision Medicine, Artificial Intelligence, Mental Health, Mental Disease, Psychiatry, Data Mining, RDoC, and Research Domain Criteria. Articles selected for review were those that were determined to be aligned with the objective of this particular paper.
Results
Results indicate that AI is a viable option to build useful predictors of outcome while offering objective and comparable accuracy metrics, a unique opportunity, particularly in mental health research. The approach has also consistently brought notable insight into disease models through processing the vast amount of already available multi-domain, semi-structured medical data. The opportunity for AI in psychiatry, in addition to disease-model refinement, is in characterizing those at risk, and it is likely also relevant to personalizing and discovering therapeutics.
Conclusions
Machine learning currently provides an opportunity to parse disease models in complex, multi-factorial disease states (e.g. mental disorders) and could possibly inform treatment selection with existing therapies and provide bases for domain-based therapeutic discovery.",https://doi.org/10.1016/j.artmed.2019.101704,2019,Andy M.Y. Tai and Alcides Albuquerque and Nicole E. Carmona and Mehala Subramanieapillai and Danielle S. Cha and Margarita Sheko and Yena Lee and Rodrigo Mansur and Roger S. McIntyre,MACHINE LEARNING AND BIG DATA: IMPLICATIONS FOR DISEASE MODELING AND THERAPEUTIC DISCOVERY IN PSYCHIATRY,article
379,17243,JOURNAL OF VASCULAR AND INTERVENTIONAL RADIOLOGY,journal,"10510443, 15357732","0,979",Q1,133,401,1086,6771,2111,715,"1,86","16,89",United States,Northern America,Elsevier Inc.,1990-2020,"Medicine (miscellaneous) (Q1); Radiology, Nuclear Medicine and Imaging (Q1); Cardiology and Cardiovascular Medicine (Q2)",JOURNAL OF VASCULAR AND INTERVENTIONAL RADIOLOGY,JOURNAL OF VASCULAR AND INTERVENTIONAL RADIOLOGY,"11,063",3.464,0.01078,"Observational data research studying access, utilization, cost, and outcomes of image-guided interventions using publicly available “big data” sets is growing in the interventional radiology (IR) literature. Publicly available data sets offer insight into real-world care and represent an important pillar of IR research moving forward. They offer insights into how IR procedures are being used nationally and whether they are working as intended. On the other hand, large data sources are aggregated using complex sampling frames, and their strengths and weaknesses only become apparent after extensive use. Unintentional misuse of large data sets can result in misleading or sometimes erroneous conclusions. This review introduces the most commonly used databases relevant to IR research, highlights their strengths and limitations, and provides recommendations for use. In addition, it summarizes methodologic best practices pertinent to all data sets for planning and executing scientifically rigorous and clinically relevant observational research.",https://doi.org/10.1016/j.jvir.2022.08.003,2022,Premal S. Trivedi and Vincent M. Timpone and Rustain L. Morgan and Alexandria M. Jensen and Margaret Reid and P. Michael Ho and Osman Ahmed,A PRACTICAL GUIDE TO USE OF PUBLICLY AVAILABLE DATA SETS FOR OBSERVATIONAL RESEARCH IN INTERVENTIONAL RADIOLOGY,article
380,17013,JOURNAL OF PETROLEUM SCIENCE AND ENGINEERING,journal,09204105,"0,975",Q1,111,1174,2764,59146,13310,2752,"4,78","50,38",Netherlands,Western Europe,Elsevier,1987-2021,Fuel Technology (Q1); Geotechnical Engineering and Engineering Geology (Q1),JOURNAL OF PETROLEUM SCIENCE AND ENGINEERING,JOURNAL OF PETROLEUM SCIENCE AND ENGINEERING,"25,616",4.346,0.02651,"Pressure transient analysis provides essential information to evaluate the dimensions of injection induced fractures, permeability damage near the wellbore, and pressure elevation in the injection horizon. For injection wells, shut-in data can be collected and analyzed after each injection cycle to evaluate the well injectivity and predict the well longevity. However, any interactive analysis of the pressure data could be subjective and time-consuming. In this study a novel cloud-based approach to automatically analyzing pressure data is presented, which aims to improve the reliability and efficiency of pressure transient analysis. There are two fundamental requirements for automated pressure transient analysis: 1) Pressure data needs to be automatically retrieved from field sites and fed to the analyzer; 2) The engineer can automatically select instantaneous shut-in pressure (ISIP), identify flow regimes, and determine the fracture closure point if any. To meet these requirements as well as to take advantage of cloud storage and computing technologies, a web-based application has been developed to pull real time injection data from any field sites and push it to a cloud database. A built-in pressure transient workflow has been also proposed to detect any stored or real-time pressure data and perform pressure analysis automatically if the required data is available. The automated pressure transient analysis technology has been applied to multiple injection projects. In general, the analysis results including formation and fracture properties (i.e. permeability, fracture half length, skin factor, and fracture closure pressure) are comparable to results from interactive analysis. Any discrepancies are mainly caused by poor data quality. Issues such as inconsistent selections of ISIP and different slopes defined for pre and after closure analyses also contribute to the divergence. Overall, the automated pressure transient analysis provides consistent results as the exact same criteria are applied to the pressure data, and analysis results are independent of the analyzer's experience and knowledge. As data from oil/gas industry increases exponentially over time, automated data transmission, storage, analysis and access are becoming necessary to maximize the value of the data and reduce operation cost. The automated pressure transient analysis presented here demonstrates that cloud storage and computing combined with automated analysis tools is a viable way to overcome big data challenges faced by oil/gas industry professionals.",https://doi.org/10.1016/j.petrol.2020.107627,2021,Yonggui Guo and Ibrahim Mohamed and Ali Zidane and Yashesh Panchal and Omar Abou-Sayed and Ahmed Abou-Sayed,AUTOMATED PRESSURE TRANSIENT ANALYSIS: A CLOUD-BASED APPROACH,article
381,17013,JOURNAL OF PETROLEUM SCIENCE AND ENGINEERING,journal,09204105,"0,975",Q1,111,1174,2764,59146,13310,2752,"4,78","50,38",Netherlands,Western Europe,Elsevier,1987-2021,Fuel Technology (Q1); Geotechnical Engineering and Engineering Geology (Q1),JOURNAL OF PETROLEUM SCIENCE AND ENGINEERING,JOURNAL OF PETROLEUM SCIENCE AND ENGINEERING,"25,616",4.346,0.02651,"In recent years, machine learning has been adopted in the Oil and Gas industry as a promising technology for solutions to the most demanding problems like downhole parameters estimations and incidents detection. A big amount of available data makes this technology an attractive option for solving a wide variety of drilling problems, as well as a reliable candidate for performing big-data analysis and interpretation. Nevertheless, this approach may cause, in some cases, that petroleum engineering concepts are disregarded in favor of more data-intensive approaches. This study aims to evaluate the impact of drilling data measurement correction on data-driven model performance. In our study, besides using the standard data processing technologies, like gap filling, outlier removal, noise reduction etc., the physics-based drilling models are also implemented for data quality improvement and data correction in consideration of the measurement physics, rarely mentioned in most of publications. In our case study, recurrent neural networks (RNN) that are able to capture temporal natures of a signal are employed for the rate of penetration (ROP) estimation with an adjustable predictive window. The results show that the RNN model produces the best results when using the drilling data recovered through analytical methods. Moreover, the comprehensive data-driven model evaluation and engineering interpretation are conducted to facilitate better understanding of the data-driven models and their applications.",https://doi.org/10.1016/j.petrol.2021.109904,2022,Mauro A. Encinas and Andrzej T. Tunkiel and Dan Sui,DOWNHOLE DATA CORRECTION FOR DATA-DRIVEN RATE OF PENETRATION PREDICTION MODELING,article
382,21100389518,JOURNAL OF BUILDING ENGINEERING,journal,23527102,"0,974",Q1,39,777,721,38204,4094,720,"5,70","49,17",Netherlands,Western Europe,Elsevier BV,2015-2021,"Architecture (Q1); Building and Construction (Q1); Civil and Structural Engineering (Q1); Mechanics of Materials (Q1); Safety, Risk, Reliability and Quality (Q1)",JOURNAL OF BUILDING ENGINEERING,JOURNAL OF BUILDING ENGINEERING,"5,990",5.318,0.00753,"With the increase in the energy consumption of air-conditioning (AC) systems in Chinese residential buildings, the realization of energy savings in AC systems has attracted increasing attention. The variable refrigerant flow (VRF) system is a common AC system for residential buildings in China. In most previous studies on VRF systems, onsite measurements or surveys were conducted to collect operational data. These traditional methods may face various data issues, such as limited sample sizes and invalid data, making them unable to capture the spatial and temporal performance features of VRF systems in residential buildings on a large scale. To fill this gap, with advances in data storage and transmission technology, Big Data methods have been widely used for data collection. In the present study, researchers adopted 16,985 sets of VRF system operation data from China as the database and conducted data analysis for both the spatial and temporal dimensions. Several key indicators were proposed from the two perspectives (spatial and temporal), including the part-space index (PSI), load ratio (LR), use duration (UD), and cooling energy consumption. The main findings were as follows: (1) The “part-time part-space” operation mode of residential VRF systems can be analyzed according to the statistical results of the UD and PSI. (2) An LR of <30% is the main operating condition for VRF systems in residential buildings. (3) Extracted typical LR patterns can reflect different user behavior. The statistical results obtained in this study provide a basis for VRF engineering projects.",https://doi.org/10.1016/j.jobe.2022.105219,2022,Hua Liu and Yi Wu and Da Yan and Shan Hu and Mingyang Qian,INVESTIGATION OF VRF SYSTEM COOLING OPERATION AND PERFORMANCE IN RESIDENTIAL BUILDINGS BASED ON LARGE-SCALE DATASET,article
383,11300153406,DISABILITY AND HEALTH JOURNAL,journal,"19366574, 18767583","0,967",Q1,36,132,307,4567,796,278,"2,29","34,60",United States,Northern America,Elsevier Inc.,2008-2020,"Medicine (miscellaneous) (Q1); Public Health, Environmental and Occupational Health (Q1)",DISABILITY AND HEALTH JOURNAL,DISABILITY AND HEALTH JOURNAL,"2,139",2.554,0.00403,,https://doi.org/10.1016/j.dhjo.2015.04.003,2015,Suzanne McDermott and Margaret A. Turk,WHAT ARE THE IMPLICATIONS OF THE BIG DATA PARADIGM SHIFT FOR DISABILITY AND HEALTH?,article
384,28249,JOURNAL OF PROFESSIONAL NURSING,journal,"15328481, 87557223","0,960",Q1,53,137,229,4125,508,205,"1,74","30,11",United Kingdom,Western Europe,W.B. Saunders Ltd,1985-2020,Medicine (miscellaneous) (Q1); Nursing (miscellaneous) (Q1),JOURNAL OF PROFESSIONAL NURSING,JOURNAL OF PROFESSIONAL NURSING,"2,171",2.104,0.00228,,https://doi.org/10.1016/j.profnurs.2017.10.005,2018,Betty Rambur and Therese Fitzpatrick,A PLEA TO NURSE EDUCATORS: INCORPORATE BIG DATA USE AS A FOUNDATIONAL SKILL FOR UNDERGRADUATE AND GRADUATE NURSES,article
385,28425,FIRE SAFETY JOURNAL,journal,03797112,"0,958",Q1,78,260,442,8386,1566,437,"3,13","32,25",United Kingdom,Western Europe,Elsevier Ltd.,1977-2020,"Chemistry (miscellaneous) (Q1); Materials Science (miscellaneous) (Q1); Physics and Astronomy (miscellaneous) (Q1); Safety, Risk, Reliability and Quality (Q1)",FIRE SAFETY JOURNAL,FIRE SAFETY JOURNAL,"5,861",2.764,0.00398,"Wildfires, whether natural or caused by humans, are considered among the most dangerous and devastating disasters around the world. Their complexity comes from the fact that they are hard to predict, hard to extinguish and cause enormous financial losses. To address this issue, many research efforts have been conducted in order to monitor, predict and prevent wildfires using several Artificial Intelligence techniques and strategies such as Big Data, Machine Learning, and Remote Sensing. The latter offers a rich source of satellite images, from which we can retrieve a huge amount of data that can be used to monitor wildfires. The method used in this paper combines Big Data, Remote Sensing and Data Mining algorithms (Artificial Neural Network and SVM) to process data collected from satellite images over large areas and extract insights from them to predict the occurrence of wildfires and avoid such disasters. For this reason, we implemented a methodology that serves this purpose by building a dataset based on Remote Sensing data related to the state of the crops (NDVI), meteorological conditions (LST), as well as the fire indicator “Thermal Anomalies”, these data, were acquired from “MODIS” (Moderate Resolution Imaging Spectroradiometer), a key instrument aboard the Terra and Aqua satellites. This dataset is available on GitHub via this link (https://github.com/ouladsayadyounes/Wildfires). Experiments were made using the big data platform “Databricks”. Experimental results gave high prediction accuracy (98.32%). These results were assessed using several validation strategies (e.g., classification metrics, cross-validation, and regularization) as well as a comparison with some wildfire early warning systems.",https://doi.org/10.1016/j.firesaf.2019.01.006,2019,Younes Oulad Sayad and Hajar Mousannif and Hassan {Al Moatassime},PREDICTIVE MODELING OF WILDFIRES: A NEW DATASET AND MACHINE LEARNING APPROACH,article
386,29253,NURSING OUTLOOK,journal,"15283968, 00296554","0,953",Q1,57,119,315,4073,747,240,"2,37","34,23",United States,Northern America,Mosby Inc.,1953-2020,Nursing (miscellaneous) (Q1),NURSING OUTLOOK,NURSING OUTLOOK,"2,751",3.250,0.00323,"Background
Chronic diseases, such as opioid use disorder (OUD) require a multifaceted scientific approach to address their evolving complexity. The Council for the Advancement of Nursing Science's (Council) four nursing science priority areas (precision health; global health, determinants of health, and big data/data analytics) were established to provide a framework to address current complex health problems.
Purpose
To examine OUD research through the nursing science priority areas and evaluate the appropriateness of the priority areas as a framework for research on complex health conditions.
Method
OUD was used as an exemplar to explore the relevance of the nursing science priorities for future research.
Findings
Research in the four priority areas is advancing knowledge in OUD identification, prevention, and treatment. Intersection of OUD research population focus and methodological approach was identified among the priority areas.
Discussion
The Council priorities provide a relevant framework for nurse scientists to address complex health problems like OUD.",https://doi.org/10.1016/j.outlook.2020.02.001,2020,Patricia Eckardt and Donald Bailey and Holli A. DeVon and Cynthia Dougherty and Pamela Ginex and Cheryl A. Krause-Parello and Rita H. Pickler and Therese S. Richmond and Eleanor Rivera and Carol F. Roye and Nancy Redeker,OPIOID USE DISORDER RESEARCH AND THE COUNCIL FOR THE ADVANCEMENT OF NURSING SCIENCE PRIORITY AREAS,article
387,29253,NURSING OUTLOOK,journal,"15283968, 00296554","0,953",Q1,57,119,315,4073,747,240,"2,37","34,23",United States,Northern America,Mosby Inc.,1953-2020,Nursing (miscellaneous) (Q1),NURSING OUTLOOK,NURSING OUTLOOK,"2,751",3.250,0.00323,"Background
Big data and cutting-edge analytic methods in nursing research challenge nurse scientists to extend the data sources and analytic methods used for discovering and translating knowledge.
Purpose
The purpose of this study was to identify, analyze, and synthesize exemplars of big data nursing research applied to practice and disseminated in key nursing informatics, general biomedical informatics, and nursing research journals.
Methods
A literature review of studies published between 2009 and 2015. There were 650 journal articles identified in 17 key nursing informatics, general biomedical informatics, and nursing research journals in the Web of Science database. After screening for inclusion and exclusion criteria, 17 studies published in 18 articles were identified as big data nursing research applied to practice.
Discussion
Nurses clearly are beginning to conduct big data research applied to practice. These studies represent multiple data sources and settings. Although numerous analytic methods were used, the fundamental issue remains to define the types of analyses consistent with big data analytic methods.
Conclusion
There are needs to increase the visibility of big data and data science research conducted by nurse scientists, further examine the use of state of the science in data analytics, and continue to expand the availability and use of a variety of scientific, governmental, and industry data resources. A major implication of this literature review is whether nursing faculty and preparation of future scientists (PhD programs) are prepared for big data and data science.",https://doi.org/10.1016/j.outlook.2016.11.021,2017,Bonnie L. Westra and Martha Sylvia and Elizabeth F. Weinfurter and Lisiane Pruinelli and Jung In Park and Dianna Dodd and Gail M. Keenan and Patricia Senk and Rachel L. Richesson and Vicki Baukner and Christopher Cruz and Grace Gao and Luann Whittenburg and Connie W. Delaney,BIG DATA SCIENCE: A LITERATURE REVIEW OF NURSING RESEARCH EXEMPLARS,article
388,29712,CLINICAL THERAPEUTICS,journal,"1879114X, 01492918","0,925",Q2,134,223,672,9030,1766,587,"2,33","40,49",United States,Northern America,Elsevier Inc.,1977-2020,Pharmacology (Q2); Pharmacology (medical) (Q2),CLINICAL THERAPEUTICS,CLINICAL THERAPEUTICS,"9,397",3.393,0.00964,"Purpose
Interest in leveraging real-world evidence (RWE) to support regulatory decision making for product effectiveness has been increasing globally as evident by the increasing number of regulatory frameworks and guidance documents. However, acceptance of RWE, especially before marketing for regulatory approval, differs across countries. In addition, guidance on the design and conduct of innovative clinical trials, such as randomized controlled registry studies, pragmatic trials, and other hybrid studies, is lacking.
Methods
We assessed the global regulatory environment with regard to RWE based on regional availability of the following 3 key regulatory elements: (1) RWE regulatory framework, (2) data quality and standards guidance. and (3) study methods guidance.
Findings
This article reviews the available frameworks and existing guidance from across the globe and discusses the observed gaps and opportunities for further development and harmonization.
Implications
Cross-country collaborations are encouraged to further shape and align RWE policies and help establish frameworks in countries without current policies with the goal of creating efficiencies when considering RWE to support regulatory decision-making globally.",https://doi.org/10.1016/j.clinthera.2022.01.012,2022,Leah Burns and Nadege Le Roux and Robert Kalesnik-Orszulak and Jennifer Christian and Mathias Hukkelhoven and Frank Rockhold and John O'Donnell,REAL-WORLD EVIDENCE FOR REGULATORY DECISION-MAKING: GUIDANCE FROM AROUND THE WORLD,article
389,29286,CLINICA CHIMICA ACTA,journal,"00098981, 18733492","0,924",Q2,142,555,1269,26479,4130,1202,"3,35","47,71",Netherlands,Western Europe,Elsevier,1956-2020,Biochemistry (Q2); Biochemistry (medical) (Q2); Clinical Biochemistry (Q2); Medicine (miscellaneous) (Q2),CLINICA CHIMICA ACTA,CLINICA CHIMICA ACTA,"22,229",3.786,0.01641,"Although reference intervals (RIs) play an important role in clinical diagnosis, there remain significant differences with respect to race, gender, age and geographic location. Accordingly, the Clinical Laboratory Standards Institute (CLSI) EP28-A3c has recommended that clinical laboratories establish RIs appropriate to their subject population. Unfortunately, the traditional and direct approach to establish RIs relies on the recruitment of a sufficient number of healthy individuals of various age groups, collection and testing of large numbers of specimens and accurate data interpretation. The advent of the big data era has, however, created a unique opportunity to “mine” laboratory information. Unfortunately, this indirect method lacks standardization, consensus support and CLSI guidance. In this review we provide a historical perspective, comprehensively assess data processing and statistical methods, and post-verification analysis to validate this big data approach in establishing laboratory specific RIs.",https://doi.org/10.1016/j.cca.2022.01.001,2022,Dan Yang and Zihan Su and Min Zhao,BIG DATA AND REFERENCE INTERVALS,article
390,23604,COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,journal,"01692607, 18727565","0,924",Q1,102,538,789,23721,4743,753,"6,27","44,09",Ireland,Western Europe,Elsevier Ireland Ltd,1985-2020,Computer Science Applications (Q1); Software (Q1); Health Informatics (Q2),COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,"12,277",5.428,0.01119,"ABSTRACT
Introduction. With biomedical imaging research increasingly using large datasets, it becomes critical to find operator-free methods to quality control the data collected and the associated analysis. Attempts to use artificial intelligence (AI) to perform automated quality control (QC) for both single-site and multi-site datasets have been explored in some neuroimaging techniques (e.g. EEG or MRI), although these methods struggle to find replication in other domains. The aim of this study is to test the feasibility of an automated QC pipeline for brain [18F]-FDOPA PET imaging as a biomarker for the dopamine system. Methods. Two different Convolutional Neural Networks (CNNs) were used and combined to assess spatial misalignment to a standard template and the signal-to-noise ratio (SNR) relative to 200 static [18F]-FDOPA PET images that had been manually quality controlled from three different PET/CT scanners. The scans were combined with an additional 400 scans, in which misalignment (200 scans) and low SNR (200 scans) were simulated. A cross-validation was performed, where 80% of the data were used for training and 20% for validation. Two additional datasets of [18F]-FDOPA PET images (50 and 100 scans respectively with at least 80% of good quality images) were used for out-of-sample validation. Results. The CNN performance was excellent in the training dataset (accuracy for motion: 0.86 ± 0.01, accuracy for SNR: 0.69 ± 0.01), leading to 100% accurate QC classification when applied to the two out-of-sample datasets. Data dimensionality reduction affected the generalizability of the CNNs, especially when the classifiers were applied to the out-of-sample data from 3D to 1D datasets. Conclusions. This feasibility study shows that it is possible to perform automatic QC of [18F]-FDOPA PET imaging with CNNs. The approach has the potential to be extended to other PET tracers in both brain and non-brain applications, but it is dependent on the availability of large datasets necessary for the algorithm training.",https://doi.org/10.1016/j.cmpb.2021.106239,2021,Antonella D. Pontoriero and Giovanna Nordio and Rubaida Easmin and Alessio Giacomel and Barbara Santangelo and Sameer Jahuar and Ilaria Bonoldi and Maria Rogdaki and Federico Turkheimer and Oliver Howes and Mattia Veronese,AUTOMATED DATA QUALITY CONTROL IN FDOPA BRAIN PET IMAGING USING DEEP LEARNING,article
391,23604,COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,journal,"01692607, 18727565","0,924",Q1,102,538,789,23721,4743,753,"6,27","44,09",Ireland,Western Europe,Elsevier Ireland Ltd,1985-2020,Computer Science Applications (Q1); Software (Q1); Health Informatics (Q2),COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,"12,277",5.428,0.01119,"Background and Objectives
Data Quality (DQ) programs are recognized as a critical aspect of new-generation research platforms using electronic health record (EHR) data for building Learning Healthcare Systems. The AP-HP Clinical Data Repository aggregates EHR data from 37 hospitals to enable large-scale research and secondary data analysis. This paper describes the DQ program currently in place at AP-HP and the lessons learned from two DQ campaigns initiated in 2017.
Materials and Methods
As part of the AP-HP DQ program, two domains - patient identification (PI) and healthcare services (HS) - were selected for conducting DQ campaigns consisting of 5 phases: defining the scope, measuring, analyzing, improving and controlling DQ. Semi-automated DQ profiling was conducted in two data sets – the PI data set containing 8.8 M patients and the HS data set containing 13,099 consultation agendas and 2122 care units. Seventeen DQ measures were defined and DQ issues were classified using a unified DQ reporting framework. For each domain, actions plans were defined for improving and monitoring prioritized DQ issues.
Results
Eleven identified DQ issues (8 for the PI data set and 3 for the HS data set) were categorized into completeness (n = 6), conformance (n = 3) and plausibility (n = 2) DQ issues. DQ issues were caused by errors from data originators, ETL issues or limitations of the EHR data entry tool. The action plans included sixteen actions (9 for the PI domain and 7 for the HS domain). Though only partial implementation, the DQ campaigns already resulted in significant improvement of DQ measures.
Conclusion
DQ assessments of hospital information systems are largely unpublished. The preliminary results of two DQ campaigns conducted at AP-HP illustrate the benefit of the engagement into a DQ program. The adoption of a unified DQ reporting framework enables the communication of DQ findings in a well-defined manner with a shared vocabulary. Dedicated tooling is needed to automate and extend the scope of the generic DQ program. Specific DQ checks will be additionally defined on a per-study basis to evaluate whether EHR data fits for specific uses.",https://doi.org/10.1016/j.cmpb.2018.10.016,2019,Christel Daniel and Patricia Serre and Nina Orlova and Stéphane Bréant and Nicolas Paris and Nicolas Griffon,INITIALIZING A HOSPITAL-WIDE DATA QUALITY PROGRAM. THE AP-HP EXPERIENCE.,article
392,23604,COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,journal,"01692607, 18727565","0,924",Q1,102,538,789,23721,4743,753,"6,27","44,09",Ireland,Western Europe,Elsevier Ireland Ltd,1985-2020,Computer Science Applications (Q1); Software (Q1); Health Informatics (Q2),COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,"12,277",5.428,0.01119,"Background and Objectives
Data curation is a tedious task but of paramount relevance for data analytics and more specially in the health context where data-driven decisions must be extremely accurate. The ambition of TAQIH is to support non-technical users on 1) the exploratory data analysis (EDA) process of tabular health data, and 2) the assessment and improvement of its quality.
Methods
A web-based tool has been implemented with a simple yet powerful visual interface. First, it provides interfaces to understand the dataset, to gain the understanding of the content, structure and distribution. Then, it provides data visualization and improvement utilities for the data quality dimensions of completeness, accuracy, redundancy and readability.
Results
It has been applied in two different scenarios. (1) The Northern Ireland General Practitioners (GPs) Prescription Data, an open data set containing drug prescriptions. (2) A glucose monitoring tele health system dataset. Findings on (1) include: Features that had significant amount of missing values (e.g. AMP_NM variable 53.39%); instances that have high percentage of variable values missing (e.g. 0.21% of the instances with > 75% of missing values); highly correlated variables (e.g. Gross and Actual cost almost completely correlated (∼ + 1.0)). Findings on (2) include: Features that had significant amount of missing values (e.g. patient height, weight and body mass index (BMI) (> 70%), date of diagnosis 13%)); highly correlated variables (e.g. height, weight and BMI). Full detail of the testing and insights related to findings are reported.
Conclusions
TAQIH enables and supports users to carry out EDA on tabular health data and to assess and improve its quality. Having the layout of the application menu arranged sequentially as the conventional EDA pipeline helps following a consistent analysis process. The general description of the dataset and features section is very useful for the first overview of the dataset. The missing value heatmap is also very helpful in visually identifying correlations among missing values. The correlations section has proved to be supportive as a preliminary step before further data analysis pipelines, as well as the outliers section. Finally, the data quality section provides a quantitative value to the dataset improvements.",https://doi.org/10.1016/j.cmpb.2018.12.029,2019,Roberto {Álvarez Sánchez} and Andoni {Beristain Iraola} and Gorka {Epelde Unanue} and Paul Carlin,"TAQIH, A TOOL FOR TABULAR DATA QUALITY ASSESSMENT AND IMPROVEMENT IN THE CONTEXT OF HEALTH DATA",article
393,23604,COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,journal,"01692607, 18727565","0,924",Q1,102,538,789,23721,4743,753,"6,27","44,09",Ireland,Western Europe,Elsevier Ireland Ltd,1985-2020,Computer Science Applications (Q1); Software (Q1); Health Informatics (Q2),COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,"12,277",5.428,0.01119,"Objective
To identify common temporal evolution profiles in biological data and propose a semi-automated method to these patterns in a clinical data warehouse (CDW).
Materials and Methods
We leveraged the CDW of the European Hospital Georges Pompidou and tracked the evolution of 192 biological parameters over a period of 17 years (for 445,000 + patients, and 131 million laboratory test results).
Results
We identified three common profiles of evolution: discretization, breakpoints, and trends. We developed computational and statistical methods to identify these profiles in the CDW. Overall, of the 192 observed biological parameters (87,814,136 values), 135 presented at least one evolution. We identified breakpoints in 30 distinct parameters, discretizations in 32, and trends in 79.
Discussion and conclusion
our method allowed the identification of several temporal events in the data. Considering the distribution over time of these events, we identified probable causes for the observed profiles: instruments or software upgrades and changes in computation formulas. We evaluated the potential impact for data reuse. Finally, we formulated recommendations to enable safe use and sharing of biological data collection to limit the impact of data evolution in retrospective and federated studies (e.g. the annotation of laboratory parameters presenting breakpoints or trends).",https://doi.org/10.1016/j.cmpb.2018.12.030,2019,Vincent Looten and Liliane {Kong Win Chang} and Antoine Neuraz and Marie-Anne Landau-Loriot and Benoit Vedie and Jean-Louis Paul and Laëtitia Mauge and Nadia Rivet and Angela Bonifati and Gilles Chatellier and Anita Burgun and Bastien Rance,WHAT CAN MILLIONS OF LABORATORY TEST RESULTS TELL US ABOUT THE TEMPORAL ASPECT OF DATA QUALITY? STUDY OF DATA SPANNING 17 YEARS IN A CLINICAL DATA WAREHOUSE,article
394,23604,COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,journal,"01692607, 18727565","0,924",Q1,102,538,789,23721,4743,753,"6,27","44,09",Ireland,Western Europe,Elsevier Ireland Ltd,1985-2020,Computer Science Applications (Q1); Software (Q1); Health Informatics (Q2),COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,"12,277",5.428,0.01119,,https://doi.org/10.1016/j.cmpb.2019.06.013,2019,Carlos Sáez and Siaw-Teng Liaw and Eizen Kimura and Pascal Coorevits and Juan M Garcia-Gomez,GUEST EDITORIAL: SPECIAL ISSUE IN BIOMEDICAL DATA QUALITY ASSESSMENT METHODS,article
395,23604,COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,journal,"01692607, 18727565","0,924",Q1,102,538,789,23721,4743,753,"6,27","44,09",Ireland,Western Europe,Elsevier Ireland Ltd,1985-2020,Computer Science Applications (Q1); Software (Q1); Health Informatics (Q2),COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,"12,277",5.428,0.01119,"Purpose
We present a Health Care System (HCS) based on integrated learning to achieve high-efficiency and high-precision integration of medical and health big data, and compared it with an internet-based integrated system.
Method
The method proposed in this paper adopts the Bagging integrated learning method and the Extreme Learning Machine (ELM) prediction model to obtain a high-precision strong learning model. In order to verify the integration efficiency of the system, we compare it with the Internet-based health big data integration system in terms of integration volume, integration efficiency, and storage space capacity.
Results
The HCS based on integrated learning relies on the Internet in terms of integration volume, integration efficiency, and storage space capacity. The amount of integration is proportional to the time and the integration time is between 170-450 ms, which is only half of the comparison system; whereby the storage space capacity reaches 8.3×28TB.
Conclusion
The experimental results show that the integrated learning-based HCS integrates medical and health big data with high integration volume and integration efficiency, and has high space storage capacity and concurrent data processing performance.",https://doi.org/10.1016/j.cmpb.2021.106293,2021,Yuguang Ye and Jianshe Shi and Daxin Zhu and Lianta Su and Jianlong Huang and Yifeng Huang,MANAGEMENT OF MEDICAL AND HEALTH BIG DATA BASED ON INTEGRATED LEARNING-BASED HEALTH CARE SYSTEM: A REVIEW AND COMPARATIVE ANALYSIS,article
396,23604,COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,journal,"01692607, 18727565","0,924",Q1,102,538,789,23721,4743,753,"6,27","44,09",Ireland,Western Europe,Elsevier Ireland Ltd,1985-2020,Computer Science Applications (Q1); Software (Q1); Health Informatics (Q2),COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,"12,277",5.428,0.01119,"Background
In the age of information superhighway, big data play a significant role in information processing, extractions, retrieving and management. In computational biology, the continuous challenge is to manage the biological data. Data mining techniques are sometimes imperfect for new space and time requirements. Thus, it is critical to process massive amounts of data to retrieve knowledge. The existing software and automated tools to handle big data sets are not sufficient. As a result, an expandable mining technique that enfolds the large storage and processing capability of distributed or parallel processing platforms is essential.
Method
In this analysis, a contemporary distributed clustering methodology for imbalance data reduction using k-nearest neighbor (K-NN) classification approach has been introduced. The pivotal objective of this work is to illustrate real training data sets with reduced amount of elements or instances. These reduced amounts of data sets will ensure faster data classification and standard storage management with less sensitivity. However, general data reduction methods cannot manage very big data sets. To minimize these difficulties, a MapReduce-oriented framework is designed using various clusters of automated contents, comprising multiple algorithmic approaches.
Results
To test the proposed approach, a real DNA (deoxyribonucleic acid) dataset that consists of 90 million pairs has been used. The proposed model reduces the imbalance data sets from large-scale data sets without loss of its accuracy.
Conclusions
The obtained results depict that MapReduce based K-NN classifier provided accurate results for big data of DNA.",https://doi.org/10.1016/j.cmpb.2016.04.005,2016,Sarwar Kamal and Shamim Hasnat Ripon and Nilanjan Dey and Amira S. Ashour and V. Santhi,A MAPREDUCE APPROACH TO DIMINISH IMBALANCE PARAMETERS FOR BIG DEOXYRIBONUCLEIC ACID DATASET,article
397,23604,COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,journal,"01692607, 18727565","0,924",Q1,102,538,789,23721,4743,753,"6,27","44,09",Ireland,Western Europe,Elsevier Ireland Ltd,1985-2020,Computer Science Applications (Q1); Software (Q1); Health Informatics (Q2),COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,"12,277",5.428,0.01119,"Background and objective
In recent years, several data quality conceptual frameworks have been proposed across the Data Quality and Information Quality domains towards assessment of quality of data. These frameworks are diverse, varying from simple lists of concepts to complex ontological and taxonomical representations of data quality concepts. The goal of this study is to design, develop and implement a platform agnostic computable data quality knowledge repository for data quality assessments.
Methods
We identified computable data quality concepts by performing a comprehensive literature review of articles indexed in three major bibliographic data sources. From this corpus, we extracted data quality concepts, their definitions, applicable measures, their computability and identified conceptual relationships. We used these relationships to design and develop a data quality meta-model and implemented it in a quality knowledge repository.
Results
We identified three primitives for programmatically performing data quality assessments: data quality concept, its definition, its measure or rule for data quality assessment, and their associations. We modeled a computable data quality meta-data repository and extended this framework to adapt, store, retrieve and automate assessment of other existing data quality assessment models.
Conclusion
We identified research gaps in data quality literature towards automating data quality assessments methods. In this process, we designed, developed and implemented a computable data quality knowledge repository for assessing quality and characterizing data in health data repositories. We leverage this knowledge repository in a service-oriented architecture to perform scalable and reproducible framework for data quality assessments in disparate biomedical data sources.",https://doi.org/10.1016/j.cmpb.2019.05.017,2019,Naresh Sundar Rajan and Ramkiran Gouripeddi and Peter Mo and Randy K. Madsen and Julio C. Facelli,TOWARDS A CONTENT AGNOSTIC COMPUTABLE KNOWLEDGE REPOSITORY FOR DATA QUALITY ASSESSMENT,article
398,28805,NURSE EDUCATION IN PRACTICE,journal,"18735223, 14715953","0,924",Q1,47,193,486,6984,1193,433,"2,24","36,19",United Kingdom,Western Europe,Churchill Livingstone,2001-2020,Education (Q1); Nursing (miscellaneous) (Q1); Medicine (miscellaneous) (Q2),NURSE EDUCATION IN PRACTICE,NURSE EDUCATION IN PRACTICE,"3,334",2.281,0.00456,"The pace of technological evolution in healthcare is advancing. In this article key technology trends are identified that are likely to influence nursing practice and education over the next decade. The complexity of curricular revision can create challenges in the face of rapid practice change. Nurse educators are encouraged to consider the role of electronic health records (EHRs), wearable technologies, big data and data analytics, and increased patient engagement as key areas for curriculum development. Student nurses, and those already in practice, should be offered ongoing educational opportunities to enhance a wide spectrum of professional informatics skills. The nurses of 2025 will most certainly inhabit a very different practice environment than what exists today and technology will be key in this transformation. Nurse educators must prepare now to lead these practitioners into the future.",https://doi.org/10.1016/j.nepr.2016.12.007,2017,Tracie Risling,EDUCATING THE NURSES OF 2025: TECHNOLOGY TRENDS OF THE NEXT DECADE,article
399,26529,FUZZY SETS AND SYSTEMS,journal,01650114,"0,902",Q1,169,333,570,11642,2262,560,"3,72","34,96",Netherlands,Western Europe,Elsevier,1978-2020,Artificial Intelligence (Q1); Logic (Q1),FUZZY SETS AND SYSTEMS,FUZZY SETS AND SYSTEMS,"17,883",3.343,0.00737,,https://doi.org/10.1016/j.fss.2018.05.022,2018,Sadok Ben Yahia and Anne Laurent and Gabriella Pasi,PREFACE: SPECIAL ISSUE ON BIG DATA,article
400,29806,INTERNATIONAL JOURNAL OF ACCOUNTING INFORMATION SYSTEMS,journal,14670895,"0,897",Q1,53,21,51,1375,290,50,"5,56","65,48",United States,Northern America,Elsevier Inc.,2000-2020,Finance (Q1); Information Systems and Management (Q1); Management Information Systems (Q1); Accounting (Q2),INTERNATIONAL JOURNAL OF ACCOUNTING INFORMATION SYSTEMS,INTERNATIONAL JOURNAL OF ACCOUNTING INFORMATION SYSTEMS,"1,009",4.400,0.00056,"Given exponential growth in the size of big data, its multi-channel sources and variability in quality that create challenges concerning cost-effective use, firms have invested significantly in databases and analytical tools to inform decision-making. In this regard, one means to avoid the costs associated with producing less than insightful reports and negative effects on performance through wasted resources is prioritizing data in terms of relevance and quality. The aim of this study is to investigate this approach by developing and testing a scale to evaluate Big Data Availability and the role of Big Data Prioritization for more effective use of big data in decision-making and performance. Focusing on the context of supply chain management (SCM), we validate this scale through a survey involving 84 managers. Findings support a positive association between Big Data Availability and its use in SCM decision-making, and suggest that Big Data Prioritization, as conceptualized in the study, has a positive impact on the use of big data in SCM decision-making and SCM performance. Through developing a scale to evaluate association between Big Data Availability and use in SCM decision-making, we make an empirical contribution to value generation from big data.",https://doi.org/10.1016/j.accinf.2020.100470,2020,Carla Wilkin and Aldónio Ferreira and Kristian Rotaru and Luigi Red Gaerlan,BIG DATA PRIORITIZATION IN SCM DECISION-MAKING: ITS ROLE AND PERFORMANCE IMPLICATIONS,article
401,29806,INTERNATIONAL JOURNAL OF ACCOUNTING INFORMATION SYSTEMS,journal,14670895,"0,897",Q1,53,21,51,1375,290,50,"5,56","65,48",United States,Northern America,Elsevier Inc.,2000-2020,Finance (Q1); Information Systems and Management (Q1); Management Information Systems (Q1); Accounting (Q2),INTERNATIONAL JOURNAL OF ACCOUNTING INFORMATION SYSTEMS,INTERNATIONAL JOURNAL OF ACCOUNTING INFORMATION SYSTEMS,"1,009",4.400,0.00056,"With corporate investment in Big Data of $34 billion in 2013 growing to $232 billion through 2016 (Gartner 2012), the Big 4 accounting firms are aiming to be at the forefront of Big Data implementations. Notably, they see Big Data as an increasingly essential part of their assurance practice. We argue that while there is a place for Big Data in auditing, its application to auditing is less clear than it is in the other fields, such as marketing and medical research. The objectives of this paper are to: (1) provide a discussion of both the inhibitors of incorporating Big Data into financial statement audits; and (3) present a research agenda to identify approaches to ameliorate those inhibitors.",https://doi.org/10.1016/j.accinf.2016.07.004,2016,Michael Alles and Glen L. Gray,INCORPORATING BIG DATA IN AUDITS: IDENTIFYING INHIBITORS AND A RESEARCH AGENDA TO ADDRESS THOSE INHIBITORS,article
402,29806,INTERNATIONAL JOURNAL OF ACCOUNTING INFORMATION SYSTEMS,journal,14670895,"0,897",Q1,53,21,51,1375,290,50,"5,56","65,48",United States,Northern America,Elsevier Inc.,2000-2020,Finance (Q1); Information Systems and Management (Q1); Management Information Systems (Q1); Accounting (Q2),INTERNATIONAL JOURNAL OF ACCOUNTING INFORMATION SYSTEMS,INTERNATIONAL JOURNAL OF ACCOUNTING INFORMATION SYSTEMS,"1,009",4.400,0.00056,"The nature of management accountants' responsibility is evolving from merely reporting aggregated historical value to also including organizational performance measurement and providing management with decision related information. Corporate information systems such as enterprise resource planning (ERP) systems have provided management accountants with both expanded data storage power and enhanced computational power. With big data extracted from both internal and external data sources, management accountants now could utilize data analytics techniques to answer the questions including: what has happened (descriptive analytics), what will happen (predictive analytics), and what is the optimized solution (prescriptive analytics). However, research shows that the nature and scope of managerial accounting has barely changed and that management accountants employ mostly descriptive analytics, some predictive analytics, and a bare minimum of prescriptive analytics. This paper proposes a Managerial Accounting Data Analytics (MADA) framework based on the balanced scorecard theory in a business intelligence context. MADA provides management accountants the ability to utilize comprehensive business analytics to conduct performance measurement and provide decision related information. With MADA, three types of business analytics (descriptive, predictive, and prescriptive) are implemented into four corporate performance measurement perspectives (financial, customer, internal process, and learning and growth) in an enterprise system environment. Other related issues that affect the successful utilization of business analytics within a corporate-wide business intelligence (BI) system, such as data quality and data integrity, are also discussed. This paper contributes to the literature by discussing the impact of business analytics on managerial accounting from an enterprise systems and BI perspective and by providing the Managerial Accounting Data Analytics (MADA) framework that incorporates balanced scorecard methodology.",https://doi.org/10.1016/j.accinf.2017.03.003,2017,Deniz Appelbaum and Alexander Kogan and Miklos Vasarhelyi and Zhaokai Yan,IMPACT OF BUSINESS ANALYTICS AND ENTERPRISE SYSTEMS ON MANAGERIAL ACCOUNTING,article
403,61490,BIOSYSTEMS ENGINEERING,journal,"15375129, 15375110","0,894",Q1,110,236,588,9794,3137,581,"5,07","41,50",United States,Northern America,Academic Press Inc.,2002-2020,Agronomy and Crop Science (Q1); Animal Science and Zoology (Q1); Control and Systems Engineering (Q1); Food Science (Q1); Soil Science (Q1),BIOSYSTEMS ENGINEERING,BIOSYSTEMS ENGINEERING,"9,924",4.123,0.00737,"The Internet of Things is allowing agriculture, here specifically arable farming, to become data-driven, leading to more timely and cost-effective production and management of farms, and at the same time reducing their environmental impact. This review is addressing an analytical survey of the current and potential application of Internet of Things in arable farming, where spatial data, highly varying environments, task diversity and mobile devices pose unique challenges to be overcome compared to other agricultural systems. The review contributes an overview of the state of the art of technologies deployed. It provides an outline of the current and potential applications, and discusses the challenges and possible solutions and implementations. Lastly, it presents some future directions for the Internet of Things in arable farming. Current issues such as smart phones, intelligent management of Wireless Sensor Networks, middleware platforms, integrated Farm Management Information Systems across the supply chain, or autonomous vehicles and robotics stand out because of their potential to lead arable farming to smart arable farming. During the implementation, different challenges are encountered, and here interoperability is a key major hurdle throughout all the layers in the architecture of an Internet of Things system, which can be addressed by shared standards and protocols. Challenges such as affordability, device power consumption, network latency, Big Data analysis, data privacy and security, among others, have been identified by the articles reviewed and are discussed in detail. Different solutions to all identified challenges are presented addressing technologies such as machine learning, middleware platforms, or intelligent data management.",https://doi.org/10.1016/j.biosystemseng.2019.12.013,2020,Andrés Villa-Henriksen and Gareth T.C. Edwards and Liisa A. Pesonen and Ole Green and Claus Aage Grøn Sørensen,"INTERNET OF THINGS IN ARABLE FARMING: IMPLEMENTATION, APPLICATIONS, CHALLENGES AND POTENTIAL",article
404,26617,EUROPEAN JOURNAL OF INTERNAL MEDICINE,journal,"09536205, 18790828","0,894",Q2,71,384,907,8947,1904,557,"2,14","23,30",Netherlands,Western Europe,Elsevier,1989-2020,Internal Medicine (Q2),EUROPEAN JOURNAL OF INTERNAL MEDICINE,EUROPEAN JOURNAL OF INTERNAL MEDICINE,"7,083",4.487,0.00933,"The big data approach offers a powerful alternative to Evidence-based medicine. This approach could guide cancer management thanks to machine learning application to large-scale data. Aim of the Thyroid CoBRA (Consortium for Brachytherapy Data Analysis) project is to develop a standardized web data collection system, focused on thyroid cancer. The Metabolic Radiotherapy Working Group of Italian Association of Radiation Oncology (AIRO) endorsed the implementation of a consortium directed to thyroid cancer management and data collection. The agreement conditions, the ontology of the collected data and the related software services were defined by a multicentre ad hoc working-group (WG). Six Italian cancer centres were firstly started the project, defined and signed the Thyroid COBRA consortium agreement. Three data set tiers were identified: Registry, Procedures and Research. The COBRA-Storage System (C-SS) appeared to be not time-consuming and to be privacy respecting, as data can be extracted directly from the single centre's storage platforms through a secured connection that ensures reliable encryption of sensible data. Automatic data archiving could be directly performed from Image Hospital Storage System or the Radiotherapy Treatment Planning Systems. The C-SS architecture will allow “Cloud storage way” or “distributed learning” approaches for predictive model definition and further clinical decision support tools development. The development of the Thyroid COBRA data Storage System C-SS through a multicentre consortium approach appeared to be a feasible tool in the setup of complex and privacy saving data sharing system oriented to the management of thyroid cancer and in the near future every cancer type.",https://doi.org/10.1016/j.ejim.2018.02.012,2018,Luca Tagliaferri and Carlo Gobitti and Giuseppe Ferdinando Colloca and Luca Boldrini and Eleonora Farina and Carlo Furlan and Fabiola Paiar and Federica Vianello and Michela Basso and Lorenzo Cerizza and Fabio Monari and Gabriele Simontacchi and Maria Antonietta Gambacorta and Jacopo Lenkowicz and Nicola Dinapoli and Vito Lanzotti and Renzo Mazzarotto and Elvio Russi and Monica Mangoni,A NEW STANDARDIZED DATA COLLECTION SYSTEM FOR INTERDISCIPLINARY THYROID CANCER MANAGEMENT: THYROID COBRA,article
405,26617,EUROPEAN JOURNAL OF INTERNAL MEDICINE,journal,"09536205, 18790828","0,894",Q2,71,384,907,8947,1904,557,"2,14","23,30",Netherlands,Western Europe,Elsevier,1989-2020,Internal Medicine (Q2),EUROPEAN JOURNAL OF INTERNAL MEDICINE,EUROPEAN JOURNAL OF INTERNAL MEDICINE,"7,083",4.487,0.00933,,https://doi.org/10.1016/j.ejim.2018.05.019,2018,Andrea Damiani and Graziano Onder and Vincenzo Valentini,LARGE DATABASES (BIG DATA) AND EVIDENCE-BASED MEDICINE,article
406,17957,COMPUTERS IN BIOLOGY AND MEDICINE,journal,"00104825, 18790534","0,884",Q1,94,383,936,19977,5223,923,"5,59","52,16",United Kingdom,Western Europe,Elsevier Ltd.,1970-2020,Computer Science Applications (Q1); Health Informatics (Q2),COMPUTERS IN BIOLOGY AND MEDICINE,COMPUTERS IN BIOLOGY AND MEDICINE,"9,751",4.589,0.01186,"Background
Clinical notes are ubiquitous resources offering potential value in optimizing critical care via data mining technologies.
Objective
To determine the predictive value of clinical notes as prognostic markers of 1-year all-cause mortality among people with diabetes following critical care.
Materials and methods
Mortality of diabetes patients were predicted using three cohorts of clinical text in a critical care database, written by physicians (n = 45253), nurses (159027), and both (n = 204280). Natural language processing was used to pre-process text documents and LASSO-regularized logistic regression models were trained and tested. Confusion matrix metrics of each model were calculated and AUROC estimates between models were compared. All predictive words and corresponding coefficients were extracted. Outcome probability associated with each text document was estimated.
Results
Models built on clinical text of physicians, nurses, and the combined cohort predicted mortality with AUROC of 0.996, 0.893, and 0.922, respectively. Predictive performance of the models significantly differed from one another whereas inter-rater reliability ranged from substantial to almost perfect across them. Number of predictive words with non-zero coefficients were 3994, 8159, and 10579, respectively, in the models of physicians, nurses, and the combined cohort. Physicians’ and nursing notes, both individually and when combined, strongly predicted 1-year all-cause mortality among people with diabetes following critical care.
Conclusion
Clinical notes of physicians and nurses are strong and novel prognostic markers of diabetes-associated mortality in critical care, offering potentially generalizable and scalable applications. Clinical text-derived personalized risk estimates of prognostic outcomes such as mortality could be used to optimize patient care.",https://doi.org/10.1016/j.compbiomed.2021.104305,2021,Kushan {De Silva} and Noel Mathews and Helena Teede and Andrew Forbes and Daniel Jönsson and Ryan T. Demmer and Joanne Enticott,CLINICAL NOTES AS PROGNOSTIC MARKERS OF MORTALITY ASSOCIATED WITH DIABETES MELLITUS FOLLOWING CRITICAL CARE: A RETROSPECTIVE COHORT ANALYSIS USING MACHINE LEARNING AND UNSTRUCTURED BIG DATA,article
407,17957,COMPUTERS IN BIOLOGY AND MEDICINE,journal,"00104825, 18790534","0,884",Q1,94,383,936,19977,5223,923,"5,59","52,16",United Kingdom,Western Europe,Elsevier Ltd.,1970-2020,Computer Science Applications (Q1); Health Informatics (Q2),COMPUTERS IN BIOLOGY AND MEDICINE,COMPUTERS IN BIOLOGY AND MEDICINE,"9,751",4.589,0.01186,"Background
Building cancer risk models from real-world data requires overcoming challenges in data preprocessing, efficient representation, and computational performance. We present a case study of a cloud-based approach to learning from de-identified electronic health record data and demonstrate its effectiveness for melanoma risk prediction.
Methods
We used a hybrid distributed and non-distributed approach to computing in the cloud: distributed processing with Apache Spark for data preprocessing and labeling, and non-distributed processing for machine learning model training with scikit-learn. Moreover, we explored the effects of sampling the training dataset to improve computational performance. Risk factors were evaluated using regression weights as well as tree SHAP values.
Results
Among 4,061,172 patients who did not have melanoma through the 2016 calendar year, 10,129 were diagnosed with melanoma within one year. A gradient-boosted classifier achieved the best predictive performance with cross-validation (AUC = 0.799, Sensitivity = 0.753, Specificity = 0.688). Compared to a model built on the original data, a dataset two orders of magnitude smaller could achieve statistically similar or better performance with less than 1% of the training time and cost.
Conclusions
We produced a model that can effectively predict melanoma risk for a diverse dermatology population in the U.S. by using hybrid computing infrastructure and data sampling. For this de-identified clinical dataset, sampling approaches significantly shortened the time for model building while retaining predictive accuracy, allowing for more rapid machine learning model experimentation on familiar computing machinery. A large number of risk factors (>300) were required to produce the best model.",https://doi.org/10.1016/j.compbiomed.2019.04.039,2019,Aaron N. Richter and Taghi M. Khoshgoftaar,EFFICIENT LEARNING FROM BIG DATA FOR CANCER RISK MODELING: A CASE STUDY WITH MELANOMA,article
408,16044,ELECTRIC POWER SYSTEMS RESEARCH,journal,03787796,"0,845",Q1,122,643,1165,20949,5369,1158,"4,25","32,58",Netherlands,Western Europe,Elsevier BV,1977-2021,Electrical and Electronic Engineering (Q1); Energy Engineering and Power Technology (Q1),ELECTRIC POWER SYSTEMS RESEARCH,ELECTRIC POWER SYSTEMS RESEARCH,"13,115",3.414,0.01287,"This paper provides a survey of big data analytics applications and associated implementation issues. The emphasis is placed on applications that are novel and have demonstrated value to the industry, as illustrated using field data and practical applications. The paper reflects on the lessons learned from initial implementations, as well as ideas that are yet to be explored. The various data science trends treated in the literature are outlined, while experiences from applying them in the electricity grid setting are emphasized to pave the way for future applications. The paper ends with opportunities and challenges, as well as implementation goals and strategies for achieving impactful outcomes.",https://doi.org/10.1016/j.epsr.2020.106788,2020,Mladen Kezunovic and Pierre Pinson and Zoran Obradovic and Santiago Grijalva and Tao Hong and Ricardo Bessa,BIG DATA ANALYTICS FOR FUTURE ELECTRICITY GRIDS,article
409,20870,TELECOMMUNICATIONS POLICY,journal,03085961,"0,840",Q1,69,87,224,5352,831,212,"3,51","61,52",United Kingdom,Western Europe,Elsevier Ltd.,1976-2020,Electrical and Electronic Engineering (Q1); Human Factors and Ergonomics (Q1); Information Systems (Q1),TELECOMMUNICATIONS POLICY,TELECOMMUNICATIONS POLICY,"2,745",3.036,0.00269,"This study seeks to understand big data ecology, how it is perceived by different stakeholders, the potential value and challenges, and the implications for the private sector and public organizations, as well as for policy makers. With Normalization Process Theory in place, this study conducts socio-technical evaluation on the big data phenomenon to understand the developmental processes through which new practices of thinking and enacting are implemented, embedded, and integrated in South Korea. It also undertakes empirical analyses of user modeling to explore the factors influencing users׳ adoption of big data by integrating cognitive motivations as well as user values as the primary determining factors. Based on the qualitative and quantitative findings, this study concludes that big data should be developed with user-centered ideas and that users should be the focus of big data design.",https://doi.org/10.1016/j.telpol.2015.03.007,2016,Dong-Hee Shin,DEMYSTIFYING BIG DATA: ANATOMY OF BIG DATA DEVELOPMENTAL PROCESS,article
410,17697,PUBLIC HEALTH,journal,"00333506, 14765616","0,826",Q2,75,433,874,11891,2120,788,"2,10","27,46",Netherlands,Western Europe,Elsevier,"1888-1913, 1915-2020","Medicine (miscellaneous) (Q2); Public Health, Environmental and Occupational Health (Q2)",PUBLIC HEALTH,PUBLIC HEALTH,"7,603",2.427,0.01222,,https://doi.org/10.1016/j.puhe.2015.02.013,2015,P. Mackie and F. Sim and C. Johnman,BIG DATA! BIG DEAL?,article
411,18676,TECHNOLOGY IN SOCIETY,journal,0160791X,"0,819",Q1,51,201,224,14468,1056,222,"4,75","71,98",United Kingdom,Western Europe,Elsevier Ltd.,1979-2020,Business and International Management (Q1); Education (Q1); Human Factors and Ergonomics (Q1); Sociology and Political Science (Q1),TECHNOLOGY IN SOCIETY,TECHNOLOGY IN SOCIETY,"2,735",4.192,0.00214,"Due to the importance of big data technology in decision-making, production and service provision, enterprises have adopted various big data technologies and platforms to improve their operational efficiency. However, the number of enterprises that have adopted big data is not promising. The purpose of this study is to explore the current status of big data adoption by Chinese enterprises and to reveal the possible factors that hinder big data adoption from the group behaviour network perspective. Based on a real case survey of 54 big data platforms (BDPs), four types of networks—i.e., the enterprise-platform network, enterprise network, platform network and industry similarity and difference (ISD) network—are constructed and analysed on the basis of social network analysis (SNA). This study finds that among Chinese enterprises, the level and scope of big data adoption are generally low and are imbalanced among industries; the cognitive level and adoption behaviour of enterprises on BDPs are inconsistent, the compatibility of BDPs is different, and the density and distance-based cohesion of networks are weak; although the current big data adoption behaviours of Chinese enterprises have formed some structural features, core-periphery structures and maximal complete cliques are found, and the current network structure has little impact on individual enterprises and platforms; enterprises in the same industry prefer to adopt the same kind of big data technology or platform. Based on these findings, several strategies and suggestions to improve big data adoption are provided.",https://doi.org/10.1016/j.techsoc.2021.101570,2021,Zhimei Lei and Yandan Chen and Ming K. Lim,MODELLING AND ANALYSIS OF BIG DATA PLATFORM GROUP ADOPTION BEHAVIOUR BASED ON SOCIAL NETWORK ANALYSIS,article
412,18676,TECHNOLOGY IN SOCIETY,journal,0160791X,"0,819",Q1,51,201,224,14468,1056,222,"4,75","71,98",United Kingdom,Western Europe,Elsevier Ltd.,1979-2020,Business and International Management (Q1); Education (Q1); Human Factors and Ergonomics (Q1); Sociology and Political Science (Q1),TECHNOLOGY IN SOCIETY,TECHNOLOGY IN SOCIETY,"2,735",4.192,0.00214,"With its rapid growth and increasing adoption, big data is producing a substantial impact in society. Its usage is opening both opportunities such as new business models and economic gains and risks such as privacy violations and discrimination. Europe is in need of a comprehensive strategy to optimise the use of data for a societal benefit and increase the innovation and competitiveness of its productive activities. In this paper, we contribute to the definition of this strategy with a research roadmap to capture the economic, social and ethical, legal and political benefits associated with the use of big data in Europe. The present roadmap considers the positive and negative externalities associated with big data, maps research and innovation topics in the areas of data management, processing, analytics, protection, visualisation, as well as non-technical topics, to the externalities they can tackle, and provides a time frame to address these topics in order to deliver social impact, skills development and standardisation. Finally, it also identifies what sectors will be most benefited by each of the research efforts. The goal of the roadmap is to guide European research efforts to develop a socially responsible big data economy, and to allow stakeholders to identify and meet big data challenges and proceed with a shared understanding of the societal impact, positive and negative externalities and concrete problems worth investigating in future programmes.",https://doi.org/10.1016/j.techsoc.2018.03.005,2018,Martí Cuquet and Anna Fensel,THE SOCIETAL IMPACT OF BIG DATA: A RESEARCH ROADMAP FOR EUROPE,article
413,24482,CHILDREN AND YOUTH SERVICES REVIEW,journal,01907409,"0,816",Q1,89,978,1349,58646,3628,1315,"2,29","59,97",United Kingdom,Western Europe,Elsevier Ltd.,1979-2020,Education (Q1); Social Work (Q1); Sociology and Political Science (Q1); Developmental and Educational Psychology (Q2),CHILDREN AND YOUTH SERVICES REVIEW,CHILDREN AND YOUTH SERVICES REVIEW,"13,303",2.393,0.01343,"Numerous approaches are available for improving governance of the child welfare system, all of which require longitudinal data reporting on child welfare clients. A substantial amount of agency administrative information – big data – can be transformed into knowledge for policy and management actions through a rigorous information generation process. Important properties of the information generation process are that it must generate accurate, timely information while protecting the confidentiality of the clients. In addition, it must be extensible to serve an ever-changing policy and technology environment. Knowledge discovery and data mining (KDD), aka data science, is a method developed in the private sector to mine consumer data and can be used in public settings to support evidence based governance. KDD consists of a rigorous 5-step process that includes a Web-based end-user interface. The relationship between KDD and governance is a continuous feedback cycle that enables ongoing development of new information and knowledge as stakeholders identify emerging needs. In this paper, we synthesis the different frameworks for utilizing big data for public governance, introduce the KDD process, describe the nature of big data in child welfare, and then present an updated KDD architecture that can support these frameworks to utilize big data for governance. We also demonstrate the role KDD plays in child welfare management through 2 case studies. We conclude with a discussion on implications for agency–university partnerships and research-to-practice.",https://doi.org/10.1016/j.childyouth.2015.09.014,2015,Hye-Chung Kum and C. {Joy Stewart} and Roderick A. Rose and Dean F. Duncan,USING BIG DATA FOR EVIDENCE BASED GOVERNANCE IN CHILD WELFARE,article
414,18838,PREVENTIVE VETERINARY MEDICINE,journal,"01675877, 18731716","0,816",Q1,95,299,618,12546,1722,609,"2,50","41,96",Netherlands,Western Europe,Elsevier,1982-2020,Animal Science and Zoology (Q1); Food Animals (Q1),PREVENTIVE VETERINARY MEDICINE,PREVENTIVE VETERINARY MEDICINE,"9,469",2.670,0.00645,"Concurrent with global economic development in the last 50 years, the opportunities for the spread of existing diseases and emergence of new infectious pathogens, have increased substantially. The activities associated with the enormously intensified global connectivity have resulted in large amounts of data being generated, which in turn provides opportunities for generating knowledge that will allow more effective management of animal and human health risks. This so-called Big Data has, more recently, been accompanied by the Internet of Things which highlights the increasing presence of a wide range of sensors, interconnected via the Internet. Analysis of this data needs to exploit its complexity, accommodate variation in data quality and should take advantage of its spatial and temporal dimensions, where available. Apart from the development of hardware technologies and networking/communication infrastructure, it is necessary to develop appropriate data management tools that make this data accessible for analysis. This includes relational databases, geographical information systems and most recently, cloud-based data storage such as Hadoop distributed file systems. While the development in analytical methodologies has not quite caught up with the data deluge, important advances have been made in a number of areas, including spatial and temporal data analysis where the spectrum of analytical methods ranges from visualisation and exploratory analysis, to modelling. While there used to be a primary focus on statistical science in terms of methodological development for data analysis, the newly emerged discipline of data science is a reflection of the challenges presented by the need to integrate diverse data sources and exploit them using novel data- and knowledge-driven modelling methods while simultaneously recognising the value of quantitative as well as qualitative analytical approaches. Machine learning regression methods, which are more robust and can handle large datasets faster than classical regression approaches, are now also used to analyse spatial and spatio-temporal data. Multi-criteria decision analysis methods have gained greater acceptance, due in part, to the need to increasingly combine data from diverse sources including published scientific information and expert opinion in an attempt to fill important knowledge gaps. The opportunities for more effective prevention, detection and control of animal health threats arising from these developments are immense, but not without risks given the different types, and much higher frequency, of biases associated with these data.",https://doi.org/10.1016/j.prevetmed.2015.05.012,2015,Dirk U. Pfeiffer and Kim B. Stevens,SPATIAL AND TEMPORAL EPIDEMIOLOGICAL ANALYSIS IN THE BIG DATA ERA,article
415,26811,COMPUTER NETWORKS,journal,13891286,"0,798",Q1,135,383,896,19762,4981,877,"5,93","51,60",Netherlands,Western Europe,Elsevier,"1977-1984, 1989-1990, 1996-2020",Computer Networks and Communications (Q1),COMPUTER NETWORKS,COMPUTER NETWORKS,"11,644",4.474,0.00957,"Data are an extremely important asset. Governments around the world encourage big data sharing and trading to promote the big data economy. However, existing data trading platforms are not fully trusted. Such platforms face the problems of a single point of failure (SPOF), opaque transactions, uncontrollability, untraceability, and issues of data privacy. Several blockchain-based big data trading methods have been proposed; however, they do not adequately address the security issues introduced by dishonesty in the data provider and data agent or the fairness of data revenue distribution and price bargaining. In this paper, we propose a blockchain-based decentralized data trading system in which data trading is completed by smart contract-based data matching, price negotiation, and reward assigning. Moreover, the proposed data trading system evaluates the data quality on the basis of three metrics, records the evaluation results in a side-chain, and distributes the data users’ application revenue to the data provider according to the evaluated data quality. We verify the security, usability, and efficiency of the proposed big data trading system.",https://doi.org/10.1016/j.comnet.2021.107994,2021,Donghui Hu and Yifan Li and Lixuan Pan and Meng Li and Shuli Zheng,A BLOCKCHAIN-BASED TRADING SYSTEM FOR BIG DATA,article
416,26811,COMPUTER NETWORKS,journal,13891286,"0,798",Q1,135,383,896,19762,4981,877,"5,93","51,60",Netherlands,Western Europe,Elsevier,"1977-1984, 1989-1990, 1996-2020",Computer Networks and Communications (Q1),COMPUTER NETWORKS,COMPUTER NETWORKS,"11,644",4.474,0.00957,"The explosive growth in the number of devices connected to the Internet of Things (IoT) and the exponential increase in data consumption only reflect how the growth of big data perfectly overlaps with that of IoT. The management of big data in a continuously expanding network gives rise to non-trivial concerns regarding data collection efficiency, data processing, analytics, and security. To address these concerns, researchers have examined the challenges associated with the successful deployment of IoT. Despite the large number of studies on big data, analytics, and IoT, the convergence of these areas creates several opportunities for flourishing big data and analytics for IoT systems. In this paper, we explore the recent advances in big data analytics for IoT systems as well as the key requirements for managing big data and for enabling analytics in an IoT environment. We taxonomized the literature based on important parameters. We identify the opportunities resulting from the convergence of big data, analytics, and IoT as well as discuss the role of big data analytics in IoT applications. Finally, several open challenges are presented as future research directions.",https://doi.org/10.1016/j.comnet.2017.06.013,2017,Ejaz Ahmed and Ibrar Yaqoob and Ibrahim Abaker Targio Hashem and Imran Khan and Abdelmuttlib Ibrahim Abdalla Ahmed and Muhammad Imran and Athanasios V. Vasilakos,THE ROLE OF BIG DATA ANALYTICS IN INTERNET OF THINGS,article
417,26811,COMPUTER NETWORKS,journal,13891286,"0,798",Q1,135,383,896,19762,4981,877,"5,93","51,60",Netherlands,Western Europe,Elsevier,"1977-1984, 1989-1990, 1996-2020",Computer Networks and Communications (Q1),COMPUTER NETWORKS,COMPUTER NETWORKS,"11,644",4.474,0.00957,"According to McKinsey & Company, about a third of food produced is lost or wasted every year, amounting to a $940 billion economic hit. Inefficiencies in planting, harvesting, water use, reduced animal contributions, as well as uncertainty about weather, pests, consumer demand and other intangibles contribute to the loss. Precision Agriculture (PA) and Precision Livestock Farming (PLF) come to assist in optimizing agricultural and livestock production and minimizing the wastes and costs aforementioned. PA is a technology-enabled, data-driven approach to farming management that observes, measures, and analyzes the needs of individual fields and crops. PLF is also a technology-enabled, data-driven approach to livestock production management, which exploits technology to quantitatively measure the behavior, health and performance of animals. Big data delivered by a plethora of data sources related to these domains, has a multitude of payoffs including precision monitoring of fertilizer and fungicide levels to optimize crop yields, risk mitigation that results from monitoring when temperature and humidity levels reach dangerous levels for crops, increasing livestock production while minimizing the environmental footprint of livestock farming, ensuring high levels of welfare and health for animals, and more. By adding analytics to these sensor and image data, opportunities also exist to further optimize PA and PLF by having continuous data on how a field or the livestock is responding to a protocol. For these domains, two main challenges exist: 1) to exploit this multitude of data facilitating dedicated improvements in performance, and 2) to make available advanced infrastructure so as to harness the power of this information in order to benefit from the new insights, practices and products, efficiently time-wise, lowering responsiveness down to seconds so as to cater for time-critical decisions. The current paper aims to introduce CYBELE, a platform aspiring to safeguard that the stakeholders involved in the agri-food value chain (research community, SMEs, entrepreneurs, etc.) have integrated, unmediated access to a vast amount of very large scale datasets of diverse types and coming from a variety of sources, and that they are capable of actually generating value and extracting insights out of these data, by providing secure and unmediated access to large-scale High Performance Computing (HPC) infrastructures supporting advanced data discovery, processing, combination and visualization services, solving computationally-intensive challenges modelled as mathematical algorithms requiring very high computing power and capability.",https://doi.org/10.1016/j.comnet.2019.107035,2020,Konstantinos Perakis and Fenareti Lampathaki and Konstantinos Nikas and Yiannis Georgiou and Oskar Marko and Jarissa Maselyne,CYBELE – FOSTERING PRECISION AGRICULTURE & LIVESTOCK FARMING THROUGH SECURE ACCESS TO LARGE-SCALE HPC ENABLED VIRTUAL INDUSTRIAL EXPERIMENTATION ENVIRONMENTS FOSTERING SCALABLE BIG DATA ANALYTICS,article
418,26811,COMPUTER NETWORKS,journal,13891286,"0,798",Q1,135,383,896,19762,4981,877,"5,93","51,60",Netherlands,Western Europe,Elsevier,"1977-1984, 1989-1990, 1996-2020",Computer Networks and Communications (Q1),COMPUTER NETWORKS,COMPUTER NETWORKS,"11,644",4.474,0.00957,"In recent years, the Internet of Things (IoT) has emerged as a new opportunity. Thus, all devices such as smartphones, transportation facilities, public services, and home appliances are used as data creator devices. All the electronic devices around us help our daily life. Devices such as wrist watches, emergency alarms, and garage doors and home appliances such as refrigerators, microwaves, air conditioning, and water heaters are connected to an IoT network and controlled remotely. Methods such as big data and data mining can be used to improve the efficiency of IoT and storage challenges of a large data volume and the transmission, analysis, and processing of the data volume on the IoT. The aim of this study is to investigate the research done on IoT using big data as well as data mining methods to identify subjects that must be emphasized more in current and future research paths. This article tries to achieve the goal by following the conference and journal articles published on IoT-big data and also IoT-data mining areas between 2010 and August 2017. In order to examine these articles, the combination of Systematic Mapping and literature review was used to create an intended review article. In this research, 44 articles were studied. These articles are divided into three categories: Architecture & Platform, framework, and application. In this research, a summary of the methods used in the area of IoT-big data and IoT-data mining is presented in three categories to provide a starting point for researchers in the future.",https://doi.org/10.1016/j.comnet.2018.04.001,2018,Shabnam Shadroo and Amir Masoud Rahmani,SYSTEMATIC SURVEY OF BIG DATA AND DATA MINING IN INTERNET OF THINGS,article
419,21728,JOURNAL OF SURGICAL RESEARCH,journal,"10958673, 00224804","0,780",Q2,108,688,1666,19607,3583,1614,"1,96","28,50",United States,Northern America,Academic Press Inc.,1961-2021,Surgery (Q2),JOURNAL OF SURGICAL RESEARCH,JOURNAL OF SURGICAL RESEARCH,"17,062",2.192,0.01875,"As more and more health systems have converted to the use of electronic health records, the amount of searchable and analyzable data is exploding. This includes not just provider or laboratory created data but also data collected by instruments, personal devices, and patients themselves, among others. This has led to more attention being paid to the analysis of these data to answer previously unaddressed questions. This is especially important given the number of therapies previously found to be beneficial in clinical trials that are currently being re-scrutinized. Because there are orders of magnitude more information contained in these data sets, a fundamentally different approach needs to be taken to their processing and analysis and the generation of knowledge. Health care and medicine are drivers of this phenomenon and will ultimately be the main beneficiaries. Concurrently, many different types of questions can now be asked using these data sets. Research groups have become increasingly active in mining large data sets, including nationwide health care databases, to learn about associations of medication use and various unrelated diseases such as cancer. Given the recent increase in research activity in this area, its promise to radically change clinical research, and the relative lack of widespread knowledge about its potential and advances, we surveyed the available literature to understand the strengths and limitations of these new tools. We also outline new databases and techniques that are available to researchers worldwide, with special focus on work pertaining to the broad and rapid monitoring of drug safety and secondary effects.",https://doi.org/10.1016/j.jss.2019.09.053,2020,Ali Zarrinpar and Ting-Yuan {David Cheng} and Zhiguang Huo,WHAT CAN WE LEARN ABOUT DRUG SAFETY AND OTHER EFFECTS IN THE ERA OF ELECTRONIC HEALTH RECORDS AND BIG DATA THAT WE WOULD NOT BE ABLE TO LEARN FROM CLASSIC EPIDEMIOLOGY?,article
420,21100780696,BIG DATA,journal,"21676461, 2167647X","0,774",Q1,27,50,86,1346,555,71,"2,55","26,92",United States,Northern America,Mary Ann Liebert Inc.,2013-2020,Information Systems (Q1); Computer Science Applications (Q2); Information Systems and Management (Q2),BIG DATA,BIG DATA,606,2.128,0.0017,,10.1145/2614512.2629588,2014,Lukesh Susan S. ,BIG DATA,inproceedings
421,21100780696,BIG DATA,journal,"21676461, 2167647X","0,774",Q1,27,50,86,1346,555,71,"2,55","26,92",United States,Northern America,Mary Ann Liebert Inc.,2013-2020,Information Systems (Q1); Computer Science Applications (Q2); Information Systems and Management (Q2),BIG DATA,BIG DATA,606,2.128,0.0017,,10.1145/3079064,2017,CACM Staff ,BIG DATA,inproceedings
422,3100147401,ECOLOGICAL INFORMATICS,journal,15749541,"0,774",Q1,55,109,273,5588,964,271,"3,43","51,27",Netherlands,Western Europe,Elsevier,2006-2020,"Modeling and Simulation (Q1); Applied Mathematics (Q2); Computational Theory and Mathematics (Q2); Computer Science Applications (Q2); Ecological Modeling (Q2); Ecology (Q2); Ecology, Evolution, Behavior and Systematics (Q2)",ECOLOGICAL INFORMATICS,ECOLOGICAL INFORMATICS,"2,893",3.142,0.00332,"The recent availability of species occurrence data from numerous sources, standardized and connected within a single portal, has the potential to answer fundamental ecological questions. These aggregated big biodiversity databases are prone to numerous data errors and biases. The data-user is responsible for identifying these errors and assessing if the data are suitable for a given purpose. Complex technical skills are increasingly required for handling and cleaning biodiversity data, while biodiversity scientists possessing these skills are rare. Here, we estimate the effect of user-level data cleaning on species distribution model (SDM) performance. We implement several simple and easy-to-execute data cleaning procedures, and evaluate the change in SDM performance. Additionally, we examine if a certain group of species is more sensitive to the use of erroneous or unsuitable data. The cleaning procedures used in this research improved SDM performance significantly, across all scales and for all performance measures. The largest improvement in distribution models following data cleaning was for small mammals (1g–100g). Data cleaning at the user level is crucial when using aggregated occurrence data, and facilitating its implementation is a key factor in order to advance data-intensive biodiversity studies. Adopting a more comprehensive approach for incorporating data cleaning as part of data analysis, will not only improve the quality of biodiversity data, but will also impose a more appropriate usage of such data.",https://doi.org/10.1016/j.ecoinf.2016.06.001,2016,Tomer Gueta and Yohay Carmel,QUANTIFYING THE VALUE OF USER-LEVEL DATA CLEANING FOR BIG DATA: A CASE STUDY USING MAMMAL DISTRIBUTION MODELS,article
423,3100147401,ECOLOGICAL INFORMATICS,journal,15749541,"0,774",Q1,55,109,273,5588,964,271,"3,43","51,27",Netherlands,Western Europe,Elsevier,2006-2020,"Modeling and Simulation (Q1); Applied Mathematics (Q2); Computational Theory and Mathematics (Q2); Computer Science Applications (Q2); Ecological Modeling (Q2); Ecology (Q2); Ecology, Evolution, Behavior and Systematics (Q2)",ECOLOGICAL INFORMATICS,ECOLOGICAL INFORMATICS,"2,893",3.142,0.00332,"Over recent years, the frequency and intensity of droughts have increased and there has been a large drying trend over many parts of the world. Consequently, drought monitoring using big data analytic has gained an explosive interest. Droughts stand among the most damaging natural disasters. It threatens agricultural production, ecological environment, and socio-economic development. For this reason, early warning, accurate evaluation, and efficient prediction are an emergency especially for the nations that are the most menaced by this danger. There are numerous emerging studies addressing big data and its applications in drought monitoring. In fact, big data handle data heterogeneity which is an additive value for the prediction of drought, it offers a view of the different dimensions such as the spatial distribution, the temporal distribution and the severity detection of this phenomenon. Big data analytic and drought are introduced and reviewed in this paper. Besides, this review includes different studies, researches and applications of big data to drought monitoring. Challenges related to data life cycle such as data challenges, data processing challenges and data infrastructure management challenges are also discussed. Finally, we conclude that big data analytic can be beneficial in drought monitoring but there is a need for statistical and artificial intelligence-based approaches.",https://doi.org/10.1016/j.ecoinf.2020.101136,2020,Hanen Balti and Ali {Ben Abbes} and Nedra Mellouli and Imed Riadh Farah and Yanfang Sang and Myriam Lamolle,"A REVIEW OF DROUGHT MONITORING WITH BIG DATA: ISSUES, METHODS, CHALLENGES AND RESEARCH DIRECTIONS",article
424,19700171401,WORLD NEUROSURGERY,journal,"18788769, 18788750","0,734",Q2,95,2675,7491,64788,14126,6927,"1,78","24,22",United States,Northern America,Elsevier Inc.,2010-2020,Neurology (clinical) (Q2); Surgery (Q2),WORLD NEUROSURGERY,WORLD NEUROSURGERY,"23,506",2.104,0.04375,"Objective
The National Inpatient Sample (NIS) (the largest all-payer inpatient database in the United States) is an important instrument for big data analysis of neurosurgical inquiries. However, earlier research has determined that many NIS studies are limited by common methodological pitfalls. In this study, we provide the first primer of NIS methodological procedures in the setting of neurosurgical research and review all reported neurosurgical studies using the NIS.
Methods
We designed a protocol for neurosurgical big data research using the NIS, based on our subject matter expertise, NIS documentation, and input and verification from the Healthcare Cost and Utilization Project. We subsequently used a comprehensive search strategy to identify all neurosurgical studies using the NIS in the PubMed and MEDLINE, Embase, and Web of Science databases from inception to August 2021. Studies underwent qualitative categorization (years of NIS studied, neurosurgical subspecialty, age group, and thematic focus of study objective) and analysis of longitudinal trends.
Results
We identified a canonical, 4-step protocol for NIS analysis: study population selection; defining additional clinical variables; identification and coding of outcomes; and statistical analysis. Methodological nuances discussed include identifying neurosurgery-specific admissions, addressing missing data, calculating additional severity and hospital-specific metrics, coding perioperative complications, and applying survey weights to make nationwide estimates. Inherent database limitations and common pitfalls of NIS studies discussed include lack of disease process–specific variables and data after the index admission, inability to calculate certain hospital-specific variables after 2011, performing state-level analyses, conflating hospitalization charges and costs, and not following proper statistical methodology for performing survey-weighted regression. In a systematic review, we identified 647 neurosurgical studies using the NIS. Although almost 60% of studies were reported after 2015, <10% of studies analyzed NIS data after 2015. The average sample size of studies was 507,352 patients (standard deviation = 2,739,900). Most studies analyzed cranial procedures (58.1%) and adults (68.1%). The most prevalent topic areas analyzed were surgical outcome trends (35.7%) and health policy and economics (17.8%), whereas patient disparities (9.4%) and surgeon or hospital volume (6.6%) were the least studied.
Conclusions
We present a standardized methodology to analyze the NIS, systematically review the state of the NIS neurosurgical literature, suggest potential future directions for neurosurgical big data inquiries, and outline recommendations to improve the design of future neurosurgical data instruments.",https://doi.org/10.1016/j.wneu.2022.02.113,2022,Oliver Y. Tang and Alisa Pugacheva and Ankush I. Bajaj and Krissia M. {Rivera Perla} and Robert J. Weil and Steven A. Toms,THE NATIONAL INPATIENT SAMPLE: A PRIMER FOR NEUROSURGICAL BIG DATA RESEARCH AND SYSTEMATIC REVIEW,article
425,21100836194,ICT EXPRESS,journal,24059595,"0,733",Q1,22,85,141,1553,813,140,"6,30","18,27",South Korea,Asiatic Region,Korean Institute of Communications Information Sciences,2015-2020,Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Information Systems (Q1); Software (Q1); Artificial Intelligence (Q2),ICT EXPRESS,ICT EXPRESS,789,4.317,0.0014,"This work assesses the quality of Internet of Things data not only as an intrinsic quality on how well it represents the related phenomenon but also, on how much information it contains to educate an artificial entity. The quality metrics here proposed are tested with real datasets. Also, they are implemented on OpenCPU, so the open data repositories can use them off-the-shelf to rate their datasets without computational cost and minimum human intervention, making them more attractive to potential users and gaining visibility and impact.",https://doi.org/10.1016/j.icte.2022.06.001,2022,Aurora González-Vidal and Alfonso P. Ramallo-González and Antonio F. Skarmeta,INTRINSIC AND EXTRINSIC QUALITY OF DATA FOR OPEN DATA REPOSITORIES,article
426,19700182731,EGYPTIAN INFORMATICS JOURNAL,journal,11108665,"0,728",Q1,34,47,59,1655,415,59,"6,94","35,21",Egypt,Africa/Middle East,"Faculty of Computers and Information, Cairo University",2010-2020,Information Systems (Q1); Computer Science Applications (Q2); Management Science and Operations Research (Q2),EGYPTIAN INFORMATICS JOURNAL,EGYPTIAN INFORMATICS JOURNAL,820,3.943,0.00093,"Internet of Things (IoT) is a fundamental concept of a new technology that will be promising and significant in various fields. IoT is a vision that allows things or objects equipped with sensors, actuators, and processors to talk and communicate with each other over the internet to achieve a meaningful goal. Unfortunately, one of the major challenges that affect IoT is data quality and uncertainty, as data volume increases noise, inconsistency and redundancy increases within data and causes paramount issues for IoT technologies. And since IoT is considered to be a massive quantity of heterogeneous networked embedded devices that generate big data, then it is very complex to compute and analyze such massive data. So this paper introduces a new model named NRDD-DBSCAN based on DBSCAN algorithm and using resilient distributed datasets (RDDs) to detect outliers that affect the data quality of IoT technologies. NRDD-DBSCAN has been applied on three different datasets of N-dimensions (2-D, 3-D, and 25-D) and the results were promising. Finally, comparisons have been made between NRDD-DBSCAN and previous models such as RDD-DBSCAN model and DBSCAN algorithm, and these comparisons proved that NRDD-DBSCAN solved the low dimensionality issue of RDD-DBSCAN model and also solved the fact that DBSCAN algorithm cannot handle IoT data. So the conclusion is that NRDD-DBSCAN proposed model can detect the outliers that exist in the datasets of N-dimensions by using resilient distributed datasets (RDDs), and NRDD-DBSCAN can enhance the quality of data exists in IoT applications and technologies.",https://doi.org/10.1016/j.eij.2019.12.001,2020,Haitham Ghallab and Hanan Fahmy and Mona Nasr,DETECTION OUTLIERS ON INTERNET OF THINGS USING BIG DATA TECHNOLOGY,article
427,27952,JOURNAL OF THE FORMOSAN MEDICAL ASSOCIATION,journal,"09296646, 18760821","0,708",Q2,54,340,615,9529,1377,512,"2,25","28,03",China,Asiatic Region,Excerpta Medica Asia Ltd.,"1961-1962, 1972-2020",Medicine (miscellaneous) (Q2),JOURNAL OF THE FORMOSAN MEDICAL ASSOCIATION,JOURNAL OF THE FORMOSAN MEDICAL ASSOCIATION,"5,341",3.282,0.00513,"Background
The need is growing to create medical big data based on the electronic health records collected from different hospitals. Errors for sure occur and how to correct them should be explored.
Methods
Electronic health records of 9,197,817 patients and 53,081,148 visits, totaling about 500 million records for 2006–2016, were transmitted from eight hospitals into an integrated database. We randomly selected 10% of patients, accumulated the primary keys for their tabulated data, and compared the key numbers in the transmitted data with those of the raw data. Errors were identified based on statistical testing and clinical reasoning.
Results
Data were recorded in 1573 tables. Among these, 58 (3.7%) had different key numbers, with the maximum of 16.34/1000. Statistical differences (P < 0.05) were found in 34 (58.6%), of which 15 were caused by changes in diagnostic codes, wrong accounts, or modified orders. For the rest, the differences were related to accumulation of hospital visits over time. In the remaining 24 tables (41.4%) without significant differences, three were revised because of incorrect computer programming or wrong accounts. For the rest, the programming was correct and absolute differences were negligible. The applicability was confirmed using the data of 2,730,883 patients and 15,647,468 patient-visits transmitted during 2017–2018, in which 10 (3.5%) tables were corrected.
Conclusion
Significant magnitude of inconsistent data does exist during the transmission of big data from diverse sources. Systematic validation is essential. Comparing the number of data tabulated using the primary keys allow us to rapidly identify and correct these scattered errors.",https://doi.org/10.1016/j.jfma.2021.12.024,2022,Yi-Chia Lee and Ying-Ting Chao and Pei-Ju Lin and Yen-Yun Yang and Yu-Cih Yang and Cheng-Chieh Chu and Yu-Chun Wang and Chin-Hao Chang and Shu-Lin Chuang and Wei-Chun Chen and Hsing-Jen Sun and Hsin-Cheng Tsou and Cheng-Fu Chou and Wei-Shiung Yang,QUALITY ASSURANCE OF INTEGRATIVE BIG DATA FOR MEDICAL RESEARCH WITHIN A MULTIHOSPITAL SYSTEM,article
428,19700174607,JOURNAL OF COMPUTATIONAL SCIENCE,journal,18777503,"0,704",Q1,46,123,485,5550,2325,465,"4,97","45,12",Netherlands,Western Europe,Elsevier,2010-2020,Computer Science (miscellaneous) (Q1); Modeling and Simulation (Q2); Theoretical Computer Science (Q2),JOURNAL OF COMPUTATIONAL SCIENCE,JOURNAL OF COMPUTATIONAL SCIENCE,"3,198",3.976,0.00489,"The application of the optimisation problems in the daily decisions of companies is able to be used for finding the best management according to the necessities of the organisations. However, optimisation problems imply a high computational complexity, increased by the current necessity to include a massive quantity of data (Big Data), for the creation of optimisation problems to customise products and services for their clients. The irruption of Big Data technologies can be a challenge but also an important mechanism to tackle the computational difficulties of optimisation problems, and the possibility to distribute the problem performance. In this paper, we propose a solution that lets the query of a data set supported by Big Data technologies that imply the resolution of Constraint Optimisation Problem (COP). This proposal enables to: (1) model COPs whose input data are obtained from distributed and heterogeneous data; (2) facilitate the integration of different data sources to create the COPs; and, (3) solve the optimisation problems in a distributed way, to improve the performance. It is done by means of a framework and supported by a tool capable of modelling, solving and querying the results of optimisation problems. The tool integrates the Big Data technologies and commercial solvers of constraint programming. The suitability of the proposal and the development have been evaluated with real data sets whose computational study and results are included and discussed.",https://doi.org/10.1016/j.jocs.2020.101180,2020,Álvaro Valencia-Parra and Ángel Jesús Varela-Vaca and Luisa Parody and María Teresa Gómez-López,UNLEASHING CONSTRAINT OPTIMISATION PROBLEM SOLVING IN BIG DATA ENVIRONMENTS,article
429,19700174607,JOURNAL OF COMPUTATIONAL SCIENCE,journal,18777503,"0,704",Q1,46,123,485,5550,2325,465,"4,97","45,12",Netherlands,Western Europe,Elsevier,2010-2020,Computer Science (miscellaneous) (Q1); Modeling and Simulation (Q2); Theoretical Computer Science (Q2),JOURNAL OF COMPUTATIONAL SCIENCE,JOURNAL OF COMPUTATIONAL SCIENCE,"3,198",3.976,0.00489,"Smart manufacturing refers to a future-state of manufacturing and it can lead to remarkable changes in all aspects of operations through minimizing energy and material usage while simultaneously maximizing sustainability enabling a futuristic more digitalized scenario of manufacturing. This research develops a big data analytics framework that optimizes the maintenance schedule through condition-based maintenance (CBM) optimization and also improves the prediction accuracy to quantify the remaining life prediction uncertainty. Through effective utilization of condition monitoring and prediction information, CBM would enhance equipment reliability leading to reduction in maintenance cost. The proposed framework uses a CBM optimization method that utilizes a new linguistic interval-valued fuzzy reasoning method for predicting the information. The proposed big data analytics framework in our study for estimating the uncertainty based on backward feature elimination and fuzzy unordered rule induction algorithm prediction errors, is an innovative contribution to the remaining life prediction field. Our paper elaborates on the basic underlying structure of CBM system that is defined by transaction matrix and the threshold value of failure probability. We developed this framework for analysing the CBM policy cost more accurately and to find the probabilistic threshold values of covariate that corresponds to the lowest price of predictive maintenance cost. The experimental results are performed on a big dataset which is generated from a sophisticated simulator of a gas turbine propulsion plant. A comparative analysis confirms that the method used in the proposed framework outpaces the classical methods in terms of classification accuracy and other statistical performance evaluation metrics.",https://doi.org/10.1016/j.jocs.2017.06.006,2018,Ajay Kumar and Ravi Shankar and Lakshman S. Thakur,A BIG DATA DRIVEN SUSTAINABLE MANUFACTURING FRAMEWORK FOR CONDITION-BASED MAINTENANCE PREDICTION,article
430,21100241218,ASTRONOMY AND COMPUTING,journal,22131337,"0,692",Q2,31,46,133,2012,475,132,"3,53","43,74",Netherlands,Western Europe,Elsevier BV,2013-2020,Astronomy and Astrophysics (Q2); Computer Science Applications (Q2); Space and Planetary Science (Q2),ASTRONOMY AND COMPUTING,ASTRONOMY AND COMPUTING,796,1.927,0.0025,"As the demand for software to support the processing and analysis of massive radio astronomy datasets increases in the era of the SKA, we demonstrate the interactive workflow building, data mining, processing, and visualisation capabilities of DUG Insight. We test the performance and flexibility of DUG Insight by processing almost 68,000 full sky radio images produced from the Engineering Development Array (EDA2) over the course of a three day period. The goal of the processing was to passively detect and identify known Resident Space Objects (RSOs: satellites and debris in orbit) and investigate how radio interferometry could be used to passively monitor aircraft traffic. These signals are observable due to both terrestrial FM radio signals reflected back to Earth and out-of-band transmission from RSOs. This surveillance of the low Earth orbit and airspace environment is useful as a contribution to space situational awareness and aircraft tracking technology. From the observations, we made 40 detections of 19 unique RSOs within a range of 1,500 km from the EDA2. This is a significant improvement on a previously published study of the same dataset and showcases the flexible features of DUG Insight that allow the processing of complex datasets at scale. Future enhancements of our DUG Insight workflow will aim to realise real-time acquisition, detect unknown RSOs, and continue to process data from SKA-relevant facilities.",https://doi.org/10.1016/j.ascom.2022.100619,2022,D. Grigg and S.J. Tingay and M. Sokolowski and R.B. Wayth,"DUG INSIGHT: A SOFTWARE PACKAGE FOR BIG-DATA ANALYSIS AND VISUALISATION, AND ITS DEMONSTRATION FOR PASSIVE RADAR SPACE SITUATIONAL AWARENESS USING RADIO TELESCOPES",article
431,26792,CLINICS IN LABORATORY MEDICINE,journal,"15579832, 02722712","0,690",Q2,55,52,173,2113,398,149,"1,61","40,63",United Kingdom,Western Europe,W.B. Saunders Ltd,1981-2020,Biochemistry (medical) (Q2); Clinical Biochemistry (Q3),CLINICS IN LABORATORY MEDICINE,CLINICS IN LABORATORY MEDICINE,"1,772",1.935,0.00186,,https://doi.org/10.1016/j.cll.2019.11.009,2020,Emily L. Gill and Stephen R. Master,BIG DATA EVERYWHERE: THE IMPACT OF DATA DISJUNCTION IN THE DIRECT-TO-CONSUMER TESTING MODEL,article
432,3200147819,PERVASIVE AND MOBILE COMPUTING,journal,15741192,"0,687",Q1,64,68,350,2760,1558,342,"4,67","40,59",Netherlands,Western Europe,Elsevier,2005-2020,Computer Networks and Communications (Q1); Computer Science (miscellaneous) (Q1); Hardware and Architecture (Q1); Information Systems (Q1); Software (Q1); Applied Mathematics (Q2); Computer Science Applications (Q2),PERVASIVE AND MOBILE COMPUTING,PERVASIVE AND MOBILE COMPUTING,"2,540",3.453,0.00335,"With gigabit networking becoming economically feasible and widely installed at homes, there are new opportunities to revisit in-home, personalized telehealth services. In this paper, we describe a novel telehealth eldercare service that we developed viz., “PhysicalTherapy-as-a-Service” (PTaaS) that connects a remote physical therapist at a clinic to a senior at home. The service leverages a high-speed, low-latency network connection through an interactive interface built on top of Microsoft Kinect motion sensing capabilities. The interface that is built using user-centered design principles for wellness coaching exercises is essentially a ‘Synchronous Big Data’ application due to its: (i) high data-in-motion velocity (i.e., peak data rate is ≈400 Mbps), (ii) considerable variety (i.e., measurements include 3D sensing, network health, user opinion surveys and video clips of RGB, skeletal and depth data), and (iii) large volume (i.e., several GB of measurement data for a simple exercise activity). The successful PTaaS delivery through this interface is dependent on the veracity analytics needed for correlation of the real-time Big Data streams within a session, in order to assess exercise balance of the senior without any bias due to network quality effects. Our experiments with PTaaS in an actual testbed involving senior homes in Kansas City with Google Fiber connections and our university clinic demonstrate the network configuration and time synchronization related challenges in order to perform online analytics. Our findings provide insights on how to: (a) enable suitable resource calibration and perform network troubleshooting for high user experience for both the therapist and the senior, and (b) realize a Big Data architecture for PTaaS and other similar personalized healthcare services to be remotely delivered at a large-scale in a reliable, secure and cost-effective manner.",https://doi.org/10.1016/j.pmcj.2015.09.004,2016,Prasad Calyam and Anup Mishra and Ronny Bazan Antequera and Dmitrii Chemodanov and Alex Berryman and Kunpeng Zhu and Carmen Abbott and Marjorie Skubic,SYNCHRONOUS BIG DATA ANALYTICS FOR PERSONALIZED AND REMOTE PHYSICAL THERAPY,article
433,23831,JOURNAL OF CARDIOTHORACIC AND VASCULAR ANESTHESIA,journal,"10530770, 15328422","0,678",Q2,82,770,1616,22269,2501,1041,"1,58","28,92",United Kingdom,Western Europe,W.B. Saunders Ltd,1991-2020,Anesthesiology and Pain Medicine (Q2); Cardiology and Cardiovascular Medicine (Q2),JOURNAL OF CARDIOTHORACIC AND VASCULAR ANESTHESIA,JOURNAL OF CARDIOTHORACIC AND VASCULAR ANESTHESIA,"7,080",2.628,0.00838,,https://doi.org/10.1053/j.jvca.2019.11.012,2020,Michael R. Mathis and Timur Z. Dubovoy and Matthew D. Caldwell and Milo C. Engoren,MAKING SENSE OF BIG DATA TO IMPROVE PERIOPERATIVE CARE: LEARNING HEALTH SYSTEMS AND THE MULTICENTER PERIOPERATIVE OUTCOMES GROUP,article
434,22285,SEMINARS IN VASCULAR SURGERY,journal,"15584518, 08957967","0,644",Q2,48,17,46,398,66,38,"0,75","23,41",United Kingdom,Western Europe,W.B. Saunders Ltd,1990-2020,Cardiology and Cardiovascular Medicine (Q2); Surgery (Q2),SEMINARS IN VASCULAR SURGERY,SEMINARS IN VASCULAR SURGERY,734,1.000,0.00065,"ABSTRACT
The field of vascular surgery is in constant evolution. Administrative data and registries can provide important contemporary evidence to inform clinical decision making and the delivery of health services. The following review outlines some important considerations for retrospective studies using administrative health databases and registries. First, these data sources have advantages (e.g. real-world applicability, timely data access, relatively lower research cost) and disadvantages (e.g. potential missing data, selection bias, confounding bias) that may be more or less relevant to different administrative databases or registries. Second, we discuss a framework to guide data source selection and provide a summary of frequently used data sources in vascular surgery research. Third, a retrospective study design warrants pre-planned exposure, outcome and covariate definitions and, when studying an exposure-outcome association, careful consideration of confounders through direct acyclic graphs. Finally, investigators must plan the most appropriate analytic approach and we distinguish descriptive, explanatory, and predictive analyses.",https://doi.org/10.1053/j.semvascsurg.2022.09.002,2022,Jean Jacob-Brassard and Charles {de Mestral},BIG DATA: USING DATABASES AND REGISTRIES,article
435,22285,SEMINARS IN VASCULAR SURGERY,journal,"15584518, 08957967","0,644",Q2,48,17,46,398,66,38,"0,75","23,41",United Kingdom,Western Europe,W.B. Saunders Ltd,1990-2020,Cardiology and Cardiovascular Medicine (Q2); Surgery (Q2),SEMINARS IN VASCULAR SURGERY,SEMINARS IN VASCULAR SURGERY,734,1.000,0.00065,"ABSTRACT
Artificial intelligence (AI) is the next great advance informing medical science. Several disciplines, including vascular surgery, use AI-based decision-making tools to improve clinical performance. Although applied widely, AI functions best when confronted with voluminous, accurate data. Consistent, predictable analytic technique selection also challenges researchers. This article contextualizes AI analyses within evidence-based medicine, focusing on “big data” and health services research, as well as discussing opportunities to improve data collection and realize AI's promise.",https://doi.org/10.1053/j.semvascsurg.2021.10.005,2021,Devin S. Zarkowsky and David P. Stonko,ARTIFICIAL INTELLIGENCE'S ROLE IN VASCULAR SURGERY DECISION-MAKING,article
436,19309,JOURNAL OF SYSTEMS AND SOFTWARE,journal,01641212,"0,642",Q1,109,183,619,11845,3058,590,"4,94","64,73",United States,Northern America,Elsevier Inc.,"1979, 1981-2021",Hardware and Architecture (Q1); Information Systems (Q2); Software (Q2),JOURNAL OF SYSTEMS AND SOFTWARE,JOURNAL OF SYSTEMS AND SOFTWARE,"6,579",2.829,0.00727,"Deep learning (DL) based software systems are difficult to develop and maintain in industrial settings due to several challenges. Data management is one of the most prominent challenges which complicates DL in industrial deployments. DL models are data-hungry and require high-quality data. Therefore, the volume, variety, velocity, and quality of data cannot be compromised. This study aims to explore the data management challenges encountered by practitioners developing systems with DL components, identify the potential solutions from the literature and validate the solutions through a multiple case study. We identified 20 data management challenges experienced by DL practitioners through a multiple interpretive case study. Further, we identified 48 articles through a systematic literature review that discuss the solutions for the data management challenges. With the second round of multiple case study, we show that many of these solutions have limitations and are not used in practice due to a combination of four factors: high cost, lack of skill-set and infrastructure, inability to solve the problem completely, and incompatibility with certain DL use cases. Thus, data management for data-intensive DL models in production is complicated. Although the DL technology has achieved very promising results, there is still a significant need for further research in the field of data management to build high-quality datasets and streams that can be used for building production-ready DL systems. Furthermore, we have classified the data management challenges into four categories based on the availability of the solutions.",https://doi.org/10.1016/j.jss.2022.111359,2022,Aiswarya Raj Munappy and Jan Bosch and Helena Holmström Olsson and Anders Arpteg and Björn Brinne,DATA MANAGEMENT FOR PRODUCTION QUALITY DEEP LEARNING MODELS: CHALLENGES AND SOLUTIONS,article
437,19309,JOURNAL OF SYSTEMS AND SOFTWARE,journal,01641212,"0,642",Q1,109,183,619,11845,3058,590,"4,94","64,73",United States,Northern America,Elsevier Inc.,"1979, 1981-2021",Hardware and Architecture (Q1); Information Systems (Q2); Software (Q2),JOURNAL OF SYSTEMS AND SOFTWARE,JOURNAL OF SYSTEMS AND SOFTWARE,"6,579",2.829,0.00727,"Context
Big Data Cybersecurity Analytics (BDCA) systems leverage big data technologies for analyzing security events data to protect organizational networks, computers, and data from cyber attacks.
Objective
We aimed at identifying the most frequently reported quality attributes and architectural tactics for BDCA systems.
Method
We used Systematic Literature Review (SLR) method for reviewing 74 papers.
Result
Our findings are twofold: (i) identification of 12 most frequently reported quality attributes for BDCA systems; and (ii) identification and codification of 17 architectural tactics for addressing the identified quality attributes. The identified tactics include six performance tactics, four accuracy tactics, two scalability tactics, three reliability tactics, and one security and usability tactic each.
Conclusion
Our study reveals that in the context of BDCA (a) performance, accuracy and scalability are the most important quality concerns (b) data analytics is the most critical architectural component (c) despite the significance of interoperability, modifiability, adaptability, generality, stealthiness, and privacy assurance, these quality attributes lack explicit architectural support (d) empirical investigation is required to evaluate the impact of the codified tactics and explore the quality trade-offs and dependencies among the tactics and (e) the reported tactics need to be modelled using a standardized modelling language such as UML.",https://doi.org/10.1016/j.jss.2019.01.051,2019,Faheem Ullah and Muhammad {Ali Babar},ARCHITECTURAL TACTICS FOR BIG DATA CYBERSECURITY ANALYTICS SYSTEMS: A REVIEW,article
438,19309,JOURNAL OF SYSTEMS AND SOFTWARE,journal,01641212,"0,642",Q1,109,183,619,11845,3058,590,"4,94","64,73",United States,Northern America,Elsevier Inc.,"1979, 1981-2021",Hardware and Architecture (Q1); Information Systems (Q2); Software (Q2),JOURNAL OF SYSTEMS AND SOFTWARE,JOURNAL OF SYSTEMS AND SOFTWARE,"6,579",2.829,0.00727,"The most successful organizations in the world are data-driven businesses. Data is at the core of the business of many organizations as one of the most important assets, since the decisions they make cannot be better than the data on which they are based. Due to this reason, organizations need to be able to trust their data. One important activity that helps to achieve data reliability is the evaluation and certification of the quality level of organizational data repositories. This paper describes the results of the application of a data quality evaluation and certification process to the repositories of three European organizations belonging to different sectors. We present findings from the point of view of both the data quality evaluation team and the organizations that underwent the evaluation process. In this respect, several benefits have been explicitly recognized by the involved organizations after achieving the data quality certification for their repositories (e.g., long-term organizational sustainability better internal knowledge of data, and a more efficient management of data quality). As a result of this experience, we have also identified a set of best practices aimed to enhance the data quality evaluation process.",https://doi.org/10.1016/j.jss.2021.110938,2021,Fernando Gualo and Moisés Rodriguez and Javier Verdugo and Ismael Caballero and Mario Piattini,DATA QUALITY CERTIFICATION USING ISO/IEC 25012: INDUSTRIAL EXPERIENCES,article
439,19309,JOURNAL OF SYSTEMS AND SOFTWARE,journal,01641212,"0,642",Q1,109,183,619,11845,3058,590,"4,94","64,73",United States,Northern America,Elsevier Inc.,"1979, 1981-2021",Hardware and Architecture (Q1); Information Systems (Q2); Software (Q2),JOURNAL OF SYSTEMS AND SOFTWARE,JOURNAL OF SYSTEMS AND SOFTWARE,"6,579",2.829,0.00727,"Apache Spark is one of the most popular big data frameworks that abstract the underlying distributed computation details. However, even though Spark provides various abstractions, developers may still encounter challenges related to the peculiarity of distributed computation and environment. To understand the challenges that developers encounter, and provide insight for future studies, in this paper, we conduct an empirical study on the questions that developers encounter. We manually analyze 1,000 randomly selected questions that we collected from Stack Overflow. We find that: 1) questions related to data processing (e.g., transforming data format) are the most common among the 11 types of questions that we uncovered. 2) Even though data processing questions are the most common ones, they require the least amount of time to receive an answer. Questions related to configuration and performance require the most time to receive an answer. 3) Most of the issues are caused by developers’ insufficient knowledge in API usages, data conversation across frameworks, and environment-related configurations. We also discuss the implication of our findings for researchers and practitioners. In summary, our work provides insights for future research directions and highlight the need for more software engineering research in this area.",https://doi.org/10.1016/j.jss.2022.111488,2022,Zehao Wang and Tse-Hsun (Peter) Chen and Haoxiang Zhang and Shaowei Wang,AN EMPIRICAL STUDY ON THE CHALLENGES THAT DEVELOPERS ENCOUNTER WHEN DEVELOPING APACHE SPARK APPLICATIONS,article
440,19309,JOURNAL OF SYSTEMS AND SOFTWARE,journal,01641212,"0,642",Q1,109,183,619,11845,3058,590,"4,94","64,73",United States,Northern America,Elsevier Inc.,"1979, 1981-2021",Hardware and Architecture (Q1); Information Systems (Q2); Software (Q2),JOURNAL OF SYSTEMS AND SOFTWARE,JOURNAL OF SYSTEMS AND SOFTWARE,"6,579",2.829,0.00727,"In this paper we present a collection of ontologies specifically designed to model the information exchange needs of combined software and data engineering. Effective, collaborative integration of software and big data engineering for Web-scale systems, is now a crucial technical and economic challenge. This requires new combined data and software engineering processes and tools. Our proposed models have been deployed to enable: tool-chain integration, such as the exchange of data quality reports; cross-domain communication, such as interlinked data and software unit testing; mediation of the system design process through the capture of design intents and as a source of context for model-driven software engineering processes. These ontologies are deployed in web-scale, data-intensive, system development environments in both the commercial and academic domains. We exemplify the usage of the suite on case-studies emerging from two complex collaborative software and data engineering scenarios: one from the legal sector and the other from the Social sciences and Humanities domain.",https://doi.org/10.1016/j.jss.2018.12.017,2019,Monika Solanki and Bojan Božić and Christian Dirschl and Rob Brennan,TOWARDS A KNOWLEDGE DRIVEN FRAMEWORK FOR BRIDGING THE GAP BETWEEN SOFTWARE AND DATA ENGINEERING,article
441,25621,JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING,journal,"07437315, 10960848","0,638",Q1,87,169,592,7166,2473,568,"4,72","42,40",United States,Northern America,Academic Press Inc.,1984-2021,Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Artificial Intelligence (Q2); Software (Q2); Theoretical Computer Science (Q2),JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING,JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING,"4,371",3.734,0.00477,"In the big data era, large amounts of data are under generation and accumulation in various industries. However, users usually feel hindered by the data quality issues when extracting values from the big data. Thus, data quality issues are gaining more and more attention from data quality management analysts. Cutting-edge solutions like data ETL, data cleaning, and data quality monitoring systems have many deficiencies in capability and efficiency, making it difficult to cope with complicated situations on big data. These problems inspire us to build SparkDQ, a generic distributed data quality management model and framework that provides a series of data quality detection and repair interfaces. Users can quickly build custom tasks of data quality computing for various needs by utilizing these interfaces. In addition, SparkDQ implements a set of algorithms that in a parallel manner with optimizations. These algorithms aim at various data quality goals. We also propose several system-level optimizations, including the job-level optimization with multi-task execution scheduling and the data-level optimization with data state caching. The experimental evaluation shows that the proposed distributed algorithms in SparkDQ run up to 12 times faster compared to the corresponding stand-alone serial and multi-thread algorithms. Compared with the cutting-edge distributed data quality solution Apache Griffin, SparkDQ has more features, and its execution time is only around half of Apache Griffin on average. SparkDQ achieves near-linear data and node scalability.",https://doi.org/10.1016/j.jpdc.2021.05.012,2021,Rong Gu and Yang Qi and Tongyu Wu and Zhaokang Wang and Xiaolong Xu and Chunfeng Yuan and Yihua Huang,SPARKDQ: EFFICIENT GENERIC BIG DATA QUALITY MANAGEMENT ON DISTRIBUTED DATA-PARALLEL COMPUTATION,article
442,25621,JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING,journal,"07437315, 10960848","0,638",Q1,87,169,592,7166,2473,568,"4,72","42,40",United States,Northern America,Academic Press Inc.,1984-2021,Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Artificial Intelligence (Q2); Software (Q2); Theoretical Computer Science (Q2),JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING,JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING,"4,371",3.734,0.00477,"One of the major applications of future generation parallel and distributed systems is in big-data analytics. Data repositories for such applications currently exceed exabytes and are rapidly increasing in size. Beyond their sheer magnitude, these datasets and associated applications’ considerations pose significant challenges for method and software development. Datasets are often distributed and their size and privacy considerations warrant distributed techniques. Data often resides on platforms with widely varying computational and network capabilities. Considerations of fault-tolerance, security, and access control are critical in many applications (Dean and Ghemawat, 2004; Apache hadoop). Analysis tasks often have hard deadlines, and data quality is a major concern in yet other applications. For most emerging applications, data-driven models and methods, capable of operating at scale, are as-yet unknown. Even when known methods can be scaled, validation of results is a major issue. Characteristics of hardware platforms and the software stack fundamentally impact data analytics. In this article, we provide an overview of the state-of-the-art and focus on emerging trends to highlight the hardware, software, and application landscape of big-data analytics.",https://doi.org/10.1016/j.jpdc.2014.01.003,2014,Karthik Kambatla and Giorgos Kollias and Vipin Kumar and Ananth Grama,TRENDS IN BIG DATA ANALYTICS,article
443,25621,JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING,journal,"07437315, 10960848","0,638",Q1,87,169,592,7166,2473,568,"4,72","42,40",United States,Northern America,Academic Press Inc.,1984-2021,Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Artificial Intelligence (Q2); Software (Q2); Theoretical Computer Science (Q2),JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING,JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING,"4,371",3.734,0.00477,"Real-time monitoring of cloud resources is crucial for a variety of tasks such as performance analysis, workload management, capacity planning and fault detection. Applications producing big data make the monitoring task very difficult at high sampling frequencies because of high computational and communication overheads in collecting, storing, and managing information. We present an adaptive algorithm for monitoring big data applications that adapts the intervals of sampling and frequency of updates to data characteristics and administrator needs. Adaptivity allows us to limit computational and communication costs and to guarantee high reliability in capturing relevant load changes. Experimental evaluations performed on a large testbed show the ability of the proposed adaptive algorithm to reduce resource utilization and communication overhead of big data monitoring without penalizing the quality of data, and demonstrate our improvements to the state of the art.",https://doi.org/10.1016/j.jpdc.2014.08.007,2015,Mauro Andreolini and Michele Colajanni and Marcello Pietri and Stefania Tosi,"ADAPTIVE, SCALABLE AND RELIABLE MONITORING OF BIG DATA ON CLOUDS",article
444,25621,JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING,journal,"07437315, 10960848","0,638",Q1,87,169,592,7166,2473,568,"4,72","42,40",United States,Northern America,Academic Press Inc.,1984-2021,Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Artificial Intelligence (Q2); Software (Q2); Theoretical Computer Science (Q2),JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING,JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING,"4,371",3.734,0.00477,,https://doi.org/10.1016/j.jpdc.2017.05.020,2017,Xian-He Sun and Marc Frincu and Charalampos Chelmis,SPECIAL ISSUE ON SCALABLE COMPUTING SYSTEMS FOR BIG DATA APPLICATIONS,article
445,25621,JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING,journal,"07437315, 10960848","0,638",Q1,87,169,592,7166,2473,568,"4,72","42,40",United States,Northern America,Academic Press Inc.,1984-2021,Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Artificial Intelligence (Q2); Software (Q2); Theoretical Computer Science (Q2),JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING,JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING,"4,371",3.734,0.00477,"This paper discusses approaches and environments for carrying out analytics on Clouds for Big Data applications. It revolves around four important areas of analytics and Big Data, namely (i) data management and supporting architectures; (ii) model development and scoring; (iii) visualisation and user interaction; and (iv) business models. Through a detailed survey, we identify possible gaps in technology and provide recommendations for the research community on future directions on Cloud-supported Big Data computing and analytics solutions.",https://doi.org/10.1016/j.jpdc.2014.08.003,2015,Marcos D. Assunção and Rodrigo N. Calheiros and Silvia Bianchi and Marco A.S. Netto and Rajkumar Buyya,BIG DATA COMPUTING AND CLOUDS: TRENDS AND FUTURE DIRECTIONS,article
446,25621,JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING,journal,"07437315, 10960848","0,638",Q1,87,169,592,7166,2473,568,"4,72","42,40",United States,Northern America,Academic Press Inc.,1984-2021,Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Artificial Intelligence (Q2); Software (Q2); Theoretical Computer Science (Q2),JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING,JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING,"4,371",3.734,0.00477,"The proliferation of Internet of Things (IoT) has led to the emergence of enabling many interesting applications within the realm of several domains including smart cities. However, the accumulation of data from smart IoT devices poses significant challenges for data storage while there are needs to deliver relevant and high quality services to consumers. In this paper, we propose QDaS, a novel domain agnostic framework as a solution for effective data storage and management of IoT applications. The framework incorporates a novel data summarisation mechanism that uses an innovative data quality estimation technique. This proposed data quality estimation technique computes the quality of data (based on their utility) without requiring any feedback from users of this IoT data or domain awareness of the data. We evaluate the effectiveness of the proposed QDaS framework using real world datasets.",https://doi.org/10.1016/j.jpdc.2018.03.013,2019,Jonathan Liono and Prem Prakash Jayaraman and A.K. Qin and Thuong Nguyen and Flora D. Salim,QDAS: QUALITY DRIVEN DATA SUMMARISATION FOR EFFECTIVE STORAGE MANAGEMENT IN INTERNET OF THINGS,article
447,25621,JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING,journal,"07437315, 10960848","0,638",Q1,87,169,592,7166,2473,568,"4,72","42,40",United States,Northern America,Academic Press Inc.,1984-2021,Computer Networks and Communications (Q1); Hardware and Architecture (Q1); Artificial Intelligence (Q2); Software (Q2); Theoretical Computer Science (Q2),JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING,JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING,"4,371",3.734,0.00477,"Record Linkage (RL) is the task of identifying duplicate entities in a dataset or multiple datasets. In the era of Big Data, this task has gained notorious attention due to the intrinsic quadratic complexity of the problem in relation to the size of the dataset. In practice, this task can be outsourced to a cloud service, and thus, a service customer may be interested in estimating the costs of a record linkage solution before executing it. Since the execution time of a record linkage solution depends on a combination of various algorithms, their respective parameter values and the employed cloud infrastructure, in practice it is hard to perform an a priori estimation of infrastructure costs for executing a record linkage task. Besides estimating customer costs, the estimation of record linkage costs is also important to evaluate whether (or not) the application of a set of RL parameter values will satisfy predefined time and budget restrictions. Aiming to tackle these challenges, we propose a theoretical model for estimating RL costs taking into account the main steps that may influence the execution time of the RL task. We also propose an algorithm, denoted as TBF, for evaluating the feasibility of RL parameter values, given a set of predefined customer restrictions. We evaluate the efficacy of the proposed model combined with regression techniques using record linkage results processed in real distributed environments. Based on the experimental results, we show that the employed regression technique has significant influence over the estimated record linkage costs. Moreover, we conclude that specific regression techniques are more suitable for estimating record linkage costs, depending on the evaluated scenario.",https://doi.org/10.1016/j.jpdc.2020.05.003,2020,Dimas Cassimiro Nascimento and Carlos Eduardo Santos Pires and Tiago Brasileiro Araujo and Demetrio Gomes Mestre,ESTIMATING RECORD LINKAGE COSTS IN DISTRIBUTED ENVIRONMENTS,article
448,20492,ANNALS OF VASCULAR SURGERY,journal,"16155947, 08905096","0,635",Q2,74,844,1561,16904,2242,1527,"1,37","20,03",United States,Northern America,Elsevier Inc.,1986-2020,Cardiology and Cardiovascular Medicine (Q2); Medicine (miscellaneous) (Q2); Surgery (Q2),ANNALS OF VASCULAR SURGERY,ANNALS OF VASCULAR SURGERY,"7,121",1.466,0.00916,,https://doi.org/10.1016/j.avsg.2020.04.022,2020,Fabien Lareyre and Cédric Adam and Marion Carrier and Juliette Raffort,ARTIFICIAL INTELLIGENCE IN VASCULAR SURGERY: MOVING FROM BIG DATA TO SMART DATA,article
449,13681,COMPUTER COMMUNICATIONS,journal,01403664,"0,627",Q1,105,616,599,24961,2424,591,"4,08","40,52",Netherlands,Western Europe,Elsevier,1978-2020,Computer Networks and Communications (Q1),COMPUTER COMMUNICATIONS,COMPUTER COMMUNICATIONS,"6,725",3.167,0.00513,"Nowadays, the world knows a high-speed development and evolution of technologies, vulnerable economic environments, market changes, and personalised consumer trends. The issue and challenge related to enterprises networks design are more and more critical. These networks are often designed for short terms since their strategies must be competitive and better adapted to the environment, social and economical changes. As a solution, to design a flexible and robust network, it is necessary to deal with the trade-off between conflicting qualitative and quantitative criteria such as cost, quality, delivery time, and competition, etc. To this end, using Big Data (BD) as emerging technology will enhance the real performances of these kinds of networks. Moreover, even if the literature is rich with BD models and frameworks developed for a single supply chain network (SCN), there is a real need to scale and extend these BD models to networked supply chains (NSCs). To do so, this paper proposes a BD architecture to drive a mixed-network of SCs that collaborate in serial and parallel fashions. The collaboration is set up by sharing their resources, capabilities, competencies, and information to imitate a unique organisation. The objective is to increase internal value to their shareholders (where value is seen as wealth) and deliver better external value to the end-customer (where value represents customer satisfaction). Within a mixed-network of SCs, both values are formally calculated considering both serial and parallel networks configurations. Besides, some performance factors of the proposed BD architecture such as security, flexibility, robustness and resilience are discussed.",https://doi.org/10.1016/j.comcom.2021.05.008,2021,Lahcen Tamym and Lyes Benyoucef and Ahmed {Nait Sidi Moh} and Moulay Driss {El Ouadghiri},A BIG DATA BASED ARCHITECTURE FOR COLLABORATIVE NETWORKS: SUPPLY CHAINS MIXED-NETWORK,article
450,13681,COMPUTER COMMUNICATIONS,journal,01403664,"0,627",Q1,105,616,599,24961,2424,591,"4,08","40,52",Netherlands,Western Europe,Elsevier,1978-2020,Computer Networks and Communications (Q1),COMPUTER COMMUNICATIONS,COMPUTER COMMUNICATIONS,"6,725",3.167,0.00513,"Mobile crowdsensing has become an efficient paradigm for performing large-scale sensing tasks. Many quality-aware incentive mechanisms for mobile crowdsensing have been proposed. However, most of them measure the data quality by one single metric from a specific perspective. Moreover, they usually use the real-time quality, which cannot provide sufficient incentive for the workers with long-term high quality. In this paper, we refine the generalized data quality into the fine-grained ability requirement. We present a mobile crowdsensing system to achieve the fine-grained quality control, and formulate the problem of maximizing the social cost such that the fine-grained ability requirement of all sensing tasks can be satisfied. To stimulate the workers with long-term high quality, we design two ability reputation systems to assess workers’ fine-grained abilities online. The incentive mechanism based on the reverse auction and fine-grained ability reputation system is proposed. We design a greedy algorithm to select the winners and determine the payment based on the bids and fine-grained ability reputation of workers. Through both rigorous theoretical analysis and extensive simulations, we demonstrate that the proposed mechanisms achieve computational efficiency, individual rationality, truthfulness, whitewashing proof, and guaranteed approximation. Moreover, the designed mechanisms show prominent advantage in terms of social cost and average ability achievement ratio.",https://doi.org/10.1016/j.comcom.2021.09.026,2021,Zhuangye Luo and Jia Xu and Pengcheng Zhao and Dejun Yang and Lijie Xu and Jian Luo,TOWARDS HIGH QUALITY MOBILE CROWDSENSING: INCENTIVE MECHANISM DESIGN BASED ON FINE-GRAINED ABILITY REPUTATION,article
451,13681,COMPUTER COMMUNICATIONS,journal,01403664,"0,627",Q1,105,616,599,24961,2424,591,"4,08","40,52",Netherlands,Western Europe,Elsevier,1978-2020,Computer Networks and Communications (Q1),COMPUTER COMMUNICATIONS,COMPUTER COMMUNICATIONS,"6,725",3.167,0.00513,"Technology has become inevitable in human life, especially the growth of Internet of Things (IoT), which enables communication and interaction with various devices. However, IoT has been proven to be vulnerable to security breaches. Therefore, it is necessary to develop fool proof solutions by creating new technologies or combining existing technologies to address the security issues. Deep learning, a branch of machine learning has shown promising results in previous studies for detection of security breaches. Additionally, IoT devices generate large volumes, variety, and veracity of data. Thus, when big data technologies are incorporated, higher performance and better data handling can be achieved. Hence, we have conducted a comprehensive survey on state-of-the-art deep learning, IoT security, and big data technologies. Further, a comparative analysis and the relationship among deep learning, IoT security, and big data technologies have also been discussed. Further, we have derived a thematic taxonomy from the comparative analysis of technical studies of the three aforementioned domains. Finally, we have identified and discussed the challenges in incorporating deep learning for IoT security using big data technologies and have provided directions to future researchers on the IoT security aspects.",https://doi.org/10.1016/j.comcom.2020.01.016,2020,Mohamed Ahzam Amanullah and Riyaz Ahamed Ariyaluran Habeeb and Fariza Hanum Nasaruddin and Abdullah Gani and Ejaz Ahmed and Abdul Salam Mohamed Nainar and Nazihah Md Akim and Muhammad Imran,DEEP LEARNING AND BIG DATA TECHNOLOGIES FOR IOT SECURITY,article
452,13681,COMPUTER COMMUNICATIONS,journal,01403664,"0,627",Q1,105,616,599,24961,2424,591,"4,08","40,52",Netherlands,Western Europe,Elsevier,1978-2020,Computer Networks and Communications (Q1),COMPUTER COMMUNICATIONS,COMPUTER COMMUNICATIONS,"6,725",3.167,0.00513,"Huge volumes of data are generated at rates faster than the speed of computing resources and executing processors available in market place. This anticipates a draft of information challenges associated with the performance capacity and the ability of big data processing systems to retort in real-time. Moreover, the elapsed time between probabilistic failures drops as the scale of information increases. An error occurred at a specific cluster node of a large Cyber–Physical System influences the overall computation requires to unfold big data transactions. Numerous failure characteristics, statistical response time and lifetime evaluation can be modeled through Weibull Distribution. In this paper, to scrutinize the latency for a data infrastructure, the three-parameter Weibull Cumulative Distribution is used through software defined networking in cyber–physical system. This speculation predicts that the shape of the response time distribution confide in the shape of the learning curve and depicts its parameters to the criterion of the input distribution.",https://doi.org/10.1016/j.comcom.2019.11.018,2020,Gifty R. and Bharathi R.,WEIBULL CUMULATIVE DISTRIBUTION BASED REAL-TIME RESPONSE AND PERFORMANCE CAPACITY MODELING OF CYBER–PHYSICAL SYSTEMS THROUGH SOFTWARE DEFINED NETWORKING,article
453,21100332403,JOURNAL OF INFORMATION SECURITY AND APPLICATIONS,journal,"22142126, 22142134","0,610",Q2,40,183,297,8559,1526,292,"5,43","46,77",United Kingdom,Western Europe,Elsevier Ltd.,2013-2020,"Computer Networks and Communications (Q2); Safety, Risk, Reliability and Quality (Q2); Software (Q2)",JOURNAL OF INFORMATION SECURITY AND APPLICATIONS,JOURNAL OF INFORMATION SECURITY AND APPLICATIONS,"1,526",3.872,0.00209,"The implementation of the GDPR that aims at protecting European citizens’ privacy is still a real challenge. In particular, in Big Data systems where data are voluminous and heterogeneous, it is hard to track data evolution through its complex life cycle ranging from collection, ingestion, storage and analytics. In this context, from 2016 to 2021 research has been conducted and several security tools designed. However, they are either specific to particular applications or address partially the regulation articles. To identify the covered parts, the missed ones and the necessary metrics for comparing different works, we propose a framework for GDPR compliance. The framework identifies the main components for the regulation implementation by mapping requirements aligned with GDPR’s provisions to IT design requirements. Based on this framework, we compare the main GDPR solutions in the Big Data domain and we propose a guideline for GDPR verification and implementation in Big Data systems.",https://doi.org/10.1016/j.jisa.2021.102896,2021,Mouna Rhahla and Sahar Allegue and Takoua Abdellatif,GUIDELINES FOR GDPR COMPLIANCE IN BIG DATA SYSTEMS,article
454,21100332403,JOURNAL OF INFORMATION SECURITY AND APPLICATIONS,journal,"22142126, 22142134","0,610",Q2,40,183,297,8559,1526,292,"5,43","46,77",United Kingdom,Western Europe,Elsevier Ltd.,2013-2020,"Computer Networks and Communications (Q2); Safety, Risk, Reliability and Quality (Q2); Software (Q2)",JOURNAL OF INFORMATION SECURITY AND APPLICATIONS,JOURNAL OF INFORMATION SECURITY AND APPLICATIONS,"1,526",3.872,0.00209,"The technical features of blockchain, including decentralization, data transparency, tamper-proofing, traceability, privacy protection and open-sourcing, make it a suitable technology for solving the information asymmetry problem in personal credit reporting transactions. Applying blockchain technology to credit reporting meets the needs of social credit system construction and may become an important technical direction in the future. This paper analyzed the problems faced by China’s personal credit reporting market, designed the framework of personal credit information sharing platform based on blockchain 3.0 architecture, studied the technical details of the platform and the technical advantages, and finally, applied the platform to the credit blacklist sharing transaction and explored the possible implementation approach. The in-depth integration of blockchain technology and personal credit reporting helps to realize the safe sharing of credit data and reduce the cost of credit data collection, thereby helping the technological and efficiency transformation of the personal credit reporting industry and promoting the overall development of the social credit system.",https://doi.org/10.1016/j.jisa.2020.102659,2020,Jing Zhang and Rong Tan and Chunhua Su and Wen Si,DESIGN AND APPLICATION OF A PERSONAL CREDIT INFORMATION SHARING PLATFORM BASED ON CONSORTIUM BLOCKCHAIN,article
455,21100332403,JOURNAL OF INFORMATION SECURITY AND APPLICATIONS,journal,"22142126, 22142134","0,610",Q2,40,183,297,8559,1526,292,"5,43","46,77",United Kingdom,Western Europe,Elsevier Ltd.,2013-2020,"Computer Networks and Communications (Q2); Safety, Risk, Reliability and Quality (Q2); Software (Q2)",JOURNAL OF INFORMATION SECURITY AND APPLICATIONS,JOURNAL OF INFORMATION SECURITY AND APPLICATIONS,"1,526",3.872,0.00209,"Nowadays, IoT, cloud computing, mobile and social networks are generating a transformation in social processes. Nevertheless, this technological change rise to new threats and security attacks that produce new and complex cybersecurity scenarios with large volumes of data and different attack vectors that can exceeded the cognitive skills of security analysts. In this context, cognitive sciences can enhance the cognitive processes, which can help to security analysts to establish actions in less time and more efficiently within cybersecurity operations. This works presents a cognitive security model that integrates technological solutions such as Big Data, Machine Learning, and Support Decision Systems with the cognitive processes of security analysts used to generate knowledge, understanding and execution of security response actions. The model considers alternatives to establish the automation process in the execution of cognitive tasks defined in the cyber operations processes and includes the analyst as the central axis in the processes of validation and decision making through the use of MAPE-K, OODA and Human in the Loop.",https://doi.org/10.1016/j.jisa.2019.06.008,2019,Roberto O Andrade and Sang Guun Yoo,COGNITIVE SECURITY: A COMPREHENSIVE STUDY OF COGNITIVE SCIENCE IN CYBERSECURITY,article
456,18732,INFORMATION AND SOFTWARE TECHNOLOGY,journal,09505849,"0,606",Q2,103,121,420,6853,2316,403,"5,12","56,64",Netherlands,Western Europe,Elsevier,"1970, 1987-2020",Computer Science Applications (Q2); Information Systems (Q2); Software (Q2),INFORMATION AND SOFTWARE TECHNOLOGY,INFORMATION AND SOFTWARE TECHNOLOGY,"5,172",2.730,0.00554,"Context
The need for business intelligence has led to advances in machine learning in the business domain, especially with the rise of big data analytics. However, the resulting predictive systems often fail to maintain a satisfactory level of performance in production. Besides, for predictive systems used in business-to-business scenarios, user trust is subject to the model performance. Therefore, the processes of creating, evaluating, and deploying machine learning systems in the business domain need innovative solutions to solve the critical challenges of assuring the quality of the resulting systems.
Objective
Applying machine learning in business-to-business situations imposes specific requirements. This paper aims at providing an integrated solution to businesses to help them transform their data into actions.
Method
The paper presents MLean, an end-to-end framework, that aims at guiding businesses in designing, developing, evaluating, and deploying business-to-business predictive systems. The framework employs the Lean Startup methodology and aims at maximizing the business value while eliminating wasteful development practices.
Results
To evaluate the proposed framework, with the help of our industrial partner, we applied the framework to a case study to build a predictive product. The case study resulted in a predictive system to predict the risks of software license cancellations. The system was iteratively developed and evaluated while adopting the management and end-user perspectives.
Conclusion
It is concluded that, in industry, it is important to be aware of the businesses requirements before considering the application of machine learning. The framework accommodates business perspective from the beginning to produce a holistic product. From the results of the case study, we think that this framework can help businesses define the right opportunities for applying machine learning, developing solutions, evaluating the effectiveness of these solutions, and maintaining their performance in production.",https://doi.org/10.1016/j.infsof.2019.05.009,2019,Mona Nashaat and Aindrila Ghosh and James Miller and Shaikh Quader and Chad Marston,M-LEAN: AN END-TO-END DEVELOPMENT FRAMEWORK FOR PREDICTIVE MODELS IN B2B SCENARIOS,article
457,18732,INFORMATION AND SOFTWARE TECHNOLOGY,journal,09505849,"0,606",Q2,103,121,420,6853,2316,403,"5,12","56,64",Netherlands,Western Europe,Elsevier,"1970, 1987-2020",Computer Science Applications (Q2); Information Systems (Q2); Software (Q2),INFORMATION AND SOFTWARE TECHNOLOGY,INFORMATION AND SOFTWARE TECHNOLOGY,"5,172",2.730,0.00554,"Context: Big Data systems are a class of software systems that ingest, store, process and serve massive amounts of heterogeneous data, from multiple sources. Despite their undisputed impact in current society, their engineering is still in its infancy and companies find it difficult to adopt them due to their inherent complexity. Existing attempts to provide architectural guidelines for their engineering fail to take into account important Big Data characteristics, such as the management, evolution and quality of the data. Objective: In this paper, we follow software engineering principles to refine the λ-architecture, a reference model for Big Data systems, and use it as seed to create Bolster, a software reference architecture (SRA) for semantic-aware Big Data systems. Method: By including a new layer into the λ-architecture, the Semantic Layer, Bolster is capable of handling the most representative Big Data characteristics (i.e., Volume, Velocity, Variety, Variability and Veracity). Results: We present the successful implementation of Bolster in three industrial projects, involving five organizations. The validation results show high level of agreement among practitioners from all organizations with respect to standard quality factors. Conclusion: As an SRA, Bolster allows organizations to design concrete architectures tailored to their specific needs. A distinguishing feature is that it provides semantic-awareness in Big Data Systems. These are Big Data system implementations that have components to simplify data definition and exploitation. In particular, they leverage metadata (i.e., data describing data) to enable (partial) automation of data exploitation and to aid the user in their decision making processes. This simplification supports the differentiation of responsibilities into cohesive roles enhancing data governance.",https://doi.org/10.1016/j.infsof.2017.06.001,2017,Sergi Nadal and Victor Herrero and Oscar Romero and Alberto Abelló and Xavier Franch and Stijn Vansummeren and Danilo Valerio,A SOFTWARE REFERENCE ARCHITECTURE FOR SEMANTIC-AWARE BIG DATA SYSTEMS,article
458,18732,INFORMATION AND SOFTWARE TECHNOLOGY,journal,09505849,"0,606",Q2,103,121,420,6853,2316,403,"5,12","56,64",Netherlands,Western Europe,Elsevier,"1970, 1987-2020",Computer Science Applications (Q2); Information Systems (Q2); Software (Q2),INFORMATION AND SOFTWARE TECHNOLOGY,INFORMATION AND SOFTWARE TECHNOLOGY,"5,172",2.730,0.00554,"Context:
Over the last decade, Agile methods have changed the software development process in an unparalleled way and with the increasing popularity of Big Data, optimizing development cycles through data analytics is becoming a commodity.
Objective:
Although a myriad of research exists on software analytics as well as on Agile software development (ASD) practice on itself, there exists no systematic overview of the research done on ASD from a data analytics perspective. Therefore, the objective of this work is to make progress by linking ASD with Big Data analytics (BDA).
Method:
As the primary method to find relevant literature on the topic, we performed manual search and snowballing on papers published between 2011 and 2019.
Results:
In total, 88 primary studies were selected and analyzed. Our results show that BDA is employed throughout the whole ASD lifecycle. The results reveal that data-driven software development is focused on the following areas: code repository analytics, defects/bug fixing, testing, project management analytics, and application usage analytics.
Conclusions:
As BDA and ASD are fast-developing areas, improving the productivity of software development teams is one of the most important objectives BDA is facing in the industry. This study provides scholars with information about the state of software analytics research and the current trends as well as applications in the business environment. Whereas, thanks to this literature review, practitioners should be able to understand better how to obtain actionable insights from their software artifacts and on which aspects of data analytics to focus when investing in such initiatives.",https://doi.org/10.1016/j.infsof.2020.106448,2021,Katarzyna Biesialska and Xavier Franch and Victor Muntés-Mulero,BIG DATA ANALYTICS IN AGILE SOFTWARE DEVELOPMENT: A SYSTEMATIC MAPPING STUDY,article
459,12888,CHINESE JOURNAL OF CHEMICAL ENGINEERING,journal,10049541,"0,595",Q2,54,355,874,15618,2755,871,"3,16","43,99",China,Asiatic Region,Chemical Industry Press,1993-2020,Chemical Engineering (miscellaneous) (Q2); Chemistry (miscellaneous) (Q2); Environmental Engineering (Q2); Biochemistry (Q3),CHINESE JOURNAL OF CHEMICAL ENGINEERING,CHINESE JOURNAL OF CHEMICAL ENGINEERING,"6,469",3.171,0.00619,"Owing to wide applications of automatic control systems in the process industries, the impacts of controller performance on industrial processes are becoming increasingly significant. Consequently, controller maintenance is critical to guarantee routine operations of industrial processes. The workflow of controller maintenance generally involves the following steps: monitor operating controller performance and detect performance degradation, diagnose probable root causes of control system malfunctions, and take specific actions to resolve associated problems. In this article, a comprehensive overview of the mainstream of control loop monitoring and diagnosis is provided, and some existing problems are also analyzed and discussed. From the viewpoint of synthesizing abundant information in the context of big data, some prospective ideas and promising methods are outlined to potentially solve problems in industrial applications.",https://doi.org/10.1016/j.cjche.2016.05.039,2016,Xinqing Gao and Fan Yang and Chao Shang and Dexian Huang,A REVIEW OF CONTROL LOOP MONITORING AND DIAGNOSIS: PROSPECTS OF CONTROLLER MAINTENANCE IN BIG DATA ERA,article
460,12370,JOURNAL OF COMPUTER AND SYSTEM SCIENCES,journal,"10902724, 00220000","0,573",Q2,99,46,252,1626,568,245,"1,87","35,35",United States,Northern America,Academic Press Inc.,1967-2021,Applied Mathematics (Q2); Computational Theory and Mathematics (Q2); Computer Networks and Communications (Q2); Theoretical Computer Science (Q2),JOURNAL OF COMPUTER AND SYSTEM SCIENCES,JOURNAL OF COMPUTER AND SYSTEM SCIENCES,"4,516",1.023,0.00268,"It is well known that processing big graph data can be costly on Cloud. Processing big graph data introduces complex and multiple iterations that raise challenges such as parallel memory bottlenecks, deadlocks, and inefficiency. To tackle the challenges, we propose a novel technique for effectively processing big graph data on Cloud. Specifically, the big data will be compressed with its spatiotemporal features on Cloud. By exploring spatial data correlation, we partition a graph data set into clusters. In a cluster, the workload can be shared by the inference based on time series similarity. By exploiting temporal correlation, in each time series or a single graph edge, temporal data compression is conducted. A novel data driven scheduling is also developed for data processing optimisation. The experiment results demonstrate that the spatiotemporal compression and scheduling achieve significant performance gains in terms of data size and data fidelity loss.",https://doi.org/10.1016/j.jcss.2014.04.022,2014,Chi Yang and Xuyun Zhang and Changmin Zhong and Chang Liu and Jian Pei and Kotagiri Ramamohanarao and Jinjun Chen,A SPATIOTEMPORAL COMPRESSION BASED APPROACH FOR EFFICIENT BIG DATA PROCESSING ON CLOUD,article
461,12370,JOURNAL OF COMPUTER AND SYSTEM SCIENCES,journal,"10902724, 00220000","0,573",Q2,99,46,252,1626,568,245,"1,87","35,35",United States,Northern America,Academic Press Inc.,1967-2021,Applied Mathematics (Q2); Computational Theory and Mathematics (Q2); Computer Networks and Communications (Q2); Theoretical Computer Science (Q2),JOURNAL OF COMPUTER AND SYSTEM SCIENCES,JOURNAL OF COMPUTER AND SYSTEM SCIENCES,"4,516",1.023,0.00268,"Big data streaming has become an important paradigm for real-time processing of massive continuous data flows in large scale sensing networks. While dealing with big sensing data streams, a Data Stream Manager (DSM) must always verify the security (i.e. authenticity, integrity, and confidentiality) to ensure end-to-end security and maintain data quality. Existing technologies are not suitable, because real time introduces delay in data stream. In this paper, we propose a Dynamic Prime Number Based Security Verification (DPBSV) scheme for big data streams. Our scheme is based on a common shared key that updated dynamically by generating synchronized prime numbers. The common shared key updates at both ends, i.e., source sensing devices and DSM, without further communication after handshaking. Theoretical analyses and experimental results of our DPBSV scheme show that it can significantly improve the efficiency of verification process by reducing the time and utilizing a smaller buffer size in DSM.",https://doi.org/10.1016/j.jcss.2016.02.005,2017,Deepak Puthal and Surya Nepal and Rajiv Ranjan and Jinjun Chen,A DYNAMIC PRIME NUMBER BASED EFFICIENT SECURITY MECHANISM FOR BIG SENSING DATA STREAMS,article
462,5100154502,JOURNAL OF FORENSIC AND LEGAL MEDICINE,journal,1752928X,"0,569",Q1,47,134,428,3921,668,407,"1,57","29,26",United Kingdom,Western Europe,Churchill Livingstone,2007-2020,Law (Q1); Medicine (miscellaneous) (Q2); Pathology and Forensic Medicine (Q2),JOURNAL OF FORENSIC AND LEGAL MEDICINE,JOURNAL OF FORENSIC AND LEGAL MEDICINE,"2,328",1.614,0.00305,"The study of cause-specific mortality data is one of the main sources of information for public health monitoring. In most industrialized countries, when a death occurs, it is a legal requirement that a medical certificate based on the international form recommended by World Health Organization's (WHO) is filled in by a physician. The physician reports the causes of death that directly led or contributed to the death on the death certificate. The death certificate is then forwarded to a coding office, where each cause is coded, and one underlying cause is defined, using the rules of the International Classification of Diseases and Related Health Problems, now in its 10th Revision (ICD-10). Recently, a growing number of countries have adopted, or have decided to adopt, the coding software Iris, developed and maintained by an international consortium1. This whole standardized production process results in a high and constantly increasing international comparability of cause-specific mortality data. While these data could be used for international comparisons and benchmarking of global burden of diseases, quality of care and prevention policies, there are also many other ways and methods to explore their richness, especially when they are linked with other data sources. Some of these methods are potentially referring to the so-called “big data” field. These methods could be applied both to the production of the data, to the statistical processing of the data, and even more to process these data linked to other databases. In the present note, we depict the main domains in which this new field of methods could be applied. We focus specifically on the context of France, a 65 million inhabitants country with a centralized health data system. Finally we will insist on the importance of data quality, and the specific problematics related to death certification in the forensic medicine domain.",https://doi.org/10.1016/j.jflm.2016.12.004,2018,Grégoire Rey and Karim Bounebache and Claire Rondet,"CAUSES OF DEATHS DATA, LINKAGES AND BIG DATA PERSPECTIVES",article
463,5100154502,JOURNAL OF FORENSIC AND LEGAL MEDICINE,journal,1752928X,"0,569",Q1,47,134,428,3921,668,407,"1,57","29,26",United Kingdom,Western Europe,Churchill Livingstone,2007-2020,Law (Q1); Medicine (miscellaneous) (Q2); Pathology and Forensic Medicine (Q2),JOURNAL OF FORENSIC AND LEGAL MEDICINE,JOURNAL OF FORENSIC AND LEGAL MEDICINE,"2,328",1.614,0.00305,"In less than a decade, big data in medicine has become quite a phenomenon and many biomedical disciplines got their own tribune on the topic. Perspectives and debates are flourishing while there is a lack for a consensual definition for big data. The 3Vs paradigm is frequently evoked to define the big data principles and stands for Volume, Variety and Velocity. Even according to this paradigm, genuine big data studies are still scarce in medicine and may not meet all expectations. On one hand, techniques usually presented as specific to the big data such as machine learning techniques are supposed to support the ambition of personalized, predictive and preventive medicines. These techniques are mostly far from been new and are more than 50 years old for the most ancient. On the other hand, several issues closely related to the properties of big data and inherited from other scientific fields such as artificial intelligence are often underestimated if not ignored. Besides, a few papers temper the almost unanimous big data enthusiasm and are worth attention since they delineate what is at stakes. In this context, forensic science is still awaiting for its position papers as well as for a comprehensive outline of what kind of contribution big data could bring to the field. The present situation calls for definitions and actions to rationally guide research and practice in big data. It is an opportunity for grounding a true interdisciplinary approach in forensic science and medicine that is mainly based on evidence.",https://doi.org/10.1016/j.jflm.2017.08.001,2018,Thomas Lefèvre,BIG DATA IN FORENSIC SCIENCE AND MEDICINE,article
464,21100356018,BIG DATA RESEARCH,journal,22145796,"0,565",Q2,25,11,76,547,324,69,"4,06","49,73",United States,Northern America,Elsevier Inc.,2014-2020,Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2),BIG DATA RESEARCH,BIG DATA RESEARCH,586,3.578,0.00126,"This paper presents the approach to Big Data Analytics (BDA) developed in the SIBDA (Sistema Innovativo Big Data Analytics) Project. The project aim is to study and develop innovative solutions in the field of BDA for three companies cooperating in a temporary association of enterprises. We discuss elements of Big Data tackled in the project, namely document processing, mass e-mail applications and Internet of Things sensor networks, to be integrated into a shared platform of common assets and services for the three cooperating companies. We comment about the “Big Data Journey” status in Italy reported by Osservatorio Politecnico di Milano. Then, the paper presents the SIBDA project approach and requirements, outlines the adopted architecture and provides implementation hints, along with some experiments and considerations on the use of the proposed architecture for Smart Cities and Smart Enterprises and Communities.",https://doi.org/10.1016/j.bdr.2021.100192,2021,Mariagrazia Fugini and Jacopo Finocchi and Paolo Locatelli,A BIG DATA ANALYTICS ARCHITECTURE FOR SMART CITIES AND SMART COMPANIES,article
465,21100356018,BIG DATA RESEARCH,journal,22145796,"0,565",Q2,25,11,76,547,324,69,"4,06","49,73",United States,Northern America,Elsevier Inc.,2014-2020,Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2),BIG DATA RESEARCH,BIG DATA RESEARCH,586,3.578,0.00126,"Research in Big Data and analytics offers tremendous opportunities to utilize evidence in making decisions in many application domains. To what extent can the paradigms of Big Data and analytics be used in the domain of transport? This article reports on an outcome of a systematic review of published articles in the last five years that discuss Big Data concepts and applications in the transportation domain. The goal is to explore and understand the current research, opportunities, and challenges relating to the utilization of Big Data and analytics in transportation. The review shows the potential of Big Data and analytics to garner insights and improve transportation systems through the analysis of various forms of data obtained from traffic monitoring systems, connected vehicles, crowdsourcing, and social media. We discuss some platforms and software architecture for the transport domain, along with a wide array of storage, processing, and analytical techniques, and describe challenges associated with the implementation of Big Data and analytics. This review contributes broadly to the various ways in which cities can utilize Big Data in transportation to guide the creation of sustainable and safer traffic systems. Since research in Big Data and transportation is, by and large, at infancy, this article does not prescribe recommendations to the various challenges identified, which also constitutes the limitation of the article.",https://doi.org/10.1016/j.bdr.2019.03.001,2019,Alex Neilson and  Indratmo and Ben Daniel and Stevanus Tjandra,SYSTEMATIC REVIEW OF THE LITERATURE ON BIG DATA IN THE TRANSPORTATION DOMAIN: CONCEPTS AND APPLICATIONS,article
466,21100356018,BIG DATA RESEARCH,journal,22145796,"0,565",Q2,25,11,76,547,324,69,"4,06","49,73",United States,Northern America,Elsevier Inc.,2014-2020,Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2),BIG DATA RESEARCH,BIG DATA RESEARCH,586,3.578,0.00126,"The management of the exponential growth of data that Next Generation Sequencing techniques produce has become a challenge for researchers that are forced to delve into an ocean of complex data in order to extract new insights to unravel the secrets of human diseases. Initially, this can be faced as a Big Data-related problem, but the genomic data have particular and relevant challenges that make them different from other Big Data working domains. Genomic data are much more heterogeneous; they are spread in hundreds of repositories, represented in multiple formats, and have different levels of quality. In addition, getting meaningful conclusions from genomic data requires considering all of the relevant surrounding knowledge that is under continuous evolution. In this scenario, the precise identification of what makes Genome Data Management so different is essential in order to provide effective Big Data-based solutions. Genomic projects require dealing with the technological problems associated with data management, nomenclature standards, and quality issues that only robust Information Systems that use Big Data techniques can provide. The main contribution of this paper is to present a Big Data-driven approach for managing genomic data, that is adapted to the particularities of the domain and to show its applicability to improve genetic diagnoses, which is the core of the development of accurate Precision Medicine.",https://doi.org/10.1016/j.bdr.2021.100253,2021,Ana León and Óscar Pastor,ENHANCING PRECISION MEDICINE: A BIG DATA-DRIVEN APPROACH FOR THE MANAGEMENT OF GENOMIC DATA,article
467,21100356018,BIG DATA RESEARCH,journal,22145796,"0,565",Q2,25,11,76,547,324,69,"4,06","49,73",United States,Northern America,Elsevier Inc.,2014-2020,Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2),BIG DATA RESEARCH,BIG DATA RESEARCH,586,3.578,0.00126,"With the boom in Internet techniques and computer science, a variety of big data have been introduced into forecasting research, bringing new knowledge and improving prediction models. This paper is the first attempt to conduct a literature review on full-scale big data in forecasting research. By source, big data in forecasting research fell into user-generated content data (from the users on social media in texts, photos, etc.), device-monitored data (by meteorological monitors, smart meters, GPS, etc.) and activity log data (for web searching/visiting, online/offline marketing, clinical treatments, laboratory experiments, etc.). Different data types, bearing distinctive information and characteristics, dominated different forecasting tasks, required different analysis technologies and improved different forecasting models. This survey provides an overall review of big data-based forecasting research, details what (regarding data types and sources), where (forecasting hotspots) and how (analysis and forecasting methods used) big data improved prediction, and offers insights into future prospects.",https://doi.org/10.1016/j.bdr.2021.100289,2022,Ling Tang and Jieyi Li and Hongchuan Du and Ling Li and Jun Wu and Shouyang Wang,BIG DATA IN FORECASTING RESEARCH: A LITERATURE REVIEW,article
468,21100356018,BIG DATA RESEARCH,journal,22145796,"0,565",Q2,25,11,76,547,324,69,"4,06","49,73",United States,Northern America,Elsevier Inc.,2014-2020,Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2),BIG DATA RESEARCH,BIG DATA RESEARCH,586,3.578,0.00126,"Renal failure is a fatal disease raising global concerns. Previous risk models for renal failure mostly rely on the diagnosis of chronic kidney disease, which lacks obvious clinical symptoms and thus is mostly undiagnosed, causing significant omission of high-risk patients. In this paper, we proposed a framework to predict the risk of renal failure directly from a big data repository of chronic disease population without prerequisite diagnosis of chronic kidney disease. The electronic health records of 42,256 patients with hypertension or diabetes in Shenzhen Health Information Big Data Platform were collected, with 398 suffered from renal failure during a 3-year follow-up. Five state-of-the-art machine learning methods are utilized to build risk prediction models of renal failure for chronic disease population. Extensive experimental results show that the proposed framework achieves quite well performance. Particularly, the XGBoost obtains the best performance with an area under receiving-operating-characteristics curve (AUC) of 0.9139. By analyzing the effect of risk factors, we identified that serum creatine, age, urine acid, systolic blood pressure, and blood urea nitrogen are the top five factors associated with renal failure risk. Compared with existing models, our model can be deployed into routine chronic disease management procedures and enable more preemptive, widely-covered screening of renal risks, which would in turn reduce the damage caused by the disease through timely intervention.",https://doi.org/10.1016/j.bdr.2021.100234,2021,Yujie Yang and Ye Li and Runge Chen and Jing Zheng and Yunpeng Cai and Giancarlo Fortino,RISK PREDICTION OF RENAL FAILURE FOR CHRONIC DISEASE POPULATION BASED ON ELECTRONIC HEALTH RECORD BIG DATA,article
469,21100356018,BIG DATA RESEARCH,journal,22145796,"0,565",Q2,25,11,76,547,324,69,"4,06","49,73",United States,Northern America,Elsevier Inc.,2014-2020,Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2),BIG DATA RESEARCH,BIG DATA RESEARCH,586,3.578,0.00126,"Production lines in pharmaceutical manufacturing generate numerous heterogeneous data sets from various embedded systems which control the multiple processes of medicine production. Such data sets should arguably ensure end-to-end traceability and data integrity in order to release a medicine batch, which is uniquely identified and tracked by its batch number/code. Consequently, auditable computerised systems are crucial on pharmaceutical production lines, since the industry is becoming increasingly regulated for product quality and patient health purposes. This paper describes the EU-funded SPuMoNI project, which aims to ensure the quality of large amounts of data produced by computerised production systems in representative pharmaceutical environments. Our initial results include significant progress in: (i) end-to-end verification taking advantage of blockchain properties and smart contracts to ensure data authenticity, transparency, and immutability; (ii) data quality assessment models to identify data behavioural patterns that can violate industry practices and/or international regulations; and (iii) intelligent agents to collect and manipulate data as well as perform smart decisions. By analysing multiple sensors in medicine production lines, manufacturing work centres, and quality control laboratories, our approach has been initially evaluated using representative industry-grade pharmaceutical manufacturing data sets generated at an IT environment with regulated processes inspected by regulatory and government agencies.",https://doi.org/10.1016/j.bdr.2020.100172,2021,Fátima Leal and Adriana E. Chis and Simon Caton and Horacio González–Vélez and Juan M. García–Gómez and Marta Durá and Angel Sánchez–García and Carlos Sáez and Anthony Karageorgos and Vassilis C. Gerogiannis and Apostolos Xenakis and Efthymios Lallas and Theodoros Ntounas and Eleni Vasileiou and Georgios Mountzouris and Barbara Otti and Penelope Pucci and Rossano Papini and David Cerrai and Mariola Mier,SMART PHARMACEUTICAL MANUFACTURING: ENSURING END-TO-END TRACEABILITY AND DATA INTEGRITY IN MEDICINE PRODUCTION,article
470,21100356018,BIG DATA RESEARCH,journal,22145796,"0,565",Q2,25,11,76,547,324,69,"4,06","49,73",United States,Northern America,Elsevier Inc.,2014-2020,Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2),BIG DATA RESEARCH,BIG DATA RESEARCH,586,3.578,0.00126,"We are living in a world where everything computes, everyone and everything is connected and sharing data. Going beyond just capturing and managing data, enterprises are tapping into IoT and Artificial Intelligence (AI) to create insights and intelligence in a revolutionary way that was not possible before. For instance, by analyzing unstructured data (such as text), call centers can extract entities, concepts, themes which can enable them to get faster insights that only few years back was not feasible. Public safety and law enforcement are only few of the examples that benefit from text analytics used to strengthen crime investigation. Sentiment Analysis, Content Classification, Language Detection and Intent Detection are just some of the Text Classification applications. The overall process model of such applications considering the complexity of the unstructured data, can be definitely challenging. In response to the chaotic emerging science of unstructured data analysis, the main goal of this paper is to first contribute to the gap of no existing methodology approach for Text Analytics projects, by introducing a methodology approach based on one of the most widely accepted and used methodology approach of CRISP-DM.",https://doi.org/10.1016/j.bdr.2021.100274,2022,Christina G. Skarpathiotaki and Konstantinos E. Psannis,CROSS-INDUSTRY PROCESS STANDARDIZATION FOR TEXT ANALYTICS,article
471,21100356018,BIG DATA RESEARCH,journal,22145796,"0,565",Q2,25,11,76,547,324,69,"4,06","49,73",United States,Northern America,Elsevier Inc.,2014-2020,Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2),BIG DATA RESEARCH,BIG DATA RESEARCH,586,3.578,0.00126,"In a fast growing big data era, volume and varieties of data processed in Internet applications drastically increase. Real-world search engines commonly use text classifiers with thousands of classes to improve relevance or data quality. These large scale classification problems lead to severe runtime performance challenges, so practitioners often resort to fast approximation techniques. However, the increase in classification speed comes at a cost, as approximations are lossy, mis-assigning classes relative to the original reference classification algorithm. To address this problem, we introduce a Lossless Pruned Naive Bayes (LPNB) classification algorithm tailored to real-world, big data applications with thousands of classes. LPNB achieves significant speed-ups by drawing on Information Retrieval (IR) techniques for efficient posting list traversal and pruning. We show empirically that LPNB can classify text up to eleven times faster than standard Naive Bayes on a real-world data set with 7205 classes, with larger gains extrapolated for larger taxonomies. In practice, the achieved acceleration is significant as it can greatly cut required computation time. In addition, it is lossless: the output is identical to standard Naive Bayes, in contrast to extant techniques such as hierarchical classification. The acceleration does not rely on the taxonomy structure, and it can be used for both hierarchical and flat taxonomies.",https://doi.org/10.1016/j.bdr.2018.05.007,2018,Nanfei Sun and Bingjun Sun and Jian (Denny) Lin and Michael Yu-Chi Wu,LOSSLESS PRUNED NAIVE BAYES FOR BIG DATA CLASSIFICATIONS,article
472,21100356018,BIG DATA RESEARCH,journal,22145796,"0,565",Q2,25,11,76,547,324,69,"4,06","49,73",United States,Northern America,Elsevier Inc.,2014-2020,Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2),BIG DATA RESEARCH,BIG DATA RESEARCH,586,3.578,0.00126,,https://doi.org/10.1016/j.bdr.2019.100123,2019,Nikos Bikakis and George Papastefanatos and Olga Papaemmanouil,"BIG DATA EXPLORATION, VISUALIZATION AND ANALYTICS",article
473,21100356018,BIG DATA RESEARCH,journal,22145796,"0,565",Q2,25,11,76,547,324,69,"4,06","49,73",United States,Northern America,Elsevier Inc.,2014-2020,Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2),BIG DATA RESEARCH,BIG DATA RESEARCH,586,3.578,0.00126,"This paper presents a system that employs information visualization techniques to analyze urban traffic data and the impact of traffic emissions on urban air quality. Effective visualizations allow citizens and public authorities to identify trends, detect congested road sections at specific times, and perform monitoring and maintenance of traffic sensors. Since road transport is a major source of air pollution, also the impact of traffic on air quality has emerged as a new issue that traffic visualizations should address. Trafair Traffic Dashboard exploits traffic sensor data and traffic flow simulations to create an interactive layout focused on investigating the evolution of traffic in the urban area over time and space. The dashboard is the last step of a complex data framework that starts from the ingestion of traffic sensor observations, anomaly detection, traffic modeling, and also air quality impact analysis. We present the results of applying our proposed framework on two cities (Modena, in Italy, and Santiago de Compostela, in Spain) demonstrating the potential of the dashboard in identifying trends, seasonal events, abnormal behaviors, and understanding how urban vehicle fleet affects air quality. We believe that the framework provides a powerful environment that may guide the public decision-makers through effective analysis of traffic trends devoted to reducing traffic issues and mitigating the polluting effect of transportation.",https://doi.org/10.1016/j.bdr.2021.100292,2022,Chiara Bachechi and Laura Po and Federica Rollo,BIG DATA ANALYTICS AND VISUALIZATION IN TRAFFIC MONITORING,article
474,21100356018,BIG DATA RESEARCH,journal,22145796,"0,565",Q2,25,11,76,547,324,69,"4,06","49,73",United States,Northern America,Elsevier Inc.,2014-2020,Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2),BIG DATA RESEARCH,BIG DATA RESEARCH,586,3.578,0.00126,"In recent years, the rapid development of Internet, Internet of Things, and Cloud Computing have led to the explosive growth of data in almost every industry and business area. Big data has rapidly developed into a hot topic that attracts extensive attention from academia, industry, and governments around the world. In this position paper, we first briefly introduce the concept of big data, including its definition, features, and value. We then identify from different perspectives the significance and opportunities that big data brings to us. Next, we present representative big data initiatives all over the world. We describe the grand challenges (namely, data complexity, computational complexity, and system complexity), as well as possible solutions to address these challenges. Finally, we conclude the paper by presenting several suggestions on carrying out big data projects.",https://doi.org/10.1016/j.bdr.2015.01.006,2015,Xiaolong Jin and Benjamin W. Wah and Xueqi Cheng and Yuanzhuo Wang,SIGNIFICANCE AND CHALLENGES OF BIG DATA RESEARCH,article
475,21100356018,BIG DATA RESEARCH,journal,22145796,"0,565",Q2,25,11,76,547,324,69,"4,06","49,73",United States,Northern America,Elsevier Inc.,2014-2020,Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2),BIG DATA RESEARCH,BIG DATA RESEARCH,586,3.578,0.00126,,https://doi.org/10.1016/j.bdr.2021.100244,2021,Lars Lundberg and Håkan Grahn and Valeria Cardellini and Andreas Polze and Sogand Shirinbab,EDITORIAL TO THE SPECIAL ISSUE ON BIG DATA IN INDUSTRIAL AND COMMERCIAL APPLICATIONS,article
476,21100356018,BIG DATA RESEARCH,journal,22145796,"0,565",Q2,25,11,76,547,324,69,"4,06","49,73",United States,Northern America,Elsevier Inc.,2014-2020,Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2),BIG DATA RESEARCH,BIG DATA RESEARCH,586,3.578,0.00126,"The increasing presence of geo-distributed sensor networks implies the generation of huge volumes of data from multiple geographical locations at an increasing rate. This raises important issues which become more challenging when the final goal is that of the analysis of the data for forecasting purposes or, more generally, for predictive tasks. This paper proposes a framework which supports predictive modeling tasks from streaming data coming from multiple geo-referenced sensors. In particular, we propose a distance-based anomaly detection strategy which considers objects described by embedding features learned via a stacked auto-encoder. We then devise a repair strategy which repairs the data detected as anomalous exploiting non-anomalous data measured by sensors in nearby spatial locations. Subsequently, we adopt Gradient Boosted Trees (GBTs) to predict/forecast values assumed by a target variable of interest for the repaired newly arriving (unlabeled) data, using the original feature representation or the embedding feature representation learned via the stacked auto-encoder. The workflow is implemented with distributed Apache Spark programming primitives and tested on a cluster environment. We perform experiments to assess the performance of each module, separately and in a combined manner, considering the predictive modeling of one-day-ahead energy production, for multiple renewable energy sites. Accuracy results show that the proposed framework allows reducing the error up to 13.56%. Moreover, scalability results demonstrate the efficiency of the proposed framework in terms of speedup, scaleup and execution time under a stress test.",https://doi.org/10.1016/j.bdr.2019.04.001,2019,Roberto Corizzo and Michelangelo Ceci and Nathalie Japkowicz,ANOMALY DETECTION AND REPAIR FOR ACCURATE PREDICTIONS IN GEO-DISTRIBUTED BIG DATA,article
477,21100356018,BIG DATA RESEARCH,journal,22145796,"0,565",Q2,25,11,76,547,324,69,"4,06","49,73",United States,Northern America,Elsevier Inc.,2014-2020,Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2),BIG DATA RESEARCH,BIG DATA RESEARCH,586,3.578,0.00126,"The emergence of new technologies such as Internet/Web/Network-of-Things and large scale wireless sensor systems enables the collection of data from an increasing volume and variety of networked sensors for analysis. In this review article, we summarize the latest developments of big sensor data systems (a term to conceptualize the application of the big data model towards networked sensor systems) in various representative studies for urban environments, including for air pollution monitoring, assistive living, disaster management systems, and intelligent transportation. An important focus is the inclusion of how value is extracted from the big data system. We also discuss some recent techniques for big data acquisition, cleaning, aggregation, modeling, and interpretation in large scale sensor-based systems. We conclude the paper with a discussion on future perspectives and challenges of sensor-based data systems in the big data era.",https://doi.org/10.1016/j.bdr.2015.12.003,2016,Li-Minn Ang and Kah Phooi Seng,BIG SENSOR DATA APPLICATIONS IN URBAN ENVIRONMENTS,article
478,21100356018,BIG DATA RESEARCH,journal,22145796,"0,565",Q2,25,11,76,547,324,69,"4,06","49,73",United States,Northern America,Elsevier Inc.,2014-2020,Computer Science Applications (Q2); Information Systems (Q2); Information Systems and Management (Q2); Management Information Systems (Q2),BIG DATA RESEARCH,BIG DATA RESEARCH,586,3.578,0.00126,"Data science has employed great research efforts in developing advanced analytics, improving data models and cultivating new algorithms. However, not many authors have come across the organizational and socio-technical challenges that arise when executing a data science project: lack of vision and clear objectives, a biased emphasis on technical issues, a low level of maturity for ad-hoc projects and the ambiguity of roles in data science are among these challenges. Few methodologies have been proposed on the literature that tackle these type of challenges, some of them date back to the mid-1990, and consequently they are not updated to the current paradigm and the latest developments in big data and machine learning technologies. In addition, fewer methodologies offer a complete guideline across team, project and data & information management. In this article we would like to explore the necessity of developing a more holistic approach for carrying out data science projects. We first review methodologies that have been presented on the literature to work on data science projects and classify them according to the their focus: project, team, data and information management. Finally, we propose a conceptual framework containing general characteristics that a methodology for managing data science projects with a holistic point of view should have. This framework can be used by other researchers as a roadmap for the design of new data science methodologies or the updating of existing ones.",https://doi.org/10.1016/j.bdr.2020.100183,2021,Iñigo Martinez and Elisabeth Viles and Igor {G. Olaizola},DATA SCIENCE METHODOLOGIES: CURRENT CHALLENGES AND FUTURE APPROACHES,article
479,26602,EVALUATION AND PROGRAM PLANNING,journal,"01497189, 18737870","0,555",Q2,62,107,353,5140,753,350,"1,93","48,04",United Kingdom,Western Europe,Elsevier Ltd.,1978-2020,"Business and International Management (Q2); Geography, Planning and Development (Q2); Public Health, Environmental and Occupational Health (Q2); Social Psychology (Q2); Strategy and Management (Q2)",EVALUATION AND PROGRAM PLANNING,EVALUATION AND PROGRAM PLANNING,"3,211",1.849,0.0031,"Data to inform and improve health care systems in low- and middle-income countries (LMICs) has been facilitated by the development of monitoring and evaluation (M&E) systems. The drivers of change in M&E systems over the last 50 years have included a series of health concerns that have animated global donors (e.g., family planning, vaccination campaigns, and HIV/AIDS); the data requirements of donors; improved national economies enabling LMICs to invest more in M&E systems; and rapid advances in digital technologies. Progress has included the training and expansion of an M&E workforce, the creation of systems for data collection and use, and processes for assessing and ensuring data quality. Controversies have included the development of disease-specific systems that do not coordinate with each other, and a growing burden on health care deliverers to collect data for a proliferating number of health and process indicators. Digital technologies offer the promise of real time data and quick adaptation but also raise ethical and privacy concerns. The desire for speed can cast large-scale evaluations, considered by some to be the gold standard, in an unfavorable light as slow and expensive. Accordingly, there is a growing demand for speedy evaluations that rely on routine health information systems and privately collected “big data” from electronic health records and social media.",https://doi.org/10.1016/j.evalprogplan.2021.101994,2021,James C. Thomas and Kathy Doherty and Stephanie Watson-Grant and Manish Kumar,ADVANCES IN MONITORING AND EVALUATION IN LOW- AND MIDDLE-INCOME COUNTRIES,article
480,12189,SIMULATION MODELLING PRACTICE AND THEORY,journal,1569190X,"0,554",Q2,69,132,316,5805,1227,312,"4,02","43,98",Netherlands,Western Europe,Elsevier,2002-2021,Hardware and Architecture (Q2); Modeling and Simulation (Q2); Software (Q2),SIMULATION MODELLING PRACTICE AND THEORY,SIMULATION MODELLING PRACTICE AND THEORY,"3,547",3.272,0.00315,"Simulation stands out as an appropriate method for the Supply Chain Management (SCM) field. Nevertheless, to produce accurate simulations of Supply Chains (SCs), several business processes must be considered. Thus, when using real data in these simulation models, Big Data concepts and technologies become necessary, as the involved data sources generate data at increasing volume, velocity and variety, in what is known as a Big Data context. While developing such solution, several data issues were found, with simulation proving to be more efficient than traditional data profiling techniques in identifying them. Thus, this paper proposes the use of simulation as a semantic validator of the data, proposed a classification for such issues and quantified their impact in the volume of data used in the final achieved solution. This paper concluded that, while SC simulations using Big Data concepts and technologies are within the grasp of organizations, their data models still require considerable improvements, in order to produce perfect mimics of their SCs. In fact, it was also found that simulation can help in identifying and bypassing some of these issues.",https://doi.org/10.1016/j.simpat.2019.101985,2020,António AC Vieira and Luís MS Dias and Maribel Y Santos and Guilherme AB Pereira and José A Oliveira,ON THE USE OF SIMULATION AS A BIG DATA SEMANTIC VALIDATOR FOR SUPPLY CHAIN MANAGEMENT,article
481,12305,INFORMATION SYSTEMS,journal,03064379,"0,547",Q2,85,100,271,4739,1027,255,"3,39","47,39",United Kingdom,Western Europe,Elsevier Ltd.,1975-2021,Hardware and Architecture (Q2); Information Systems (Q2); Software (Q2),INFORMATION SYSTEMS,INFORMATION SYSTEMS,"2,604",2.309,0.00331,"Nowadays, social network data of ever increasing size is gathered, stored and analyzed by researchers from a range of disciplines. This data is often automatically gathered from API’s, websites or existing databases. As a result, the quality of this data is typically not manually validated, and the resulting social networks may be based on false, biased or incomplete data. In this paper, we investigate the effect of data quality issues on the analysis of large networks. We focus on the global board interlock network, in which nodes represent firms across the globe, and edges model social ties between firms – shared board members holding a position at both firms. First, we demonstrate how we can automatically assess the completeness of a large dataset of 160 million firms, in which data is missing not at random. Second, we present a novel method to increase the accuracy of the entries in our data. By comparing the expected and empirical characteristics of the resulting network topology, we develop a technique that automatically prunes and merges duplicate nodes and edges. Third, we use a case study of the board interlock network of Sweden to show how poor quality data results in distorted network topologies, incorrect community division, biased centrality values and abnormal influence spread under a well-known diffusion model. Finally, we demonstrate how the proposed data quality assessment methods help restore the network structure, ultimately allowing us to derive meaningful and correct results from the analysis of the network.",https://doi.org/10.1016/j.is.2017.10.005,2018,Javier Garcia-Bernardo and Frank W. Takes,THE EFFECTS OF DATA QUALITY ON THE ANALYSIS OF CORPORATE BOARD INTERLOCK NETWORKS,article
482,12305,INFORMATION SYSTEMS,journal,03064379,"0,547",Q2,85,100,271,4739,1027,255,"3,39","47,39",United Kingdom,Western Europe,Elsevier Ltd.,1975-2021,Hardware and Architecture (Q2); Information Systems (Q2); Software (Q2),INFORMATION SYSTEMS,INFORMATION SYSTEMS,"2,604",2.309,0.00331,"In the recent years the problems of using generic storage (i.e., relational) techniques for very specific applications have been detected and outlined and, as a consequence, some alternatives to Relational DBMSs (e.g., HBase) have bloomed. Most of these alternatives sit on the cloud and benefit from cloud computing, which is nowadays a reality that helps us to save money by eliminating the hardware as well as software fixed costs and just pay per use. On top of this, specific querying frameworks to exploit the brute force in the cloud (e.g., MapReduce) have also been devised. The question arising next tries to clear out if this (rather naive) exploitation of the cloud is an alternative to tuning DBMSs or it still makes sense to consider other options when retrieving data from these settings. In this paper, we study the feasibility of solving OLAP queries with Hadoop (the Apache project implementing MapReduce) while benefiting from secondary indexes and partitioning in HBase. Our main contribution is the comparison of different access plans and the definition of criteria (i.e., cost estimation) to choose among them in terms of consumed resources (namely CPU, bandwidth and I/O).",https://doi.org/10.1016/j.is.2014.09.005,2015,Oscar Romero and Victor Herrero and Alberto Abelló and Jaume Ferrarons,TUNING SMALL ANALYTICS ON BIG DATA: DATA PARTITIONING AND SECONDARY INDEXES IN THE HADOOP ECOSYSTEM,article
483,12305,INFORMATION SYSTEMS,journal,03064379,"0,547",Q2,85,100,271,4739,1027,255,"3,39","47,39",United Kingdom,Western Europe,Elsevier Ltd.,1975-2021,Hardware and Architecture (Q2); Information Systems (Q2); Software (Q2),INFORMATION SYSTEMS,INFORMATION SYSTEMS,"2,604",2.309,0.00331,"Our work is motivated by the fact that there is an increasing need to perform complex analytics jobs over streaming data as close to the edge devices as possible and, in parallel, it is important that data quality is considered as an optimization objective along with performance metrics. In this work, we develop a solution that trades latency for an increased fraction of incoming data, for which data quality-related measurements and operations are performed, in jobs running over geo-distributed heterogeneous and constrained resources. Our solution is hybrid: on the one hand, we perform search heuristics over locally optimal partial solutions to yield an enhanced global solution regarding task allocations; on the other hand, we employ a spring relaxation algorithm to avoid unnecessarily increased degree of partitioned parallelism. Through thorough experiments, we show that we can improve upon state-of-the-art solutions in terms of our objective function that combines latency and extent of quality checks by up to 2.56X. Moreover, we implement our solution within Apache Storm, and we perform experiments in an emulated setting. The results show that we can reduce the latency in 86.9% of the cases examined, while latency is up to 8 times lower compared to the built-in Storm scheduler, with the average latency reduction being 52.5%.",https://doi.org/10.1016/j.is.2021.101953,2022,Anna-Valentini Michailidou and Anastasios Gounaris and Moysis Symeonides and Demetris Trihinas,EQUALITY: QUALITY-AWARE INTENSIVE ANALYTICS ON THE EDGE,article
484,12305,INFORMATION SYSTEMS,journal,03064379,"0,547",Q2,85,100,271,4739,1027,255,"3,39","47,39",United Kingdom,Western Europe,Elsevier Ltd.,1975-2021,Hardware and Architecture (Q2); Information Systems (Q2); Software (Q2),INFORMATION SYSTEMS,INFORMATION SYSTEMS,"2,604",2.309,0.00331,"The efficiency and effectiveness of business processes are usually evaluated by Process Performance Indicators (PPIs), which are computed using process event logs. PPIs can be insightful only when they are measurable, i.e., reliable. This paper proposes to define PPI measurability on the basis of the quality of the data in the process logs. Then, based on this definition, a framework for PPI measurability assessment and improvement is presented. For the assessment, we propose novel definitions of PPI accuracy, completeness, consistency, timeliness and volume that contextualise the traditional definitions in the data quality literature to the case of process logs. For the improvement, we define a set of guidelines for improving the measurability of a PPI. These guidelines may concern improving existing event logs, for instance through data imputation, implementation or enhancement of the process monitoring systems, or updating the PPI definitions. A case study in a large-sized institution is discussed to show the feasibility and the practical value of the proposed framework.",https://doi.org/10.1016/j.is.2021.101874,2022,Cinzia Cappiello and Marco Comuzzi and Pierluigi Plebani and Matheus Fim,ASSESSING AND IMPROVING MEASURABILITY OF PROCESS PERFORMANCE INDICATORS BASED ON QUALITY OF LOGS,article
485,12305,INFORMATION SYSTEMS,journal,03064379,"0,547",Q2,85,100,271,4739,1027,255,"3,39","47,39",United Kingdom,Western Europe,Elsevier Ltd.,1975-2021,Hardware and Architecture (Q2); Information Systems (Q2); Software (Q2),INFORMATION SYSTEMS,INFORMATION SYSTEMS,"2,604",2.309,0.00331,"Cyber-physical systems (CPSs) are integrated systems engineered to combine computational control algorithms and physical components such as sensors and actuators, effectively using an embedded communication core. Smart cities can be viewed as large-scale, heterogeneous CPSs that utilise technologies like the Internet of Things (IoT), surveillance, social media, and others to make informed decisions and drive the innovations of automation in urban areas. Such systems incorporate multiple layers and complex structure of hardware, software, analytical algorithms, business knowledge and communication networks, and operate under noisy and dynamic conditions. Thus, large-scale CPSs are vulnerable to enormous technical and operational challenges that may compromise the quality of data of their applications and accordingly reduce the quality of their services. This paper presents a systematic literature review to investigate data quality challenges in smart-cities large-scale CPSs and to identify the most common techniques used to address these challenges. This systematic literature review showed that significant work had been conducted to address data quality management challenges in smart cities, large-scale CPS applications. However, still, more is required to provide a practical, comprehensive data quality management solution to detect errors in sensor nodes’ measurements associated with the main data quality dimensions of accuracy, timeliness, completeness, and consistency. No systematic or generic approach was demonstrated for detecting sensor nodes and sensor node networks failures in large-scale CPS applications. Moreover, further research is required to address the challenges of ensuring the quality of the spatial and temporal contextual attributes of sensor nodes’ observations.",https://doi.org/10.1016/j.is.2021.101951,2022,Ahmed Abdulhasan Alwan and Mihaela Anca Ciupala and Allan J. Brimicombe and Seyed Ali Ghorashi and Andres Baravalle and Paolo Falcarin,DATA QUALITY CHALLENGES IN LARGE-SCALE CYBER-PHYSICAL SYSTEMS: A SYSTEMATIC REVIEW,article
486,12305,INFORMATION SYSTEMS,journal,03064379,"0,547",Q2,85,100,271,4739,1027,255,"3,39","47,39",United Kingdom,Western Europe,Elsevier Ltd.,1975-2021,Hardware and Architecture (Q2); Information Systems (Q2); Software (Q2),INFORMATION SYSTEMS,INFORMATION SYSTEMS,"2,604",2.309,0.00331,"Cloud computing is a powerful technology to perform massive-scale and complex computing. It eliminates the need to maintain expensive computing hardware, dedicated space, and software. Massive growth in the scale of data or big data generated through cloud computing has been observed. Addressing big data is a challenging and time-demanding task that requires a large computational infrastructure to ensure successful data processing and analysis. The rise of big data in cloud computing is reviewed in this study. The definition, characteristics, and classification of big data along with some discussions on cloud computing are introduced. The relationship between big data and cloud computing, big data storage systems, and Hadoop technology are also discussed. Furthermore, research challenges are investigated, with focus on scalability, availability, data integrity, data transformation, data quality, data heterogeneity, privacy, legal and regulatory issues, and governance. Lastly, open research issues that require substantial research efforts are summarized.",https://doi.org/10.1016/j.is.2014.07.006,2015,Ibrahim Abaker Targio Hashem and Ibrar Yaqoob and Nor Badrul Anuar and Salimah Mokhtar and Abdullah Gani and Samee {Ullah Khan},THE RISE OF “BIG DATA” ON CLOUD COMPUTING: REVIEW AND OPEN RESEARCH ISSUES,article
487,12305,INFORMATION SYSTEMS,journal,03064379,"0,547",Q2,85,100,271,4739,1027,255,"3,39","47,39",United Kingdom,Western Europe,Elsevier Ltd.,1975-2021,Hardware and Architecture (Q2); Information Systems (Q2); Software (Q2),INFORMATION SYSTEMS,INFORMATION SYSTEMS,"2,604",2.309,0.00331,"Data quality evaluation is built upon data quality measurement results. “Data quality evaluation” uses the “data quality rules” representing the risk appetite of the organization to decide on the usability of the data; “data quality measurement” uses the business rules describing the “data requirements” or “data specifications” to determine the validity of the data. Consequently, to conduct meaningful and useful data quality evaluations, business rules must be first completely identified and captured at the beginning of the evaluation to perform sound measurements. We propose that the evaluation leads to better and more interpretable and useful results when the potential contribution of these business rules to the measurement of the data quality characteristics is first evaluated, avoiding the inclusion in the evaluation of those not having potential contribution and the resulting waste of resources. Considering this, we feel that for a better management of business rules for data quality evaluation, it makes sense to group all business rules having an important contribution to the evaluation of data quality characteristics, something that other business rules management methodologies have not covered yet. Through our experiences in conducting industrial projects of data quality evaluations we identified six problems when collecting and grouping the business rules. These problems make data quality evaluation processes less efficient and more costly. The main contribution of this paper is a methodology to systematically collect, group and validate the business rules to avoid or to alleviate these problems. For the sake of generalization, comparability, and reusability, we propose to do the grouping for data quality characteristics and properties defined in ISO/IEC 25012 and ISO/IEC 25024, respectively. Lastly, we validate the methodology in three case studies of real projects. From this validation, it is possible to raise the conclusion that the methodology is useful, applicable in the real world, and valid to capture and group the business rules used as a basis for data quality evaluation.",https://doi.org/10.1016/j.is.2022.102058,2022,Ismael Caballero and Fernando Gualo and Moisés Rodríguez and Mario Piattini,BR4DQ: A METHODOLOGY FOR GROUPING BUSINESS RULES FOR DATA QUALITY EVALUATION,article
488,12305,INFORMATION SYSTEMS,journal,03064379,"0,547",Q2,85,100,271,4739,1027,255,"3,39","47,39",United Kingdom,Western Europe,Elsevier Ltd.,1975-2021,Hardware and Architecture (Q2); Information Systems (Q2); Software (Q2),INFORMATION SYSTEMS,INFORMATION SYSTEMS,"2,604",2.309,0.00331,"The production of analytic datasets is a significant big data trend and has gone well beyond the scope of traditional IT-governed dataset development. Analytic datasets are now created by data scientists and data analysts using big data frameworks and agile data preparation tools. However, despite the profusion of available datasets, it remains quite difficult for a data analyst to start from a dataset at hand and customize it with additional attributes coming from other existing datasets. This article describes a model and algorithms that exploit automatically extracted and user-defined semantic relationships for extending analytic datasets with new atomic or aggregated attribute values. Our framework is implemented as a REST service in SAP HANA and includes a careful theoretical analysis and practical solutions for several complex data quality issues.",https://doi.org/10.1016/j.is.2020.101495,2020,Rutian Liu and Eric Simon and Bernd Amann and Stéphane Gançarski,DISCOVERING AND MERGING RELATED ANALYTIC DATASETS,article
489,28436,JOURNAL OF ATMOSPHERIC AND SOLAR-TERRESTRIAL PHYSICS,journal,13646826,"0,515",Q2,89,180,628,8954,1256,620,"1,89","49,74",United Kingdom,Western Europe,Elsevier Ltd.,1997-2020,Geophysics (Q2); Atmospheric Science (Q3); Space and Planetary Science (Q3),JOURNAL OF ATMOSPHERIC AND SOLAR-TERRESTRIAL PHYSICS,JOURNAL OF ATMOSPHERIC AND SOLAR-TERRESTRIAL PHYSICS,"7,321",1.735,0.00523,"Starting from a set of 6190 meteorological stations we are choosing 6130 of them and only for Northern Hemisphere we are computing average values for absolute annual Mean, Minimum, Q1, Median, Q3, Maximum temperature plus their standard deviations for years 1800–2013, while we use 4887 stations and 389467 rows of complete yearly data. The data quality and the seasonal bias indices are defined and used in order to evaluate our dataset. After the year 1969 the data quality is monotonically decreasing while the seasonal bias is positive in most of the cases. An Extreme Value Distribution estimation is performed for minimum and maximum values, giving some upper bounds for both of them and indicating a big magnitude for temperature changes. Finally suggestions for improving the quality of meteorological data are presented.",https://doi.org/10.1016/j.jastp.2015.03.009,2015,Demetris T. Christopoulos,EXTRACTION OF THE GLOBAL ABSOLUTE TEMPERATURE FOR NORTHERN HEMISPHERE USING A SET OF 6190 METEOROLOGICAL STATIONS FROM 1800 TO 2013,article
490,17833,MEDICAL HYPOTHESES,journal,"03069877, 15322777","0,441",Q3,87,816,979,28666,1452,883,"1,40","35,13",United States,Northern America,Churchill Livingstone,1975-2020,Medicine (miscellaneous) (Q3),MEDICAL HYPOTHESES,MEDICAL HYPOTHESES,"9,727",1.538,0.005,"For the first time in history, it is possible to study human behavior on great scale and in fine detail simultaneously. Online services and ubiquitous computational devices, such as smartphones and modern cars, record our everyday activity. The resulting Big Data offers unprecedented opportunities for tracking and analyzing behavior. This paper hypothesizes the applicability and impact of Big Data technologies in the context of psychometrics both for research and clinical applications. It first outlines the state of the art, including the severe shortcomings with respect to quality and quantity of the resulting data. It then presents a technological vision, comprised of (i) numerous data sources such as mobile devices and sensors, (ii) a central data store, and (iii) an analytical platform, employing techniques from data mining and machine learning. To further illustrate the dramatic benefits of the proposed methodologies, the paper then outlines two current projects, logging and analyzing smartphone usage. One such study attempts to thereby quantify severity of major depression dynamically; the other investigates (mobile) Internet Addiction. Finally, the paper addresses some of the ethical issues inherent to Big Data technologies. In summary, the proposed approach is about to induce the single biggest methodological shift since the beginning of psychology or psychiatry. The resulting range of applications will dramatically shape the daily routines of researches and medical practitioners alike. Indeed, transferring techniques from computer science to psychiatry and psychology is about to establish Psycho-Informatics, an entire research direction of its own.",https://doi.org/10.1016/j.mehy.2013.11.030,2014,Alexander Markowetz and Konrad Błaszkiewicz and Christian Montag and Christina Switala and Thomas E. Schlaepfer,PSYCHO-INFORMATICS: BIG DATA SHAPING MODERN PSYCHOMETRICS,article
491,21100201770,HEALTH POLICY AND TECHNOLOGY,journal,"22118845, 22118837","0,393",Q3,21,86,168,3120,325,151,"1,84","36,28",Netherlands,Western Europe,,2012-2020,Biomedical Engineering (Q3); Health Policy (Q3),HEALTH POLICY AND TECHNOLOGY,HEALTH POLICY AND TECHNOLOGY,630,1.931,0.0009,"Background
Big data analytics are becoming more prevalent due to the recent availability of health data. Yet in spite of evidence supporting the potential contribution of big data analytics to health policy makers and care providers, these tools are still too complex to be routinely used. Further, access to comprehensive datasets required for more accurate results is complex and costly. Consequently, big data analytics are mostly used by researchers and experts who are far removed from actual clinical practice. Hence, policy makers should allocate resources to encourage studies that clarify and simplify big data analytics so it can be used by non-experts (e.g., clinicians, practitioners and decision-makers who may not have advanced computer skills). It is also important to fund data collection and integration from various health IT, a pre-condition for any big data analytics project.
Objectives
To methodologically clarify the rationale and logic behind several analytics algorithms to help non-expert users employ big data analytics by understanding how to implement relatively easy to use platforms as Azure ML.
Methods
We demonstrate the predictive power of four known algorithms and compare their accuracy in predicting early mortality of Congestive Heart Failure (CHF) patients.
Results
The results of our models outperform those reported in the literature, attesting to the strength of some of the models, and the utility of comprehensive data.
Conclusions
The results support our call to policy makers to allocate resources to establishing comprehensive, integrated health IT systems, and to projects aimed at simplifying ML analytics.",https://doi.org/10.1016/j.hlpt.2018.12.003,2019,Ofir Ben-Assuli and Tsipi Heart and Nir Shlomo and Robert Klempfner,BRINGING BIG DATA ANALYTICS CLOSER TO PRACTICE: A METHODOLOGICAL EXPLANATION AND DEMONSTRATION OF CLASSIFICATION ALGORITHMS,article
492,18370,MEDICINA INTENSIVA,journal,"15786749, 02105691","0,336",Q3,28,237,401,4303,416,346,"0,98","18,16",Spain,Western Europe,"Ediciones Doyma, S.L.",1988-2020,Critical Care and Intensive Care Medicine (Q3),MEDICINA INTENSIVA,MEDICINA INTENSIVA,"1,177",2.491,0.00147,"The introduction of clinical information systems (CIS) in Intensive Care Units (ICUs) offers the possibility of storing a huge amount of machine-ready clinical data that can be used to improve patient outcomes and the allocation of resources, as well as suggest topics for randomized clinical trials. Clinicians, however, usually lack the necessary training for the analysis of large databases. In addition, there are issues referred to patient privacy and consent, and data quality. Multidisciplinary collaboration among clinicians, data engineers, machine-learning experts, statisticians, epidemiologists and other information scientists may overcome these problems. A multidisciplinary event (Critical Care Datathon) was held in Madrid (Spain) from 1 to 3 December 2017. Under the auspices of the Spanish Critical Care Society (SEMICYUC), the event was organized by the Massachusetts Institute of Technology (MIT) Critical Data Group (Cambridge, MA, USA), the Innovation Unit and Critical Care Department of San Carlos Clinic Hospital, and the Life Supporting Technologies group of Madrid Polytechnic University. After presentations referred to big data in the critical care environment, clinicians, data scientists and other health data science enthusiasts and lawyers worked in collaboration using an anonymized database (MIMIC III). Eight groups were formed to answer different clinical research questions elaborated prior to the meeting. The event produced analyses for the questions posed and outlined several future clinical research opportunities. Foundations were laid to enable future use of ICU databases in Spain, and a timeline was established for future meetings, as an example of how big data analysis tools have tremendous potential in our field.
Resumen
La aparición de los sistemas de información clínica (SIC) en el entorno de los cuidados intensivos brinda la posibilidad de almacenar una ingente cantidad de datos clínicos en formato electrónico durante el ingreso de los pacientes. Estos datos pueden ser empleados posteriormente para obtener respuestas a preguntas clínicas, para su uso en la gestión de recursos o para sugerir líneas de investigación que luego pueden ser explotadas mediante ensayos clínicos aleatorizados. Sin embargo, los médicos clínicos carecen de la formación necesaria para la explotación de grandes bases de datos, lo que supone un obstáculo para aprovechar esta oportunidad. Además, existen cuestiones de índole legal (seguridad, privacidad, consentimiento de los pacientes) que deben ser abordadas para poder utilizar esta potente herramienta. El trabajo multidisciplinar con otros profesionales (analistas de datos, estadísticos, epidemiólogos, especialistas en derecho aplicado a grandes bases de datos), puede resolver estas cuestiones y permitir utilizar esta herramienta para investigación clínica o análisis de resultados (benchmarking). Se describe la reunión multidisciplinar (Critical Care Datathon) realizada en Madrid los días 1, 2 y 3 de diciembre de 2017. Esta reunión, celebrada bajo los auspicios de la Sociedad Española de Medicina Intensiva, Crítica y Unidades Coronarias (SEMICYUC) entre otros, fue organizada por el Massachusetts Institute of Technology (MIT), la Unidad de Innovación y el Servicio de Medicina Intensiva del Hospital Clínico San Carlos, así como el grupo de investigación «Life Supporting Technologies» de la Universidad Politécnica de Madrid. Tras unas ponencias de formación sobre big data, seguridad y calidad de los datos, y su aplicación al entorno de la medicina intensiva, un grupo de clínicos, analistas de datos, estadísticos, expertos en seguridad informática de datos realizaron sesiones de trabajo colaborativo en grupos utilizando una base de datos reales anonimizada (MIMIC III), para analizar varias preguntas clínicas establecidas previamente a la reunión. El trabajo colaborativo permitió establecer resultados relevantes con respecto a las preguntas planteadas y esbozar varias líneas de investigación clínica a desarrollar en el futuro. Además, se sentaron las bases para poder utilizar las bases de datos de las UCI con las que contamos en España, y se estableció un calendario de trabajo para planificar futuras reuniones contando con los datos de nuestras unidades. El empleo de herramientas de big data y el trabajo colaborativo con otros profesionales puede permitir ampliar los horizontes en aspectos como el control de calidad de nuestra labor cotidiana, la comparación de resultados entre unidades o la elaboración de nuevas líneas de investigación clínica.",https://doi.org/10.1016/j.medin.2018.06.002,2019,Antonio {Núñez Reiz} and Fernando {Martínez Sagasti} and Manuel {Álvarez González} and Antonio {Blesa Malpica} and Juan Carlos {Martín Benítez} and Mercedes {Nieto Cabrera} and Ángela {del Pino Ramírez} and José Miguel {Gil Perdomo} and Jesús {Prada Alonso} and Leo Anthony Celi and Miguel Ángel {Armengol de la Hoz} and Rodrigo Deliberato and Kenneth Paik and Tom Pollard and Jesse Raffa and Felipe Torres and Julio Mayol and Joan Chafer and Arturo {González Ferrer} and Ángel Rey and Henar {González Luengo} and Giuseppe Fico and Ivana Lombroni and Liss Hernandez and Laura López and Beatriz Merino and María Fernanda Cabrera and María Teresa Arredondo and María Bodí and Josep Gómez and Alejandro Rodríguez and Miguel {Sánchez García},BIG DATA AND MACHINE LEARNING IN CRITICAL CARE: OPPORTUNITIES FOR COLLABORATIVE RESEARCH,article
493,15552,MICROPROCESSORS AND MICROSYSTEMS,journal,01419331,"0,323",Q3,38,462,428,12804,962,423,"2,34","27,71",Netherlands,Western Europe,Elsevier,1978-2020,Artificial Intelligence (Q3); Computer Networks and Communications (Q3); Hardware and Architecture (Q3); Software (Q3),MICROPROCESSORS AND MICROSYSTEMS,MICROPROCESSORS AND MICROSYSTEMS,"1,490",1.525,0.00207,"Cross-Language Information Retrieval (CLIR) the purpose of another language (target language), a collection of documents written question from one language (source language).CLIR employees publish documents based on user queries, dictionary translation, machine translation methods, and promotion problems. Through various detection methods and translation, this word applies to single words, translation, transliteration of names, including transcription and translation and disambiguation. CLIR Recovery in Question Semantics Technology is the most appropriate) Translation to retrieve documents (dictionary related method) based on English Question Concentrations and Question translation. To the proposed model of translating Arabic to English, and provides the high accuracy Optical Character Recognition (OCR) errors in handling orthography, expanding outside and transliteration dubbed gives higher accuracy in resolving ambiguities. Thus, the single question expands with additional meanings and related words that improve significantly with semantic input. However, the documents related to the questions recover those cross language boundaries. The development of large data systems is completely different from the actual goal of small (traditional, structured) data system development. When the in-depth learning technology is developed, face some space and environmental barriers in different laboratory environments. To describe the requirements when running embedded applications on computers for deep learning.",https://doi.org/10.1016/j.micpro.2021.103928,2021,Zhihong Li,SEARCH QUERY OF ENGLISH TRANSLATION TEXT BASED ON EMBEDDED SYSTEM AND BIG DATA,article
494,15552,MICROPROCESSORS AND MICROSYSTEMS,journal,01419331,"0,323",Q3,38,462,428,12804,962,423,"2,34","27,71",Netherlands,Western Europe,Elsevier,1978-2020,Artificial Intelligence (Q3); Computer Networks and Communications (Q3); Hardware and Architecture (Q3); Software (Q3),MICROPROCESSORS AND MICROSYSTEMS,MICROPROCESSORS AND MICROSYSTEMS,"1,490",1.525,0.00207,"Data mining and big data computing are the emerging domains in the current era of predictions for societal applications. Millions of people are interested in sharing their views through tweets. Healthcare predictions are one of the attractive researches in big data social mining. Healthcare predictions are derived by implementing topic models by the ailments data. An ailment refers to either illness or sign of a particular health problem. Millions of tweets are collected based on conditions and assessed with ailment topic aspect models. The existing topic model, Latent Dirichlet Allocation (LDA), Latent Semantic Indexing, Probabilistic LSI (PLSI), limits the healthcare results assessment concerning any one of the ailments aspects. Recent ailments topic aspect model (ATAM) overcome the problems of these topic models and delivers the healthcare assessment results concerning the fundamental aspects of ailments data except side-effects analysis of treatments. The scalability performance of ATAM is degraded in showing healthcare results over the massive amounts of health data. A high-performance computing model of ATAM has been developed in the distributed environment to address scalability. Its intelligent model is designed in the cloud and multi-node Hadoop environment to deliver high-performance social computing results for healthcare. Experiments are conducted on many comparative studies is demonstrated between the existing and proposed high-performance models using the massive amount of health-related tweets concerning the ailments aspects.",https://doi.org/10.1016/j.micpro.2022.104690,2022,K Narasimhulu and K.T. {Meena Abarna},HIGH PERFORMANCE SOCIAL DATA COMPUTING WITH DEVELOPMENT OF INTELLIGENT TOPIC MODELS FOR HEALTHCARE,article
495,27632,CRITICAL CARE NURSING CLINICS OF NORTH AMERICA,journal,"08995885, 15583481","0,320",Q3,29,51,164,1964,188,140,"1,04","38,51",United Kingdom,Western Europe,W.B. Saunders Ltd,1989-2020,Critical Care Nursing (Q3),CRITICAL CARE NURSING CLINICS OF NORTH AMERICA,CRITICAL CARE NURSING CLINICS OF NORTH AMERICA,592,1.326,0.00061,,https://doi.org/10.1016/j.cnc.2018.07.005,2018,Lynn E. Bayne,"BIG DATA IN NEONATAL HEALTH CARE: BIG REACH, BIG REWARD?",article
496,21100904890,JOURNAL OF COMPUTER LANGUAGES,journal,"25901184, 26659182","0,254",Q3,6,36,49,1889,104,47,"2,12","52,47",United Kingdom,Western Europe,Elsevier Ltd.,2019-2020,Computer Networks and Communications (Q3); Human-Computer Interaction (Q3); Software (Q3),JOURNAL OF COMPUTER LANGUAGES,JOURNAL OF COMPUTER LANGUAGES,78,1.271,0.0001,"We present BiDaML 2.0, an integrated suite of visual languages and supporting tool to help multidisciplinary teams with the design of big data analytics solutions. BiDaML tool support provides a platform for efficiently producing BiDaML diagrams and facilitating their design, creation, report and code generation. We evaluated BiDaML using two types of evaluations, a theoretical analysis using the “physics of notations”, and an empirical study with 1) a group of 12 target end-users and 2) five individual end-users. Participants mostly agreed that BiDaML was straightforward to understand/learn, and prefer BiDaML for supporting complex data analytics solution modeling than other modeling languages.",https://doi.org/10.1016/j.cola.2020.100964,2020,Hourieh Khalajzadeh and Andrew J. Simmons and Mohamed Abdelrazek and John Grundy and John Hosking and Qiang He,AN END-TO-END MODEL-BASED APPROACH TO SUPPORT BIG DATA ANALYTICS DEVELOPMENT,article
497,27052,AORN JOURNAL,journal,"18780369, 00012092","0,222",Q2,43,288,682,2461,276,475,"0,25","8,55",United States,Northern America,John Wiley &amp; Sons Inc.,1963-2020,Medical and Surgical Nursing (Q2),AORN JOURNAL,AORN JOURNAL,"1,553",0.676,0.0013,"Big data are large volumes of digital data that can be collected from disparate sources and are challenging to analyze. These data are often described with the five “Vs”: volume, velocity, variety, veracity, and value. Perioperative nurses contribute to big data through documentation in the electronic health record during routine surgical care, and these data have implications for clinical decision making, administrative decisions, quality improvement, and big data science. This article explores methods to improve the quality of perioperative nursing data and provides examples of how these data can be combined with broader nursing data for quality improvement. We also discuss a national action plan for nursing knowledge and big data science and how perioperative nurses can engage in collaborative actions to transform health care. Standardized perioperative nursing data has the potential to affect care far beyond the original patient.",https://doi.org/10.1016/j.aorn.2016.07.009,2016,Bonnie L. Westra and Jessica J. Peterson,BIG DATA AND PERIOPERATIVE NURSING,article
