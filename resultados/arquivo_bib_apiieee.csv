,doi,title,abstract,year,issn
0,10.1109/CTISC54888.2022.9849793,RESEARCH ON THE BIG DATA-BASED PRODUCT QUALITY DATA PACKAGE CONSTRUCTION AND APPLICATION,"In the new environment of intelligent manufacturing, enterprise quality data has increased exponentially. How to manage, utilize, mine and analyze quality data has become a key issue in modern quality management. This article expands the definition of the product quality data package in the intelligent manufacturing environment, and proposes a big data-based product quality data package construction and management solution, gives a quality data fusion method based on business decision, outlines the application of quality data package. Finally, a chip manufacturing company was used to verify the feasibility of the product quality data package construction and management plan.",2022,
1,10.1109/WISA.2017.29,A BIG DATA FRAMEWORK FOR ELECTRIC POWER DATA QUALITY ASSESSMENT,"Since a low-quality data may influence the effectiveness and reliability of applications, data quality is required to be guaranteed. Data quality assessment is considered as the foundation of the promotion of data quality, so it is essential to access the data quality before any other data related activities. In the electric power industry, more and more electric power data is continuously accumulated, and many electric power applications have been developed based on these data. In China, the power grid has many special characteristic, traditional big data assessment frameworks cannot be directly applied. Therefore, a big data framework for electric power data quality assessment is proposed. Based on big data techniques, the framework can accumulate both the real-time data and the history data, provide an integrated computation environment for electric power big data assessment, and support the storage of different types of data.",2017,
2,10.1109/BigDataCongress.2017.73,BIG DATA PRE-PROCESSING: CLOSING THE DATA QUALITY ENFORCEMENT LOOP,"In the Big Data Era, data is the core for any governmental, institutional, and private organization. Efforts were geared towards extracting highly valuable insights that cannot happen if data is of poor quality. Therefore, data quality (DQ) is considered as a key element in Big data processing phase. In this stage, low quality data is not penetrated to the Big Data value chain. This paper, addresses the data quality rules discovery (DQR) after the evaluation of quality and prior to Big Data pre-processing. We propose a DQR discovery model to enhance and accurately target the pre-processing activities based on quality requirements. We defined, a set of pre-processing activities associated with data quality dimensions (DQD's) to automatize the DQR generation process. Rules optimization are applied on validated rules to avoid multi-passes pre-processing activities and eliminates duplicate rules. Conducted experiments showed an increased quality scores after applying the discovered and optimized DQR's on data.",2017,
3,10.1109/IBDAP50342.2020.9245455,SAKDAS: A PYTHON PACKAGE FOR DATA PROFILING AND DATA QUALITY AUDITING,"Data Profiling and data quality management become a more significant part of data engineering, which an essential part of ensuring that the system delivers quality information to users. In the last decade, data quality was considered to need more managing. Especially in the big data era that the data comes from many sources, many data types, and an enormous amount. Thus it makes the managing of data quality is more difficult and complicated. The traditional system was unable to respond as needed. The data quality managing software for big data was developed but often found in a high-priced, difficult to customize as needed, and mostly provide as GUI, which is challenging to integrate with other systems. From this problem, we have developed an opensource package for data quality managing. By using Python programming language, Which is a programming language that is widely used in the scientific and engineering field today. Because it is a programming language that is easy to read syntax, small, and has many additional packages to integrate. The software developed here is called “Sakdas” this package has been divided into three parts. The first part deals with data profiling provide a set of data analyses to generate a data profile, and this profile will help to define the data quality rules. The second part deals with data quality auditing that users can set their own data quality rules for data quality measurement. The final part deals with data visualizing that provides data profiling and data auditing report to improve the data quality. The results of the profiling and auditing services, the user can specify both the form of a report for self-review. Or in the form of JSON for use in post-process automation.",2020,
4,10.1109/ICBDA.2017.8078796,SOME KEY PROBLEMS OF DATA MANAGEMENT IN ARMY DATA ENGINEERING BASED ON BIG DATA,"This paper analyzed the challenges of data management in army data engineering, such as big data volume, data heterogeneous, high rate of data generation and update, high time requirement of data processing, and widely separated data sources. We discussed the disadvantages of traditional data management technologies to deal with these problems. We also highlighted the key problems of data management in army data engineering including data integration, data analysis, representation of data analysis results, and evaluation of data quality.",2017,
5,10.1109/CIBDA50819.2020.00024,IMPLEMENTATION OF WATER QUALITY MANAGEMENT PLATFORM FOR AQUACULTURE BASED ON BIG DATA,"In order to ensure the quality and quantity of aquaculture, aquaculture farmers need to grasp the water quality in time. However, most farmers have to collect water quality data manually at present, and cannot store and reuse that information rapidly. This paper aims to use SpringBoot framework and JPA framework to build a big data platform of acquisition automation and visualization, which realizes the data analysis and display of heterogeneous water quality and breeding information. The platform can make the water quality prediction and real-time warning. Meanwhile, it realizes the management of robots, users and breeding experts. The application of this platform will bring better social benefits to aquaculture farmers.",2020,
6,10.1109/BigData47090.2019.9005614,AN EFFECTIVE AND SCALABLE DATA MODELING FOR ENTERPRISE BIG DATA PLATFORM,"The enormous growth of the internet, enterprise applications, social media, and IoT devices in the current time caused a huge spike in enterprise data growth. Big data platform provided scalable storage to manage enterprise data growth and served easier data access to decision-makers, stakeholders and business users. It is a well-known challenge to classify, organize and store all this data and process it to provide business insights. Due to nature, variety, velocity, volume and value of data make it difficult to effectively process big data. Enterprises face challenges to apply complex business rules, to generate insights and to support data-driven decisions in a timely fashion. As big data lake integrates streams of data from a bunch of business units, stakeholders usually analyze enterprise-wide data from various data models. Data models are a vital component of Big data platform. Users may do complex processing, run queries and perform big table joins to generate required metrics depending on the available data models. It is usually a time consuming and resource-intensive process to find the value from data. It is a no-brainer that big data platform in the enterprise needs high-quality data modeling methods to reach an optimal mix of cost, performance, and quality. This paper addresses these challenges by proposing an effective and scalable way to organize and store data in Big Data Lake. It presents some of the basic principles and methodology to build scalable data models in a distributed environment. It also describes how it overcomes common challenges and presents findings.",2019,
7,10.1109/INNOVATIONS.2018.8605945,BIG DATA QUALITY ASSESSMENT MODEL FOR UNSTRUCTURED DATA,"Big Data has gained an enormous momentum the past few years because of the tremendous volume of generated and processed Data from diverse application domains. Nowadays, it is estimated that 80% of all the generated data is unstructured. Evaluating the quality of Big data has been identified to be essential to guarantee data quality dimensions including for example completeness, and accuracy. Current initiatives for unstructured data quality evaluation are still under investigations. In this paper, we propose a quality evaluation model to handle quality of Unstructured Big Data (UBD). The later captures and discover first key properties of unstructured big data and its characteristics, provides some comprehensive mechanisms to sample, profile the UBD dataset and extract features and characteristics from heterogeneous data types in different formats. A Data Quality repository manage relationships between Data quality dimensions, quality Metrics, features extraction methods, mining methodologies, data types and data domains. An analysis of the samples provides a data profile of UBD. This profile is extended to a quality profile that contains the quality mapping with selected features for quality assessment. We developed an UBD quality assessment model that handles all the processes from the UBD profiling exploration to the Quality report. The model provides an initial blueprint for quality estimation of unstructured Big data. It also, states a set of quality characteristics and indicators that can be used to outline an initial data quality schema of UBD.",2018,23255498
8,10.1109/BigData47090.2019.9006294,QUALIBD: A TOOL FOR MODELLING QUALITY REQUIREMENTS FOR BIG DATA APPLICATIONS,"The development of Big Data applications is not well-explored, to our knowledge. Embracing Big Data in system building, questions arise as to how to elicit, specify, analyse, model, and document Big Data quality requirements. In our ongoing research, we explore a requirements modelling language for Big Data software applications. In this paper, we introduce QualiBD, a modelling tool that implements the proposed goal-oriented requirements language that facilitates the modelling of Big Data quality requirements.",2019,
9,10.1109/QRS-C.2018.00115,A METHOD OF QUALITY IMPROVEMENT BASED ON BIG QUALITY WARRANTY DATA ANALYSIS,"Quality warranty data includes big data of product use and customer services, which is foundation of product quality and reliability improvement. This paper presents a method of quality warranty data analysis, which is based on the big data analysis technology. By means of the method of association rules mining, it distinguishes the association rules of failure modes while feeding back the information to the process of product design, production, and usage. To achieve product fault location and fault disposal, the key factors such as fault type and fault cause are analyzed. Meanwhile, this paper adopted the principles of PDCA circulation to propose a procedure of product quality improvement. The quality improvement procedure based on quality warranty data analysis provides a comprehensive and systematic quality improvement for different stages and different types of products. Finally, a case study of household appliances in China is given to illustrate the method.",2018,
10,10.1109/BigData.2018.8622388,USING BIG DATA ANALYTICS TO CREATE A PREDICTIVE MODEL FOR JOINT STRIKE FIGHTER,"The amount of information needed to acquire knowledge on today's acquisition systems is growing exponentially due to more complex, higher resolution, software-intensive acquisition systems that need to operate in System-of-Systems (SoS), Family-of-Systems (FoS), Joint, and Coalition environments. Unfortunately, the tools and methods necessary to rapidly collect, aggregate, and analyze this information have not evolved as a whole in conjunction with this increased system complexity and, therefore, has made analysis and evaluation increasingly deficient and ineffective. The Test Resource Management Center's (TRMC's) vision is to build a DoD test and evaluation (T&E) knowledge management (KM) and analysis capability that leverages commercial big data analysis and cloud computing technologies to improve evaluation quality and reduce decision-making time. An evaluation revolution, starting with the Joint Strike Fighter (JSF) program, is underway to ensure the T&E community can support the demands of next-generation weapon systems.The true product of T&E is knowledge ascertained through the collection of information about a system or item under test. However, the T&E community's ability to provide this knowledge is hampered by more complex systems, more complex environments, and the need to be more agile in support of strategic initiatives, such as agile acquisition and the 3rd Offset Strategy. This increased complexity and need for speed cause delayed analysis and problems that go undetected during T&E. The primary reason for these shortfalls is antiquated tools and processes that make data hard to locate, aggregate, and convert into knowledge. In short, DoD has not evolved its evaluation infrastructure as its weapon systems have evolved.Conversely, commercial entities, such as medical observation and diagnosis, electric power distribution, retail, and industrial manufacturing, have embraced agility in their methodologies while modernizing analytics capabilities to keep up with the massive influx of data. Raw physical sensors could provide data, higher-quality image or video cameras, radio frequency identification (RFID) devices, faster data collectors, more detailed point-of-sale information or digitized records, and ultimately is providing more data to analysts in size and complexity than ever before. As more data has become available, an interrelated phenomenon is the desire of analysts to ask more detailed questions about their consumers and their business infrastructure. To drive the process of implementing big data analytics, businesses have begun establishing analytics centers which either take pre-defined business cases and apply methods to address them or implement existing knowledge within the data architecture to create a higher level of awareness to business groups or the company at-large. To meet these demands, data storage and computation architectures have become more sophisticated, dozens of technologies were developed for large-scale processing (such as Apache Hadoop or GreenPlum), and streaming architectures which allow data to be processed and actioned on in real-time as it is collected have become commonplace. The net result of these commercial best practices is a solid foundation for the DoD to transform how it uses data to achieve faster, better, and smarter decisions throughout the acquisition lifecycle.",2018,
11,10.1109/ICBASE51474.2020.00008,METHODS OF ENTERPRISE ELECTRONIC FILE CONTENT INFORMATION MINING UNDER BIG DATA ENVIRONMENT,"As the product of the digital age, big data technology and computer information technology can greatly improve the efficiency and quality of file management and promote the development of enterprises. Based on this, this paper first analyzes the current status of enterprise archives management; Secondly, this paper discusses the countermeasures of information mining of electronic documents of innovative enterprises in the digital age. Text information mining is beneficial to improve the efficiency of text information search and utilization, aiming at the existing problems of traditional methods, the text information mining method is proposed.",2020,
12,10.26599/BDMA.2019.9020019,MINING CONDITIONAL FUNCTIONAL DEPENDENCY RULES ON BIG DATA,"Current Conditional Functional Dependency (CFD) discovery algorithms always need a well-prepared training dataset. This condition makes them difficult to apply on large and low-quality datasets. To handle the volume issue of big data, we develop the sampling algorithms to obtain a small representative training set. We design the fault-tolerant rule discovery and conflict-resolution algorithms to address the low-quality issue of big data. We also propose parameter selection strategy to ensure the effectiveness of CFD discovery algorithms. Experimental results demonstrate that our method can discover effective CFD rules on billion-tuple data within a reasonable period.",2020,20960654
13,10.1109/ICCCBDA.2018.8386521,DATA QUALITY ASSESSMENT FOR ON-LINE MONITORING AND MEASURING SYSTEM OF POWER QUALITY BASED ON BIG DATA AND DATA PROVENANCE THEORY,"Currently, on-line monitoring and measuring system of power quality has accumulated a huge amount of data. In the age of big data, those data integrated from various systems will face big data application problems. This paper proposes a data quality assessment system method for on-line monitoring and measuring system of power quality based on big data and data provenance to assess integrity, redundancy, accuracy, timeliness, intelligence and consistency of data set and single data. Specific assessment rule which conforms to the situation of on-line monitoring and measuring system of power quality will be devised to found data quality problems. Thus it will provide strong data support for big data application of power quality.",2018,
14,10.1109/BigData50022.2020.9378148,TOWARDS AUTOMATIC DATA CLEANSING AND CLASSIFICATION OF VALID HISTORICAL DATA AN INCREMENTAL APPROACH BASED ON MDD,"The project Death and Burial Data: Ireland 1864-1922 (DBDIrl) examines the relationship between historical death registration data and burial data to explore the history of power in Ireland from 1864 to 1922. Its core Big Data arises from historical records from a variety of heterogeneous sources, some aspects are pre-digitized and machine readable. A huge data set (over 4 million records in each source) and its slow manual enrichment (ca 7,000 records processed so far) pose issues of quality, scalability, and creates the need for a quality assurance technology that is accessible to non-programmers. An important goal for the researcher community is to produce a reusable, high-level quality assurance tool for the ingested data that is domain specific (historic data), highly portable across data sources, thus independent of storage technology.This paper outlines the step-wise design of the finer granular digital format, aimed for storage and digital archiving, and the design and test of two generations of the techniques, used in the first two data ingestion and cleaning phases.The first small scale phase was exploratory, based on metadata enrichment transcription to Excel, and conducted in parallel with the design of the final digital format and the discovery of all the domain-specific rules and constraints for the syntax and semantic validity of individual entries. Excel embedded quality checks or database-specific techniques are not adequate due to the technology independence requirement. This first phase produced a Java parser with an embedded data cleaning and evaluation classifier, continuously improved and refined as insights grew. The next, larger scale phase uses a bespoke Historian Web Application that embeds the Java validator from the parser, as well as a new Boolean classifier for valid and complete data assurance built using a Model-Driven Development technique that we also describe. This solution enforces property constraints directly at data capture time, removing the need for additional parsing and cleaning stages. The new classifier is built in an easy to use graphical technology, and the ADD-Lib tool it uses is a modern low-code development environment that auto-generates code in a large number of programming languages. It thus meets the technology independence requirement and historians are now able to produce new classifiers themselves without being able to program. We aim to infuse the project with computational and archival thinking in order to produce a robust data set that is FAIR compliant (Free Accessible Inter-operable and Re-useable).",2020,
15,10.1109/ICBDA.2017.8078781,MISSING DATA OF QUALITY INSPECTION IMPUTATION ALGORITHM BASE ON STACKED DENOISING AUTO-ENCODER,"Analyzing and processing big data of quality inspection is the key factor in ensuring product quality and People's property security. Big data of quality inspection collected by social network and E-commerce is missing in most cases. And the incompleteness of data brings huge challenge for analyzing and processing. Therefore, the algorithm of data filling based on stacked denoising auto-encoder is proposed in this text. As the experiment shows that the algorithm proposed in this text is effective in dealing with big data of quality inspection.",2017,
16,10.1109/BigData.2015.7364093,GRADIENT-BASED SIGNATURES FOR BIG MULTIMEDIA DATA,"With the continuous increase of heterogeneous multimedia data, the question of how to access big multimedia data efficiently has become of crucial importance. In order to provide fast access to complex multimedia data, we propose to approximate content-based features of multimedia objects by means of generative models. The proposed gradient-based signatures epitomize a high quality content-based approximation of multimedia objects and facilitate efficient indexing and query processing at large scale.",2015,
17,10.1109/COMITCon.2019.8862267,BIG DATA QUALITY FRAMEWORK: PRE-PROCESSING DATA IN WEATHER MONITORING APPLICATION,"Big Data has become an imminent part of all industries and business sectors today. All organizations in any sector like energy, banking, retail, hardware, networking, etc all generate huge quantum of heterogenous data which if mined, processed and analyzed accurately can reveal immensely useful patterns for business heads to apply to generate and grow their businesses. Big Data helps in acquiring, processing and analyzing large amounts of heterogeneous data to derive valuable results. Quality of information is affected by size, speed and format in which data is generated. Hence, Quality of Big Data is of great relevance and importance. We propose addressing various aspects of the raw data to improve its quality in the pre-processing stage, as the raw data may not usable as-is. We are exploring process like Cleansing to fix as much data as feasible, Noise filters to remove bad data, as well sub-processes for Integration and Filtering along with Data Transformation/Normalization. We evaluate and profile the Big Data during acquisition stage, which is adapted to expectations to avoid cost overheads later while also improving and leading to accurate data analysis. Hence, it is imperative to improve Data quality even it is absorbed and utilized in an industry's Big Data system. In this paper, we propose a Pre-Processing Framework to address quality of data in a weather monitoring and forecasting application that also takes into account global warming parameters and raises alerts/notifications to warn users and scientists in advance.",2019,
18,10.1109/ICCCBDA.2019.8725668,RESEARCH ON WIDE-AREA DISTRIBUTED POWER QUALITY DATA FUSION TECHNOLOGY OF POWER GRID,"With the advancement of the ""big operation"" system construction, the online monitoring system for power quality has been integrated, and various power quality data have been incorporated into relevant organizations for unified management. Power quality management has a larger range of data, more types, and higher frequency. It needs to realize the unified storage management and efficient access of massive heterogeneous power quality data for the characteristics of data applications and the collection and aggregation of these effective data. This paper proposes a new type of grid wide-area distributed power quality data integration architecture, which is designed for multi-source, heterogeneous, distributed data integration technology and wide-area distributed data storage technology to solve the big data source problem and realize the sharing of power quality data information of the whole network.",2019,
19,10.1109/BigData.2015.7364064,"BIG DATA, BIG DATA QUALITY PROBLEM","A USAF sponsored MITRE research team undertook four separate, domain-specific case studies about Big Data applications. Those case studies were initial investigations into the question of whether or not data quality issues encountered in Big Data collections are substantially different in cause, manifestation, or detection than those data quality issues encountered in more traditionally sized data collections. The study addresses several factors affecting Big Data Quality at multiple levels, including collection, processing, and storage. Though not unexpected, the key findings of this study reinforce that the primary factors affecting Big Data reside in the limitations and complexities involved with handling Big Data while maintaining its integrity. These concerns are of a higher magnitude than the provenance of the data, the processing, and the tools used to prepare, manipulate, and store the data. Data quality is extremely important for all data analytics problems. From the study's findings, the ""truth about Big Data"" is there are no fundamentally new DQ issues in Big Data analytics projects. Some DQ issues exhibit return-s-to-scale effects, and become more or less pronounced in Big Data analytics, though. Big Data Quality varies from one type of Big Data to another and from one Big Data technology to another.",2015,
20,10.1109/DSAA49011.2020.00119,BIG DATA QUALITY PREDICTION ON BANKING APPLICATIONS: EXTENDED ABSTRACT,"Big data has been transformed into knowledge by information systems to add value in businesses. Enterprises relying on it benefit from risk management to a certain extent. The value, however, depends on the quality of data. The quality needs to be verified before any use of the data. Specifically, measuring the quality by simulating the real life situation and even forecast it accurately turns into a hot topic. In recent years, there have been numerous researches on the measurement and assessment of data quality. These are yet to utilize a scientific computational method for the measurement and prediction. Current methods either fail to make an accurate prediction or do not consider the correlation and time sequence factors of the data. To address this, we design a model to extend machine learning technique to business applications predicting this. Firstly, we implement the model to detect data noises from a risk dataset according to an international data quality standard from banking industry and then estimate their impacts with Gaussian and Bayesian methods. Secondly, we direct sequential learning in multiple deep neural networks for the prediction with an attention mechanism. The model is experimented with various network methodologies to show the predictive power of machine learning technique and is evaluated by validation data to confirm the model effectiveness. The model is scalable to apply to any industries utilizing big data other than the banking industry.",2020,
21,10.1109/BigData.2015.7364065,DATA QUALITY ISSUES IN BIG DATA,"Though the issues of data quality trace back their origin to the early days of computing, the recent emergence of Big Data has added more dimensions. Furthermore, given the range of Big Data applications, potential consequences of bad data quality can be for more disastrous and widespread. This paper provides a perspective on data quality issues in the Big Data context. it also discusses data integration issues that arise in biological databases and attendant data quality issues.",2015,
22,10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0122,BIG DATA QUALITY: A QUALITY DIMENSIONS EVALUATION,"Data is the most valuable asset companies are proud of. When its quality degrades, the consequences are unpredictable, can lead to complete wrong insights. In Big Data context, evaluating the data quality is challenging, must be done prior to any Big data analytics by providing some data quality confidence. Given the huge data size, its fast generation, it requires mechanisms, strategies to evaluate, assess data quality in a fast, efficient way. However, checking the Quality of Big Data is a very costly process if it is applied on the entire data. In this paper, we propose an efficient data quality evaluation scheme by applying sampling strategies on Big data sets. The Sampling will reduce the data size to a representative population samples for fast quality evaluation. The evaluation targeted some data quality dimensions like completeness, consistency. The experimentations have been conducted on Sleep disorder's data set by applying Big data bootstrap sampling techniques. The results showed that the mean quality score of samples is representative for the original data, illustrate the importance of sampling to reduce computing costs when Big data quality evaluation is concerned. We applied the Quality results generated as quality proposals on the original data to increase its quality.",2016,
23,10.1109/ICABCD.2018.8465410,"MODELING OF AN EFFICIENT LOW COST, TREE BASED DATA SERVICE QUALITY MANAGEMENT FOR MOBILE OPERATORS USING IN-MEMORY BIG DATA PROCESSING AND BUSINESS INTELLIGENCE USE CASES","Network Operators are shifting their business interest towards Data services in a geometric progression manner, as Data services is becoming the major source of Telco revenue. The wide use of Data platforms; such as WhatsApp, Skype, Hangout and other Over the Top (OTT) voice applications over the traditional voice services is a clear indication that Network Operators need to adjust their business model and needs. And couple with the adoption of Smartphones usage which grows continuously year by year, this means more subscribers to manage, large amount of transactions generated, more network resources to be added and evidently more human technical expertise required to ensure good service quality. That has led to high investment on Robust Service Quality Management (SQM) and Customer Experience Management (CEM) to stay competitive in the market. The high investment is justified by the integration of Big Data Solutions, Machine Learning capabilities and good visualization of insight data. However, the Return on Investment (ROI) of the expensive systems are not as conspicuous as the provided functionalities and business rules. Therefore, in this paper an efficient model for low cost SQM system is presented, exploring the advantages of In-Memory Big Data processing and low cost business Intelligence tools to showcase how a good Service Quality Management can be implemented with no big investment.",2018,
24,10.1109/CBMS.2017.71,MEETING TECHNOLOGY AND METHODOLOGY INTO HEALTH BIG DATA ANALYTICS SCENARIOS,"Health organizations are collecting more data from a wider array of sources at greater speed every day. The analysis of this vast amount of data creates new opportunities to deliver modern personalized health and social care services. Big Data Analytics and underlying technologies have the potential to process and analyze these data to extract meaningful insights for improving quality of care, efficiency and sustainability of health and social care systems. Health organizations face therefore a new scenario where analytical tools must accommodate both traditional business intelligence and Big Data approaches, resulting in important technological and methodological challenges to be tackled. In this paper, we present a methodological approach to address the introduction of Big Data Analytics technologies into an integrated care provider.",2017,23729198
25,10.1109/UIC-ATC.2017.8397554,"DATA QUALITY IN BIG DATA PROCESSING: ISSUES, SOLUTIONS AND OPEN PROBLEMS","With the rapid development of social networks, Internet of things, Cloud computing as well as other technologies, big data age is arriving. The increasing number of data has brought great value to the public and enterprises. Meanwhile how to manage and use big data better has become the focus of all walks of life. The 4V characteristics of big data have brought a lot of issues to the big data processing. The key to big data processing is to solve data quality issue, and to ensure data quality is a prerequisite for the successful application of big data technique. In this paper, we use recommendation systems and prediction systems as typical big data applications, and try to find out the data quality issues during data collection, data preprocessing, data storage and data analysis stages of big data processing. According to the elaboration and analysis of the proposed issues, the corresponding solutions are also put forward. Finally, some open problems to be solved in the future are also raised.",2017,
26,10.1109/BigData.2017.8258406,A BIG DATA ANALYTICS FRAMEWORK FOR FORECASTING RARE CUSTOMER COMPLAINTS: A USE CASE OF PREDICTING MA MEMBERS' COMPLAINTS TO CMS,"Centers for Medicare & Medicaid Services (CMS) publishes Medicare Part C Star Ratings each year to measure the quality of care of Medicare Advantage (MA) contracts. One of the key measures is Complaints about the Health Plan, which is captured in Complaints Tracking Module (CTM). Complaints resulted in CTM are rare events: for MA contracts with 2-5 star ratings, number of complaints for every 1,000 members range from .10 to 1.84 over last 5 years. Reducing number of complaints is extremely important to MA plans as they impact CMS reimbursements to MA plans. Forecasting and reducing complaints is an extremely technically challenging task, and involves ethics considerations in patients' rights and privacy. In this research, we constructed a big data analytics framework for forecasting rare customer complaints. First, we built a big data ingestion pipelines on a Hadoop platform: a) Ingest MA plan's customer complaints data from CTM from past 3 years. b) Ingest health plan's call center data for MA members from past 3 years, including both structured data and unstructured text script for the calls. c) Ingest MA members' medical claims, including members' demographics and enrollment history. d) Ingest MA members' pharmacy claims. e) Integrate and unified data from above sources, and enrich the data with additional engineered features into a big wide table, one row per member for analysis and modeling. Second, we designed a unique decision tree based Large Ensemble with Over-Sampling (LEOS) algorithm, which mimics random forest but with extreme oversampling of target class to increase bias, and leverages the parallel computing of Hadoop clusters by generating thousands of fixed size training data sets, and for each such dataset training a decision trees with similar fixed tree structure, and ensemble them. Third, we validated our framework and LEOS learning algorithm with real data, and also discussed ethics issues we encountered in handling data and applying findings from research.",2017,
27,10.1109/IBDAP52511.2021.9552131,THE FRAMEWORK OF EXTRACTING UNSTRUCTURED USAGE FOR BIG DATA PLATFORM,"Big Data becomes crucial tools for new era of data analytics. The amount of unstructured data is also increasing. As a result, the number of unstructured data projects are increased. However, several organizations are still lack of knowledge how to determine the unstructured data in the organization and exploit it. Therefore, the tool of extracting unstructured data is needed. This research aims to propose the framework to identify the unstructured usage in the organization. The framework has been derived from the interview of the experts in areas. After that, the framework has been used to verify the results. The success case and failed are also shown. This can be seen that the proposed framework can be used in the organization to help the user extract the unstructured data usage in the organization. It can help to make the decision related to unstructured data project.",2021,
28,10.1109/BigDataCongress.2016.65,AN HYBRID APPROACH TO QUALITY EVALUATION ACROSS BIG DATA VALUE CHAIN,"While the potential benefits of Big Data adoption are significant, and some initial successes have already been realized, there remain many research and technical challenges that must be addressed to fully realize this potential. The Big Data processing, storage and analytics, of course, are major challenges that are most easily recognized. However, there are additional challenges related for instance to Big Data collection, integration, and quality enforcement. This paper proposes a hybrid approach to Big Data quality evaluation across the Big Data value chain. It consists of assessing first the quality of Big Data itself, which involve processes such as cleansing, filtering and approximation. Then, assessing the quality of process handling this Big Data, which involve for example processing and analytics process. We conduct a set of experiments to evaluate Quality of Data prior and after its pre-processing, and the Quality of the pre-processing and processing on a large dataset. Quality metrics have been measured to access three Big Data quality dimensions: accuracy, completeness, and consistency. The results proved that combination of data-driven and process-driven quality evaluation lead to improved quality enforcement across the Big Data value chain. Hence, we recorded high prediction accuracy and low processing time after we evaluate 6 well-known classification algorithms as part of processing and analytics phase of Big Data value chain.",2016,
29,10.1109/BigData47090.2019.9005530,FEDERATED QUERY PROCESSING FOR BIG DATA IN DATA SCIENCE,As the number of databases continues to grow data scientists need to use data from different sources to run machine learning algorithms for analysis. Data science results depend upon the quality of data been extracted. The objective of this research paper is to implement a federated query processing framework which extracts data from different data sources and stores the result datasets in a common in-memory data format. This helps data scientists to perform their analysis and execute machine learning algorithms using different data engines without having to convert the data into their native data format and improve the performance.,2019,
30,10.1109/BigData47090.2019.9006358,PROVENANCE–AWARE WORKFLOW FOR DATA QUALITY MANAGEMENT AND IMPROVEMENT FOR LARGE CONTINUOUS SCIENTIFIC DATA STREAMS,"Data quality assessment, management and improvement is an integral part of any big data intensive scientific research to ensure accurate, reliable, and reproducible scientific discoveries. The task of maintaining the quality of data, however, is non-trivial and poses a challenge for a program like the Department of Energy's Atmospheric Radiation Measurement (ARM) that collects data from hundreds of instruments across the world, and distributes thousands of streaming data products that are continuously produced in near-real-time for an archive 1.7 Petabyte in size and growing. In this paper, we present a computational data processing workflow to address the data quality issues via an easy and intuitive web-based portal that allows reporting of any quality issues for any site, facility or instruments at a granularity down to individual variables in the data files. This portal allows instrument specialists and scientists to provide corrective actions in the form of symbolic equations. A parallel processing framework applies the data improvement to a large volume of data in an efficient, parallel environment, while optimizing data transfer and file I/O operations; corrected files are then systematically versioned and archived. A provenance tracking module tracks and records any change made to the data during its entire life cycle which are communicated transparently to the scientific users. Developed in Python using open source technologies, this software architecture enables fast and efficient management and improvement of data in an operational data center environment.",2019,
31,10.1109/BigData.2015.7364060,EVALUATION OF DATA QUALITY OF MULTISITE ELECTRONIC HEALTH RECORD DATA FOR SECONDARY ANALYSIS,"Currently, a large amount of data is amassed in electronic health records (EHRs). However, EHR systems are largely information silos, that is, uses of these systems are often confined to management of patient information and analytics specific to a clinician's practice. A growing trend in healthcare is combining multiple databases to support epidemiological research. The College Health Surveillance Network is the first national data warehouse containing EHR data from 31 different student health centers. Each member university contributes to the data warehouse by uploading select EHR data including patient demographics, diagnoses, and procedures to a common server on a monthly basis. In this paper, we focus on the data quality dimensions from a subsample of the data comprised of over 5.7 million patient visits for approximately 980,000 patients with 4,465 unique diagnoses from 23 of those universities. We examine the data for measures of completeness, consistency, and availability for secondary use for epidemiological research. Additionally, clinical documentation practices and EHR vendor were evaluated as potential contributors to data quality. We found that overall about 70% of the data in the data warehouse is available for secondary use, and identified clinical documentation practices that are correlated to a reduction in data quality. This suggests that automated quality control and proactive clinical documentation support could reduce ad-hoc data cleaning needs resulting in greater data availability for secondary use.",2015,
32,10.1109/ICITBS.2019.00160,EVALUATION MODEL OF EDUCATION SERVICE QUALITY SATISFACTION IN COLLEGES AND UNIVERSITIES DEPENDENT ON CLASSIFICATION ATTRIBUTE BIG DATA FEATURE SELECTION ALGORITHM,"In view of the insufficiency in the education service quality in colleges and universities, a kind of evaluation model of the education service quality satisfaction in the colleges and universities that is dependent on the classification attribute big data feature selection algorithm is put forward in this paper based on the existing work. On the basis of detailed description of the model components, further study on the evaluation method of the proposed model for the education service quality satisfaction in the colleges and universities is carried out. Under the guidance of the evaluation model of the education service quality satisfaction in the colleges and universities, the method for the construction of the evaluation model of the education service quality satisfaction in the colleges and universities is studied with the orientation to the education service resources in the colleges and universities under the open big data environment. In addition, experimental verification is carried out on the basis of the evaluation data in the 360 Encyclopedia on the education service quality satisfaction in the colleges and universities. The experimental results show that the model and method put forward in this paper can effectively evaluate the quality of the education service in the colleges and universities.",2019,
33,10.1109/ITCA52113.2020.00070,MULTI DIMENSIONAL DATA DISTRIBUTION MONITORING BASED ON OLAP,"With the rapid development of the Internet, society is gradually entering the information age, and various data in enterprises have become the most important strategic core resources of all enterprises. The operation and decision-making of enterprises all require a large amount of data analysis. Nowadays, many companies do not pay enough attention to the monitoring of data asset distribution. In addition, various internal systems such as financial management and ERP systems are relatively independent. Each system has its own data organization standard, which makes it difficult to conduct a unified management of data. This also directly leads to the one-sided and subjective problem of enterprise managers' distribution of data assets. With the construction of the data center of each enterprise, the data of each system is aggregated to the center through data integration technology. Therefore, all enterprises need to build a multi-dimensional data distribution monitoring model around data links to comprehensively monitor the status of various data distributions across the company's entire network, and improve data service capabilities and sharing capabilities as well as the company's operational capabilities. This article uses OLAP technology to construct a multi-dimensional data distribution monitoring model for the data link in the process of power enterprise data integration. This article first selects the dimensions and metrics that need to be monitored in the multidimensional data, and then constructs the conceptual model, logical model and physical model of the multidimensional data using on line analytical processing technology. Finally, an example analysis of OLAP system architecture based on B/S structure is realized. The overall data distribution of the enterprise can be grasped by analyzing the various dimensions of the data link, such as System type, location distribution, and time.",2020,
34,10.1109/BigDataCongress.2018.00029,BIG DATA QUALITY: A SURVEY,"With the advances in communication technologies and the high amount of data generated, collected, and stored, it becomes crucial to manage the quality of this data deluge in an efficient and cost-effective way. The storage, processing, privacy and analytics are the main keys challenging aspects of Big Data that require quality evaluation and monitoring. Quality has been recognized by the Big Data community as an essential facet of its maturity. Yet, it is a crucial practice that should be implemented at the earlier stages of its lifecycle and progressively applied across the other key processes. The earlier we incorporate quality the full benefit we can get from insights. In this paper, we first identify the key challenges that necessitates quality evaluation. We then survey, classify and discuss the most recent work on Big Data management. Consequently, we propose an across-the-board quality management framework describing the key quality evaluation practices to be conducted through the different Big Data stages. The framework can be used to leverage the quality management and to provide a roadmap for Data scientists to better understand quality practices and highlight the importance of managing the quality. We finally, conclude the paper and point to some future research directions on quality of Big Data.",2018,
35,10.1109/BigData.2017.8258267,MY (FAIR) BIG DATA,"Policy making has the strict requirement to rely on quantitative and high quality information. This paper will address the data quality issue for policy making by showing how to deal with Big Data quality in the different steps of a processing pipeline, with a focus on the integration of Big Data sources with traditional sources. In this respect, a relevant role is played by metadata and in particular by ontologies. Integration systems relying on ontologies enable indeed a formal quality evaluation of inaccuracy, inconsistency and incompleteness of integrated data. The paper will finally describe data confidentiality as a Big Data quality dimension, showing the main issues to be faced for its assurance.",2017,
36,10.1109/IAEAC50856.2021.9391025,RESEARCH ON DATA SECURITY IN BIG DATA CLOUD COMPUTING ENVIRONMENT,"In the big data cloud computing environment, data security issues have become a focus of attention. This paper delivers an overview of conceptions, characteristics and advanced technologies for big data cloud computing. Security issues of data quality and privacy control are elaborated pertaining to data access, data isolation, data integrity, data destruction, data transmission and data sharing. Eventually, a virtualization architecture and related strategies are proposed to against threats and enhance the data security in big data cloud environment.",2021,2689663X
37,10.1109/ACIT-CSII-BCD.2017.49,A SURVEY ON BIG DATA PRE-PROCESSING,"In this paper, we briefly introduce some basic concepts and characteristics of big data. We are surrounded by massive amount of data but starving for knowledge. In the era of Big Data, how to quickly obtain high-quality and valuable information from massive amounts of data has become an important research direction. Hence, we focus our attention to the data pre-processing which is a sub-content of the data processing workflow. In this paper, the four phases of data pre-processing, including data cleansing, data integration, data reduction, and data transformation, have been discussed. And different approaches for a variety of purposes have been presented, which show current methods and techniques need to be further modified in order to improve the quality of data before data analysis.",2017,
38,10.26599/BDMA.2021.9020009,LOTUSSQL: SQL ENGINE FOR HIGH-PERFORMANCE BIG DATA SYSTEMS,"In recent years, Apache Spark has become the de facto standard for big data processing. SparkSQL is a module offering support for relational analysis on Spark with Structured Query Language (SQL). SparkSQL provides convenient data processing interfaces. Despite its efficient optimizer, SparkSQL still suffers from the inefficiency of Spark resulting from Java virtual machine and the unnecessary data serialization and deserialization. Adopting native languages such as C++ could help to avoid such bottlenecks. Benefiting from a bare-metal runtime environment and template usage, systems with C++ interfaces usually achieve superior performance. However, the complexity of native languages also increases the required programming and debugging efforts. In this work, we present LotusSQL, an engine to provide SQL support for dataset abstraction on a native backend Lotus. We employ a convenient SQL processing framework to deal with frontend jobs. Advanced query optimization technologies are added to improve the quality of execution plans. Above the storage design and user interface of the compute engine, LotusSQL implements a set of structured dataset operations with high efficiency and integrates them with the frontend. Evaluation results show that LotusSQL achieves a speedup of up to 9× in certain queries and outperforms Spark SQL in a standard query benchmark by more than 2× on average.",2021,20960654
39,10.1109/BigData.2018.8622590,BIG DATA STREAMING ANALYTICS FOR QOE MONITORING IN MOBILE NETWORKS: A PRACTICAL APPROACH,"Traditionally, Mobile Network Operators (MNOs) use a set of Key Performance Indicators (KPIs) to measure the quality offered to their customers. However, these KPIs do not reflect the quality perceived by the customers because they are high-level and network-based metrics. Instead, Quality of Experience (QoE) monitoring of the most common mobile applications can help MNOs to determine when and where customer experience is degraded. In this paper, a customized tool based on Big Data Streaming is proposed to solve the needs of customer experience monitoring in a real-life MNO and to overcome the challenges of processing a large amount of data collected in 3G and 4G mobile networks. Moreover, real-life case studies of value creation through Big Data Analytics for telecommunication industry are also defined. Results show that the streaming data processing enables new opportunities for the MNO to take actions focused on customer experience improvement in near real-time.",2018,
40,10.1109/ICCCBDA.2019.8725744,ANALYSIS OF THE APPLICATION OF MILITARY BIG DATA IN EQUIPMENT QUALITY INFORMATION MANAGEMENT,"This At present, big data has risen to the national strategy. Big data is fully integrated into the military field, becoming the driving force of military scientific research, the core element of construction management, and an important resource for war success. This paper mainly expounds the basic connotation of big data technology and military big data, and analyzes the application of military big data in equipment quality information management, and proposes information collection, storage, analysis, processing, exchange and feedback on equipment quality information management. The countermeasures provide methods and basis for military big data in equipment information management.",2019,
41,10.1109/ICBDCI.2019.8686099,BIG DATA QUALITY CHALLENGES,"Big Data, is a growing technique these days. There are many uses of Big Data; Artificial Intelligence, Health Care, Business, and many more. For that reason, it becomes necessary to deal with this massive volume of data with caution and care in a term to make sure that the data used and produced is in high quality. Therefore, the Big Data quality is must, and its rules have to be satisfied. In this paper, the main Big Data Quality Factors, which need to be measured, is presented in the perspective of the data itself, the data management, data processing, and data users. This research highlights the quality factors that may be used later to create different Big Data quality models.",2019,
42,10.1109/CECIT53797.2021.00045,AN INTELLIGENT SELECTION METHOD BASED ON ELECTRONIC COMPONENT QUALITY DATA SYSTEM,"Aiming at the difficulty of electronic component quality data management and application, and the lack of data system and application methods required for data management in selection scenarios, this paper proposes an intelligent selection method based on electronic component quality data system, uses Bi-LSTM-ATT model for entity identification, and identifies data association based on entity relationship. By calculating the Tanimoto coefficient, the intelligent matching and push of similar products and substitute products are realized, and the intellectualization of component selection is fully supported. Finally, taking the scenario of fast switching diode selection as an example, the feasibility of the method proposed in this paper is verified, which provides a model for the intelligent application of quality data resources.",2021,
43,10.1109/BigData50022.2020.9377734,REQUIREMENTS ENGINEERING PRACTICES AND CHALLENGES IN THE CONTEXT OF BIG DATA SOFTWARE DEVELOPMENT PROJECTS: EARLY INSIGHTS FROM A CASE STUDY,"This paper reports on the results of an exploratory case study on a large-scale Big Data systems development project in the Oil&Gas domain within a non-profit organisation. The aim of this study was to investigate the RE practices and challenges in such projects, currently bereft in the scientific literature. This investigation was focused on: (a) RE practices; (b) sources and distribution of requirements; (c) the role of Big Data characteristics and technologies in RE and systems design; and (d) RE challenges in engineering Big Data Systems. The main results show that (a) there is a lack of specific project tailored RE practices, tools, and frameworks for elicitation, specification and modelling, analysis, and prioritisation of requirements; (b) 40% of the system's requirements are considered Big Data-related from which 75% are identified from internal sources; (c) Big Data characteristics and technologies play an important role in defining quality requirements and system's architecture; (d) five challenges in eliciting, documenting, and analysing Big Data related requirements were identified and discussed. The findings suggest academics and practitioners opportunities to engage in further research in this area.",2020,
44,10.1109/BigData.2015.7363987,"BUSINESS INFORMATION MODELING: A METHODOLOGY FOR DATA-INTENSIVE PROJECTS, DATA SCIENCE AND BIG DATA GOVERNANCE","This paper discusses an integrated methodology to structure and formalize business requirements in large data-intensive projects, e.g. data warehouses implementations, turning them into precise and unambiguous data definitions suitable to facilitate harmonization and assignment of data governance responsibilities. We place a business information model in the center - used end-to-end from analysis, design, development, testing to data quality checks by data stewards. In addition, we show that the approach is suitable beyond traditional data warehouse environments, applying it also to big data landscapes and data science initiatives - where business requirements analysis is often neglected. As proper tool support has turned out to be inevitable in many real-world settings, we also discuss software requirements and their implementation in the Accurity Glossary tool. The approach is evaluated based on a large banking data warehouse project the authors are currently involved in.",2015,
45,10.1109/IGARSS39084.2020.9323615,A MACHINE LEARNING APPROACH FOR DATA QUALITY CONTROL OF EARTH OBSERVATION DATA MANAGEMENT SYSTEM,"In the big data era, innovative technologies like cloud computing, artificial intelligence, and machine learning are increasingly utilized in the large-scale data management systems of many industry sectors to make them more scalable and intelligent. Applying them to automate and optimize earth observation data management is a hot topic. To improve data quality control mechanisms, a machine learning method in combination with built-in quality rules is presented in this paper to evolve processes around data quality and enhance management of earth observation data. The rules of quality check are set up to detect the common issues, including data completeness, data latency, bad data, and data duplication, and the machine learning model is trained, tested, and deployed to address these quality issues automatically and reduce manual efforts.",2020,21536996
46,10.1109/DSN-W.2018.00023,"EUBRA-BIGSEA, A CLOUD-CENTRIC BIG DATA SCIENTIFIC RESEARCH PLATFORM","This paper describes the achievements of project EUBra-BIGSEA, which has delivered programming models and data analytics tools for the development of distributed Big Data applications. As framework components, multiple data models are supported (e.g. data streams, multidimensional data, etc.) and efficient mechanisms to ensure privacy and security, on top of a QoS-aware layer for the smart and rapid provisioning of resources in a cloud-based environment.",2018,23256664
47,10.1109/ICBAIE52039.2021.9389892,RESEARCH AND APPLICATION OF BIG DATA ANALYSIS FOR OIL AND GAS PRODUCTION,"The history of oil and gas development and production is a history of data development. The generation of a large amount of information data has laid the cornerstone for the application of big data analysis. How to effectively mine data resources, use big data analysis to guide oilfield production practices, and provide a theoretical basis for decision-making to improve quality and efficiency is the technology core. In recent years, Huabei Oilfield has explored the application of big data analysis in oil and gas production. According to the types and characteristics of oilfield data, it has proposed and created a closed-loop big data analysis “seven-step method” system from acquisition, processing, tracking, and evaluation, preliminary designed and developed a data mining platform for oil production engineering based on Hadoop/Spark; The platform has been applied in 6 oil and gas production units and achieved remarkable social and economic benefits.",2021,
48,10.1109/ICBDA51983.2021.9403054,BIG DATA DRIVEN MODEL FOR NEW YORK TAXI TRIPS ANALYSIS,"Due to the accumulation of large amount of evolution in metropolitan areas, urban data is understood and has become the first-hand prospect of manageable data, the driven analysis which can be recycled to improve life quality in urban areas. In this study, the influential factors on total fare amount charged to passengers are explored and analysed by using the taxi trip data in the year 2015 to provide the insights for people in NYC to plan their trips in a most economically efficient way.",2021,
49,10.1109/BigDataCongress.2015.35,BIG DATA PRE-PROCESSING: A QUALITY FRAMEWORK,"With the abundance of raw data generated from various sources, Big Data has become a preeminent approach in acquiring, processing, and analyzing large amounts of heterogeneous data to derive valuable evidences. The size, speed, and formats in which data is generated and processed affect the overall quality of information. Therefore, Quality of Big Data (QBD) has become an important factor to ensure that the quality of data is maintained at all Big data processing phases. This paper addresses the QBD at the pre-processing phase, which includes sub-processes like cleansing, integration, filtering, and normalization. We propose a QBD model incorporating processes to support Data quality profile selection and adaptation. In addition, it tracks and registers on a data provenance repository the effect of every data transformation happened in the pre-processing phase. We evaluate the data quality selection module using large EEG dataset. The obtained results illustrate the importance of addressing QBD at an early phase of Big Data processing lifecycle since it significantly save on costs and perform accurate data analysis.",2015,23797703
50,10.1109/ICABCD.2018.8465129,DISCOVERING MOST IMPORTANT DATA QUALITY DIMENSIONS USING LATENT SEMANTIC ANALYSIS,"Big Data quality is a field which is emerging. Many authors nowadays agree that data quality is still very relevant, even for Big Data uses. However, there is a lack of frameworks or guidelines about how to carry out those big data quality initiatives. The starting point of any data quality work is to determine the properties of data quality, termed as data quality dimensions (DQDs). Even those dimensions lack precise rigour in terms of definition from existing literature. This current research aims to contribute towards identifying the most important DQDs for big data in the health industry. It is a continuation of a previous work, which already identified five most important DQDs, using a human judgement based technique known as inner hermeneutic cycle. To remove potential bias coming from the human judgement aspect, this research uses the same set of literature but applies a statistical technique known to extract knowledge from a set of documents known as latent semantic analysis. The results confirm only 2 similar most important DQDs, namely accuracy and completeness.",2018,
51,10.1109/DeSE.2019.00072,DATA QUALITY MANAGEMENT FOR BIG DATA APPLICATIONS,"Currently, as a result of the continuous increase of data, one of the key issues is the development of systems and applications to deal with storage, management and processing of big numbers of data. These data are found in unstructured ways. Data management with traditional approaches is inappropriate because of the large and complex data sizes. Hadoop is a suitable solution for the continuous increase in data sizes. The important characteristics of the Hadoop are distributed processing, high storage space, and easy administration. Hadoop is better known for distributed file systems. In this paper, we have proposed techniques and algorithms that deal with big data including data collecting, data preprocessing, algorithms for data cleaning, A Technique for Converting Unstructured Data to Structured Data using metadata, distributed data file system (fragmentation algorithm) and Quality assurance algorithms by using the model is the statistical model to evaluate the highest educational institutions. We concluded that Metadata accelerates query response required and facilitates query execution, metadata will be content for reports, fields and descriptions. Total time access for three complex queries in distributed processing it is 00: 03: 00 per second while in nondistributed processing it is at 00: 15: 77 per second, average is approximately five minutes per second. Quality assurance note values (T-test) is 0.239 and values (T-dis) is 1.96, as a result of dealing with scientific sets and humanities sets. In the comparison law, it can be deduced that if the t-test is smaller than the t-dis; so there is no difference between the mean of the scientific and humanities samples, the values of C.V for both scientific is (8.585) and humanities sets is (7.427), using the law of homogeneity know whether any sets are more homogeneous whenever the value of a small C.V was more homogeneous however the humanity set is more homogeneity.",2019,21611343
52,10.1109/BigData.2018.8621924,SPRING BOOT BASED REST API TO IMPROVE DATA QUALITY REPORT GENERATION FOR BIG SCIENTIFIC DATA: ARM DATA CENTER EXAMPLE,"Web application technologies are growing rapidly with continuous innovation and improvements. This paper focuses on the popular Spring Boot [1] java-based framework for building web and enterprise applications and how it provides the flexibility for service-oriented architecture (SOA). One challenge with any Spring-based applications is its level of complexity with configurations. Spring Boot makes it easy to create and deploy stand-alone, production-grade Spring applications with very little Spring configuration. Example, if we consider Spring Model-View-Controller (MVC) framework [2], we need to configure dispatcher servlet, web jars, a view resolver, and component scan among other things. To solve this, Spring Boot provides several Auto Configuration options to setup the application with any needed dependencies. Another challenge is to identify the framework dependencies and associated library versions required to develop a web application. Spring Boot offers simpler dependency management by using a comprehensive, but flexible, framework and the associated libraries in one single dependency, which provides all the Spring related technology that you need for starter projects as compared to CRUD web applications. This framework provides a range of additional features that are common across many projects such as embedded server, security, metrics, health checks, and externalized configuration. Web applications are generally packaged as war and deployed to a web server, but Spring Boot application can be packaged either as war or jar file, which allows to run the application without the need to install and/or configure on the application server. In this paper, we discuss how Atmospheric Radiation Measurement (ARM) Data Center (ADC) at Oak Ridge National Laboratory, is using Spring Boot to create a SOA based REST [4] service API, that bridges the gap between frontend user interfaces and backend database. Using this REST service API, ARM scientists are now able to submit reports via a user form or a command line interface, which captures the same data quality or other important information about ARM data.",2018,
53,10.1109/ICDS47004.2019.8942297,TOWARDS A MULTI-AGENTS MODEL FOR ERRORS DETECTION AND CORRECTION IN BIG DATA FLOWS,"The quality of data for decision-making will always be a major factor for companies that want to remain competitors. In addition, the era of Big Data has brought new challenges for the processing, management, storage of data and in particular the challenge represented by the veracity of these data which is one of the 5Vs that characterizes Big Data. This characteristic that defines the quality or reliability of the data and its sources must be verified in the future systems of each company. In this paper, we present an approach that helps to improve the quality of Big Data by the distributed execution of algorithms for detecting and correcting data errors. The idea is to have a multi-agents model for errors detection and correction in big data flow. This model linked to a repository specific to each company. This repository contains the most frequent errors, metadata, error types, error detection algorithms and error correction algorithms. Each agent of this model represents an algorithm and will be deployed in multiple instances when needed. The use of these agents will go through two steps. In the first step, the detection agents and error correction agents manage each flow entering the system in real time. In the second step, all the processed data flows in first step will be a dataset to which the error detection and correction agents are applied in batch in order to process other types of errors. Among architectures who allow this processing type, we have chosen Lambda architecture.",2019,
54,10.1109/WOCC.2018.8372743,BIG DATA PLATFORM FOR AIR QUALITY ANALYSIS AND PREDICTION,"With the advance of industry, air quality (AQ) is increasingly becoming worse. There are increasingly AQ monitors device have been deployed around country for monitoring air-quality all year long. To estimate and predict AQ, such as PM (particulate matter) 2.5, become an important issue for government to improve people's quality of life. As we can know, there are many factors can affect the AQ, such as traffic, factory exhaust emissions, weather, incineration of garbage, and so on. In most well-developed countries, these pollution sources are monitored for future environmental policy making. In this paper, we will propose a semantic ETL (Extract-Transform-Load) framework on cloud platform for AQ prediction. In the platform, we exploit ontology to concretize the relationship of PM 2.5 from various data sources and to merge those data with the same concept but different naming into the unified database. We implement the ETL framework on the cloud platform, which includes computing nodes and storage nodes. The computing nodes are used to execute data mining algorithms for predicting, and storage modes are used to store retrieved, preprocessed, and analyzed data. We utilize restful web service as the front end API to retrieve analyzed data, and finally we exploit browser to show the visualized result to demonstrate the estimation and prediction. It shows that the big data access framework on the cloud platform can work well for air quality analysis.",2018,23791276
55,10.1109/BigDataSecurity-HPSC-IDS.2016.52,ASPECTS OF DATA CATALOGUING FOR ENTERPRISE DATA PLATFORMS,"As the adoption of enterprise Big Data platforms mature, the necessity to maintain systematic catalogue of data being processed and managed by these platforms becomes imperative. Enterprise data catalogues serve as centralized repositories of storing such metadata about data being handled by such platforms, enabling better data governance, security and control. Standardized approaches, methodologies and tools for data catalogues are being discussed and evolved. This paper introduces some key variables, attributes and indexes that need to be handled in such cataloguing solutions -- such as data contexts, data-system relationships, data quality, reliability, sensitivity and accessibility. The paper also discusses specific approaches on how each of these aspects can be adopted and applied to different enterprise contexts effectively.",2016,
56,10.1109/BigData.2017.8258338,BIG DATA MACHINE LEARNING USING APACHE SPARK MLLIB,"Artificial intelligence, and particularly machine learning, has been used in many ways by the research community to turn a variety of diverse and even heterogeneous data sources into high quality facts and knowledge, providing premier capabilities to accurate pattern discovery. However, applying machine learning strategies on big and complex datasets is computationally expensive, and it consumes a very large amount of logical and physical resources, such as data file space, CPU, and memory. A sophisticated platform for efficient big data analytics is becoming more important these days as the data amount generated in a daily basis exceeds over quintillion bytes. Apache Spark MLlib is one of the most prominent platforms for big data analysis which offers a set of excellent functionalities for different machine learning tasks ranging from regression, classification, and dimension reduction to clustering and rule extraction. In this contribution, we explore, from the computational perspective, the expanding body of the Apache Spark MLlib 2.0 as an open-source, distributed, scalable, and platform independent machine learning library. Specifically, we perform several real world machine learning experiments to examine the qualitative and quantitative attributes of the platform. Furthermore, we highlight current trends in big data machine learning research and provide insights for future work.",2017,
57,10.1109/ACCESS.2015.2490723,EVALUATING THE QUALITY OF SOCIAL MEDIA DATA IN BIG DATA ARCHITECTURE,"The use of freely available online data is rapidly increasing, as companies have detected the possibilities and the value of these data in their businesses. In particular, data from social media are seen as interesting as they can, when properly treated, assist in achieving customer insight into business decision making. However, the unstructured and uncertain nature of this kind of big data presents a new kind of challenge: how to evaluate the quality of data and manage the value of data within a big data architecture? This paper contributes to addressing this challenge by introducing a new architectural solution to evaluate and manage the quality of social media data in each processing phase of the big data pipeline. The proposed solution improves business decision making by providing real-time, validated data for the user. The solution is validated with an industrial case example, in which the customer insight is extracted from social media data in order to determine the customer satisfaction regarding the quality of a product.",2015,21693536
58,10.1109/ELECOM49001.2020.9297009,A QUALITATIVE ASSESSMENT OF MACHINE LEARNING SUPPORT FOR DETECTING DATA COMPLETENESS AND ACCURACY ISSUES TO IMPROVE DATA ANALYTICS IN BIG DATA FOR THE HEALTHCARE INDUSTRY,"Tackling Data Quality issues as part of Big Data can be challenging. For data cleansing activities, manual methods are not efficient due to the potentially very large amount of data.. This paper aims to qualitatively assess the possibilities for using machine learning in the process of detecting data incompleteness and inaccuracy, since these two data quality dimensions were found to be the most significant by a previous research study conducted by the authors. A review of existing literature concludes that there is no unique machine learning algorithm most suitable to deal with both incompleteness and inaccuracy of data. Various algorithms are selected from existing studies and applied against a representative big (healthcare) dataset. Following experiments, it was also discovered that the implementation of machine learning algorithms in this context encounters several challenges for Big Data quality activities. These challenges are related to the amount of data particualar machine learning algorithms can scale to and also to certain data type restrictions imposed by some machine learning algorithms. The study concludes that 1) data imputation works better with linear regression models, 2) clustering models are more efficient to detect outliers but fully automated systems may not be realistic in this context. Therefore, a certain level of human judgement is still needed.",2020,
59,10.1109/ICIICII.2016.0052,BIG DATA ORIENTED MACRO-QUALITY INDEX BASED ON CUSTOMER SATISFACTION INDEX AND PLS-SEM FOR MANUFACTURING INDUSTRY,"The aim of this paper was to develop a novel macro-quality index driven by big quality data regarding the macro-quality management demands of manufacturing enterprises in Industry 4.0. Firstly, the connotation of big data of macro-quality management in manufacturing industry is expounded, which is the collection of the quality data in product lifecycle including the product quality data, process quality data and organizational ability data. Secondly, referring to the customer satisfaction index theory, a new big data oriented macro-quality index computation model based on the partial least square-structural equation modeling(PLS-SEM) theory is proposed, and the partial least square(PLS) is adopted to estimate the path parameters. Finally, a case study of macro-quality situation evaluation for the manufacturing industry of a city in China is presented. The final result shows that the proposed macro-quality index model is applicable and predictable.",2016,
60,10.1109/BigData.2016.7840595,ANTECEDENTS OF BIG DATA QUALITY: AN EMPIRICAL EXAMINATION IN FINANCIAL SERVICE ORGANIZATIONS,"Big data has been acknowledged for its enormous potential. In contrast to the potential, in a recent survey more than half of financial service organizations reported that big data has not delivered the expected value. One of the main reasons for this is related to data quality. The objective of this research is to identify the antecedents of big data quality in financial institutions. This will help to understand how data quality from big data analysis can be improved. For this, a literature review was performed and data was collected using three case studies, followed by content analysis. The overall findings indicate that there are no fundamentally new data quality issues in big data projects. Nevertheless, the complexity of the issues is higher, which makes it harder to assess and attain data quality in big data projects compared to the traditional projects. Ten antecedents of big data quality were identified encompassing data, technology, people, process and procedure, organization, and external aspects.",2016,
61,10.1109/ICCSE.2017.8085555,EXPERIMENTAL TEACHING DESIGN AND PRACTICE ON BIG DATA COURSE,"With the rapid development of big data technology and the rapid growth of big data industry market, big data talent demand is also a substantial increase in China. In order to cultivate more talented people satisfying the needs of the community, we have designed the big data course for undergraduates. The big data course stresses not only on many theories but also lots of practice. The project of “big data talent development trend analysis” is designed in the experimental teaching on big data. By doing this project, students can master all the technologies of big data processing lifecycle, including data collection, data preprocessing, data mining and data visualization. We evaluate students who master big data core technology with a multi-evaluation method and design the experiment evaluation system on big data. Through our two years' practice, the results show that all these designs have achieved the good effect and improved the teaching quality.",2017,24739464
62,10.1109/ICSCDS53736.2022.9760944,COMPUTER-AIDED REALIZATION OF INNOVATIVE CLOTHING DESIGN UNDER THE BACKGROUND OF BIG DATA: FROM THE PERSPECTIVE OF IMAGE QUALITY EVALUATION,"This paper studies the computer-aided realization of innovative clothing design under the background of big data from the perspective of image quality evaluation. Now it explains the application of computer technology in the production of renderings, analyzes the current situation of computer-aided design in clothing design, and proposes some application strategies. To promote the innovative development of the apparel design industry. Combining the background of big data, this article proposes an innovative clothing design strategy based on image quality evaluation. Starting from the various elements of clothing design, from the perspective of data mining, discover fashion elements to achieve the purpose of clothing design innovation, so as to meet the ever-changing social needs.",2022,
63,10.1109/BigData.2016.7840935,BAD BIG DATA SCIENCE,"As hardware and software technologies have improved, our definition of a “manageable amount of data” has increased in its scope dramatically. The term “big data” can be applied to any of several different projects and technologies sharing the ultimate goal of supporting analysis on these large, heterogeneous, and evolving data sets. The term “data science” refers to the statistical, technical, and domain-specific knowledge required to ensure that the analysis is done properly. Techniques for managing some common causes for bad data and invalid analysis have been used in other areas, such as data warehousing and distributed database. However, big data projects face special challenges when trying to combine big data and data science without producing inaccurate, misleading, or invalid results. This paper discusses potential causes for “bad big data science”, focusing primarily on the data quality of the input data, and suggests methods for minimizing them based on techniques originally developed for data warehousing and distributed database projects.",2016,
64,10.1109/CAC48633.2019.8996332,AIR QUALITY DATA ANALYSIS AND FORECASTING PLATFORM BASED ON BIG DATA,"Nowadays, with the continuous development of big data technology, various industries use big data technology to process and mine massive data, and realize the value of data efficiently. In terms of air quality data processing, big data technology can also play a certain advantage. The platform is based on big data technology to design an air quality data analysis and prediction platform including data layer, business layer, interaction layer and visualization platform. Data is cleaned, calibrated, and stored in the data layer to ensure data consistency, integrity, and security. The air quality data is analyzed and predicted at the business layer. The interaction layer includes the functions of algorithm management, data query, and the data visualization platform provides intuitive information display. This design is a significant application for fully exploiting environmental data information. It has powerful data processing functions and scalability, which is a reliable data analysis and prediction platform.",2019,2688092X
65,10.1109/BigData.2016.7840586,DD-RTREE: A DYNAMIC DISTRIBUTED DATA STRUCTURE FOR EFFICIENT DATA DISTRIBUTION AMONG CLUSTER NODES FOR SPATIAL DATA MINING ALGORITHMS,"Parallelizing data mining algorithms has become a necessity as we try to mine ever increasing volumes of data. Spatial data mining algorithms like Dbscan, Optics, Slink, etc. have been parallelized to exploit a cluster infrastructure. The efficiency achieved by existing algorithms can be attributed to spatial locality preservation using spatial indexing structures like k-d-tree, quad-tree, grid files, etc. for distributing data among cluster nodes. However, these indexing structures are static in nature, i.e., they need to scan the entire dataset to determine the partitioning coordinates. This results in high data distribution cost when the data size is large. In this paper, we propose a dynamic distributed data structure, DD-Rtree, which preserves spatial locality while distributing data across compute nodes in a shared nothing environment. Moreover, DD-Rtree is dynamic, i.e., it can be constructed incrementally making it useful for handling big data. We compare the quality of data distribution achieved by DD-Rtree with one of the recent distributed indexing structure, SD-Rtree. We also compare the efficiency of queries supported by these indexing structures along with the overall efficiency of DBSCAN algorithm. Our experimental results show that DD-Rtree achieves better data distribution and thereby resulting in improved overall efficiency.",2016,
66,10.1109/WHISPERS.2015.8075482,BIG DATA CHALLENGES IN CHINA CENTRE FOR RESOURCES SATELLITE DATA AND APPLICATION,"China Centre for Resources Satellite Data and Application (abbreviate as CRESDA) is a core platform to store, process, analyze, and distribute land observing satellite data in China. It can provide high quality and effective services for the State Council and the relevant departments of government and local authorities. In the era of big data, the data center benefits from big data opportunities as well as suffering from big data challenges. In the paper, the big data challenges of the CRESDA are summarized. In particular, four major challenges are comprised of the 3V dimensions of big data (i.e. Volume, Variety, and Velocity) and one specific challenge (i.e., extensibility) in the data center.",2015,21586276
67,10.1109/CIBDA50819.2020.00049,QUALITY MANAGEMENT OF CROWD SENSING DATA BASED ON MACHINE LEARNING,"Recently, research on crowd sensing data quality management is a new subject area developed based on wireless sensor network related concepts. Crowd sensing data has also experienced many links in the process of network propagation, so it is inevitable that there are abnormal data in the database. Therefore, how to filter these unreliable data to get more real data is particularly important. This paper takes somatosensory temperature as an example, solves the problem of calculating the similarity of unequal long-term sequences by using DTW technology, and then clusters and compares the data in the database to find out the abnormal data. Thereby, the accuracy of the somatosensory temperature database is improved, and relevant users can obtain more accurate information. The experimental results show that when the minimum DTW value exceeds the threshold t = 0.7, the more the number of simulation sequences, the more stable the accuracy rate, and the faster the growth rate of the running time.",2020,
68,10.1109/BDC.2015.34,LINKED 'BIG' DATA: TOWARDS A MANIFOLD INCREASE IN BIG DATA VALUE AND VERACITY,"The Web of Data is an increasingly rich source of information, which makes it useful for Big Data analysis. However, there is no guarantee that this Web of Data will provide the consumer with truthful and valuable information. Most research has focused on Big Data's Volume, Velocity, and Variety dimensions. Unfortunately, Veracity and Value, often regarded as the fourth and fifth dimensions, have been largely overlooked. In this paper we discuss the potential of Linked Data methods to tackle all five V's, and particularly propose methods for addressing the last two dimensions. We draw parallels between Linked and Big Data methods, and propose the application of existing methods to improve and maintain quality and address Big Data's veracity challenge.",2015,
69,10.1109/MIPR.2019.00093,A BIG DATA PLATFORM FOR SURFACE ENHANCED RAMAN SPECTROSCOPY DATA WITH AN APPLICATION ON IMAGE-BASED SENSOR QUALITY CONTROL,"Surface-enhanced Raman spectroscopy (SERS) significantly enhances the Raman scattering by molecules, enabling detection and identification of small quantities of relevant bio-/chemical markers in a wide range of applications. In this paper, we present a big data platform with both a local client and cloud server built for acquiring, processing, visualizing and storing SERS sensor data. The local client controls the hardware (i.e., spectrometer and stage) to collect SERS spectra from HP designed sensors, and offers the options to analyze, visualize and save the spectra with meta-data records, including relevant experimental conditions. The cloud server contains remote databases and web interface for centralized data management to users from different locations. Here we describe how this platform was built and demonstrate its use for automated sensor quality control based on sensor images. Sensor quality control is a common practice, employed in sensor production to select high performing sensors. Image-based approach is a natural way to perform sensor quality control without destructing the sensors. Automating this process using the proposed platform can also reduce the time spent and achieve consistent result by avoiding human visual inspection.",2019,
70,,TOWARDS A COMPREHENSIVE DATA LIFECYCLE MODEL FOR BIG DATA ENVIRONMENTS,"A huge amount of data is constantly being produced in the world. Data coming from the IoT, from scientific simulations, or from any other field of the eScience, are accumulated over historical data sets and set up the seed for future Big Data processing, with the final goal to generate added value and discover knowledge. In such computing processes, data are the main resource, however, organizing and managing data during their entire life cycle becomes a complex research topic. As part of this, Data LifeCycle (DLC) models have been proposed to efficiently organize large and complex data sets, from creation to consumption, in any field, and any scale, for an effective data usage and big data exploitation. 2. Several DLC frameworks can be found in the literature, each one defined for specific environments and scenarios. However, we realized that there is no global and comprehensive DLC model to be easily adapted to different scientific areas. For this reason, in this paper we describe the Comprehensive Scenario Agnostic Data LifeCycle (COSA-DLC) model, a DLC model which: i) is proved to be comprehensive as it addresses the 6Vs challenges (namely Value, Volume, Variety, Velocity, Variability and Veracity, and ii), it can be easily adapted to any particular scenario and, therefore, fit the requirements of a specific scientific field. In this paper we also include two use cases to illustrate the ease of the adaptation in different scenarios. We conclude that the comprehensive scenario agnostic DLC model provides several advantages, such as facilitating global data management, organization and integration, easing the adaptation to any kind of scenario, guaranteeing good data quality levels and, therefore, saving design time and efforts for the scientific and industrial communities.",2016,
71,10.1109/ICDSBA53075.2021.00038,THE ROLE AND APPLICATION OF BIG DATA TECHNOLOGY IN DECISION-MAKING MANAGEMENT OF COLLEGES AND UNIVERSITIES,"with the advent of the era of big data, people’s way of thinking and habits have changed greatly. In Colleges and universities, big data has the characteristics of massive, high growth and diversity. Colleges and universities should seize the opportunity to tap the value of campus big data. Data driven self-development, from the ideological and political education, teaching quality, advantageous disciplines, social services, scientific research achievements and international influence and other aspects of data collection, extraction, cleaning, association analysis and mining, construction of university data ""one table"" project; and on this basis, build big data auxiliary system to assist managers in administrative decision-making, in order to promote the comprehensive construction of universities It brings new ideas for the management of colleges and universities in the new era.",2021,
72,10.1109/ICECOCS50124.2020.9314391,BIG DATA VALUE CHAIN: A UNIFIED APPROACH FOR INTEGRATED DATA QUALITY AND SECURITY,"Big Data has grown significantly in recent years. This growth has led organizations to adopt Big Data Value Chains (BDVC) as the appropriate framework for unlocking the value to make suitable decisions. Despite its promising opportunities, Big Data raises new concerns such as data quality and security that could radically impact the effectiveness of the BDVC. These two essential aspects have become an urgent need for any Big Data project to provide meaningful datasets and reliable insights. In this contribution, we highlight the importance of considering data quality and security requirements. Then, we propose a coherent, unified framework that extends BDVC with security and quality aspects. Through quality and security reports, the model can self-evaluate and arrange tasks according to orchestration and monitoring process, allowing the BDVC to evolve at the organization pace and to align strategically with its objectives as well as to federate a sustainable ecosystem.",2020,
73,10.1109/ICCBE56101.2022.9888215,RESEARCH AND DESIGN OF CONSTRUCTION ENGINEERING QUALITY MANAGEMENT SYSTEM BASED ON BIG DATA AND BIM TECHNOLOGY,"With the rapid development of information technologies such as big data, cloud computing, the Internet of Things, and 5G in recent years, the construction industry, as the traditional industry, urgently requests the transformation of its project management from extensive and low-efficiency mode to high-quality development mode. Under the background of the gradual development of newly-emerged technologies and concepts such as BIM technology, smart construction, and digital twin and based on the full investigation of the quality management requirements of construction enterprises, closed-loop management is focused on the whole process of construction engineering. From the perspective of construction engineering quality management, current theoretical tools such as the Internet of Things, big data, and BIM technology are combined. Taking Python as a basis, the mature and open-source WEB framework, database, and front-end and back-end technologies are used to design and construct a set of construction engineering quality management systems. We explore the digital potential of construction engineering quality management systems to provide a new way of thinking for the informatization, systematization, and system process-oriented development of construction engineering quality management.",2022,
74,10.1109/ICIS51600.2021.9516862,ANALYSIS ON THE QUALITY MODEL OF BIG DATA SOFTWARE,"With the rapid development of the big data system, The big data system has the characteristics of large data scale, diverse data and high computational complexity. Its testing method has to be constantly improved. By analyzing the general software quality model, and combining the characteristics of the big data software, a set of quality model for the big data software is formed.",2021,
75,10.1109/IRI.2019.00039,TOWARDS FEDERATED LEARNING APPROACH TO DETERMINE DATA RELEVANCE IN BIG DATA,"In the past few years, data has proliferated to astronomical proportions; as a result, big data has become the driving force behind the growth of many machine learning innovations. However, the incessant generation of data in the information age poses a needle in the haystack problem, where it has become challenging to determine useful data from a heap of irrelevant ones. This has resulted in a quality over quantity issue in data science where a lot of data is being generated, but the majority of it is irrelevant. Furthermore, most of the data and the resources needed to effectively train machine learning models are owned by major tech companies, resulting in a centralization problem. As such, federated learning seeks to transform how machine learning models are trained by adopting a distributed machine learning approach. Another promising technology is the blockchain, whose immutable nature ensures data integrity. By combining the blockchain's trust mechanism and federated learning's ability to disrupt data centralization, we propose an approach that determines relevant data and stores the data in a decentralized manner.",2019,
76,10.1145/3180155.3182534,[JOURNAL FIRST] EXPERIENCES AND CHALLENGES IN BUILDING A DATA INTENSIVE SYSTEM FOR DATA MIGRATION,"Recent analyses[2, 4, 5] report that many sectors of our economy and society are more and more guided by data-driven decision processes (e.g., health care, public administrations, etc.). As such, Data Intensive (DI) applications are becoming more and more important and critical. They must be fault-tolerant, they should scale with the amount of data, and be able to elastically leverage additional resources as and when these last ones are provided [3]. Moreover, they should be able to avoid data drops introduced in case of sudden overloads and should offer some Quality of Service (QoS) guarantees. Ensuring all these properties is, per se, a challenge, but it becomes even more difficult for DI applications, given the large amount of data to be managed and the significant level of parallelism required for its components. Even if today some technological frameworks are available for the development of such applications (for instance, think of Spark, Storm, Flink), we still lack solid software engineering approaches to support their development and, in particular, to ensure that they offer the required properties in terms of availability, throughput, data loss, etc. In fact, at the time of writing, identifying the right solution can require several rounds of experiments and the adoption of many different technologies. This implies the need for highly skilled persons and the execution of experiments with large data sets and a large number of resources, and, consequently, a significant amount of time and budget. To experiment with currently available approaches, we performed an action research experiment focusing on developing- testing-reengineering a specific DI application, Hegira4Cloud, that migrates data between widely used NoSQL databases, including so-called Database as a Service (DaaS), as well as on-premise databases. This is a representative DI system because it has to handle large volumes of data with different structures and has to guarantee that some important characteristics, in terms of data types and transactional properties, are preserved. Also, it poses stringent requirements in terms of correctness, high performance, fault tolerance, and fast and effective recovery. In our action research, we discovered that the literature offered some high level design guidelines for DI applications, as well as some tools to support modelling and QoS analysis/simulation of complex architectures, however the available tools were not yet. suitable to support DI systems. Moreover, we realized that the available big data frameworks we could have used were not flexible enough to cope with all possible application-specific aspects of our system. Hence, to achieve the desired level of performance, fault tolerance and recovery, we had to adopt a time-consuming, experiment-based approach [1, 6], which, in our case, consisted of three iterations: (1) the design and implementation of a Mediation Data Model capable of managing data extracted from different databases, together with a first monholitical prototype of Hegira4Cloud; (2) the improvement of performance of our prototype when managing and transferring huge amounts of data; (3) the introduction of fault-tolerant data extraction and management mechanisms, which are independent from the targeted databases. Among the others, an important issue that has forced us to reiterate in the development of Hegira4Cloud concerned the DaaS we interfaced with. In particular these DaaS, which are well-known services with a large number of users: (1) were missing detailed information regarding the behaviour of their APIs; (2) did not offer a predictable service; (3) were suffering of random downtimes not correlated with the datasets we were experimenting with. In this journal first presentation, we describe our experience and the issues we encountered that led to some important decisions during the software design and engineering process. Also, we analyse the state of the art of software design and verification tools and approaches in the light of our experience, and identify weaknesses, alternative design approaches and open challenges that could generate new research in these areas. More details can be found in the journal publication.",2018,15581225
77,10.1109/BigData.2017.8257913,ENHANCING DATA QUALITY BY CLEANING INCONSISTENT BIG RDF DATA,"We address the problem of dealing with inconsistencies in fusion of big data sources using Resource Description Framework (RDF) and ontologies. We propose a scalable approach ensuring data quality for query answering over big RDF data in a distributed way on a Spark ecosystem. In so doing, the cleaning inconsistent big RDF data approach is built on the following steps (1) modeling consistency rules to detect the inconsistency triples even if it is implicitly hidden including inference and inconsistent rules (2) detecting inconsistency through rule evaluation based on Apache Spark framework to discover the minimally sub-set of inconsistent triples (3) cleaning the inconsistency through finding the best repair for consistent query answering.",2017,
78,10.1109/ICITBS.2015.220,THE GEOGRAPHIC ENVIRONMENT ANALYSIS OF REGIONAL ECONOMIC DEVELOPMENT OF YUNNAN PROVINCE OF CHINA BASED ON THE BIG DATA TECHNOLOGY,"This paper uses the analysis methods of data mining, data quality and management, semantic engine and prediction in big data to analyze the geographic environment of the Yunnan's economic development from its special location and geographic elements, beholding that the geographic elements lay basic material conditions for its economic development of Yunnan province. Geographic factors on one hand encourage the economic development and on the other hand affect together the economic development of Yunnan province.",2015,
79,10.1109/BigData47090.2019.9006187,"DQA: SCALABLE, AUTOMATED AND INTERACTIVE DATA QUALITY ADVISOR","Fueled with growth in the fields of Internet of Things (IoT) and Big Data, data has become one of the most valuable assets in today's world. While we are leveraging this data for analyzing complex systems using machine learning and deep learning, a considerable amount of time and effort is spent on addressing data quality issues. If undetected, data quality issues can cause large deviations in the analysis, misleading data scientists. To ease the effort of identifying and addressing data quality challenges, we introduce DQA, a scalable, automated and interactive data quality advisor. In this paper, we describe the DQA framework, provide detailed description of its components and the benefits of integrating it in a data science process. We propose a programmatic approach for implementing the data quality framework which automatically generates dynamic executable graphs for performing data validations fine-tuned for a given dataset. We discuss the use of DQA to build a library of validation checks common to many applications. We provide insight into how DQA addresses many persistence and usability issues which currently make data cleaning a laborious task for data scientists. Finally, we provide a case study of how DQA is implemented in a realworld system and describe the benefits realized.",2019,
80,10.1109/ICCCBDA.2018.8386523,RESEARCH ON RELIABILITY EVALUATION OF BIG DATA SYSTEM,"The application of big data system is now more pervasive. The reliability of the large data system is crucial to both the academic and the industry. However, to date there are few studies on the reliability of the big data system, and lack of evaluation model. This paper uses the fault tree to model the reliability of the big data system on the cloud. The type of faults is summarized and the cause of fault is analyzed by experiments. The fault tree analysis (FTA) is used to evaluate the reliability of the big data system, which can provide reference for the fault processing and quality assurance of big data system.",2018,
81,10.1109/BigData.2015.7363865,ONLINE ANOMALY DETECTION OVER BIG DATA STREAMS,"Data quality is a challenging problem in many real world application domains. While a lot of attention has been given to detect anomalies for data at rest, detecting anomalies for streaming applications still largely remains an open problem. For applications involving several data streams, the challenge of detecting anomalies has become harder over time, as data can dynamically evolve in subtle ways following changes in the underlying infrastructure. In this paper, we describe and empirically evaluate an online anomaly detection pipeline that satisfies two key conditions: generality and scalability. Our technique works on numerical data as well as on categorical data and makes no assumption on the underlying data distributions. We implement two metrics, relative entropy and Pearson correlation, to dynamically detect anomalies. The two metrics we use provide an efficient and effective detection of anomalies over high velocity streams of events. In the following, we describe the design and implementation of our approach in a Big Data scenario using state-of-the-art streaming components. Specifically, we build on Kafka queues and Spark Streaming for realizing our approach while satisfying the generality and scalability requirements given above. We show how a combination of the two metrics we put forward can be applied to detect several types of anomalies - like infrastructure failures, hardware misconfiguration or user-driven anomalies - in large-scale telecommunication networks. We also discuss the merits and limitations of the resulting architecture and empirically evaluate its scalability on a real deployment over live streams capturing events from millions of mobile devices.",2015,
82,10.1109/BigData.2015.7363900,BIG DATA PROCESS ANALYTICS FOR CONTINUOUS PROCESS IMPROVEMENT IN MANUFACTURING,"One of the most important challenges in manufacturing is the continuous process improvement that requires new insights about the behavior/quality control of processes in order to understand the optimization/improvement potential. The paper elaborates on usage of big data-driven clustering for an efficient discovering of real-time unusualities in the process and their route-cause analysis. Our approach extends traditional clustering algorithms (like k-Means) with methods for better understanding the nature of clusters and provides a very efficient big data realization. We argue that this approach paves the way for a new generation of quality management tools based on big data analytics that will extend traditional statistical process control and empower Lean Six Sigma through big data processing. The proposed approach has been applied for improving process control in Whirlpool (washing machine tests, factory in Italy) and we present the most important finding from the evaluation study.",2015,
83,10.1109/CIST.2018.8596389,A STUDY OF HANDLING MISSING DATA METHODS FOR BIG DATA,"Improving data quality is not a recent field but in the context of big data this is a challenging area as there is a crucial need for data quality to, for example, increase the accuracy of big data analytics or avoid storing redundant data. Missing data is one of the major problem that faces the quality of data. There are several methods and approaches that have been used in relational databases to handle missing data most of which have been adapted to big data. This paper aims to provide an overview of some methods and approaches for handling missing data in big data contexts.",2018,2327185X
84,10.1109/SNAMS53716.2021.9732098,STILL OPEN PROBLEMS IN DATA WAREHOUSE AND DATA LAKE RESEARCH: EXTENDED ABSTRACT,"During recent years, we observe a widespread of new data sources, especially all types of social media and IoT devices, which produce huge data volumes, whose content ranges from fully structured to totally unstructured. All these types of data are commonly referred to as big data. They are typically described by the three most important characteristics, called 3V [1], namely: an extremely large volume, a variety of data models and structures (data representations), as well as a high velocity at which data are generated. We argue that out of these three Vs, the most challenging is variety [2]. Such data need to be integrated and transformed into a common representation, which is suitable for analysis, in a similar manner as traditional (mainly table-like) data.",2021,
85,10.1109/CSCloud/EdgeCom.2018.00025,REVIEW ON BIG DATA FUSION METHODS OF QUALITY INSPECTION FOR CONSUMER GOODS,"Quality big data comes from a wide range, the problem is how to eliminate the structure barriers between various types of data from different sources, to achieve the effective integration for isolated or fragmented data, and then to mine the information, knowledge and wisdom according to the actual needs. It is urgent for the quality inspection departments to solve the problem of making a decision the first time and take preventive measures. This paper introduced the research status of named entity recognition, solid unified detection, data conflict resolution, data fusion method from the characteristics of quality inspection data. The existence problems are analyzed and the research direction is looking forward of the research on data fusion in this paper.",2018,
86,10.1109/iThings-GreenCom-CPSCom-SmartData.2017.28,TOWARDS A DATA QUALITY FRAMEWORK FOR HETEROGENEOUS DATA,"Every industry has significant data output as a product of their working process, and with the recent advent of big data mining and integrated data warehousing it is the case for a robust methodology for assessing the quality for sustainable and consistent processing. In this paper a review is conducted on Data Quality (DQ) in multiple domains in order to propose connections between their methodologies. This critical review suggests that within the process of DQ assessment of heterogeneous data sets, not often are they treated as separate types of data in need of an alternate data quality assessment framework. We discuss the need for such a directed DQ framework and the opportunities that are foreseen in this research area and propose to address it through degrees of heterogeneity.",2017,
87,10.1109/BigData.2017.8258270,IMPROVING DATA SHARING IN DATA RICH ENVIRONMENTS,"The increasing use of big data comes along with the problem of ensuring correct and secure data access. There is a need to maximise the data dissemination whilst controlling their access. Depending on the type of users different qualities and parts of data are shared. We introduce an alteration mechanism, more precisely a restriction one, based on a policy analysis language. The alteration reflects the level of trust and relations the users have, and are represented as policies inside the data sharing agreements. These agreements are attached to the data and are enforced every time the data are accessed, used or shared. We show the use of our alteration mechanism with a military use case, where different parties are involved during the missions, and they have different relations of trust and partnership.",2017,
88,10.1109/BigDataService.2017.42,A SURVEY ON QUALITY ASSURANCE TECHNIQUES FOR BIG DATA APPLICATIONS,"With the rapid advance of big data and cloud computing, building high quality big data systems in different application fields has gradually became a popular research topic in academia and industry as well as government agencies. However, more quality problems lead to application errors. Although the current research work has discussed how to ensure the quality of big data applications from several aspects, there is no systematic discussion on how to ensure the quality of large data applications. Therefore, a systematic study on big data application quality assurance is very necessary and critical. This paper focuses on the survey of quality assurance techniques of big data applications, and it introduces big data properties and quality attributes. It mainly discusses the key approaches to ensure the quality of big data applications and they are testing, model-driven architecture (MDA), monitoring, fault tolerance, verification and also prediction techniques. In addition, this paper also discusses the impact of big data characteristics on big data applications.",2017,
89,10.26599/BDMA.2018.9020037,A NOVEL CLUSTERING TECHNIQUE FOR EFFICIENT CLUSTERING OF BIG DATA IN HADOOP ECOSYSTEM,"Big data analytics and data mining are techniques used to analyze data and to extract hidden information. Traditional approaches to analysis and extraction do not work well for big data because this data is complex and of very high volume. A major data mining technique known as data clustering groups the data into clusters and makes it easy to extract information from these clusters. However, existing clustering algorithms, such as k-means and hierarchical, are not efficient as the quality of the clusters they produce is compromised. Therefore, there is a need to design an efficient and highly scalable clustering algorithm. In this paper, we put forward a new clustering algorithm called hybrid clustering in order to overcome the disadvantages of existing clustering algorithms. We compare the new hybrid algorithm with existing algorithms on the bases of precision, recall, F-measure, execution time, and accuracy of results. From the experimental results, it is clear that the proposed hybrid clustering algorithm is more accurate, and has better precision, recall, and F-measure values.",2019,20960654
90,10.1109/ICCASIT50869.2020.9368701,DATA MINING ON THE FLIGHT QUALITY OF AN AIRLINE BASED ON QAR BIG DATA,"At present, the airlines have made some achievements in event analysis and investigation by using their quick access record (QAR) data. But where each airline's flight quality is in the industry, and whether there is a problem in itself, the airline can't find. In order to help airlines discover the existing flight quality problems, this article uses the QAR big data of the flight operational quality assurance (FOQA) Station of CAAC, and compares the industry-wide QAR data with the QAR data of individual airlines, and founds that the take-off pitch angle of a certain aircraft of A321 models is too small, by using mathematical statistics t test to verify, found the airline's the take-off pitch angle and the industry's the take-off pitch angle exist significant difference. The correlative speed at rotation and the speed at liftoff are also analyzed, and the significant difference is found. The FOQA Station of CAAC feeds back the problem to the airline and the authority. After the investigation of the airline and the authority, there are problems with the airline. And the airline immediately starts to rectify it.",2020,
91,10.1109/BigData52589.2021.9671672,UNSUPERVISED ANOMALY DETECTION IN DATA QUALITY CONTROL,"Data is one of the most valuable assets of an organization and has a tremendous impact on its long-term success and decision-making processes. Typically, organizational data error and outlier detection processes perform manually and reactively, making them time-consuming and prone to human errors. Additionally, rich data types, unlabeled data, and increased volume have made such data more complex. Accordingly, an automated anomaly detection approach is required to improve data management and quality control processes. This study introduces an unsupervised anomaly detection approach based on models comparison, consensus learning, and a combination of rules of thumb with iterative hyper-parameter tuning to increase data quality. Furthermore, a domain expert is considered a human in the loop to evaluate and check the data quality and to judge the output of the unsupervised model. An experiment has been conducted to assess the proposed approach in the context of a case study. The experiment results confirm that the proposed approach can improve the quality of organizational data and facilitate anomaly detection processes.",2021,
92,10.1109/BigData52589.2021.9671504,"AN ADVANCED OPEN DATA PLATFORM FOR INTEGRATED SUPPORT OF DATA MANAGEMENT, DISTRIBUTION, AND ANALYSIS","With the growing applications of big data and artificial intelligence, the quality of the service is an outcome of the quality of the data. Nevertheless, there is still a significant lack of data that has practical application value. To solve these problems, we propose SODAS (Smart Open Data As a Service) as a novel open data platform for efficient data sharing and utilization. We first analyze the major problems in the legacy CKAN and then draw up their solutions through core strategies. We next define four components and nine function blocks of SODAS for each core strategy. As a result, SODAS drives Open Data Portal, Open Data Reference Model, DataMap Publisher, and ADE Provisioning (Analytics and Development Environment Provisioning) by connecting the defined function blocks. We confirm that each function works correctly through the SODAS Web portal and apply SODAS to actual data distribution sites to prove its efficiency and practical use. SODAS is the first open data platform that provides secure interoperability between heterogeneous platforms based on international standards and enables domain-free data management with flexible metadata.",2021,
93,10.1109/ICBDA.2017.8078802,INTERNET OF THINGS AND BIG DATA ANALYTICS FOR SMART OIL FIELD MALFUNCTION DIAGNOSIS,"With the rapid development of information technology and digital communication, the data types are more abundant by integration of various technologies. In this paper, based on the analysis of a large number of historical data of oil and water wells, the changes of some important parameters of the wells can be monitored and then used in the trend prediction and the early warning system. Subsequently, we use 6 Sigma algorithm to process the historical data, and by the big data trend analysis combining with various parameters, we can diagnose six operating conditions, such as sand production, abnormal of moisture content etc. Through experiments, the algorithm is stable and reliable in practical application, and it has great significance to ensure the normal production of oil field and improve the management ability for oil field.",2017,
94,10.1109/ECIT50008.2020.00026,DESIGN OF NETWORK PRECISION MARKETING BASED ON BIG DATA ANALYSIS TECHNOLOGY,"In the process of big data processing, big data analysis is the core work content. After obtaining a large amount of data, we use related analysis technology to perform data processing and analysis to obtain knowledge. Its related content includes visual analysis, data mining, predictive analysis, semantic analysis and data quality management. How to obtain big data, classify and store according to data types, mine valuable information from big data, and effectively apply big data in the field of precision marketing are hot topics of research. On the basis of researching the key technologies of big data analysis, this paper uses Hadoop big data to analyze and store the massive online logs generated by users' mobile terminals, and calculates and builds user characteristic databases. We use relevant analysis technology to analyze the user's location information, browsing and usage habits, hobbies, and focus content. At the same time, a precise marketing model is established according to user behavior characteristics and attributes, thereby improving the marketing effect of the enterprise.",2020,
95,10.1109/DSC50466.2020.00051,A DATA TRACEABILITY METHOD TO IMPROVE DATA QUALITY IN A BIG DATA ENVIRONMENT,"In the actual project of data sharing and data governance, the problems of data heterogeneity and low data quality have not been solved. Data quality detection based on syntax rules can effectively find data quality problems, but it had not improved the data quality, especially in a variety of heterogeneous data source environments. This paper designs a data governance model based on data traceability, and we can get data feedback and revision through this model. The proposed method considers the different ownership of data and may form a closed-loop data service chain including effective data validation, data tracking, and data revision, data release. The analysis of the case shows that the method is effective to improve the data quality and meet the requirements of data security also.",2020,
96,10.1109/BigData.2017.8258221,IS DATA QUALITY ENOUGH FOR A CLINICAL DECISION?: APPLY MACHINE LEARNING AND AVOID BIAS,"This paper provides a practical guideline for the assurance and (re-)usage of clinical data. It proposes a process which aims to provide a systematic data quality assurance even without involving a medical domain expert. Especially when (re-)using clinical data, data quality is an important topic because clinical data are not purposely collected. Therefore, data driven conclusions might be false, because a given dataset is not representative. These false data driven conclusions could even harm the life of patients. Thus, all researchers should adhere to some basic principles that can prevent false conclusions. Twelve empirical experiments were conducted in order to prove that my process is able to assure data quality with respect to the descriptive and predictive analysis. Descriptive results obtained by applying stratified sampling are conflicting in four out of nine population inputs. Sampling is carried based on the top ranked feature drawn by the Contextual Data Quality Assurance (CDQA). Between datasets these features are confirmed by the Mutual Data Quality Assurance (MDQA). Stratified sampled inputs improve predictive results compared to raw data. Both Area Under the Curve (AUC) scores and accuracy scores increase by three percent.",2017,
97,10.1109/MLBDBI54094.2021.00095,"RESEARCH ON THE RELATIONSHIP BETWEEN BIG DATA MANAGEMENT, INFORMATIZATION AND ENTERPRISE PERFORMANCES OF TEXTILE AND GARMENT ENTERPRISES UNDER THE BACKGROUND OF DIGITAL ECONOMY","With the rapid development of digital economy, textile and garment enterprises begin to focus on digital and intelligent transformation to seek high-quality development. In this process, big data management and information construction are indispensable basic work. Through the analysis of the background and current situation, this study builds a model of the relationship between big data management, enterprise informatization and enterprise performance, aiming to explore how data management under big data technology affects textile and garment enterprises and its influence. By collecting data of textile and garment enterprises and conducting empirical analysis, it is concluded that big data technology will affect the level of enterprise informatization through big data management in production, research and development and marketing. The improvement of enterprise informatization level can significantly boost the increase of enterprise performance and play a certain intermediary role. The research results of this paper provide some suggestions for textile and garment enterprise managers in the aspects of technology introduction, enterprise internal management and enterprise strategy formulation.",2021,
98,10.1109/IBIGDELFT.2018.8625289,APPLICATIONS OF STREAM DATA MINING ON THE INTERNET OF THINGS: A SURVEY,"In the era of the Internet of Things (IoT), enormous amount of sensing devices collect and/or generate various sensory data over time for a wide range of fields and applications. Based on the nature of the application, these devices result in big or fast/real time data streams. The analytics technique on the subject matter used to discover new information, anticipate future predictions and make decisions on important issues makes IoT technology valuable for both the business world and the quality of everyday life. In this study, first of all, the concept of IoT and its architecture and relation with big and streaming data are emphasized. Information discovery process applied to the IoT streaming data is investigated and deep learning frameworks covered by this process are described comparatively. Finally, the most commonly used tools for analyzing IoT stream data are introduced and their characteristics are revealed.",2018,
99,10.1109/ICBDIE50010.2020.00015,PRODUCT QUALITY PREDICTION OF ROLLING MILL IN BIG DATA ENVIRONMENT,"With the wide use of rolling mill in iron and steel industry, the quality of rolling mill products has become the primary goal of people. However, due to design defects and manufacturing quality problems, the quality of steel products is seriously affected, and the surface roughness and thickness of steel plate are important quality indicators. In this paper, by analyzing a large number of monitoring data of rolling mill condition and using BP neural network model [1], the discrete system model between monitoring data and “surface roughness” and “thickness error” of rolling steel plate is further established.",2020,
100,10.1109/ICITBS.2019.00091,BIG DATA ANALYSIS OF E-COMMERCE BASED ON THE INTERNET OF THINGS,"In the era of big data, while providing massive information, it also challenges the development of related activities in the overall environment. In the context of the rapid development of e-commerce, the opportunities of the development of the Internet of things technology are analyzed from the aspects of logistics distribution, quality control and facilities promotion. Electronic commerce is a new form of trade under the development of modern information technology, while cloud computing and the Internet of Things provide related services. Under the exertion of their related functions, the revolutionary improvement of e-commerce mode has been realized, and to a certain extent, it has promoted the development and operation of modern market economy. This article analyzes the development strategy of e-commerce based on Internet of things and cloud computing under the overall environment of big data era.",2019,
101,10.1109/BigData52589.2021.9671340,MULTISPECTRAL DRONE DATA ANALYSIS ON COASTAL DUNES,We devise a method to study coastal dune vegetation based on drone data orthomosaic mapping and machine learning algorithms to distinguish objects with spatial and color accuracy from numerous high-quality drone multispectral images. It allows accurate surveying on the density of coastal dune vegetation and potentially individual species. We thus analyze big data to develop tools for coastal resiliency and sustainability.,2021,
102,10.1109/TASE.2020.3040400,ADJUSTING LEARNING DEPTH IN NONNEGATIVE LATENT FACTORIZATION OF TENSORS FOR ACCURATELY MODELING TEMPORAL PATTERNS IN DYNAMIC QOS DATA,"A nonnegative latent factorization of tensors (NLFT) model precisely represents the temporal patterns hidden in multichannel data emerging from various applications. It often adopts a single latent factor-dependent, nonnegative and multiplicative update on tensor (SLF-NMUT) algorithm. However, learning depth in this algorithm is not adjustable, resulting in frequent training fluctuation or poor model convergence caused by overshooting. To address this issue, this study carefully investigates the connections between the performance of an NLFT model and its learning depth via SLF-NMUT to present a joint learning-depth-adjusting scheme for it. Based on this scheme, a Depth-adjusted Multiplicative Update on tensor algorithm is innovatively proposed, thereby achieving a novel depth-adjusted nonnegative latent-factorization-of-tensors (DNL) model. Empirical studies on two industrial data sets demonstrate that compared with the state-of-the-art NLFT models, a DNL model achieves significant accuracy gain when performing missing data estimation on a high-dimensional and incomplete tensor with high efficiency. Note to Practitioners—Multichannel data are often encountered in various big-data-related applications. It is vital for a data analyzer to correctly capture the temporal patterns hidden in them for efficient knowledge acquisition and representation. This article focuses on analyzing temporal QoS data, which is a representative kind of multichannel data. To correctly extract their temporal patterns, an analyzer should correctly describe their nonnegativity. Such a purpose can be achieved by building a nonnegative latent factorization of tensors (NLFT) model relying on a single latent factor-dependent, nonnegative and multiplicative update on tensor (SLF-NMUT) algorithm. But its learning depth is not adjustable, making an NLFT model frequently suffer from severe fluctuations in its training error or even fail to converge. To address this issue, this study carefully investigates the learning rules for an NLFT model’s decision parameters using an SLF-NMUT and proposes a joint learning-depth-adjusting scheme. This scheme manipulates the multiplicative terms in SLF-NMUT-based learning rules linearly and exponentially, thereby making the learning depth adjustable. Based on it, this study builds a novel depth-adjusted nonnegative latent-factorization-of-tensors (DNL) model. Compared with the existing NLFT models, a DNL model better represents multichannel data. It meets industrial needs well and can be used to achieve high performance in data analysis tasks like temporal-aware missing data estimation",2021,15583783
103,10.1109/Agro-Geoinformatics.2015.7248139,DESIGN AND IMPLEMENTATION OF THE REAL-TIME GIS DATA MODEL AND SENSOR WEB SERVICE PLATFORM FOR ENVIRONMENTAL BIG DATA MANAGEMENT WITH THE APACHE STORM,"An abstract real-time GIS data model and Sensor Web service platform was proposed to manage real-time environmental data. With the development of sensor technology, more and more sensor networks are deployed to monitor our environment, and then generate environmental big data. How to improve the real-time GIS data model and Sensor Web service platform for real-time environmental big data manage is a problem. In this paper, the Apache Storm is adopted to deal with the question. A design and implementation of the real-time GIS data model and Sensor Web service platform for environmental big data management with Apache Storm is proposed. The main studied contents include integrating the Apache Strom with the Sensor Web service as the Sensor Observation Service, and processing the environmental big data timely. To test the feasibility of the design and implementation, two use cases of real-time air quality monitoring and real-time soil moisture monitoring based on the real-time GIS data model in the Sensor Web service platform are realized and demonstrated. The experimental results show that the implementation of real-time GIS data model and Sensor Web Service Platform with the Apache Storm is an effective way to manage real-time environmental big data.",2015,
104,10.1109/TENCON.2016.7848697,QUALITY ANALYTICS IN A BIG DATA SUPPLY CHAIN: COMMODITY DATA ANALYTICS FOR QUALITY ENGINEERING,"While the world is experiencing a global shortage of natural resources, a new one in the form of Digital Data has emerged! The ability to harness this new resource has become a renewed basis for competitive advantage where leveraging Big Data effectively means winning in the marketplace. It is going to transform industries and professions around the world. However, traditional data management techniques and analytical methodologies that has taken us from the late 20th century and into the early 21st century are not sustainable in today's business environment where organizations are constantly being challenged to right size the work force, increase labor productivity, increase customer satisfaction and at the same time improving product quality and reliability.",2016,21593450
105,10.1109/BigData52589.2021.9671612,"BUILDING AN ACCESSIBLE, USABLE, SCALABLE, AND SUSTAINABLE SERVICE FOR SCHOLARLY BIG DATA","Since the emergence of scholarly big data, there have been several efforts for web-based services such as digital library search engines (DLSEs). However, much of the design and specifications of an accessible, usable, scalable, and sustainable DLSE have not been well represented and discussed in the literature. We argue that these four characteristics are essential to providing a high-quality service for scholarly big data from both the user and developer’s perspectives. This paper reviews the design, implementation, and operation experiences, and lessons of CiteSeerX, a real-world digital library search engine. We analyze the strengths and weaknesses of the current design, and proposed a new design with a revised architecture, enhanced hardware, and software infrastructure. The Alpha version of the new design has been implemented and tested. The new system replaces MySQL and Apache Solr with a single instance of Elasticsearch, which plays a dual role of data storage and search. Another major improvement is the integration of extraction and ingestion, which significantly boosts document ingestion speed. The web application is re-engineered to enhance the user experience by applying a learning-to-rank model and offering more refined search tools. The system is also improved in many other aspects. We believe the design considerations and experience can benefit researchers and engineers who plan, design, and upgrade future systems with comparable scales and functionalities.",2021,
106,10.1109/eScience.2017.38,SCIENCEDB: A PUBLIC MULTIDISCIPLINARY RESEARCH DATA REPOSITORY FOR ESCIENCE,"Research data repositories are necessary infrastructures that ensure the data generated for research are accessible, stable, reliable, and reusable. Based on years of accumulated data work experience, the Computer Network Information Center of the Chinese Academy of Sciences has built a multi-disciplinary data repository ScienceDB for research users and teams using its big data storage, analysis and computing environments. This paper firstly introduces the motivation to develop ScienceDB and gives a profile to it. Then the overall technical framework of ScienceDB is introduced, and the key technologies such as the support for multidiscipline extensibility, data collaboration and data recommendation are analyzed deeply. And then this paper presents the functions and features of ScienceDB's current version and discusses some issues such as its data policy, data quality assurance measures, and current application status. Finally, it summarizes and puts forward that it needs to carry out more in-depth research and practice of ScienceDB in order to meet the higher requirements of eScience in terms of thorough data association and fusion, data analysis and mining, data evaluation, and so on.",2017,
107,10.1109/CBMS.2019.00116,SEMANTIC DATA INTEGRATION TECHNIQUES FOR TRANSFORMING BIG BIOMEDICAL DATA INTO ACTIONABLE KNOWLEDGE,"FAIR principles and the Open Data initiatives have motivated the publication of large volumes of data. Specifically, in the biomedical domain, the size of the data has increased exponentially in the last decade, and with the advances in the technologies to collect and generate data, a faster growth rate is expected for the next years. The available collections of data are characterized by the dominant dimensions of big data, i.e., they are not only large in volume, but they can be also heterogeneous and present quality issues. These data complexity problems impact on the typical tasks of data management, and particularly, in the task of integrating big biomedical data sources. We tackle the problem of big data integration and present a knowledge-driven framework able to extract and integrate data collected from structured and unstructured data sources. The proposed framework resorts to Natural Language Processing techniques to extract knowledge from unstructured data and short text. Furthermore, ontologies and controlled vocabularies, e.g., UMLS, are utilized to annotate the extracted entities and relations with terms from the ontology or controlled vocabulary. The annotated data is integrated into a knowledge graph. A unified schema is used to describe the meaning of the integrated data as well as the main properties and relations. As proof of concept, we show the results of applying the proposed framework to integrate clinical records from lung cancer patients with data extracted from open data sources like Drugbank and PubMed. The created knowledge graph enables the discovery of interactions between drugs in the treatments prescribed to lung cancer patients.",2019,2372918X
108,10.1109/BigData52589.2021.9671890,QMLEX: DATA DRIVEN DIGITAL TRANSFORMATION IN MARKETING ANALYTICS,"This paper presents a data driven approach to replace expert driven business processes. The QMLEx methodology combines NLP algorithms to improve data quality, ML techniques to perform the task and exploits external data sources to eliminate the need for the expert input. The methodology is applied to our internal process devoted to creating groups of products with similar features, one of the most relevant use case in marketing analytics.",2021,
109,10.1109/TBDATA.2017.2725904,A NEW METHODOLOGY FOR STORING CONSISTENT FUZZY GEOSPATIAL DATA IN BIG DATA ENVIRONMENT,"In this era of big data, as relational databases are inefficient, NoSQL databases are a workable solution for data storage. In this context, one of the key issues is the veracity and therefore the data quality. Indeed, as with classic data, geospatial big data are generally fuzzy even though they are stored as crisp data (perfect data). Hence, if data are geospatial and fuzzy, additional complexities appear because of the complex syntax and semantic features of such data. The NoSQL databases do not offer strict data consistency. Therefore, new challenges are needed to be overcome to develop efficient methods that simultaneously ensure the performance and the consistency in storing fuzzy geospatial big data. This paper presents a new methodology that tackles the storage issues and validates the fuzzy spatial entities' consistency in a document-based NoSQL system. Consequently, first, to better express the structure of fuzzy geospatial data in such a system, we present a logical model called Fuzzy GeoJSON schema. Second, for consistent storage, we implement a schema-driven pipeline based on the Fuzzy GeoJSON schema and semantic constraints.",2021,23722096
110,10.1109/ICTech55460.2022.00068,EVALUATION OF ONLINE TOOL DATA MANAGEMENT FOR WAREHOUSE MANAGEMENT FOR POWER BIG DATA,"With the rapid development of China's data industry, power big data has gradually become the main object of national construction and innovation. Especially in the promotion of sensors and intelligent equipment and so on, more and more electric power data sources, the type characteristics shown more complex, the use of big data related to technology, the hidden data information, not only can improve the efficiency of the power system, can also provide effective basis for warehouse management. As the main management tool for power big data, power load prediction can guarantee the power system and power supply quality on the one hand, and can provide more effective information by warehouse management on the other hand, and the actual prediction results directly affect the accuracy of the whole system operation. Therefore, on the basis of understanding the development trend of power big data, this paper takes data mining technology as the core to improve and explore the power load prediction, so as to ensure the accuracy and effectiveness of online tools and data management of warehouse management.",2022,
111,10.1109/EEBDA53927.2022.9744998,RESEARCH ON ACTIVE MONITORING SYSTEM OF POWER SUPPLY SERVICE DEMAND BASED ON COMPUTER BIG DATA,"With the improvement of the overall service level of society, people's requirements for power supply services are getting higher and higher. In order to improve the quality of power supply service, reduce the risks in power supply business activities, improve the business efficiency and management level of enterprises, build a unified audio and video monitoring system, and achieve a three-level management model of centralized supervision at the provincial level and local monitoring at the city and county levels. The main requirements function of the customer appeal information management system of the electric power company include the acceptance and review of appeal information, the appeal return visit and response information management function, the responsibility determination function, the monitoring and reminding function, the work order query function, the case information maintenance management function and the software system Operation and maintenance management functions. The thesis outlines the overall software design and database logic scheme of the information system on the basis of demand analysis, and further discusses the relational data model of database tables and the design of main business processes. On the basis of the system design, the program implementation and testing of the main modules of the system are completed.",2022,
112,10.1109/ISAIEE55071.2021.00071,FINANCIAL BIG DATA RECONCILIATION METHOD,"For data errors in distributed financial system caused by multi-system interaction, asynchronous processing and system bug, this paper proposes offline and quasi real-time data reconciliation methods based on the combination of Alibaba big data processing platform and accounting theory. In offline data reconciliation, Full data reconciliation and hour level incremental data reconciliation are introduced. And in quasi real-time data reconciliation, single system and distributed multi-system reconciliation models are introduced. These data reconciliation methods are then verified against 7 million pieces of daily data of the distributed loan system in a financial company. Results show that these methods can complete the financial big data processing, discover the data quality problems timely, and minimize the financial system capital loss.",2021,
113,10.1109/BigData.2016.7840777,BIG-DATA-DRIVEN ANOMALY DETECTION IN INDUSTRY (4.0): AN APPROACH AND A CASE STUDY,"In this paper we present a novel approach for data-driven Quality Management in industry processes that enables a multidimensional analysis of the anomalies that can appear and their real-time detection in the running system. The approach revolutionizes the way how quality control (and esp. anomaly detection) will be realized in production processes influenced by many parameters that can be in complex nonlinear correlations. It consists of two main steps: learning the normal behavior of the system (based on past data) and detecting an anomalous behavior in the real-time (by processing real-time data). The approach is especially suitable for modern industry systems that follow Industry 4.0 principles of ubiquity sensing and proactive responding. One of the main advantages is the self-adaptive nature of the approach due to its data-driven orientation, so that the model and parameters of the approach will be continuously updated to the dynamicity of data. The approach has been applied in the process of manufacturing microwave ovens (Whirlpool) and in this paper we present results for the data-driven quality control of one of the most critical parts - microwave oven fan. Due to the high speed of the rotation, every item has to be very precisely produced (according to the CAD model), which requires very strong quality control process.",2016,
114,10.1109/BDCloud.2014.38,"A DOMAIN-DRIVEN, GENERATIVE DATA MODEL FOR BIG PET STORE","Generating large amounts of semantically-rich data for testing big data workflows is paramount for scalable performance benchmarking and quality assurance in modern machine-learning and analytics workloads. The most obvious use case for such a generative algorithm is in conjunction with a big data application blueprint, which can be used by developers (to test their emerging big data solutions) as well as end users (as a starting point for validating infrastructure installations, building novel applications, and learning analytics methods). We present a new domain-driven, generative data model for Big Pet Store, a big data application blueprint for the Hadoop ecosystem included in the Apache Big Top distribution. We describe the model and demonstrate its ability to generate semantically-rich data at variable scale ranging from a single machine to a large cluster. We validate the model by using the generated data to answer questions about customer locations and purchasing habits for a fictional targeted advertising campaign, a common business use case.",2014,
115,10.1109/CCCS.2015.7374131,OVERVIEW OF DATA QUALITY CHALLENGES IN THE CONTEXT OF BIG DATA,"Data quality management systems are thoroughly researched topics and have resulted in many tools and techniques developed by both academia and industry. However, the advent of Big Data might pose some serious questions pertaining to the applicability of existing data quality concepts. There is a debate concerning the importance of data quality for Big Data; one school of thought argues that high data quality methods are essential for deriving higher level analytics while another school of thought argues that data quality level will not be so important as the volume of Big Data would be used to produce patterns and some amount of dirty data will not mask the analytic results which might be derived. This paper aims to investigate various components and activities forming part of data quality management such as dimensions, metrics, data quality rules, data profiling and data cleansing. The result list existing challenges and future research areas associated with Big Data for data quality management.",2015,
116,10.1109/BigDataCongress.2017.91,CHALLENGES OF SOFTWARE TESTING FOR ASTRONOMICAL BIG DATA,"Astronomy has been one of the first areas of science to embrace and learn from big data. The amount of data we have on our universe is doubling every year thanks to big telescopes and better light detectors. Most leading research is based on data from a handful of very expensive telescopes. Undoubtedly, the data quality is the key basis for the leading scientific findings. It is imperative that software testers understand that big data is about far more than simply data volume. This paper will analyze characteristics, types, methods, strategies, problems, challenges and propose some possible solutions of software testing for astronomical big data.",2017,
117,10.1109/CIPAE53742.2021.00035,PRACTICAL TEACHING OF MANAGEMENT ACCOUNTING COURSE UNDER THE BACKGROUND OF ARTIFICIAL INTELLIGENCE AND BIG DATA,"With the rapid development of Internet technology and artificial intelligence, the accounting profession is undergoing earth-shaking changes, and its service model has changed from data processing after the event to planning in advance and participating in management decisions. These are the characteristics of management accounting. If accountants are simply proficient in finance, they lack the connection with business, do not understand business, and cannot extract valuable information from financial big data to provide solutions or suggestions for business development, which is a waste of data resources and even misleading decision-making. Today&#x0027;s finance should not only integrate into business, but also deal with data, so as to give full play to the functions of supervision and management decision-making, help improve business quality, and at the same time, make value creation develop to a higher end. The important task of higher education is to train high-quality professionals and cultivate a group of top-notch talents in this major. The same is true of management accounting [1]. At present, there is a shortage of market management accounting talents. How to cultivate high-quality management accounting talents, improve the practical ability of management accounting students, and quickly integrate into society to be a qualified management accounting, the key to professional teaching is to make students familiar with business and understand business processes, and to have management practice courses with quality and quantity.",2021,
118,10.1109/ICCNEA50255.2020.00045,RESEARCH ON AIR QUALITY FORECASTING BASED ON BIG DATA AND NEURAL NETWORK,"Aiming at the problem that existing air quality prediction models cannot efficiently and accurately predict air quality in a big data environment, an air quality prediction method based on a big data platform to implement a distributed neural network is proposed. Collect historical data of the six pollutant concentrations that affect the air quality index and use it as input to a neural network model; A distributed neural network model containing the AQI change rule in the distributed neural network structure is adopted to realize the short-term prediction of the AQI. Experimental results show that air quality prediction models based on big data and neural networks can reveal the development trend of air quality through self-learning characteristics. And has higher prediction accuracy, It can provide a scientific basis for the degree of urban air pollution and help people make appropriate measures for different AQI levels.",2020,
119,10.1109/IGEHT.2017.8094109,AN ENHANCED PRE-PROCESSING MODEL FOR BIG DATA PROCESSING: A QUALITY FRAMEWORK,"With the ever growing trends and technologies a huge volume of data is being evolved each and every second big data has become a supreme approach in data inception, accession, processing and analyzing the heterogeneous, huge amount of data so as to derive useful insights out of it. With data and without quality there is no point in having the data. Thus, data with quality is required to use or leverage the data in a more appropriate manner. With the evolution of big data many technologies are being developed. The input to it must be processed in such a way that the quality data yields quality effective results. An effective pre-processing model is proposed in this paper for the processing of the big data. Using relief algorithm and fast mRMR together as a hybrid approach can be used for the pre-processing of the data. Analysis shows that this hybrid approach is more effective and can greatly enhance the quality of the data. This approach can yield better performance upon the big data platform using the Spark framework.",2017,
120,10.1109/BCD54882.2022.9900751,A STUDY ON BIG DATA UTILIZATION INTENTION OF SMALL AND MEDIUM-SIZED MANUFACTURING COMPANIES,"Big data, which is attracting attention as a game-changer in the manufacturing industry and innovative technology, is an important factor for securing the competitiveness of manufacturing companies and innovative growth. However, introducing and using big data in manufacturing companies requires a lot of effort from the company and the capabilities of its members due to the characteristics of the manufacturing process. To apply big data to manufacturing, developed countries and Korea are building pilot projects and use cases as experimental steps to this end. Through the review of previous studies, it was confirmed that big data utilization by manufacturing companies requires a lot of academic research from a management perspective. Therefore, this study examined the applicable fields, the actual conditions and problems related to big data utilization by manufacturing companies by checking Industry 4.0, changes in the manufacturing industry, and related prior studies for big data utilization in the manufacturing industry. It also aims to examine the intention of big data utilization through empirical research on the previous researches on big data in small and medium-sized manufacturing industries.",2022,
121,10.1109/ACCESS.2019.2932259,ACTIVE DATA REPLICA RECOVERY FOR QUALITY-ASSURANCE BIG DATA ANALYSIS IN IC-IOT,"QoS-aware big data analysis is critical in Information-Centric Internet of Things (IC-IoT) system to support various applications like smart city, smart grid, smart health, intelligent transportation systems, and so on. The employment of non-volatile memory (NVM) in cloud or edge system provides good opportunity to improve quality of data analysis tasks. However, we have to face the data recovery problem led by NVM failure due to the limited write endurance. In this paper, we investigate the data recovery problem for QoS guarantee and system robustness, followed by proposing a rarity-aware data recovery algorithm. The core idea is to establish the rarity indicator to evaluate the replica distribution and service requirement comprehensively. With this idea, we give the lost replicas with distinguishing priority and eliminate the unnecessary replicas. Then, the data replicas are recovered stage by stage to guarantee QoS and provide system robustness. From our extensive experiments and simulations, it is shown that the proposed algorithm has significant performance improvement on QoS and robustness than the traditional direct data recovery method. Besides, the algorithm gives an acceptable data recovery time.",2019,21693536
122,10.1109/ICBASE51474.2020.00023,RESEARCH ON THE TEACHING REFORM OF FINANCE AND ACCOUNTING MAJOR UNDER THE BACKGROUND OF BIG DATA,"With the development of information and intelligent technology such as Internet, big data and cloud computing, the concept of big data in education and teaching has also been widely concerned by the society and colleges and universities. The application of big data technology broadens educational resources and teaching channels, and provides a new space for teaching reform and talent training of accounting majors. Based on the background of big data and combined with the characteristics of finance and accounting majors, this paper integrates big data thinking into the teaching reform process of finance and accounting majors, integrates the needs of big data information resources and application ability cultivation of finance and accounting majors, improves the training quality of finance and accounting professionals, and promotes the teaching reform of finance and accounting majors.",2020,
123,10.1109/TBDATA.2017.2710346,SEMANTIC-BASED AND ENTITY-RESOLUTION FUSION TO ENHANCE QUALITY OF BIG RDF DATA,"Within an organisation, the quality in big data is a cornerstone to operational, transactional processes and to the reliability of business analytics for decision making. In fact, as organizations are harnessing multi-sources data to rise the benefits of their business, the quality of data becomes important and crucial. This paper presents a new approach to query big data sources using Resource Description Framework (RDF) representation to ensure data quality by harvesting more relevant and complete query results. Our approach handles two important types of heterogeneity over multiple data sources: semantic heterogeneity and URI-based entity identification. It proposes (1) a semantic entity resolution method based on inference mechanism using rules to manage the misunderstanding of data, in real world entities (2) Data Quality enhancement using MapReduce-based query rewriting approach includes the entity resolution results to infer and adds implicit data into query results (3) a parallel combination of MapReduce jobs of saturation and query rewriting inferences to handle transitive and cyclic rules for a richer rules' expression language (4) experiments to assess the efficiency of the proposed approach over real big RDF data originating from insurance and synthetic data sets.",2021,23722096
124,10.1109/BigData.2015.7364047,"BIG DATA PROVENANCE: CHALLENGES, STATE OF THE ART AND OPPORTUNITIES","Ability to track provenance is a key feature of scientific workflows to support data lineage and reproducibility. The challenges that are introduced by the volume, variety and velocity of Big Data, also pose related challenges for provenance and quality of Big Data, defined as veracity. The increasing size and variety of distributed Big Data provenance information bring new technical challenges and opportunities throughout the provenance lifecycle including recording, querying, sharing and utilization. This paper discusses the challenges and opportunities of Big Data provenance related to the veracity of the datasets themselves and the provenance of the analytical processes that analyze these datasets. It also explains our current efforts towards tracking and utilizing Big Data provenance using workflows as a programming model to analyze Big Data.",2015,
125,10.1109/BigData47090.2019.9006234,BENCH4GIS: BENCHMARKING PRIVACY-AWARE GEOCODING WITH OPEN BIG DATA,"Geocoding, the process of translating addresses to geographic coordinates, is a relatively straight-forward and well-studied process, but limitations due to privacy concerns may restrict usage of geographic data. The impact of these limitations are further compounded by the scale of the data, and in turn, also limits viable geocoding strategies. For example, healthcare data is protected by patient privacy laws in addition to possible institutional regulations that restrict external transmission and sharing of data. This results in the implementation of “in-house” geocoding solutions where data is processed behind an organization's firewall; quality assurance for these implementations is problematic because sensitive data cannot be used to externally validate results. In this paper, we present our software framework called bench4gis which benchmarks privacy-aware geocoding solutions by leveraging open big data as surrogate data for quality assurance; the scale of open big data sets for address data can ensure that results are geographically meaningful for the locale of the implementing institution.",2019,
126,10.1002/9781119690962.ch1,AN INTRODUCTION: WHAT'S A MODERN BIG DATA PLATFORM,"This chapter discusses the different aspects of designing Big Data platforms, in order to define what makes a big platform and to set expectations for these platforms. The solutions for Big Data processing vary based on the company strategy. A modern Big Data platform has several requirements, and to meet them correctly, expectations with regard to data should be set. Securing data has become a crucial aspect of a modern Big Data platform. The data quality depends on factors such as accuracy, consistency, reliability, and visibility. One of the hard problems of Big Data is backups as the vast amount of storage needed is overwhelming for backups. The Big Data platform should provide an extract, transform, and load (ETL) solution/s that manages the experience end to end. ETL developers should be able to develop, test, stage, and deploy their changes. Big Data platforms are quite complex as they are built based on distributed systems.",2021,
127,10.1109/IWCMC48107.2020.9148224,CONSTRUCTION OF BIG DATA MONITORING PLATFORM FOR TEACHING QUALITY UNDER INTELLIGENT EDUCATION,"To a great extent, the quality of teaching determines the level of trained talents. Now it has entered the era of intelligent education, coupled with the rapid development of the Internet, has produced a large number of teaching data, which also makes the monitoring and evaluation of teaching quality become particularly difficult. In view of the above problems, this paper proposes the construction of big data monitoring platform for teaching quality under intelligent education. In this paper, the OPC UA unified architecture is used for communication between devices, and the information configuration is based on the spring boot framework to achieve data collection. Then, data processing is based on GRU neural network, and spark distributed computing framework is used to improve the efficiency of data operation. Finally, the monitoring effect is realized by constructing the evaluation system.",2020,23766492
128,10.1109/CBD.2015.22,AN AUTOMATIC DISCOVERY FRAMEWORK OF CROSS-SOURCE DATA INCONSISTENCY FOR WEB BIG DATA,"The vigorous growth of big data has triggered both opportunities and challenges in business and industry. However, Web big data distributed in diverse sources with multiple data structures frequently conflict with each other, i.e. inconsistency in cross-source Web big data. In this paper, we propose a state-of-the-art architecture of auto-discovering inconsistency with Web big data. Our contributions include: (1) we classify the inconsistency features to formalize inconsistency data and establish an algebraic operation system, (2) we propose three algorithms to auto-discover inconsistency, including constraint-based, SDA-based and HPDM-based method and (3) we conduct experiments on real-world dataset to compare aforesaid schemes with Oracle-based inconsistency detection framework. The empirical results show that our methods outperform traditional framework both on accuracy and efficiency under Web big data.",2015,
129,10.1109/ICMeCG.2014.51,COMPARATIVE STUDY OF PRODUCTS QUALITY CONTROL SYSTEM OF COUNTRIES IN THE ERA OF BIG DATA,"Product quality control is an important outcome of the development of human society and production management of core areas, while its system is an important content of the quality and safety system. National regulatory system for product quality very seriously, however, product quality and safety are occurring. What is a quality management system? Regulatory information and what is the relationship between implementation of the system? Internet and ""big data"" but also can lead to changes in the regulatory process and innovation of social governance and strict regulatory regime covering the whole process? This series of quality control problems have been highlighted. Thus, drawing on the experience of other countries, along the historical context, reflections on both sorting and summarizing, and building more accurate inference and description are very urgent and necessary.",2014,
130,10.1109/EPQU.2007.4424094,DATA SYSTEM FOR THE MONITORING OF POWER QUALITY IN THE TRANSMISSION SUBSTATIONS SUPPLYING BIG CONSUMERS,"During 2006, at CN Transelectrica - OMEPA Branch request, ISPE - Power Systems Department has conceived the designing documentations (Feasibility Study and Tender Documents) for “Power Quality Analyzing System at the big consumers”. The present paper reports the purpose and technical endowment proposed by ISPE for “Power Quality Monitoring and Analyzing System” that will be developed at OMEPA.",2007,21506655
131,10.1109/INOCON50539.2020.9298378,A HYBRID APPROACH TO DATA PRE-PROCESSING METHODS,"This is an era of big data, as data is growing exponentially and resources are running out of infrastructure, so it is required to accommodate all the data that gets generated. We collect data in enormous amounts to derive meaningful conclusions, perform effective data analytics and improve decision making. As we don't have enough infrastructures to support data storage for huge volumes, it is needed to clean the data in compulsion. It is a mandatory to carry out a step before doing anything with the data. We call it pre-processing of data and this is carried out in various steps. Pre-processing includes data cleaning, data integration, data filtering, and data transformation and so on. As such preprocessing is not limited to the number of steps or a number of methods or definitive methods. We must innovatively preprocess the data before it is being consumed for data analytics. It has become a responsibility for every data analyst or big data researcher to handpick data for his or her analytics. Considering all these techniques in mind we are proposing a hybrid technique to leverage various algorithms available to pre-process our data along with minor modifications such as at the run time, choosing an algorithm or technique wisely based on the data that we have.",2020,
132,10.1002/9781119690962.ch2,A BIRD'S EYE VIEW ON BIG DATA,"This chapter presents the qualities of Big Data, components of Big Data platform, and use cases of Big Data. Big Data objects are produced from large‐scale simulations of computational dynamics and weather modeling. Hadoop distributed file system aims to deliver the following promises: failure recovery, stream data access, and support very large data sets. Hadoop became the open‐source solution for the Big Data explosion. The chapter aims to define Big Data as a new breed of data, resulting from technological advancements in storage and growing understanding of value extraction, different than traditional structured data in terms of volume, velocity, variety, and complexity. Big Data platforms may consist of many parts such as batch processing, real‐time analytics, reporting, business intelligence, machine learning and so on. The chapter combines these parts into four main components: ingestion, storage, computation, and presentation. Institutions make use of Big Data in the form querying, reporting, alerting, searching, exploring, mining, and modeling.",2021,
133,10.1109/ACCESS.2019.2939196,CURRICULUM REFORM IN BIG DATA EDUCATION AT APPLIED TECHNICAL COLLEGES AND UNIVERSITIES IN CHINA,"With the boom in data science, big data education has received increasing attention from all kinds of colleges and universities in China, and many of them are in a rush to offer big data education. This paper first analyzes the major areas of big data capability training and the Chinese market needs for various kinds of data science talent. Then, it discusses the curriculum design process for the “Data Science & Big Data Technology” bachelor's degree program, and summarizes some detailed approaches to improving teaching experiments. Finally, this paper proposes a graduating student profile for big data education at applied technical colleges and universities in China. The authors' main ideas include that, at the applied technical colleges and universities, a) a suitable graduating student orientation should be determined as the big data talent needs are hierarchical; b) the redesigned curriculum in big data education should provide students more practical capabilities and knowledge; c) the teaching of the existing mainstream big data technologies and tools should be significant components in the syllabi of big data education.",2019,21693536
134,10.23919/CJEE.2019.000025,A MISSING POWER DATA FILLING METHOD BASED ON IMPROVED RANDOM FOREST ALGORITHM,"Missing data filling is a key step in power big data preprocessing, which helps to improve the quality and the utilization of electric power data. Due to the limitations of the traditional methods of filling missing data, an improved random forest filling algorithm is proposed. As a result of the horizontal and vertical directions of the electric power data are based on the characteristics of time series. Therefore, the method of improved random forest filling missing data combines the methods of linear interpolation, matrix combination and matrix transposition to solve the problem of filling large amount of electric power missing data. The filling results show that the improved random forest filling algorithm is applicable to filling electric power data in various missing forms. What's more, the accuracy of the filling results is high and the stability of the model is strong, which is beneficial in improving the quality of electric power data.",2019,20961529
135,10.1109/ICBDSS51270.2020.00008,RESEARCH ON MULTIDIMENSIONAL TEACHING MODE OF COLLEGE ENGLISH BASED ON DATA MINING,"The rapid development of computer technology and Internet technology has promoted the development of the era of big data. The acquisition and application of data information is an important foundation for improving the level of data application in the era of big data. In the current college English teaching process, the multi-dimensional English teaching model based on data mining has a positive effect on improving college English teaching efficiency and ensuring teaching quality. In the context of big data, teachers not only need to understand the problems in college English teaching, but also teachers must analyze the way of multi-dimensional college English teaching mode based on data mining, so as to effectively improve the quality of English teaching.",2020,
136,10.1109/WiMOB.2019.8923233,CHECKING THE PLAUSIBILITY OF NUTRIENT DATA IN FOOD DATASETS USING KNIME AND BIG DATA,"As there is no standardized food database with all products available in Europe, many developers of health apps fall back on databases of communities whose quality is often insufficient. In health apps, the quality of the data sets is critical, as poor quality lowers the user's confidence. This paper examines the plausibility of nutrient data from such data sources using similarity analysis, decision support methods and Big Data technology. During a special developed process, the plausibility of the data is to be increased. Finally, the methods used will be evaluated on the basis of test data.",2019,21604886
137,10.1109/FTC.2016.7821610,BIG DATA ANALYTICS ON MANET ROUTING STANDARDIZATION USING QUALITY ASSURANCE METRICS,"An ad hoc network is a collection of wireless mobile nodes dynamically forming a temporary network without the use of any existing network infrastructure or centralized administration. Routing in ad-hoc network is a challenging issue. Big data analytics have been suggested for proper evaluation and decision making in routing and placement of ad hoc network nodes. This Paper analyses the performance of AODV and DSR routing protocols for the quality assurance metrics. The performance differentials of AODV and DSR protocols are analyzed using NS-2 which is the main network simulator, NAM (Network Animator) and compared in terms of scales applied on Packet Delivery Ratio (PDR), in different environments specified by varying pause time, speed and number of nodes.",2016,
138,10.1109/BigData47090.2019.9005952,FEDERATED RECOMMENDATION SYSTEMS,"Despite its great progress so far, artificial intelligence (AI) is facing a serious challenge in the availability of high-quality Big Data. In many practical applications, data are in the form of isolated islands. Efforts to integrate the data are increasingly difficult partly due to serious concerns over user privacy and data security. The problem is exacerbated by strict government regulations such as Europe's General Data Privacy Regulations (GDPR). In this talk, I will review these challenges and describe efforts to address them in recommendation systems area. In particular, I will give an overview of recent advances in federated learning and then focus on developments of “federated recommendation systems”, which aims to build high-performance recommendation systems by bridging data repositories without compromising data security and privacy.",2019,
139,10.1109/ICBDIE52740.2021.00055,RESEARCH ON THE MODEL OF COLLEGE ENGLISH NETWORK LEARNING PLATFORM BASED ON EDUCATION BIG DATA,"With the development of society and the progress of science, network technology is changing the living state of modern people little by little. It also brings some challenges and opportunities to the field of education in China. Nowadays, the network learning platform of Big Data provides a lot of convenience for English teaching in colleges and universities in China. At present, many colleges and universities do not realize the importance of online teaching mode to learning English. Many teachers still adhere to the traditional idea and adopt the teaching mode of full hall irrigation, which leads to the extremely low quality of English teaching and the low interest of students in learning, so it is difficult to achieve the ideal learning effect. Therefore, under the background of Big Data, it is necessary to construct the model of college English network platform reasonably to help students improve their learning efficiency. This paper briefly summarizes the model of Big Data and network learning platform, and analyzes the application of Big Data in the field of education. The principle of model construction of college English online learning platform based on Big Data is deeply studied. At the same time, the problems of college English education in China are put forward, and the way of constructing English network learning platform model under the background of educational Big Data is analyzed.",2021,
140,10.1109/CAIBDA53561.2021.00057,RESEARCH ON INSTANCE-LEVEL DATA CLEANING TECHNOLOGY,"Effectivedata analysis and data mining are based on data availability and data quality. Data cleaning is a commonly used technique to improve data quality. Instance-level data cleaning is an important part of data cleaning. The focus is on the comparison and analysis of the detection and cleaning methods of attributes and recorded values in the instance-level data cleaning technology, and the experimental analysis of the repeated record cleaning methods. This paper introduces the application field of data cleaning technology represented by the electrical engineering field combined with the application situation, and provides valuable selection suggestions for the characteristics of different data sets and the applicable instancelevel data cleaning technology. Summarizing and analyzing the existing detection and cleaning technology methods, it is concluded that instance-level data cleaning has a lot of research and development space in long text, unstructured data and specific fields. Finally, the challenges and development directions of the instance-level data cleaning technology are prospected.",2021,
141,10.1109/BigData52589.2021.9672082,BIG DATA TESTING FRAMEWORK FOR RECOMMENDATION SYSTEMS IN E-SCIENCE AND E-COMMERCE DOMAINS,"Software testing is an important process to evaluate whether the developed software applications meet the required specifications. There is an emerging need for testing frameworks for big data software projects to ensure the quality of the big data applications and satisfy the user requirements. In this study, we propose a software testing framework that can be utilized in big data projects both in e-science and e-commerce. In particular, we design the proposed framework to test big data-based recommendation applications. To show the usability of the proposed framework, we provide a reference prototype implementation and use the prototype to test a big data recommendation application. We apply the prototype implementation to test both functional and non-functional methods of the recommendation application. The results indicate that the proposed testing framework is usable and efficient for testing the recommendation systems that use big data processing techniques.",2021,
142,10.1109/COMPSAC51774.2021.00235,TOWARD A NOVEL MEASUREMENT FRAMEWORK FOR BIG DATA (MEGA),"Big Data is quickly becoming a chief part of the decision-making process in both industry and academia. As more and more institutions begin relying on Big Data to make strategic decisions, the quality of the underlying data comes into question. The quality of Big Data isn’t always transparent and large-scale systems may even lack its visibility, which adversely affects the credibility of the Big Data systems. Continuous monitoring and measurement of data quality is therefore paramount in assessing whether the information can serve its purpose in a particular context (such as Big Data analytics, for example). This research addresses the need for Big Data quality measurement modeling and automation by proposing a novel conceptual quality measurement framework for Big Data (MEGA) with the purpose of assessing the underlying quality characteristics of Big Data (also known as the V’s of Big Data) at each step of the Big Data Pipelines. The theoretical quality measurement models for four of the Big Data V’s (Volume, Variety, Velocity, Veracity) are currently automated; the remaining 6 V’s (Vincularity, Validity, Value, Volatility, Valence and Vitality) will be tackled in our future work. The approach is illustrated on a case study.",2021,07303157
143,10.1109/ICITBS.2019.00079,RESEARCH ON MEDICAL SERVICE SYSTEM BASED ON BIG DATA TECHNOLOGY,"The purpose of the medical information sharing system based on big data in this paper is promoting the development of medical informatization, providing technical solutions and supporting conditions for the construction of regional medical informatization in China. The system is bound to promote the quality of medical information service by the application of big data and cloud computing and solve the ""high cost but low efficiency"" problem. This paper designs a service pattern that takes the patients as the center and realizes the reasonable distribution and sharing of medical resources. The system also provides experience and references for the medical service innovation in China. In this paper, a medical service system BDMSP which based on big data technology, is introduced. It describes the design of BDMSP from system architecture, key technologies, its application and etc. It may be a helpful research for application of big data technology.",2019,
144,10.1109/BigDataService.2017.40,DATA-DRIVEN WATER QUALITY ANALYSIS AND PREDICTION: A SURVEY,"Water quality becomes one of the important quality factors for the quality life in smart cities. Recently, water quality has been degraded due to diverse forms of pollution caused by disposal of human wastes, industrial wastes, automobile wastes. The increasing pollution affects water quality and the quality of people's life. Hence, water quality evaluation, monitoring, and prediction become an important and hot research subject. In the past, many environmental researchers have dedicated their research efforts on this subject using conventional approaches. Recently, many researchers begin to use the big data analytics approach to studying, evaluating, and predicting water quality due to the advances of big data applications and the availability of environmental sensing networks and sensor data. This paper reviews the published research results relating to water quality evaluation and prediction. Moreover, the paper classifies and compares the applied big data analytics approaches and big data based prediction models for water quality assessment. Furthermore, the paper also discusses the future research needs and challenges.",2017,
145,10.1109/BigData50022.2020.9378008,DATA REDUCTION AND DEEP-LEARNING BASED RECOVERY FOR GEOSPATIAL VISUALIZATION AND SATELLITE IMAGERY,"The storage, retrieval, and distribution of data are some critical aspects of big data management. Data scientists and decision-makers often need to share large datasets and make decisions on archiving or deleting historical data to cope with resource constraints. A potential approach to mitigate such problems is to reduce big datasets into smaller ones, which will not only lower storage requirements but also allow light load transfer over the network. Carefully prepared data by removing redundancies, along with a machine learning model capable of reconstructing the whole dataset from its reduced version, can improve the storage scalability, data transfer, and speed up the overall data management pipeline. In this paper, we explore some data reduction strategies for big datasets, while ensuring that the data can be transferred and used ubiquitously by all stakeholders, i.e., the entire dataset can be reconstructed with high quality whenever necessary. Our approach guarantees a minimum of 75% data size reduction, where the reconstruction accuracy observed is as high as 98.75% on an average for geospatial meteorological data (e.g., soil moisture and albedo), and 99.09% for satellite imagery. We propose a novel variance based reduction technique that can further reduce the data size without losing the accuracy significantly, and adopt various deep learning approaches for high-quality reconstruction.",2020,
146,10.1109/ICBAR55169.2021.00022,RESEARCH ON THE PATH OF COLLEGE STUDENTS' SELF-EDUCATION IN THE ERA OF BIG DATA,"The era of big data is an era of high-speed information exchange and processing, openness of information, and personalized information acquisition. Faced with the continuous emergence of massive amounts of data and information in the era of big data, it has brought risks and challenges to the self-education of college students. The massive amount of information in the era of big data makes the content of college students' self-education more abundant, and the openness of the era of big data improves the subjectivity of college students' self-education. The risk challenges of the era of big data put forward higher requirements for college students' self-education ability. therefore, it is necessary to improve college students' self-education ability from the perspective of creating a high-quality big data environment and optimizing the social environment in the era of big data.",2021,
147,10.1109/BigDataCongress.2017.56,DATA GOVERNANCE FRAMEWORK FOR BIG DATA IMPLEMENTATION WITH A CASE OF KOREA,"Big Data governance requires a data governance that can satisfy the needs for corporate governance, IT governance, and ITA/EA. While the existing data governance focuses on the processing of structured data, Big Data governance needs to be established in consideration of a broad sense of Big Data services including unstructured data. To achieve the goals of Big Data, strategies need to be established together with goals that are aligned with the vision and objective of an organization. In addition to the preparation of the IT infrastructure, a proper preparation of the components is required to effectively implement the strategy for Big Data services. We propose the Big Data Governance Framework in this paper. The Big Data governance framework presents criteria different from existing criteria at the data quality level. It focuses on timely, reliable, meaningful, and sufficient data services, focusing on what data attributes should be achieved based on the data attributes of Big Data services. In addition to the quality level of Big Data, the personal information protection strategy and the data disclosure/accountability strategy are also needed to achieve goals and to prevent problems. This paper performed case analysis based on the Big Data Governance Framework with the National Pension Service of South Korea. Big Data services in the public sector are an inevitable choice to improve the quality of people's life. Big Data governance and its framework are the essential components for the realization of Big Data service.",2017,
148,10.1109/ICBDIE52740.2021.00033,RESEARCH ON DYNAMIC MONITORING OF THIRD LANGUAGE TEACHING QUALITY BASED ON BIG DATA,"The traditional teaching quality monitoring in colleges and universities has drawbacks such as long data collection time, difficulty in summarizing, feedback lag, one-sided evaluation, lack of systematization and integrity, especially the third language teaching has been hovering in the ""gray zone"" of monitoring due to its own characteristics. With the concept of ""Internet plus"" extending in the field of higher education, big data technology can be used to choose the appropriate teaching cloud platform, collect and analyze the data generated in the teaching process comprehensively. It can also be used to recommend personalized learning sources by capturing the click rate, retention time, stay location and learning mood of learners; to set up questionnaires for different evaluation objects, customize diversified indicators of teaching and learning evaluation; to form an efficient feedback and improvement mechanism among students, teachers, supervisors, secondary colleges and teaching management institutions, which can bring a new perspective for college third language teaching, and also provide a new path for teaching quality monitoring.",2021,
149,10.1109/BDEIM52318.2020.00024,ANALYSIS OF FACTORS AFFECTING THE SALES OF POPULAR SCIENCE BOOKS BASED ON BIG DATA,"The consumption data of popular science books on Taobao is obtained by using the technology of web crawler, and an empirical study is carried out on the indexes that affect the sales. Through the research, it is found that the sales rank of search engines, the price, the quantity and quality and content of online reviews have a significant impact on the sales of popular science books. Although the basic variable is an important reference index for consumers to make decisions, it has no significant impact on sales. As an important manifestation of the value of popular science books, price has a positive impact on sales, but buyers are usually willing to pay a certain premium for high-quality products. Sales rank is an important function of major e-commerce websites, which has a strong correlation and boosting effect with sales. Different variables have different effects on the sales of popular science books.",2020,
150,10.1109/ICoAC44903.2018.8939061,BIG DATA ANALYTICS IN HEALTHCARE,"The pace of both digital innovation and technology disruption is refining the healthcare industry at an exponential rate. The large volume of healthcare data continues to mount every second, making it harder and very difficult to find any form of useful information. Recently, big data is shifting the traditional way of data delivery into valuable insights using big data analytics method. Big data analytics provides a lot of benefits in the healthcare sector to detect critical diseases at the initial stage and deliver better healthcare services to the right patient at the right time so that it improves the quality of life care. Big data analytics tools play an essential role to analyze and integrate large volumes of structured, semi-structured and unstructured vital data rapidly produced by the various clinical, hospitals, other social web sources and medical data lakes. However, there are several issues to be addressed in the current health data analytics platforms that offer technical mechanisms for data collection, aggregation, process, analysis, visualization, and interpretation. Due to lack of detailed study in the previous literature, this article inspects the promising field of big data analytics in healthcare. This article examines the unique characteristics of big data, big data analytical tools, different phases followed by the healthcare economy from data collection to the data delivery stage. Further, this article briefly summarizes the open research challenges with feasible findings, and then finally offers the conclusion.",2018,
151,10.1109/BigDataService.2016.26,AIR QUALITY SIMULATIONS USING BIG DATA PROGRAMMING MODELS,"Forecasts of daily pollutant levels have become a standard part of weather predictions in television, on-line, and in newspapers. Research groups also need to analyze larger timeframes across more locations to correlate long term developments for different pollutants with multiple serious health effects such as asthma. This paper presents a comparison of the Hadoop MapReduce and Spark programing models for air quality simulations, guiding future code development for the research groups interested in these analyses. Two use cases have been used, namely (i) calculating the eight hour rolling average of pollutants in a restricted region, (ii) identifying clusters of sensors showing similar patterns in pollutant concentration over multiple years in the state of Texas. The data set used in this analysis is air pollution data collected over fifteen years at 179 monitor sites across the state of Texas for a variety of pollutants. Our results reveal 20-25% performance benefits for the Spark solutions over MapReduce. Furthermore, it documents performance benefits of the Spark MLlib machine learning library over the Mahout library which is based on the MapReduce programing model.",2016,
152,10.1109/ACCESS.2019.2904286,BIG DATA QUALITY ASSURANCE THROUGH DATA TRACEABILITY: A CASE STUDY OF THE NATIONAL STANDARD REFERENCE DATA PROGRAM OF KOREA,"In the era of big data, the scientific and social demand for quality data is aggressive and urgent. This paper sheds light on the expanded role of metrology of verifying validated procedures of data production and developing adequate uncertainty evaluation methods to ensure the trustworthiness of data and information. In this regard, I explore the mechanism of the national standard reference data (SRD) program of Korea, which connects various scientific and social sectors to metrology by applying useful metrological concepts and methods to produce reliable data and convert such data into national standards. In particular, the changing interpretation of metrological key concepts, such as “measurement,” “traceability,” and “uncertainty,” will be explored and reconsidered from the perspective of data quality assurance. As a result, I suggest the concept of “data traceability” with “the matrix of data quality evaluation” according to the elements of a data production system and related evaluation criteria. To conclude, I suggest social and policy implications for the new role of metrology and standards for producing and disseminating reliable knowledge sources from big data.",2019,21693536
153,10.1109/BigData.2017.8258218,IDENTIFYING AND MITIGATING RISKS TO THE QUALITY OF OPEN DATA IN THE POST-TRUTH ERA,"Big Data analysis often relies on open data, integrating it with large private data sets, using it as ground truth information, or providing it as part of the input to large simulations. Data can be released openly by governments to achieve various objectives: transparency, informing citizen engagement, or supporting private enterprise, to name a few. To the latter objective, Big Data analytics algorithms rely on high-quality, timely access to various data sources, including open data. Examples include retail analytics drawing on open demographic data and weather forecast systems drawing on open weather and climate data. In this paper, we describe the rise of post-truth in society, and the risks this poses to the quality, integrity, and authenticity of open data. We also discuss approaches to identifying, assessing, and mitigating these risks, and suggest future steps to manage this data quality concern.",2017,
154,10.1109/COMITCon.2019.8862445,"CLOUD, BIG DATA & IOT: RISK MANAGEMENT","The heart of research pumps for analyzing risks in today's competitive business environment where big, massive computations are performed on interconnected devices pervasively. Advanced computing environments i.e. Cloud, big data and Internet of things are taken under consideration for finding and analyzing business risks developed from evolutionary, interoperable and digital devices communications with massive volume of data generated. Various risks in advanced computational environment have been identified in this research and are provided with risks mitigation strategies. We have also focused on how risk management affects these environments and how that effect can be mitigated for software and business quality improvement.",2019,
155,10.1109/MLBDBI51377.2020.00068,INFLUENCE OF BIG DATA ON MODERN RISK-ORIENTED AUDIT AND COUNTERMEASURES,"The level of economic development determines the actual state of social development. With the advent of the era of fragmentation, science and technology are constantly evolving. Big data and multiple linkage technologies are newly presented in response to the trend of the new era. At the same time, big data can be widely used in various fields. It can also play an effective role in risk-oriented audit. Therefore, this article starts from reality, fully explores the problems of big data in modern risk-oriented audit, and then proposes corresponding solutions.",2020,
156,10.1109/BigData.2014.7004319,INCREASING THE ACCESSIBILITY TO BIG DATA SYSTEMS VIA A COMMON SERVICES API,"Despite the plethora of polls, surveys, and reports stating that most companies are embracing Big Data, there is slow adoption of Big Data technologies, like Hadoop, in enterprises. One of the primary reasons for this is that companies have significant investments in legacy languages and systems and the process of migrating to newer (Big Data) technologies would represent a substantial commitment of time and money, while threatening the ir short-term service quality and revenue goals. In this paper, we propose a possible solution that enables existing infrastructure to access Big Data systems via a services application programming interface (API); minimizing the migration drag and (possibly negative) business repercussions.",2014,
157,10.1109/BigData.Congress.2013.60,SERVICE-GENERATED BIG DATA AND BIG DATA-AS-A-SERVICE: AN OVERVIEW,"With the prevalence of service computing and cloud computing, more and more services are emerging on the Internet, generating huge volume of data, such as trace logs, QoS information, service relationship, etc. The overwhelming service-generated data become too large and complex to be effectively processed by traditional approaches. How to store, manage, and create values from the service-oriented big data become an important research problem. On the other hand, with the increasingly large amount of data, a single infrastructure which provides common functionality for managing and analyzing different types of service-generated big data is urgently required. To address this challenge, this paper provides an overview of service-generated big data and Big Data-as-a-Service. First, three types of service-generated big data are exploited to enhance system performance. Then, Big Data-as-a-Service, including Big Data Infrastructure-as-a-Service, Big Data Platform-as-a-Service, and Big Data Analytics Software-as-a-Service, is employed to provide common big data related services (e.g., accessing service-generated big data and data analytics results) to users to enhance efficiency and reduce cost.",2013,23797703
158,10.1109/IWBIS.2017.8275101,IMPROVING DATA QUALITY THROUGH BIG DATA: CASE STUDY ON BIG DATA-MOBILE POSITIONING DATA IN INDONESIA TOURISM STATISTICS,"Big Data is a new concept that has become widely popularised in recent years. The revolutionized meaning of information communication technologies and Internet technologies refers to mobile communications which enable individuals to move and generate, transmit and receive different kinds of information. There are many communication options where users can search, interact and share information with other users such as website, social media, online communities blogs, and email called as Digital transformation. In Digital transformation era, over 95 % of travellers today use digital resources. Digital traveler can be as data source for official statistics. One of methods to capture the number of tourist can use Mobile Positioning Data (MPD). This method is considered able to improve the quality of survey data. This article will discuss further how to improve the quality of survey data through Big Data with case studies of MPD users in Indonesian Tourism Statistics.",2017,
159,10.1109/ICDMW.2016.0116,THE PROMINENT ROLE OF PROBE DATA AND BIG DATA IN MODERN TECHNOLOGY,"In today's society, the management of probe data and big data is becoming increasingly important with new technological advancements. This can be demonstrated in the ongoing development of automated driving systems and the dynamic map. The safety of these new systems depends on high quality data and a reliable shared platform. As the transition to artificial intelligence continues, it is the responsibility of humans to create a system that can effectively control information with minimal risks.",2016,23759259
160,10.1109/ICBDIE52740.2021.00019,BIG DATA TECHNOLOGY BOOSTS RESEARCH ON THE MODERNIZATION OF VOCATIONAL EDUCATION MANAGEMENT IN SHANDONG PROVINCE,"Big data technology has flourished in recent years. As a subdivision of big data, education big data has great potential to promote education reform. Big data technology is developing rapidly, which provides new possibilities for the application of big data. In order to deeply analyze the development of big data in education, starting from the latest development of big data technology, the development trend of big data is explained from the aspects of infrastructure, analysis technology and field application. Then through the analysis of the composition and characteristics of big data in the education field, the meaning of big data in education is analyzed. In order to promote the modern management of vocational education, this article analyzes the challenges of vocational education in the era of big data on the basis of analyzing the status quo of vocational education. First, special skills and innovation capabilities will become the subject of talent training. The second is that learning and personalized training will be deeply integrated. The third is that technical education and comprehensive capabilities will achieve multi-subject training. Fourth, information technology and teaching resources will promote the reform of teaching models. Fifth, super skill and good organization will become the basic requirements of teachers. Sixth, management innovation and technology research and development will become important measures for the modern management of vocational colleges. Finally, this article discusses the strategies of modern management of vocational education, including improving the management ability of vocational colleges, building a new vocational education training model, improving the teaching quality evaluation system, and building a ""dual-teacher"" teaching team.",2021,
161,10.1109/JIOT.2021.3096637,AN INTEGRATED FRAMEWORK FOR HEALTH STATE MONITORING IN A SMART FACTORY EMPLOYING IOT AND BIG DATA TECHNIQUES,"With the rapid growth in the use of various smart digital sensors, the Internet of Things (IoT) is a swiftly growing technology, which has contributed significantly to Industry 4.0 and the promotion of IoT-based smart factories, which gives rise to the new challenges of big data analytics and the implementation of machine learning techniques. This article proposes a practical framework that combines IoT techniques, a data lake, data analysis, and cloud computing for manufacturing equipment health-state monitoring and diagnostics in smart manufacturing. It addresses all the required aspects in the realization of such a system and allows the seamless interchange of data and functionality. Due to the specific characteristics of IoT sensor data (low quality, redundant multisources, partial labeling), we not only provide a promising framework but also give detailed insights and pay considerable attention to data quality issues. In the proposed framework, an ingestion procedure is designed to manage data collection, data security, data transformation and data storage issues. To improve the quality of IoT big data, a high-noise feature filter is proposed for automated preliminary sensor selection to suppress noisy features, followed by a noisy data cleaning module to provide good quality data for unbiased diagnosis modeling. The proposed framework can achieve seamless integration between IoT big data ingestion from the physical factory and machine learning-based data analytics in the virtual systems. It is built on top of the Apache Spark processing engine, being capable of working in both big data and real-time environments. One case study has been conducted based on a four-stage syngas compressor from real industries, which won the Best Industry Application of IoT at the BigInsights Data &#x0026; AI Innovation Awards. The experimental results demonstrate the effectiveness of both the proposed IoT-architecture and techniques to address the data quality issues.",2022,23722541
162,10.1109/MLBDBI51377.2020.00058,RESEARCH ON HOW TO STRENGTHEN IDEOLOGICAL AND POLITICAL EDUCATION IN COLLEGES AND UNIVERSITIES IN THE ERA OF BIG DATA,"Analyzing, judging and sorting out data through big data technology helps us grasp the prospects and laws of the development of things. Especially for the future development direction of things, more accurate calculations can be made, and scientific and reasonable measures can be taken to promote the development of things according to the calculation results. At present, in the process of ideological and political education in my country's colleges and universities, the teaching methods are mainly traditional teaching methods. In classroom teaching, teachers instill the corresponding curriculum theory, the teaching method is relatively simple, and the curriculum content is boring, which seriously affects the teaching efficiency and teaching quality of college ideological and political classrooms. The use of big data technology to build a curriculum network system that is more suitable for students' learning needs. Besides, the use of some simple extracurricular activities to assist students' education and teaching can mobilize students' enthusiasm and enthusiasm for participating in ideological and political education in colleges and universities, which is conducive to giving full play to ideological and political education in colleges and universities positive effect. Therefore, we need to pay attention to the positive role of big data technology in college ideological and political education, and use big data technology to strengthen college ideological and political education.",2020,
163,10.1109/BDEIM55082.2021.00083,THE APPLICATION OF BIG DATA TECHNOLOGY IN COMPUTER NETWORK INFORMATION MANAGEMENT,"Nowadays, big data technology has been frequently used in computer network information management. Big data technology can make computer network information management more convenient and improve the quality and technical level of information management. At the same time, big data technology can also analyze and organize large and complex data, making information resource sharing more real and accurate. This article focuses on the authority module in the computer network information management, and divides the authority level according to the needs of users, which can realize the field-level access control in the database, thereby improving the information protection of the computer network information management. The correct use of big data in the authority management module can also improve the security of network information management.",2021,
164,10.1109/ICBDSS51270.2020.00045,RESEARCH ON THE DEVELOPMENT TREND OF AQUACULTURE BASED ON BIG DATA ANALYSIS,"Aquaculture economy plays an important role in guaranteeing the food supply and quality protein output of our residents. By cleaning, aggregating and analyzing the big data of China, Japan and the United States from 2010 to 2017, published by the fisheries and aquaculture department of FAO, we study the development trend of aquaculture production and value in the three countries and make a prediction in 5 years by linear regression fitting. Finally, suggestions were put forward for the aquaculture development. This paper found that China's aquaculture production and value have been developing steadily, and would continue to stay progression in five years, but the growth rate would decline. The variety of aquaculture species is diversified, the production and value of aquaculture farming species are relatively balanced. China should strengthen aquaculture resources and environmental conservation and vigorously develop high technology in aquaculture. As for Japan, development prospects are not bright in the near future. Due to the combined effects of tsunami and aging fishery communities, the aquaculture is generally showing a declining trend. Japan should take measures to restore and strengthen fishery resources and revitalize fishing communities. In the United States, aquaculture production declined gradually after 2004 due mainly to the shrinking scale of channel catfish farming. Since then, it has remained quite stable. Besides, the aquaculture species are relatively poor, the production and value of the farming species are extremely uneven. At present, the United States should expand aquaculture species, substantially increase aquaculture production and maintain a growing trend.",2020,
165,10.1109/BDEIM55082.2021.00098,RESEARCH AND EXPLORATION OF COLLEGE STUDENT AWARD MANAGEMENT SYSTEM BASED ON INFORMATION SYSTEM UNDER THE BACKGROUND OF BIG DATA,"University award management plays an important role in cultivating students ‘ good quality. However, at the present stage, most colleges and universities have scattered award management structures and generally use traditional manual means of information office. The shortcomings of low system efficiency, a high error rate and complex repetition have made students and faculty complain. In this paper, the current situation of the award management system for college students is investigated. In line with the characteristics of the big data era, the causes of the problems are analyzed in depth according to the phenomenon. Combined with the practical experience of the management of colleges and universities, the literature review method, research method and object-oriented method are used to analyze the significance of the construction of the system. A multi-parameter award management system is constructed and realized by using the management information system.",2021,
166,10.1109/ICBASE51474.2020.00028,ANALYSIS OF COMPUTER SCIENCE BASED ON BIG-DATA MINING,"The scientific construction of a first-class discipline construction evaluation system is of great significance to the promotion of discipline construction. As an important evaluation reference system, the third-party evaluation system must pay attention to its underlying data sources and calculation methods. Using the massive underlying data of the Scopus database, through the analysis of the computer disciplines of four Chinese universities, the development trend is discussed from the aspects of overall academic output, scientific research quality, and hot topics.",2020,
167,10.1109/ICCCN49398.2020.9209633,USING TRUST AS A MEASURE TO DERIVE DATA QUALITY IN DATA SHARED IOT DEPLOYMENTS,"Recent developments in Internet of Things have heightened the need for data sharing across application domains to foster innovation. As most of these IoT deployments are based on heterogeneous sensor types, there is increased scope for sharing erroneous, inaccurate or inconsistent data. This in turn may lead to inaccurate models built from this data. It is important to evaluate this data as it is collected to establish its quality. This paper presents an analysis of data quality as it is represented in Internet of Things (IoT) systems and some of the limitations of this representation. The paper then introduces the use of trust as a heuristic to drive data quality measurements. Trust is a well-established metric that has been used to determine the validity of a piece or source of data in crowd sourced or other unreliable data collection techniques. The analysis extends to detail an appropriate framework for representing data quality within the big data model. To demonstrate the application of a trust backed framework, we used data collected from a IoT deployment of sensors to measure air quality in which a low cost sensor was co-located with a gold reference sensor. Using data streams modeled based on a dataset from an IoT deployment, our initial results show that the framework's trust score are consistent with the accuracy measure of the machine learning models.",2020,10952055
168,10.26599/BDMA.2020.9020024,MULTI-ATTENTION FUSION MODELING FOR SENTIMENT ANALYSIS OF EDUCATIONAL BIG DATA,"As an important branch of natural language processing, sentiment analysis has received increasing attention. In teaching evaluation, sentiment analysis can help educators discover the true feelings of students about the course in a timely manner and adjust the teaching plan accurately and timely to improve the quality of education and teaching. Aiming at the inefficiency and heavy workload of college curriculum evaluation methods, a Multi-Attention Fusion Modeling (Multi-AFM) is proposed, which integrates global attention and local attention through gating unit control to generate a reasonable contextual representation and achieve improved classification results. Experimental results show that the Multi-AFM model performs better than the existing methods in the application of education and other fields.",2020,20960654
169,10.1109/ITCA49981.2019.00070,RESEARCH OF THE INTEGRATION OF HUMANISTIC QUALITY IN EDUCATION UNDER THE BACKGROUND OF INTELLIGENT BIG DATA,"In the context of the new era, it is very indispensable to integrate humanistic quality education into college and university. However, there are still some problems because of the impact of big data information, which is not conducive to students' humanity and even affects their future development. The following is an in-depth analysis of the integration of humanistic quality in education under the background of big data information. The purpose of this paper is to effectively promote the integration of humanistic quality between college and university education, in order to improve the humanistic quality of students as well as better development of students.",2019,
170,10.1109/ICCCBDA.2017.7951917,INFORMATION SERVICE QUALITY EVALUATION STUDY OF CLOUD COMPUTING ENVIRONMENT BASED ON BIG DATA,"Give full play to the big data in the cloud computing environment application advantages has become an important information service mode of the era of Internet +, paper with information service quality evaluation as the main line, using fuzzy comprehensive evaluation method to analyze a set of cloud computing environment information service quality evaluation process, at the same time, using the case of project construction example analysis, the evaluation of cloud computing environment based on large data information service quality has important reference value.",2017,
171,10.1109/ACCESS.2021.3072196,BLOCKCHAIN-WATERMARKING FOR COMPRESSIVE SENSED IMAGES,"With the application of multimedia big data, the problems such as information leakage and data tampering have emerged. The security of images which is one of the most typical multimedia has become a major problem facing the large-scale open network environment. This paper proposed a blockchain-watermarking scheme to protect the privacy, integrity and availability of compressed sensed images, which effectively combines multimedia watermarking, compressed sensing, Interplanetary File System (IPFS) and blockchain technologies. Based on the reliable authentication of watermarking, the confidentiality protection of compressed sensing, the secure storage of IPFS, and the decentralization and non-tamperability of blockchain, the all-round security protection of the image big data based on compressive sensing can be realized. Experiments show that the proposed scheme is effective and feasible.",2021,21693536
172,10.1109/ICBDIE52740.2021.00016,RESEARCH ON THE DEVELOPMENT OF UNIVERSITY INNOVATION AND ENTREPRENEURSHIP EDUCATION UNDER THE BACKGROUND OF BIG DATA,"Today, information technologies such as big data, artificial intelligence, mobile Internet, and intelligent integrated networks are gradually changing people's work and life, and technological innovation and technological entrepreneurship will surely become mainstream models. Innovation and entrepreneurship education in colleges and universities closely follows the trend of the era of ""mass entrepreneurship and innovation"", and effectively integrates with big data to promote the development of innovation and entrepreneurship education. This article re-organizes the current problems of innovation and entrepreneurship education in universities, analyzes the existing research, value and role of big data for innovation and entrepreneurship education in universities, and expounds that big data promotes innovation in universities from the three aspects of education philosophy, quality evaluation and education system.",2021,
173,10.1109/BigData50022.2020.9377918,EDGE COMPUTING WITH BIG DATA CLOUD ARCHITECTURE: A CASE STUDY IN SMART BUILDING,"The growth of buildings embedded with technologies that can monitor the internal building environment with respect to energy consumption such as heating, ventilation, air conditioning, wind, motion as well occupancy have an immense potential. From energy management, occupancy administration, security maintenance as well as improving the health and quality of life for humans in indoor or outdoor spaces. These potentials can be realized by a clear understanding of the interplay between vast environmental conditions, humans and their health as well as the many smart products they interact with in their lives. This is a complex process that requires thorough testing and evaluation within smart buildings simulation environments where multiple buildings data can be generated and then effectively analyzed. This can be facilitated by a robust data management process that utilizes big data computing technologies to harness large volumes, variety and velocity of data that can be captured within smart buildings while maintain the security and privacy of data sources.In this paper we describe a smart building architecture that has been designed and developed for management of data from a smart building. In particular the architecture enables acquisition, processing and distribution of simulated environmental building data to multiple consumers and workflows for further processing and analysis locally and in a high performance cloud computing platform. The research premise is that such an architecture enables effective management of multiple data sources within climatic based simulated testing in smart buildings to further research.",2020,
174,10.1109/BigData.2018.8622229,A PATH TO BIG DATA READINESS,"""Big Data readiness"" begins at the source where data are first created and extends along a path through an organization to the outside world. This paper focuses on practical solutions to common problems experienced when integrating diverse datasets from disparate sources. Following the Introduction, Section 2 situates Big Data in the larger context of open government, open science, science integrity, and Standards, internationally and in Canada. Section 3 analyses the Big Data problem space, while Section 4 proposes a Big Data solution space. Section 5 proposes eight data checklist modules and suggests implementation strategies to effectively meet a variety of organizational needs. Section 6 summarizes conclusions and describes future work.",2018,
175,10.1109/ICBDIE52740.2021.00038,RESEARCH ON THE INFLUENCING FACTORS OF TEACHING ABILITY OF TEXTILE UNIVERSITY PROFESSIONAL TEACHERS UNDER THE COMBINATION OF TEACHING AND BIG DATA INFORMATION--TAKING DALIAN POLYTECHNIC UNIVERSITY AS AN EXAMPLE,"Nowadays, textile professionals in the era of big data do not just meet a skill requirement. Because the business operation model brought about by big data is very different from the past, different positions are not independent of each other, but are interconnected and interdependent, and jointly serve the enterprise information operation system. In this respect, the professionalism and diversification of teachers' teaching abilities are the core and key to the development of higher education institutions. With the progress of the times, the rapid development of higher education in China, among the many factors that affect the teaching quality of colleges and universities, the teaching ability of professional teachers is an important factor. The teaching ability of college teachers is of great significance to the development of china's education and the growth of students[1]. This research combines the analysis of big data informatization with textile professional teachers as the research object. Through the sorting and analysis of teaching combined big data, the current situation of the teaching ability of textile teachers in my country is found, and the form of data analysis is used to find out the influence of textiles[2]. Influencing factors of professional teachers' teaching ability, scientifically and effectively find effective strategies for the development of professional teachers' teaching ability in my country's colleges and universities.",2021,
176,10.1109/CICED.2016.7575978,DISCUSSION OF APPLICATION ABOUT USING OF BIG DATA TECHNOLOGY IN HARMONIC MONITORING PLATFORM,"At present, several provinces' power supply companies have established harmonic monitoring platform in China, by real-time monitoring the voltage and current of power grid, then upload these data to the main station to analyze and settle. The platform can collect about 1 TB(1TB=1000GB) data every day[12], these data contain all the indexes of power quality, but we can only use this platform to do some basal data analysis work, but not processing analysis with multi-indexes, so that it does not show strong supporting effort to the power grid operation. With the development of energy internet and the third industry, more and more unstructured data are impacting the power grid operation, how to make better use of big data technology to deal with the data from monitoring platform has became the focus of the power grid company. In this paper, some introduces of contents and characteristics of harmonic real-time monitoring platform and big data technology has been made, by comparing the way of using and settling data of platform to the big data technology, meanwhile by combining with the characteristics of power quality data, we analyze the combination of the two prospects. At last, the development direction of the advanced application for the harmonic line monitoring system is discussed in this paper.",2016,2161749X
177,,QUALITY ISSUES WITH BIG DATA ANALYTICS,"The promise of data-driven decision-making is now being recognized broadly, and there is growing enthusiasm for the notion of “Big Data.” In this paper techniques related with the big data pipelining or processing are discussed. There are number of issues related with the analytical results of the big data and the big data processing software. The major issues are included in this paper and will be helpful for the new beginners in this area of big data analytics.",2016,
178,10.1109/AUTEST.2018.8532518,MEASURING MANUFACTURING TEST DATA ANALYSIS QUALITY,"Manufacturing test data volumes are constantly increasing. While there has been extensive focus in the literature on big data processing, less focus has existed on data quality, and considerably less focus has been placed specifically on manufacturing test data quality. This paper presents a fully automated test data quality measurement developed by the authors to facilitate analysis of manufacturing test operations, resulting in a single number used to compare manufacturing test data quality across programs and factories, and focusing effort cost-effectively. The automation enables program and factory users to see, understand, and improve their test data quality directly. Immediate improvements in test data quality speed manufacturing test operation analysis, reducing elapsed time and overall spend in test operations. Data quality has significant financial impacts to businesses [1]. While manufacturing cost models are well understood, data quality cost models are less well understood (see Eppler & Helfert [2] who review manufacturing cost models and create a taxonomy for data quality costs). Kim & Choi [3] discuss measuring data quality costs, and a rudimentary data quality cost calculation is described in [4]. Haug et al. [5] describe a classification of costs for poor data quality, and while they do not provide a cost calculation, they do define optimality for data quality. Laranjeiro et al. [6] have a recent survey of poor data quality classification. Ge & Helfert [7] extend the work in [2], and provide an updated review of data quality costs. Test data is specifically addressed in the context of data processing in [8]. Big data quality efforts are reviewed in [9, 10]. Data quality metrics are discussed in [11], and requirements for data quality metrics are identified in [12]. Data inconsistencies are detailed in [13], while categorical data inconsistencies are explained in [14]. In the current work, manufacturing test data quality is directly correlated to the speed of manufacturing test operations analysis. A measurement for manufacturing test data quality indicates the speed at which analysis can be performed, and increases in the test data quality score have precipitated increases in the speed of analysis, described herein.",2018,10887725
179,10.1109/TKDE.2020.3014302,A DATA-CHARACTERISTIC-AWARE LATENT FACTOR MODEL FOR WEB SERVICES QOS PREDICTION,"How to accurately predict unknown quality-of-service (QoS) data based on observed ones is a hot yet thorny issue in Web service-related applications. Recently, a latent factor (LF) model has shown its efficiency in addressing this issue owing to its high accuracy and scalability. An LF model can be improved by identifying user and service neighborhoods based on user and service geographical information. However, such information can be difficult to acquire in most applications with the considerations of information security, identity privacy, and commercial interests in a real system. Besides, the existing LF model-based QoS predictors mostly ignore the reliability of given QoS data where noises commonly exist to cause accuracy loss. To address the above issues, this paper proposes a data-characteristic-aware latent factor (DCALF) model to implement highly accurate QoS predictions, where ‘data-characteristic-aware’ indicates that it can appropriately implement QoS prediction according to the characteristics of given QoS data. Its main idea is two-fold: a) it detects the neighborhoods and noises of users and services based on the dense LFs extracted from the original sparse QoS data, b) it incorporates a density peaks-based clustering method into its modeling process for achieving the simultaneous detections of both neighborhoods and noises of QoS data. With such designs, it precisely represents the given QoS data in spite of their sparsity, thereby achieving highly accurate predictions for unknown ones. Experimental results on two QoS datasets generated by real-world Web services demonstrate that the proposed DCALF model outperforms state-of-the-art QoS predictors, making it highly competitive in addressing the issue of Web service selection and recommendation.",2022,23263865
180,10.1109/ICBASE51474.2020.00010,A RESEARCH ON THE IMPROVEMENT OF THE COLLEGE ENGLISH CLASSROOM STUDY IN THE CONTEXT OF NEW MEDIA AND BIG DATA,"As a basic subject, college English is of great significance to the all-around development of college students. But traditionally, there exist certain problems, such as undiversified teaching methods, boredom in classroom teaching, lack of learning interests and ineffective classroom learning among students, facing college English teaching. With the develop of science and technology, the technology of new media has become an important part of people's life and work. In the age of new media and big data, the new media technology with its openness, diversity, plurality, and interactivity, can be combined with and facilitate classroom teaching. Such a combination not only provides abundance of learning materials, but also promotes students' learning interest as well as motivation. What's more, it could stimulate students' full development. Under this background, this paper will probe into the college English teaching status quo and the existing problems first, and then give advices regarding the improvement of students' classroom learning efficiency and quality by utilizing the new media technology.",2020,
181,10.1109/ICBDACI.2017.8070826,BIG DATA IN HEALTH CARE: A MOBILE BASED SOLUTION,"In the present Indian scenario, healthcare information is independently maintained by hospitals, institutions and not readily accessible in a centralized, informed manner. This greatly limits the health providers' efforts to improve quality and efficiency. Through this paper, we address this issue on bringing various information from many sources into one place in realtime which can be truly life saving. Also, low ratio of doctor to patient and the low per capita income in India hikes the medical expenses thereby increasing the patient's inaccessibility to receive proper health care in their reach especially for people in the rural areas. A means by which the bridge between the patients and doctors can be gapped and how patients can be treated at a lower expense is the prime concern. This paper focuses on the development of a mobile/web application, through which patients sends their symptomatic query to the doctors through a server. The mobile application will be equipped with first aid instructions, according to the nature and severity of the symptoms, either the patients are directed to respective departments or given emergency help for further treatment. Within the time huge amount of data is collected from users and doctors, this big data will be used to train machines to automate the tasks to some extent. The information gained from analyzing massive amounts of aggregated health data can provide useful insight to improve quality and efficiency for providers and insurers alike. This makes the patients reach out for healthcare solutions easily and cheaply and makes healthcare a easy reach for the unprivileged also. Thus, this unified model can serve as a data collection, delivery as well as an analytic tool in the healthcare domain.",2017,
182,10.1109/CSE.2013.168,"INFORMATION GOVERNANCE, BIG DATA AND DATA QUALITY","The value of information as a competitive differential has been taken into consideration in companies all over the world for some time already. In recent years, there has been heated debate about some terms originated from new concepts related to information, such as big data, due to the promise that such topic might revolutionise world trade. Hence, data and information governance and quality have been increasingly discussed in the business world.",2013,
183,10.1109/ICBASE51474.2020.00007,EVALUATION OF LIVABLE AND RESILIENT URBAN DESIGN BASED ON BIG DATA,"In the background of the information age, modern information technology is constantly updated and improved, especially the rapid development of mobile information technology, which provides great convenience for the use of big data. In the context of big data, the livability of cities has attracted more and more attention. The construction of livable city is not only the best interpretation of the construction of resource-saving and environment-friendly social policies, but also the most urgent desire of urban residents for high-level economic development and high-quality living environment. This paper analyzes the ideas and methods of urban planning evaluation under the background of big data, which can further strengthen the function of urban space urban planning and promote the construction and development of urban modernization.",2020,
184,10.1109/ICBAIE49996.2020.00028,DISCUSSION ON THE TRANSFORMATION OF COMPUTER REMOTE NETWORK COMMUNICATION TECHNOLOGY IN THE ERA OF BIG DATA,"With the advent of the big data era, computer network communication technology has brought certain conveniences to the development of people and enterprises, and it has also broken through the traditional time and geographical limitations. However, under the influence of the big data era, China s computers There are still many problems in the development of long-distance network communication technology. The existence of these problems will not only affect the data transmission speed and quality, but also have a certain negative impact on people’s production and life. Therefore, in the context of the era of big data, relevant departments should revolutionize the network communication technology of the Computer College. This article describes the relative advantages and existing problems of computer remote network communication technology in the era of big data, and puts forward reasonable suggestions for computer network remote communication technology.",2020,
185,10.1109/ICDE.2016.7498367,BIG DATA QUALITY - WHOSE PROBLEM IS IT?,"The increased reliance on data driven enterprise has seen an unprecedented investment in big data initiatives. Organizations averaged US$8M in investments in big data-related initiatives and programs in 2014, with 70% of large enterprises and 56% of small and medium enterprises (SMEs) having already deployed, or planning to deploy, big-data projects [1]. As companies intensify their efforts to get value from big data, the growth in the amount of data being managed continues at an exponential rate, leaving organizations with a massive footprint of unexplored, unfamiliar datasets. On February 8th, 2015, a group of global thought leaders from the database research community outlined the grand challenges in getting value from big data [2]. The key message was the need to develop the capacity to `understand how the quality of data affects the quality of the insight we derive from it'.",2016,
186,10.1109/CTEMS.2018.8769141,PROTAGONIST OF BIG DATA AND PREDICTIVE ANALYTICS USING DATA ANALYTICS,"Big Data has created as a fundamental locale of eagerness of study and research among experts and academicians. Movement in advancement is making it fiscally conceivable to store and examine gigantic proportions of information. Enormous Data consolidates a mix of composed, semi-sorted out and unstructured progressing information starting from combination of sources. Farsighted Analytics gives method in tapping learning from broad information files. Various visionary associations, for instance, Google, Amazon, etc have comprehended the ability of Big Data and Analytics in expanding upper hand. These techniques give a couple of chances like discovering precedents or better improvement figurings. Directing and breaking down. Big data similarly sets up couple of troubles - specifically estimate, quality, steadfast quality and satisfaction of data. This paper gives an expansive review of composing on Big Data and Predictive Analytics. It gives unpretentious components of urgent thoughts in this rising field. Finally, we have completed up with disclosures of our examination and structure future research orientation in this field.",2018,
187,10.1109/ICAIBD49809.2020.9137451,RESEARCH ON BIG DATA REFERENCE ARCHITECTURE MODEL,"For ISO / / IEC TS 25011:2017 some deficiencies of big data reference system. In this paper, we redesigned the big data reference system structure model, and proposed a goal (big data information service quality), three chains (information value chain, information technology value chain and information assurance value chain) and five roles (big data application provider, big data framework provider and big data information assurance provider) 1-3-5 model of data provider and data consumer, and all the attributes of the model are described. In the part of big data information service quality, the deficiencies in ISO / / IEC TS 25011:2017 are corrected. Then, the basic system of big data is proposed. Under the big data reference architecture model proposed in this paper, compared with some other typical models, the conclusion is that other typical models can be replaced completely.",2020,
188,10.1109/ICBDA55095.2022.9760321,RESEARCH ON DATA FUSION OF INTELLIGENT MANUFACTURING DRIVEN BY BIG DATA,"At present, people pay close attention to intelligent manufacturing. The application of information technology such as big data and artificial intelligence can accelerate the development process of intelligent manufacturing. The common problems encountered by small and medium-sized manufacturing enterprises is that the information isolated island leads to the non-flow of data, which leads to the low efficiency of business flow. Combined with the actual business data of the enterprise, the intelligent manufacturing data fusion scheme driven by big data is proposed, which opens up the information isolated island from the horizontal and vertical paths, realizes the fusion of business flow and data flow, improves the efficiency, and achieves the real improvement of quality and efficiency. The data fusion scheme proposed in this paper has been successfully applied in a practical enterprise.",2022,
189,10.1109/ICCSE.2017.8085513,DESIGN OF HIGHER EDUCATION QUALITY MONITORING AND EVALUATION PLATFORM BASED ON BIG DATA,"Through the continuous collection and in-depth analysis of the quality monitoring data of colleges and universities, we combine the efficiency processing of big data and data evaluation, monitor the status of higher education normally, and construct a higher education quality monitoring and evaluation platform based on Spark. This platform is teaching centered with schools as its basis, including subsystems of data acquisition, data analysis, machine learning, data storage, data analysis and other areas. Through the application of the higher education quality monitoring platform, we can understand the current situation of the development of higher education scientifically, and provide the basis for the macro-decision of education administration department.",2017,24739464
190,10.1109/BigDataCongress.2015.53,RISK-ADJUSTED MONITORING METHOD FOR SURGICAL DATA: METHODOLOGY FOR DATA ANALYTICS (WORK IN PROGRESS),"Hospital Authority (HA) has launched a Surgical Outcome Monitoring and Improvement Program (SOMIP), which is used to audit the surgical performance of all public hospitals in Hong Kong. One of the most important information provided by the annual SOMIP report is the changes of 30-day mortality and identify whether and when there is a significant deterioration for each hospital. However, the routine monitoring method used such as Variable Life-adjusted Display (VLAD) and Cumulative Sum Charting (CUSUM) may not be able to detect the change of surgical performance efficiently. Expected improvement or deterioration in surgical outcome may not be exactly the same as the truth. In this paper, we develop a more effective risk-adjusted monitoring method to detect the change in surgical performance. By adapting this method to SOMIP, the proposed monitoring procedure is expected to not only benefit frontline surgeons, anaesthetists and intensivists when they decide on the operations for patients, but also help managers in HA to evaluate the surgical performance and further improve surgical quality.",2015,23797703
191,10.1109/ICCMC53470.2022.9753721,BIG DATA ANALYSIS AND MINING TECHNOLOGY OF SMART GRID BASED ON PRIVACY PROTECTION,"Aiming at the big data security and privacy protection issues in the smart grid, the current key technologies for big data security and privacy protection in smart grids are sorted out, and a privacy-protecting smart grid association rule is proposed according to the privacy-protecting smart grid big data analysis and mining technology route The mining plan specifically analyzes the risk factors in the operation of the new power grid, and discusses the information security of power grid users from the perspective of the user, focusing on the protection of privacy and security, using safe multi-party calculation of the support and confidence of the association rules. Privacy-protecting smart grid big data mining enables power companies to improve service quality to 7.5% without divulging customer private information.",2022,
192,10.1109/ICSCTI.2015.7489625,ENHANCING QOS AND QOE USING BIG DATA IN IPTV DOMAIN,"Big Data analytics has brought remarkable difference in the way businesses are operating these days. IPTV service providers are also looking for ways to augment the value of their offerings by using the power of Big Data Analytics. For IPTV Service Providers, Enhancing Quality of Service (QoS) and Quality of Experience (QoE) are some of the crucial areas which always have significant scope of improvement. These can be directly related to higher customer satisfaction and subsequently to the revenues. Using Big Data Analytics, IPTV Service Providers can anticipate several risks and observe user behavior. This can help the Service Providers to take proactive/appropriate actions which leads to better QoS and QoE.",2015,
193,10.1109/ICITBS.2015.18,A PRELIMINARY DISCUSSION ON THE APPLICATION OF BIG DATA IN URBAN RESIDENTS TRAVEL GUIDANCE,"This paper discusses the application of big data in the residents travel guidance system. The system is based on traffic service information platform and tries to make the information collection and analysis system suitable for more types of data through applying the big data with characteristics of volume, variety, velocity. The application of big data processing technology in this system will form a complete transportation guidance system with the big data, geographic information system and the cloud storage. The big data processing technology will be used to simplify the program of residents travel guidance and raise the quality of urban traffic information.",2015,
194,10.1109/TENCON.2016.7848472,A BIG DATA APPROACH FOR MEMORY QUALITY MANAGEMENT,"As memory technology scaling continues to advance to sub 20nm technology and memory capacity becomes higher, memory quality management becomes more challenging to achieve client's quality expectations especially in this new era of computing. Transformation of traditional quality management approaches becomes necessary to drive memory quality improvements. A big data analytics approach is presented in this paper to demonstrate its application on end to end quality management process to drive continuous memory quality improvements.",2016,21593450
195,10.1109/ICBDACI.2017.8070831,BIG DATA TECHNOLOGIES FOR PREDICTING EPIDEMICS AND ENHANCING THE QUALITY OF HUMAN LIFE,"With a lot of medical data coming from various sources, steer decisions can be made from the insights expand through big data by using various Machine Learning Algorithms. Traditionally, physicians use their knowledge while making treatment assessments, but in the last few years there has been a shift towards evidence-based medicine. This involves systematical appraisal of clinical data and making treatment decisions based on the finest accessible information. In this paper, we apply the predictive analysis technique in Hadoop/Map Reduce background to predict and classify the type of diabetes. And also focuses on how MapReduce is used, how map and reduce evaluations are adapted, implemented in various situations such as in medical field to produce medical reports by processing large medical data sets. This system provides capable way to care and cure the patients at low cost with improved outcomes like affordability and accessibility.",2017,
196,10.1109/BigData.2016.7841003,APPLICATION OF BIG DATA ANALYTICS FOR AUTOMATED ESTIMATION OF CT IMAGE QUALITY,"With the increasing applications of Big Data analytics in medical image processing systems, there has been a growing need for quantitative medical image quality assessment techniques. Specifically for computed tomography (CT) images, quantitative image assessment can allow for benchmarking image processing methods and optimization of image acquisition parameters. In this work, large volumes of CT images from phantoms and patients are analyzed using 3 data models that vary in their implementation time complexities. The goal here is to identify the optimal method that scales across data set variabilities for predictive modeling of CT image quality (CTIQ). The first two models rely on spatial segmentation of regions-of-interest (ROIs) and estimate CTIQs in terms of segmented pixel variabilities. The third, convolutional neural network (CNN) model relies on error back-propagation from the training set of images to learn the regions indicative of CTIQ. We observe that for 70/30 data split, the average multi-class classification accuracies for CTIQ prediction using the 3 data models range from 73.6-100% and 50-100% for the phantom and patient CT images, respectively. Using variance of pixels within the segmented ROIs as a CTIQ classification parameter, the spatial segmentation data models are found to be more generalizable that the CNN model. However, the CNN model is found to be more suitable for CT image texture classification in the absence of structural variabilities. Our analysis demonstrates that spatial ROI segmentation data models are consistent CTIQ estimators while the CNN models are consistent identifiers of structural similarities for CT image data sets.",2016,
197,10.1109/BDEIM52318.2020.00029,RESEARCH ON THE DEVELOPMENT OF RETAIL E-COMMERCE IN CHINA FROM THE PERSPECTIVE OF BIG DATA,"After more than 20 years of development, China's retail e-commerce industry shows an explosive growth trend. At the present stage, the over-speed growth of the retail e-commerce industry has gradually leveled off, with many platforms showing a trend of giant standing. The rapid rise of emerging technologies such as big data, artificial intelligence and cloud computing has brought new opportunities and challenges to the development of retail e-commerce, and further exposed the problems of retail e-commerce. In order to satisfy consumers' high-quality and high-demand experience in the future, this paper puts forward countermeasures to improve the development of retail e-commerce in China based on the analysis of big data.",2020,
198,10.1109/BigData50022.2020.9378487,FACILITATING THE HPC DATA CENTER HOST EFFICIENCY THROUGH BIG DATA ANALYTICS,"Quality of service is important feature for a High Performance Computing Center (HPC) center like Partnership for an Advanced Computing Environment (PACE) center in Georgia Institute of Technology (Georgia Tech). The user's job fails running on a HPC center may due to a spectral of reasons, one of major contributor is the hardware and network failure. Reducing the hardware failure rate can significantly increase a data center's quality of service as well as reducing the cost of human intervention. This is critical during PACE's transition to a fee-based service model in which uptime correlates directly with revenue. PACE has around 9 millions jobs each year with 12% of job failure rate. In order to extend service life of hardware and reduce the potential failure and data center's cost, we present a machine learning method to understand the center's host usage pattern. By clustering the hosts based on multiple features, we reshuffle the host list to avoid the hosts being overused over time. We build a test framework which runs the complex combination of experiments, and presents the ad hoc comparisons. We intend to make the machine learning method in a rack aware fashion, and show the meaningful result with rack information included.",2020,
199,10.1109/TBDATA.2017.2750699,DATA-PATTERN ENABLED SELF-RECOVERY LOW-POWER STORAGE SYSTEM FOR BIG VIDEO DATA,"The growing popularity of powerful mobile devices such as smart phones and tablet devices has resulted in the exponential growth of demand for video applications. However, due to the large video data size and intensive computation, mobile video applications require frequent embedded memory access, which consumes a large amount of power and limits battery life. In this paper, we present a low-cost self-recovery video storage system by investigating meaningful data patterns hidden in big video data, by introducing data mining techniques to the hardware design process. We propose a two-dimensional data-pattern approach to explore horizontal data-association and vertical data-correlation characteristics. Such data relationship discovery and pattern identification enable a new dimension for the hardware design space and bring self-recovery ability to memories in the presence of bitcell failures. Based on the identified optimal data patterns, we present a low-cost and efficient SRAM design to enable data self-recovery at low voltages. A 45nm 32 kb SRAM is implemented that delivers good video quality at near-threshold voltage (0.5 V) with negligible area overhead (7.94 percent).",2019,23722096
0,10.1109/BigDataService.2016.37,BIG DATA QUALITY: A ROADMAP FOR OPEN DATA,"Open Data (OD) is one of the most discussed issue of Big Data which raised the joint interest of public institutions, citizens and private companies since 2009. However, the massive amount of freely available data has not yet brought the expected effects: as of today, there is no application that has fully exploited the potential provided by large and distributed information sources in a non-trivial way, nor any service has substantially changed for the better the lives of people. The era of a new generation applications based on OD is far to come. In this context, we observe that OD quality is one of the major threats to achieving the goals of the OD movement. The starting point of this case study is the quality of the OD released by the five Constitutional offices of Italy. Our exploratory case study aims to assess the quality of such releases and the real implementations of OD. The outcome suggests the need of a drastic improvement in OD quality. Finally we highlight some key quality principles for OD, and propose a roadmap for further research.",2016,
1,10.1109/IPEC49694.2020.9115136,A BIG DATA ANALYSIS BASED SYSTEM FOR THE COMPREHENSIVE EVALUATION OF THE CULTIVATION QUALITY OF THE INNOVATION AND ENTREPRENEURSHIP SKILLS AMONG CHINESE COLLEGE STUDENTS,"The big data analysis based model for tracking the cultivation quality of college students’ innovation and entrepreneurship skills relies on the matching and synergy between elements such as diversified participants, funding, technology and network design, and institutional systems to achieve its swift and efficient operation. The creation and integration of these elements would be the future focus of this project. Through diversified data collection from existing systems and other databases, we employ the indigenously developed data scheduling system (DataX) to carry out centralized scheduling of data extraction, thereby building a big data analysis based system for the comprehensive evaluation of the cultivation quality of innovation and entrepreneurship skills.",2020,
2,10.1109/ICABCD.2018.8465132,IMPROVE DECISION MAKING TOWARDS UNIVERSITIES PERFORMANCE THROUGH BIG DATA ANALYTICS,"The technology Big Bang has seen organizations universities inclusive generating big volumes of data in various formats and at high speed than they used to do. Such data is referred to as Big Data. This voluminous data can be of great significance to organizations if better insights are drawn for management to improve decision making. However, to draw valued insight from Big Data, advanced forms of analytics need to be employed and such techniques are commonly known as Big Data analytics (BDA), This paper sough to report on analysis of factors influencing the leverage of BDA to improve performance in universities.",2018,
3,10.1109/ICSGRC49013.2020.9232648,A STUDY ON THE ASPECTS OF QUALITY OF BIG DATA ON ONLINE BUSINESS AND RECENT TOOLS AND TRENDS TOWARDS CLEANING DIRTY DATA,"The reliability, efficiency, and accuracy of e-business depend on the quality of data that is associated with a buyer, seller, brokers, e-business portals, admins, managers, decision-makers and so on. However, maintaining the quality of data in e-business is very challenging. It is because e-business data typically comes from different communication channels and sources. Integrating and managing the data quality of different sources is generally much troublesome than dealing with traditional business data. Even though there are several data cleaning methods and tools exist those methods and tools have some constraints. None of them directly working, particularly on e-business data that motivates to do research to highlight the aspects of big data quality related to e-business. Therefore, this research demonstrates the problems related to data quality related to online business, discusses the existing literature of data quality, the current tools and techniques that are being used for data quality and provides a research finding highlighting the weaknesses of current tools to address the problem of online business.",2020,
4,10.1109/ICCBE56101.2022.9888218,RESEARCH ON BIG DATA MINING EVALUATION SYSTEM OF BEIJING OPERA ART PERFORMANCE,"Considering the important position of Peking opera art performance in Chinese traditional culture, it is necessary to comprehensively and accurately understand and monitor the quality of Peking opera art performance. In order to achieve this, we build a comprehensive and representative evaluation system of Beijing opera art performance. Taking the satisfaction index of Peking opera artistic performance as an important indicator of perfection, a Peking opera artistic performance evaluation system is established with 4 dimensions and 10 indicators. By using factor analysis, the evaluation index model of Peking opera artistic performance is validated.",2022,
5,10.1109/BDACS53596.2021.00008,DESIGN OF SOCIAL MEDIA USER SATISFACTION EVALUATION SYSTEM FROM THE PERSPECTIVE OF BIG DATA SERVICES,"In the era of big data, an important way for country to develop digital economy and improve information management capabilities is to develop service-oriented social media. Based on this background, this article studies the significant impact of service methods, information content, problem-solving quality, and infrastructure facilities of new social media platform on the social media services maturity and user satisfaction. According to the functional characteristics of the social media platform, the user satisfaction evaluation model of social media is constructed.",2021,
6,10.1109/TEM.2020.2971717,QOS-AWARE DATA PLACEMENT FOR MAPREDUCE APPLICATIONS IN GEO-DISTRIBUTED DATA CENTERS,"With growing data volumes and the scaling of data center clusters, communication resources often become a bottleneck in service provisioning for many MapReduce applications (e.g., training machine learning models). Therefore, data placements that bring data blocks closer to data consumers (e.g., MapReduce applications) are seen as a promising solution. In this article, we propose an efficient data-placement technique that considers network traffic reduction as well as QoS guarantees for the data blocks to optimize the communication resources. We first formulate the joint optimization of the data-placement problem, propose a generic model for minimizing communication costs, and show that the joint data-placement problem is NP-hard. To solve this problem, we propose a heuristic algorithm considering traffic flows in the network topology of data centers by first seeking optimal QoS-aware data placement based on golden division on a Zipflike replica distribution, then transforming the joint data-placement problem into a block-dependence tree (BDT) construction problem, and finally reducing the BDT construction to a graph-partitioning problem. The experimental results demonstrate that our data-placement approach could effectively improve the performance of MapReduce jobs with lower communication costs and less job execution time for big-data processing.",2021,15580040
7,10.1109/BigData.2018.8622487,MEASURING COMPLETENESS AS METADATA QUALITY METRIC IN EUROPEANA,"Europeana, the European digital platform for cultural heritage, has a heterogeneous collection of metadata records ingested from more than 3200 data providers. The original nature and context of these records were different. In order to create effective services upon them we should know the strength and weakness or in other words the quality of these data. This paper proposes a method and an open source implementation to measure some structural features of these data, such as completeness, multilinguality, uniqueness, record patterns, to reveal quality issues.",2018,
8,10.1109/ICITBS.2016.69,THE OPTIMIZATION DESIGN OF PRO-POOR TOURISM INFORMATION SYSTEM IN SICHUAN AREA WITH INTRODUCTION OF BIG DATA ANALYSIS,"For any countries and regions, and all sectors of society, poverty has always been the generally concerned and long-term content. The devotion of the area has fully embodied the social common development and the humanities. Sichuan is an important tourism province. The development value space of tourism resource is extremely broad. The pro-poor tourism has a strong maneuverability. The distribution of tourism resources and the concentration distribution of poor population in Sichuan area are highly overlapped. The poverty-stricken area is the most potential area of tourist resources. The distribution of tourism resources is more concentrated, and the quality of resources is higher. Therefore, it is an effective way to combine the tourism development with the anti-poverty organically. For this purpose, the big data analysis is introduced into the optimization design of Sichuan area pro-poor tourism information system, and the guarantee system of pro-poor tourism is established creatively.",2016,
9,10.1109/BigData50022.2020.9378024,PREDICTING ESCALATIONS IN CUSTOMER SUPPORT: ANALYSIS OF DATA MINING CHALLENGE RESULTS,"We summarize IEEE Big Data Cup: Predicting Escalations in Customer Support - a data mining competition organized jointly by companies Information Builders and QED Software at the KnowledgePit platform, in the frame of the 2020 IEEE International Conference on Big Data. We discuss the motivation for organizing this event and highlight the factors that make it such a challenging topic. We describe the data provided to participants and formulate the competition task. We also provide an overview of competition results with a detailed analysis of a few selected solutions. Finally, we present a novel functionality of the KnowledgePit platform - an analytic module that allows organizers to investigate selected solutions using a convenient GUI and provides in-depth insights about their quality.",2020,
10,10.1109/BigData.2017.8258380,UNDERSTANDING DATA QUALITY: ENSURING DATA QUALITY BY DESIGN IN THE RAIL INDUSTRY,"The railways worldwide are increasingly looking to the integration of their data resources coupled with advanced analytics to enhance traffic management, to provide new insights on the health of infrastructure assets, to provide soft linkages to other transport modes, and ultimately to enable them to better serve their customers. As in many industrial sectors, over the past decade the rail industry has been investing heavily in sensing technologies that record every aspect of the operation of the railway network. However, as any data scientist knows, it does not matter how good an algorithm is, if you put rubbish in, you get rubbish out; and as the traditional industry model of working with data only within the system that it was collected by becomes increasingly fragile, the industry is discovering that it knows less than it thought about the data it is gathering. When coupled with legacy data resources of unknown accuracy, such as design diagrams for assets that in many cases are decades old, the rail industry now faces a crisis in which its data may become essentially worthless due to a poor understanding of the quality of its data. This paper reports the findings of the first phase of a three-phase systematic review of literature about how data quality can be managed and evaluated in the rail domain. It begins by discussing why data quality matters in a rail context, before going on to define the quality, introduce and expand the concept of a data quality schema.",2017,
11,10.1109/TPDS.2015.2457924,A CROWDSOURCING WORKER QUALITY EVALUATION ALGORITHM ON MAPREDUCE FOR BIG DATA APPLICATIONS,"Crowdsourcing is a new emerging distributed computing and business model on the backdrop of Internet blossoming. With the development of crowdsourcing systems, the data size of crowdsourcers, contractors and tasks grows rapidly. The worker quality evaluation based on big data analysis technology has become a critical challenge. This paper first proposes a general worker quality evaluation algorithm that is applied to any critical tasks such as tagging, matching, filtering, categorization and many other emerging applications, without wasting resources. Second, we realize the evaluation algorithm in the Hadoop platform using the MapReduce parallel programming model. Finally, to effectively verify the accuracy and the effectiveness of the algorithm in a wide variety of big data scenarios, we conduct a series of experiments. The experimental results demonstrate that the proposed algorithm is accurate and effective. It has high computing performance and horizontal scalability. And it is suitable for large-scale worker quality evaluations in a big data environment.",2016,21619883
12,10.1109/BigDataService.2017.44,BIG DATA VALIDATION CASE STUDY,"With the advent of big data, data is being generated, collected, transformed, processed and analyzed at an unprecedented scale. Since data is created at a fast velocity and with a large variety, the quality of big data is far from perfect. Recent studies have shown that poor quality can bring serious erroneous data costs on the result of big data analysis. Data validation is an important process to recognize and improve data quality. In this paper, a case study that is relevant to big data quality is designed to study original big data quality, data quality dimension, data validation process and tools.",2017,
13,10.1109/ICCOINS49721.2021.9497187,TOWARDS IMPROVED DATA ANALYTICS THROUGH USABILITY ENHANCEMENT OF UNSTRUCTURED BIG DATA,"A high volume of unstructured data is being generated from diverse and heterogeneous sources. The unstructured data analytics process is used to extract valuable insights from these unstructured data sources but unlocking useful and usable information is critical for analytics. Despite advancements in technologies, data preparation requires an inordinate amount of time in unstructured data manipulation into a usable form. Although several data manipulation and preparation techniques have been proposed for unstructured big data, relatively limited research has addressed the usability issues of unstructured data. This study identifies the usability issues of unstructured big data for the analytical process to bridge the identified gap. The usability enhancement model has been proposed for unstructured big data to facilitate the subjective and objective efficacy of unstructured big data for data preparation and manipulation activities. Moreover, concept mapping is an essential element to improve the usability of unstructured big data incorporated in the proposed model with usability rules. These rules reduce the usability gap between data availability and its usefulness for an intended purpose. The proposed research model will help to improve the efficiency of unstructured big data analytics.",2021,
14,10.1109/BigData47090.2019.9006446,AN INTERACTIVE DATA QUALITY TEST APPROACH FOR CONSTRAINT DISCOVERY AND FAULT DETECTION,"Data quality tests validate heterogeneous data to detect violations of syntactic and semantic constraints. The specification of these constraints can be incomplete because domain experts typically specify them in an ad hoc manner. Existing automated test approaches can generate false alarms and do not explain the constraint violations while reporting faulty data records. In previous work, we proposed ADQuaTe, which is an automated data quality test approach that uses an unsupervised deep learning techni que (1) to discover constraints from big datasets that may have been missed by experts, and (2) to label as suspicious those records that violate the constraints. These records are grouped and explanations for constraint violations are presented to domain experts who determine whether or not the groups are actually faulty. This paper presents ADQuaTe2, which extends ADQuaTe to use an interactive learning technique that incorporates expert feedback to retrain the learning model and improve the accuracy of constraint discovery and fault detection. We evaluate the effectiveness of the approach on real-world datasets from a health data warehouse and a plant diagnosis database. We also use datasets with known faults from the UCI repository to evaluate the improvement in the accuracy of the approach after incorporating ground truth knowledge.",2019,
15,10.1109/ICSESS.2017.8343025,ANALYSIS OF CIVIL AIRCRAFT QUALITY DATA UNDER THE SUPPORT OF BIG DATA,"In the production assembly manufacturing process, a large amount of quality data has been generated by civil aircraft equipment system. With the passage of time and the accumulation of data, these massive data cannot be dealt with effectively using traditional statistical analysis of discrete manufacturing industry. To solve this problem, the method of quality data analysis for unsupervised learning presented in this paper was developed, after evaluating the generating characteristics of the civil aircraft quality data and the problems associated with the processing of the traditional quality data analysis. On this basis, in this paper, according to the disorder association, complex structure and large amount of data of the civil aircraft quality data, the data mining association analysis Apriori algorithm and the big data Splunk platform are introduced to effectively reduce the complexity of the quality data analysis through the complementary advantages of both, and put the data in an orderly, coherent state. The results show that the developed method is effective with high efficiency value.",2017,23270594
16,10.1109/ICISAT54145.2021.9678209,"A SURVEY ON DATA QUALITY: PRINCIPLES, TAXONOMIES AND COMPARISON OF APPROACHES","Nowadays, data generation keeps increasing exponentially due to the emergence of the Internet of Things (IoT) and Big data technologies. The manipulation of such Big amount of data becomes more and more difficult because of its size and its variety. For better governance of organizations (decision making, data analysis, earnings increase …), data quality and data governance at present of Big data are two major pillars for the design of any system handling data within the organization. This explains the number of researches conducted as it constitutes a research subject with several gaps and opportunities. Many works were conducted to define and standardize Data Quality (DQ) and its dimensions, others were directed to design and propose data quality assessment and improvement models or frameworks. This work aims to recall the data quality principles starting by the needed background knowledge, then identify and compare the relevant taxonomies existing in the literature, next surveys and compares the available Data quality assessment and improvement approaches. After that, we propose a metamodel highlighting the main concepts of DQ assessment and we describe a generic process for DQ assessment and improvement. Finally, we evoke the main challenges in the field of DQ before and after the emergence of Big Data.",2021,
17,10.1109/IRI.2014.7051902,TOWARDS A TECHNOLOGY ROADMAP FOR BIG DATA APPLICATIONS IN THE HEALTHCARE DOMAIN,"Big Data technologies can be used to improve the quality and efficiency of healthcare delivery. The highest impact of Big Data applications is expected when data from various healthcare areas, such as clinical, administrative, financial, or outcome data, can be integrated. However, as of today, the seamless access to the various healthcare data pools is only possible in a very constrained and limited manner. For enabling the seamless access several technical requirements, such as data digitalization, semantic annotation, data sharing, data privacy and security as well as data quality need to be addressed. In this paper, we introduce a detailed analysis of these technical requirements and show how the results of our analysis lead towards a technical roadmap for Big Data in the healthcare domain.",2014,
18,10.1109/ICBDIE50010.2020.00014,RESEARCH ON THE CONSTRUCTION AND INNOVATION OF LIFELONG EDUCATION SYSTEM UNDER THE BACKGROUND OF BIG DATA,"The development of information technology provides technical support for the construction of lifelong education think tanks. It is an urgent need for lifelong education development to create a new type of life think tanks in combination with regional development. The advent of the era of big data has profoundly changed the social structure, social relations, social production and lifestyle, and people's way of thinking. The entire education ecosystem has been reshaped, information and knowledge are constantly changing, and continuous learning has become the basic needs of society members. In addition, China is in a period of social transformation, the quality of the population needs to be improved, the issue of equity in education needs to be solved, and the vitality of social innovation needs to be activated. The unique advantages of big data will help solve the above problems. Therefore, building a learning society for the whole people, lifelong and comprehensive learning is not only an effective way to promote China's sustainable development, but also an inevitable choice in the era of big data.",2020,
19,10.1109/BigData.2016.7841033,CONCISE ESSENCE-PRESERVING BIG DATA REPRESENTATION,"Controversially, more data is not necessary better than less data. The explosion of the data lead to a number of interesting practical and theoretical problems. Among those problems are the need to filter, process, verify, index, distribute, protect and make redundant copies of the data. This data “massaging” usually take a lot of time and processing power. However, the quantity of the collected data does not necessary mean quality, as a lot of data is repetitive or does not contain any new information. Nevertheless, it still has to be processed, filtered, consumes high communication volume, has to be protected from breaches and from storage failures. In this position paper we propose to perform data reduction techniques on the collected (big) data prior to gathering of the data in a single location. In many cases (exemplified by two use-cases), especially in Internet-of-Things (IoT), those techniques might save tremendous amounts of power, processing time and network traffic.",2016,
20,10.1109/NYSDS.2016.7747827,MODEL-DRIVEN VISUAL ANALYTICS FOR BIG DATA,"The growth of digital data is tremendous. Any aspect of life and matter is being recorded and stored on cheap disks, either in the cloud, in businesses, or in research labs. We can now afford to explore very complex relationships with many variables playing a part. But for this we need powerful tools that allow us to be creative, to sculpt this intricate insight formulated as models from the raw block of data. High-quality visual feedback plays a decisive role here. The subject of this poster is a framework we have developed over the years to make the exploration of large multivariate data more intuitive and direct. The components of this framework were conceived in tight collaborations with domain experts in the fields of climate science, health informatics, computer systems, and others.",2016,
21,10.1109/COMPSAC.2019.00066,BIG DATA ANALYTICS IN TELECOMMUNICATION USING STATE-OF-THE-ART BIG DATA FRAMEWORK IN A DISTRIBUTED COMPUTING ENVIRONMENT: A CASE STUDY,"Predictive Analytics is of great interest when it comes to enhancing Business Intelligence. Businesses have already started to use Big Data Analytics, particularly predictive and prescriptive analytics, to strengthen and increase their business yields. Not only has analytics resulted in business growth, but has also provided a significant competitive edge over others. The voluminous data generated from various resources is highly unstructured in nature and adding a structure to it would leverage the actual potential of the data. New techniques and frameworks should serve as human aids in automatically and intelligently analyzing large datasets in order to acquire useful information. In this paper, we attempt to perform Big Data Analytics on data from one of the most important and growing sources, namely, Telecommunication. To keep pace with the growing telecommunication market and ever increasing demands of the consumers for quality service, the telecom service providers are required to observe and estimate various trends in customer's usage to plan future upgrades and deployments driven by real data. We have attempted to use several data mining techniques to find hidden and interesting patterns from the telecom data generated by Telecoms Italia cellular network for the city of Milano, Italy. K-means clustering is used to categorize the usage statistics while several machine learning algorithms like Decision Tree, Random Forest, Logistic Regression and SVM are used for predicting the usage of telecom services. In the end, a performance comparison matrix is generated to rate the performance of these algorithms for the given dataset. All these experiments are performed on the big data environment set up at the supercomputing infrastructure of C-DAC. Given such a matrix, the result can be applied to similar dataset pertaining to other domains as well.",2019,07303157
22,10.1109/ICISCAE52414.2021.9590700,RESEARCH ON THE CONSTRUCTION OF EDUCATIONAL DATA QUALITY MODEL BASED ON MULTIPLE CONSTRAINTS MODEL,"With the development of Internet and information technology, data has become an important asset related to the development prospects of society and all walks of life. At present, there are many quality problems in the use of educational data, which has brought great obstacles to exerting the value of educational data. Only by using scientific statistical methods, obtaining real, objective, comprehensive, scientific and effective basic data, and carrying out systematic and comprehensive analysis on the obtained data, can we give full play to its command and decision-making role. The development of big data and artificial intelligence technology provides new ideas for the analysis and evaluation of educational data quality, and is committed to restoring the overall picture of the education system and promoting the change of regional educational ecology. In this paper, an educational data quality analysis model based on multiple constraint model is proposed, which classifies the data in the database, divides the information in the database into several different categories according to the data characteristics, and establishes a quality management system for educational data in universities, so as to effectively improve the quality of educational data.",2021,
23,10.1109/ICITBS53129.2021.00109,INTELLIGENT EVALUATION OF COLLEGE STUDENTS’ IDEOLOGICAL AND POLITICAL EDUCATION EFFECT BASED ON BIG DATA TECHNOLOGY,"In the current ideological and political teaching evaluation, due to many comprehensive evaluation standards, complex process, resource consumption and other reasons, the evaluation is insufficient or mere formality. Thus, this paper proposes a classroom evaluation assistant strategy based on big data technology. The scheme mainly uses a new multi class classification algorithm which takes the similarity direction between classes as the IBTSVM generation algorithm, reads the data from the web system, and trains it by SVM train. Through the method of cross validation, the parameter setting of SVM is obtained. Finally, the algorithm is applied to the evaluation of teachers’ teaching quality, and the samples are trained and verified by case analysis, which proves that the application of intelligent teaching evaluation method based on big data in teaching evaluation classification is feasible and effective.",2021,
24,10.1109/BigData.2014.7004459,PROBABILISTIC ESTIMATES OF ATTRIBUTE STATISTICS AND MATCH LIKELIHOOD FOR PEOPLE ENTITY RESOLUTION,"For big data practitioners, data integration/entity resolution/record linkage is one of the key challenges we face from day to day. Entity resolution/record linkage with high precision and recall on a large graph with billions of nodes, and hundreds of times more edges poses significant scalability challenges. Similarity based graph partition is still the most scalable method available. This paper presents a probabilistic method to approximate the match likelihood of a pair of records by incorporating values of different attributes and their aggregates/statistics. The quality of the approximates depend on the accuracy of the estimates of the aggregated values. The paper adapts the GTM model described in [1] to obtain the estimates. We present experimental results based on real world commercial data sources to show that the estimates obtained via GTM model is better than the baseline. Our experimental results also showed that the approximate match likelihood can improve the recall of the similarity function.",2014,
25,10.1109/BigData.2018.8622589,BIG DATA STORAGE TECHNOLOGIES: A CASE STUDY FOR WEB-BASED LIDAR VISUALIZATION,"Big data technologies have been growing up quickly during past years. New storage and computing solutions appear while those already established in the market are improved with new features and better performance. Along with this growth also rises the number of applications and fields where the inclusion of big data technologies provides a large number of benefits, from the reduction in computational costs and economic resources to the improvement in the quality of the services provided which has a direct impact on the customers satisfaction. LiDAR (Light Detection and Ranging) data processing is one of the topics that could benefit from the adoption of these kind of technologies due to the massive datasets that are being gathered nowadays, with applications in archaeology, geography, geology or forestry, among many others. An efficient management of this volume of data becomes a key point especially in visualization, computing and analytic processes. In this paper, we analyse how web applications for the visualization of LiDAR data can benefit from the adoption of big data storage technologies, as well as the advantages and disadvantages that may determine the choice of one of them.",2018,
26,10.1109/CSCI51800.2020.00070,A LOW COST LORA-BASED IOT BIG DATA CAPTURE AND ANALYSIS SYSTEM FOR INDOOR AIR QUALITY MONITORING,"This paper presents a low cost LoRa-based IoT big data capture and analysis system for indoor air quality monitoring. This system is presented as an alternative solution to expensive and bulky indoor air quality monitors. It enables multiple low cost nodes to be distributed within a building such that extensive location-based indoor air quality data is generated. This data is captured by a gateway and forwarded to a cloud-based LoRaWAN network which in turn publishes the received data via MQTT. A cloud-based data forwarding server is used to capture, format and store this big data on a cloud-based document-oriented database. Cloud-based services are used for data visualization and analysis. Periodic indoor air quality graphs along with air quality index and thermal comfort index heat maps are generated.",2020,
27,10.1109/BigData.2018.8622335,REPUTATION-AWARE DATA FUSION AND MALICIOUS PARTICIPANT DETECTION IN MOBILE CROWDSENSING,"Mobile crowdsensing, an emerging sensing paradigm, promotes scalability and reduction in the deployment of specialized sensing devices for large-scale data collection in a decentralized fashion. However, its open structure allows malicious entities to interrupt a system by reporting fabricated or erroneous data, making trust evaluation a highly important issue in mobile crowdsensing applications. The goal of this research is to show that an introduction of a reputation system in the process of correlated sensor-based data fusion will enhance the overall quality of the sensed data. To do so, we design a reputation-aware data fusion mechanism to ensure data integrity. We use Gompertz function in our reputation method to rate the trustworthiness of the data reported by a crowdsensing participant. The proposed mechanism, on one hand, is capable of defending a data corruption attack and identifying malicious or honest participants based on their reported data in real time. On the other hand, this mechanism yields more accurate data prediction in terms of lower data prediction error. We conducted experiments using two different real-world datasets. We compare our correlated data and reputation-aware data prediction (CDR) method with other popular methods, and the results show that our effective method incurs lower data prediction error.",2018,
28,10.1109/ITQMIS53292.2021.9642918,BIG DATA AS A MANAGEMENT DECISION-MAKING TOOL IN DIGITAL BUSINESS ENVIRONMENTS,"The article examines the problem of using big data in the process of making managerial decisions. Approaches to the definition of the concept and main characteristics of big data are considered, the sources of big data, methods of big data analysis are revealed, the comparison of traditional management information analytics and big data analytics is given, the possibilities of using big data in the planning of functional strategies of the organization are shown, the prospects for the development of the big data market in the conditions of business digitalization are evaluated.",2021,
29,10.1109/ICBAIE49996.2020.00014,IDENTIFICATION OF THE RESIDENTIAL AREAS FOR URBAN RENEWAL BASED ON BIG DATA : TAKE GUANGZHOU AS AN EXAMPLE,"Urban renewal is an important direction for urban development in China. Affected by urban diseases and lack of land resources, megacities urgently need to carry out urban renewal. The residential areas in Guangzhou also face problems such as low efficiency, poor quality, and insufficient vitality. This paper uses big data to identify the urban residential areas that is in urgent need of transformation and provides a reference for the planning and decision-making of Guangzhou urban renewal.",2020,
30,10.1109/CBD.2019.00013,PCP-2LSTM: TWO STACKED LSTM-BASED PREDICTION MODEL FOR POWER CONSUMPTION IN DATA CENTERS,"As the size of data centers and cloud computing continue to expand, power consumption in data centers is rapidly increasing. It has a great significance to predict and analyze power consumption in the data center because power consumption prediction can help data center operators perform workflow scheduling, manage energy efficiency, provide high quality-of-service (QoS), and meet the requirements of green energy use. The current methods are mainly divided into two scopes: the one is establishing a static relationship between power consumption and relevant components/applications, and the other one is treating power consumption as sequential temporal data. However, the first scope does not consider the dynamic fluctuation of power, and the other one ignores the characteristics of the power consumption data. To solve these issues, in this paper, we present a power consumption prediction framework called PCP-2LSTM based on the mean smoothing and long short-term memory (LSTM) network. We first build a power consumption system to collect data and analyze the stationary of the power series. Then we use the mean smoothing to remove the noise from the time series of power consumption. After data preprocessing, because the time for workflow and container scheduling is usually 30 seconds, we use a stacked LSTM model to predict 30s power consumption in the future. The experimental result indicates that our approach outperforms other baselines.",2019,
31,10.1109/CCBD.2016.073,BIG DATA ANALYSIS ON RADIOGRAPHIC IMAGE QUALITY,"Mass data generated from in-service radiographic product contain assignable information on Image Quality (IQ). Analyzing data from routine work might supplement the time-consuming Image Quality Assurance Test Procedure (IQATP) to evaluate IQ and to know product type performance on site, which can also locate risks and give manufacturer directions for the further actions as well. This article illustrates methodologies of extracting IQ information from mass data and visual quality track, analysis, control, and risk mitigation in Big Data environments.",2016,
32,10.1109/CIEEC50170.2021.9510379,A NOVEL IDENTIFICATION AND LOCATION METHOD FOR TRANSIENT POWER QUALITY DISTURBANCE SOURCES,"The application of power electronics and high penetration of new energy generation have brought great economic and social benefits. Meanwhile, new power quality phenomena and new issues have come into existence, such as three-phase unbalance, harmonic and low-frequency resonance, etc. In order to solve power quality problems fundamentally. The paper proposes a novel identifying and locating method utilizing transient power quality data. Based on the operational big data of power system, spatio-temporal data model of power quality is established. Then, based on big data, by adopting random forest algorithm, the data is analyzed to identify and locate transient power quality problem disturbance sources. This paper simulates in IEEE 30 bus system, and the results indicate the accuracy in identifying and locating short circuit disturbance sources.",2021,
33,10.1109/CONFLUENCE.2016.7508117,A ROBUST MODEL FOR BIG HEALTHCARE DATA ANALYTICS,"The big data technologies are offering varied challenges for research and scientists in healthcare application domain to improve the quality of life in patients. Yet in this phase of technology explosion, retrieving effective and efficient information from big data is a value care for future medical diagnostic. However. unprecedented growth in data analytical technology has proven fruitful for discovery of hidden patterns from such databases. The purpose of this study is to discuss current developments in big data analytics with healthcare application domain. However, paper explains the emerging role of predictive data analytics with focused study on patient's quality care with several states of examples. Further, comprehensive suggestive novel framework is discussed with the approach to offer significant benefits to computing technology for effective patient care diagnosis.",2016,
34,10.1109/ICCCBDA49378.2020.9095695,FINDING NEXT HIGH-QUALITY PASSENGER BASED ON SPATIO-TEMPORAL BIG DATA,"Finding high-quality passenger can provide timely recommendations for taxi drivers, thus decreasing the waiting time of passengers and increasing the cab driver's efficiency. This paper proposes a high-quality passenger recommendation model which combines the value evaluation formula of non-occupied status and clustering. Firstly, the GPS big data of taxi is preprocessed to get the trajectory data by Map Reduce, which is simple, clean and labeled with status. Then, every trajectory pick-up point is extracted, and the profits of pick-up point set is calculated by using the formula considering non-occupied status. At the same time, the density based clustering method DBSCAN clustering is used for different value of pickup points. Finally, the top high probability passenger area with high value is extracted as the recommended result. In this paper, the real 10357 taxis equipping GPS in Beijing collected the big trajectory data as the experimental dataset, using our method to calculate and recommend, the results show that it can accurately predict the high-quality passenger area, further to significantly improve the income of taxi drivers and reduce the waiting time of passengers.",2020,
35,10.1109/ICCCBDA.2017.7951899,INTEGRATING PRIVACY IN ARCHITECTURE DESIGN OF STUDENT INFORMATION SYSTEM FOR BIG DATA ANALYTICS,"Educational Data Mining (EDM) is an area of growing interest in academia with significant challenges and tremendous opportunities. Most EDM initiatives are based on finding patterns to aid and enhance student learning and performance, while others focus on program efficiency, service improvement, and college readiness. This paper is related to a case study being conducted at Sta. Teresa School, a high school in the Northern District of a West African country looking to improve service quality. By combining its Relational Database School Management System data sets with its anticipated online community forum, and aggregating it with Social media data, the school is expected to gain new actionable insights to enhance student services. In this paper, we present a model for incorporating privacy into big data analytics architecture integration with Social media, discuss some of the school's concern related to privacy and security, and offer some delivery options for its online community forum initiative.",2017,
36,10.1109/BigData50022.2020.9378153,ON THE LARGE-SCALE GRAPH DATA PROCESSING FOR USER INTERFACE TESTING IN BIG DATA SCIENCE PROJECTS,"In functional User Interface testing, test scenarios are written with respect to the requirements that are specified by test analysts. Usually, a test analyst focuses on base URLs and HTML components while collecting requirements of User Interface test scenarios. A base URL is essentially a unit segment of large scale graph data. It has mostly dynamic shape and is used to navigate pages amongst application's pages. We argue that even though dynamic URLs have additional important information about the content of the page, they are not being utilized in generating User Interface test scenarios. In this study, we address this lack of capability and focus on the development of a methodology that can support the usage of large-scale dynamic URL datasets in UI test script generation. Our proposed methodology is designed as an add-on tool that can be used on the top of the existing UI test automation tools to improve testing quality. We introduce a higher quality testing methodology to make the results more accurate, and we discuss the proposed methodology and give an overview of the implementation details followed by the evaluation results. We perform various performance evaluations to investigate how well the proposed algorithms scale under increasing data sizes. The results are promising and show the usability of the proposed methodology.",2020,
37,10.1109/ICBDSS51270.2020.00018,RESEARCH AND APPLICATION ON THE GOVERNANCE OF PASSENGER CAR PRODUCT DATA RESOURCES,"With the development of digital economy and big data technology, data resources owned by enterprises have become one of the important production factors in the era of digital economy. Via data empowerment, enterprises analyze the intrinsic value of data, and then realize their own business transformation and innovation and development. After more than ten years of rapid development, China is now the world's largest automobile production country. Behind the huge automobile market, there are a lot of data resources related to the market and products. However, due to the problems of scattered data sources, inconsistent statistical caliber and untimely updating, it brings great inconvenience to the researchers engaged in automobile industry. This paper proposes a set of passenger car product data governance framework, which aims to provide high-quality passenger car product database for enterprises in the process of studying the market and technology of passenger car products, mining the value of data assets, and assisting managers in decision-making.",2020,
38,10.1109/CISCE50729.2020.00051,RESEARCH ON HIGHER VOCATIONAL TEACHING QUALITY IMPROVEMENT BASED ON EDUCATIONAL BIG DATA,"The vigorous development of educational big data already has great potential to promote educational reform, which provides conditions and opportunities for the improvement of teaching quality. Education big data has the characteristics of real-time, multi-dimensionality, authenticity and so on. This paper analyzes the current problems in the teaching quality of higher vocational education, and with the continuous improvement of data mining and learning analysis technology, puts forward the path of educational big data to promote the quality of teaching.",2020,
39,10.1109/BigData.2016.7840906,THE SMART APPROACH TO COMPREHENSIVE QUALITY ASSESSMENT OF SITE-BASED SPATIAL-TEMPORAL DATA,"There is a need for comprehensive solutions to address the challenges of spatio-temporal data quality assessment. Emphasis is often placed on the quality assessment of individual observations from sensors but not on the sensors themselves nor upon site metadata such as location and timestamps. The focus of this paper is on the development and evaluation of a representative and comprehensive, interpolation-based methodology for assessment of spatio-temporal data quality. We call our method the SMART method, short for Simple Mappings for the Approximation and Regression of Time series. When applied to a real-world, meteorological data set, we show that our method outperforms standard interpolators and we identify numerous problematic sites that otherwise would not have been flagged as bad. We further identify sites for which metadata is incorrect. We believe that there are many problems with real data sets like these and, in the absence of an approach like ours, these problems have largely gone unidentified. Our results bring into question the validity of provider-based quality control indicators. In addition to providing a comprehensive solution, our approach is novel for the simple but effective way that it accounts for spatial and temporal variation.",2016,
40,10.1109/FSKD.2017.8393094,DESIGN AND IMPLEMENTATION OF THE WATER ENVIRONMENTAL QUALITY MONITORING SYSTEM BASED ON BIG DATA,"To realize the comprehensive scientific decision-making of ecological environment, a hybrid storage structure based on a combination of big data technology and traditional database is designed. First, HBase has high expansibility and reliability to the data storage capacity in big environmental monitoring data. Second, relational database MySql provides powerful SQL query language and data analysis capabilities, which can provide a real time statistical analysis and multi dimension display on recent environmental quality monitoring data. Given method can effectively improve the data storage size and query speed, compared with the traditional method.",2017,
41,10.1109/ICCSE.2017.8085517,WINE QUALITY IDENTIFICATION BASED ON DATA MINING RESEARCH,"For the quality of the wine big data identification technology, the introduction of data mining classification algorithm, effectively according to the content of several impact compounds in wine level identification;Are introduced including the Logistic regression and BP neural network and SVM classification algorithm, in view of the three algorithms identify the modeling analysis of wine quality. Data mining is closely related to big data, applying data mining to the wine in the quality detection of big data, can quickly to the quality of the wine.",2017,24739464
42,,SSMDM: AN APPROACH OF BIG DATA FOR SEMANTICALLY MASTER DATA MANAGEMENT,"Master data is critical for any business organization. Big organizations like Oracle, Infosys, IBM, Google, Facebook and TCS started working on Master Data Management (MDM) in early 20's. Multinational corporations spend millions of dollars for Managing their Master Data, so as to ensure quality of service and customer retention as well. Unlike big organizations, Small and Midsized Enterprises (SME's), because of their limited resources, are unable to exploit the economies of scale associated with master data management. In this paper a Synthetic Semantic Master Data Modeler (SSMDM) has been proposed, this modeler primarily uses the concept of Google's knowledge graph to identify semantics within data sets. Using SSMDM, synthetic yet realistic master data was generated to find out probable ontologies within synthetic data sets. Based on these ontologies, some rules were framed to produce synthetic facts. These synthetic facts were further used to decide services and cuisines to be offered at a newly opened eating joint. Since inception, we keep on collecting actual customer data. Once sufficient data was available we statistically analyzed the facts originated from actual data with those created synthetically. The results were promising and justify the use of SSMDM for the purpose of policy making in SMEs.",2015,
43,10.1109/MIS.2014.82,EMBEDDING AI AND CROWDSOURCING IN THE BIG DATA LAKE,"Daniel E. O'Leary examines the notion of the Big Data Lake and contrasts it with decision support-based data warehouses. In addition, some of the risks of the emerging Lake concept that ultimately require data governance are analyzed. O'Leary investigates using different AI and crowdsourcing (human intelligence) applications in that lake in order to integrate disparate data sources, facilitate master data management and analyze data quality. Although data governance often is not seen as a technology issue, it is seen as a critical component of making the Big Data Lake ""work"".",2014,19411294
44,10.1109/TNSE.2018.2843326,REDUNDANCY AVOIDANCE FOR BIG DATA IN DATA CENTERS: A CONVENTIONAL NEURAL NETWORK APPROACH,"As the innovative data collection technologies are applying to every aspect of our society, the data volume is skyrocketing. Such phenomenon poses tremendous challenges to data centers with respect to enabling storage. In this paper, a hybrid-stream big data analytics model is proposed to perform multimedia big data analysis. This model contains four procedures, i.e., data pre-processing, data classification, data recognition and data load reduction. Specifically, an innovative multi-dimensional Convolution Neural Network (CNN) is proposed to assess the importance of each video frame. Thus, those unimportant frames can be dropped by a reliable decision-making algorithm. In order to ensure video quality, minimal correlation and minimal redundancy (MCMR) are combined to optimize the decision-making algorithm. Simulation results show that the amount of processed video is significantly reduced, and the quality of video is preserved due to the addition of MCMR. The simulation also proves that the proposed model performs steadily and is robust enough to scale up to accommodate the big data crush in data centers.",2020,2334329X
45,10.1109/ICIT52682.2021.9491629,RECENT QUALITY MODELS IN BIGDATA APPLICATIONS,"In this time the big data became an important part of all areas, it can be used in multiple industrials such as banking, education, government, networking, energy, health care, etc. So, because of that the huge amount of data became have problems or unnecessary data, and so that comes from the difficulty of measure the quality of these data. In this research we show the quality characteristic that can be help to increase the efficiency of quality measurement process of BDA by comparing it with other quality model of BDA and applying it on the 7V's of big data.",2021,
46,10.1109/ACCESS.2021.3056486,RESEARCH ON THE IMPACT OF BIG DATA CAPABILITIES ON GOVERNMENT’S SMART SERVICE PERFORMANCE: EMPIRICAL EVIDENCE FROM CHINA,"The government of China seeks to improve e-government service quality and build a service-oriented government that citizens find satisfactory. To this end, big data is being used as a new tool of government service innovation. However, there is a lack of research on how big data affects the performance of government smart services. This article explores the influence mechanisms of government big data capabilities on the performance of smart service provision, utilizing the carding analysis of relevant literature, published both in China and abroad. To this end, a structural equation model was constructed. Using data from 289 valid questionnaires in Jiangsu, Shandong, Zhejiang, and other provinces and cities in China, the study tests internal mechanisms of big data capabilities and its effect on smart service performance. Following a new definition of government big data capability, the paper divides the capability into three dimensions: big data system capability, big data human capability and big data management capability. The main conclusions are as follows: (1) Big data management capability has a significant positive impact on big data human capability and big data system capability. (2) Big data system capability has a significant positive impact on big data human capability. (3) Big data system capability and big data management capability have a significant positive effect on smart service performance. (4) The impact of big data human capability on smart service performance is not however significant enough to bring about the improvements which the government seeks.",2021,21693536
47,10.1109/ISPAN-FCST-ISCC.2017.29,AN ELECTRIC POWER SENSOR DATA ORIENTED DATA CLEANING SOLUTION,"With the development of Smart Grid Technology, more and more electric power sensor data are utilized in various electric power systems. To guarantee the effectiveness of such systems, it is necessary to ensure the quality of electric power sensor data, especially when the scale of electric power sensor data is large. In the field of large-scale electric power sensor data cleaning, the computational efficiency and accuracy of data cleaning are two vital requirements. In order to satisfy these requirements, this paper presents an electric power sensor data oriented data cleaning solution, which is composed of a data cleaning framework and a data cleaning method. Based on Hadoop, the given framework is able to support large-scale electric power sensor data acquisition, storage and processing. Meanwhile, the proposed method which achieves outlier detection and reparation is implemented on the basis of a time-relevant k-means clustering algorithm in Spark. The feasibility and effectiveness of the proposed method is evaluated on a data set which originates from charging piles. Experimental results show that the proposed data cleaning method is able to improve the data quality of electric power sensor data by finding and repairing most outliers. For large-scale electric power sensor data, the proposed data cleaning method has high parallel performance and strong scalability.",2017,2375527X
48,10.1145/2896825.2896838,UNDERSTANDING QUALITY REQUIREMENTS IN THE CONTEXT OF BIG DATA SYSTEMS,"While the domain of big data is anticipated to affect many aspects of human endeavour, there are numerous challenges in building big data applications among which is how to address big data characteristics in quality requirements. In this paper, we propose a novel, unified, approach for specifying big data characteristics (e.g., velocity of data arrival) in quality requirements (i.e., those requirements specifying attributes such as performance, reliability, availability, security, etc.). Several examples are given to illustrate the integrated specifications. As this is early work, further experimentation is needed in different big data situations and quality requirements and, beyond that, in a variety of project settings.",2016,
49,10.1109/BigData47090.2019.9006190,"KRATOS: A SECURE, AUTHENTICATED AND PUBLICLY VERIFIABLE SYSTEM FOR EDUCATIONAL DATA USING THE BLOCKCHAIN","Growing interest in educational data mining (EDM) and learning analytics (LA) to leverage big data and to benefit education and the science of learning has made data ownership an important focus point for institutions and students. While EDM and LA can provide important information that help enhance the quality of teaching and learning, it has become critical to ensure data privacy and student agency over data. In this paper, we introduce Kratos: an immutable and publicly verifiable data management system that enables EDM and LA, while maintaining data privacy and empowering students with a user interface for data governance and participation in school processes. The system aims to achieve data interoperability, which facilitates EDM and LA as incentives to educational stakeholders (policy makers, educators, developers of education technologies, etc.), while prioritizing student agency over their data. Our system gives students and schools an immutable log along with comprehensive access to data that is otherwise scattered across systems and vendors. The underlying set of rules of the system are defined in a set of smart contracts, codified from existing non-virtual agreements [1] between schools and education technology (edutech) vendors. We propose the smart contracts to be deployed on a public blockchain (like Ethereum or Bitcoin), for notarizing and time-stamping various interactions which users of Kratos may have with data. Third parties requesting access to school data have a unique virtual token assigned to them on the blockchain which helps keep track of data modifications, access and use.",2019,
50,10.23919/ICACT.2019.8701948,CONSIDERATION ON THE VARIATION OF FINANCIAL DATA OF INSTITUTIONS FOR CANONICAL CORRELATION ANALYSIS,"In these days, progress of e-government and spread of electrical data lead to the prevail of public open databases, and also lead to the large amount of data analysis applications applying to these official open data. With regard to data analysis method, Canonical Correlation Analysis, which is one of the basic data analysis method and also data visualization method, is becoming the requisite skill for data scientists in this Big Data era. This paper examines the open data of financial data of education institutions. Especially, we focus on the higher education institutions and their financial data. In addition, we examine the variation of data and its problem to the data analysis. We aim to apply this analysis method and the result of consideration for supporting the improvement of quality assurance of higher education institutions.",2019,17389445
51,10.1109/HONET.2017.8102206,BENEFITS OF SDN FOR BIG DATA APPLICATIONS,"Big data applications depend on underlying networks that make the transfer of information possible. These networks may be real (conventional) or virtual (in case of services hosted in data centers). Either way, the responsibility of smooth execution of the application, despite increasing traffic volume, lies with the service provider. The service providers face many challenges with respect to providing a high quality of service. It is therefore in the best interest of the service providers that efficiency of the applications is increased. SDN has the potential to improve big data application performance. In this paper we have a look at the recent advancements in technology that helps improve big data applications using SDN and discuss our observations.",2017,19494106
52,10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.129,BIG DATA TRANSFORMATION TESTING BASED ON DATA REVERSE ENGINEERING,"During the transformation of huge volume of data, there might exist data mismatch, miscalculation and/or loss of useful data that leads to an unsuccessful data transformation. To check out the occurrence of such possible errors, testing is a crucial requirement. The existing quality testing methods are either unreliable, return biased results, fail to provide answers for data differences or have several limitations which does not treat each and every part of the data into the process. We propose an approach of big data transformation testing based on the concept of data reverse engineering. It is a comprehensive approach that reverse the whole transformation process and does a comparison testing on each and every entry of the data if the original source data can be constructed back from the target data, once successful ETL process is done.",2015,
53,10.1109/IAEAC47372.2019.8997699,MULTI-DIMENSIONAL INDEX CONSTRUCTION OF ELECTRIC POWER MULTI-SOURCE MEASUREMENT DATA CONSIDERING SPATIO-TEMPORAL CORRELATION,"The operation of complex AC/DC power grid changes rapidly and dynamically, which objectively puts forward higher requirements for on-line analysis, and it is urgent to improve the basic data quality of power grid. Because of low quality and poor synchronization of the basic data of power grid, it is impossible to accurately map the actual operation of the power grid. At the same time, the cross-system data matching degree is low and the data correlation is poor, so it can not support the multi-scale data analysis for all kinds of applications. In this paper, the associated method of multi-source heterogeneous data in the power grid is studied. Combined with big data's access characteristics, big data storage, big data retrieval and artificial intelligence technology, the high-speed data storage and index architecture of power big data are constructed, and a multi-dimensional index reflecting the associated relationship of operating data is established from the dimensions of time, space, application, device and so on. It is easier to analyze multi-source data, to improve the basic data quality of power grid, which provides effective support for accurate data analysis and evaluation of power grid.",2019,23810947
54,10.1109/BigDataService.2018.00014,SCHEDULING BIG DATA WORKFLOWS IN THE CLOUD UNDER DEADLINE CONSTRAINTS,"With the advent of cloud computing, an unbound number of compute resources can be leased from the cloud providers. In such an environment, the number of assigned resources to a workflow can be elastically scaled in and out on a demand basis using the added Quality of Service (QoS) constraints such as the budget and the deadline. The heterogeneous nature of the cloud resources makes the decision of selecting resource type for each workflow a challenging problem. Although there are several existing research studies that propose both static and dynamic scheduling algorithms for both homogeneous and heterogeneous cloud resource types, they do not take advantage of the data dependency information that is part of the workflow structure during the scheduling process. There is still room for improvement, since the scheduling problem is an NP-hard problem. In this paper we propose a new Big data wOrkflow scheduleR undeR deadlIne conStraint (BORRIS) that is used to minimize the execution cost of the workflow under a provided deadline constraint in a heterogeneous cloud computing environment. We have implemented the proposed algorithm in our big data workflow system called DATAVIEW and the experimental results show the competitive advantage of our approach.",2018,
55,10.1109/ICBDA.2017.8078801,RESEARCH ON PRODUCT QUALITY EVALUATION BASED ON BIG DATA ANALYSIS,"In order to evaluate product quality from nonnumerical data, we propose the product quality evaluation model based on big data analysis including data collecting, data preprocessing, quality feature extraction, vector quantization and quality classification. Quality feature word extension algorithm, reviews quantization algorithm and machine learning algorithm are applied. We finally obtain the qualified rate(88.94%) and 7 features that most concerned by consumers through the analysis of 184,967 effective product reviews of wooden toys. In the end, we compare the SVM machine learning algorithm with decision tree and naive bayes, and discuss the credibility of the results. Our research on product quality evaluation extends the application of big data analysis, and also presents a new method to evaluate product quality in the field of manufacture.",2017,
56,10.1109/KACSTIT.2016.7756075,THE MOTIVATIONS FOR BIG DATA MINING TECHNOLOGIES ADOPTION IN SAUDI BANKS,"Significant shifts in the business environment, economic instability, changes in the desires and expectations of customers and employees, led the banking sector to find new technology strategies that corresponded with these changes. The Banking sector realized the need for innovative Information Technology solutions, and this has led to the use of Big Data and Data Mining tools, where both are playing significant and effective roles in creating business value in banking products and services. This research seeks to investigate the motivational factors affecting the implementation of data mining techniques to harness big data in Saudi banks. According to the findings, the adoption and implementation of data mining to harness big data is affected by motivational factors including: system quality, information quality, service quality and perceived benefits. The paper highlights the importance of these issues and their role in the adoption and implementation of big data mining technology.",2016,
57,10.1109/CIPAE53742.2021.00058,DESIGN AND REALIZATION OF COLLEGE STUDENT MANAGEMENT SYSTEM BASED ON INFORMATION TECHNOLOGY UNDER BIG DATA TECHNOLOGY,"With the popularization of education informatization, colleges and universities generally have information management systems, which can manage school information, teacher information, student information, and performance information, and are equipped with special databases or data clusters to store this information. How to effectively use these data to extract and mine valuable information from these data, so as to provide schools and teachers with auxiliary decision-making, and truly improve the level and quality of school running has become an issue worthy of attention. The purpose of this paper is to design and implement the university management system based on big data technology. This paper firstly summarizes the basic theory of big data and derives the core technology of big data data mining and other core technologies, combined with the current university student management system in our country. The status quo, analysis of its problems and shortcomings, on this basis, combined with big data technology to design and analyze the college student management system. This article systematically expounds the design and use of the database, function modules and related technologies of the management system. And conduct research through research methods such as comparison and field investigation. Experimental research shows that compared with the traditional college student management system, the college student management system based on big data technology is more practical and more powerful.",2021,
58,10.1109/BigData.2016.7840715,H2O: A HYBRID AND HIERARCHICAL OUTLIER DETECTION METHOD FOR LARGE SCALE DATA PROTECTION,"Data protection is the process of backing up data in case of a data loss event. It is one of the most critical routine activities for every organization. Detecting abnormal backup jobs is important to prevent data protection failures and ensure the service quality. Given the large scale backup endpoints and the variety of backup jobs, from a backup-as-a-service provider viewpoint, we need a scalable and flexible outlier detection method that can model a huge number of objects and well capture their diverse patterns. In this paper, we introduce H2O, a novel hybrid and hierarchical method to detect outliers from millions of backup jobs for large scale data protection. Our method automatically selects an ensemble of outlier detection models for each multivariate time series composed by the backup metrics collected for each backup endpoint by learning their exhibited characteristics. Interactions among multiple variables are considered to better detect true outliers and reduce false positives. In particular, a new seasonal-trend decomposition based outlier detection method is developed, considering the interactions among variables in the form of common trends, which is robust to the presence of outliers in the training data. The model selection process is hierarchical, following a global to local fashion. The final outlier is determined through an ensemble learning by multiple models. Built on top of Apache Spark, H2O has been deployed to detect outliers in a large and complex data protection environment with more than 600,000 backup endpoints and 3 million daily backup jobs. To the best of our knowledge, this is the first work that selects and constructs large scale outlier detection models for multivariate time series on Big Data platforms.",2016,
59,10.1109/BDEE52938.2021.00032,RESEARCH ON CURRICULUM CONSTRUCTION OF BIG DATA AND ACCOUNTING UNDER THE BACKGROUND OF BIG DATA,"In the era of big data and artificial intelligence, society has put forward new requirements for the quality structure, ability structure, knowledge structure, skill structure and learning ability of accounting talents. In response to the new needs of the society for the skills, structure and learning ability of accounting professionals in the big data era, this article analyzes the reconstruction of the accounting professional curriculum system from the perspective of big data technology and management accounting, and analyzes the accounting professional curriculum system in the big data era Based on the core courses of engineering and management, it puts forward suggestions and countermeasures for the cross-border integration of big data and accounting curriculum system, hoping to help the accounting professional teaching in colleges and universities to achieve better development.",2021,
60,10.1109/TBDATA.2017.2757942,A BIG DATA-AS-A-SERVICE FRAMEWORK: STATE-OF-THE-ART AND PERSPECTIVES,"Due to the rapid advances of information technologies, Big Data, recognized with 4Vs characteristics (volume, variety, veracity, and velocity), bring significant benefits as well as many challenges. A major benefit of Big Data is to provide timely information and proactive services for humans. The primary purpose of this paper is to review the current state-of-the-art of Big Data from the aspects of organization and representation, cleaning and reduction, integration and processing, security and privacy, analytics and applications, then present a novel framework to provide high-quality so called Big Data-as-a-Service. The framework consists of three planes, namely sensing plane, cloud plane and application plane, to systemically address all challenges of the above aspects. Also, to clearly demonstrate the working process of the proposed framework, a tensor-based multiple clustering on bicycle renting and returning data is illustrated, which can provide several suggestions for rebalancing of the bicycle-sharing system. Finally, some challenges about the proposed framework are discussed.",2018,23722096
61,10.1109/ISCIT.2016.7751629,MOBILE TERMINAL QUALITY OF EXPERIENCE ANALYSIS BASED ON BIG DATA,"In this paper, we proposes a method to analyze and evaluate the quality of experience (QoE) in mobile terminals using “big data”. The feature parameters of key quality indicator (KQI) are obtained from operators and quantized through the use of a scoring system. Then the scores of customer experience indicator (CEI) and QoE are calculated based on our proposed analytical model. In combination with the data of market operation, the terminal QoE evaluation scores contribute to offer effective suggestions on the promotion of mobile terminal.",2016,
62,10.1109/SC2.2017.30,QUALITY PROFILE-BASED CLOUD SERVICE SELECTION FOR FULFILLING BIG DATA PROCESSING REQUIREMENTS,"Big data has emerged as promising technology to handle huge and special data. Processing Big data involves selecting the appropriate services and resources thanks to the variety of services offered by different Cloud providers. Such selection is difficult, especially if a set of Big data requirements should be met. In this paper, we propose a dynamic cloud service selection scheme that assess Big data requirements, dynamically map these to the most available cloud services, and then recommend the best match services that fulfill different Big data processing requests. Our selection is conducted in two stages: 1) relies on a Big data task profile that efficiently capture Big data task's requirements and map them to QoS parameters, and then classify cloud providers that best satisfy these requirements, 2) uses the list of selected providers from stage 1 to further select the appropriate Cloud services to fulfill the overall Big Data task requirements. We extend the Analytic Hierarchy Process (AHP) based ranking mechanism to cope with the problem of multi-criteria selection. We conduct a set of experiments using simulated cloud setup to evaluate our selection scheme as well as the extended AHP against other selection techniques. The results show that our selection approach outperforms the others and select efficiently the appropriate cloud services that guarantee Big data task's QoS requirements.",2017,
63,10.1109/BigData47090.2019.9006283,PRIVACY AND SECURITY OF BIG DATA IN AI SYSTEMS: A RESEARCH AND STANDARDS PERSPECTIVE,"The huge volume, variety, and velocity of big data have empowered Machine Learning (ML) techniques and Artificial Intelligence (AI) systems. However, the vast portion of data used to train AI systems is sensitive information. Hence, any vulnerability has a potentially disastrous impact on privacy aspects and security issues. Nevertheless, the increased demands for high-quality AI from governments and companies require the utilization of big data in the systems. Several studies have highlighted the threats of big data on different platforms and the countermeasures to reduce the risks caused by attacks. In this paper, we provide an overview of the existing threats which violate privacy aspects and security issues inflicted by big data as a primary driving force within the AI/ML workflow. We define an adversarial model to investigate the attacks. Additionally, we analyze and summarize the defense strategies and countermeasures of these attacks. Furthermore, due to the impact of AI systems in the market and the vast majority of business sectors, we also investigate Standards Developing Organizations (SDOs) that are actively involved in providing guidelines to protect the privacy and ensure the security of big data and AI systems. Our far-reaching goal is to bridge the research and standardization frame to increase the consistency and efficiency of AI systems developments guaranteeing customer satisfaction while transferring a high degree of trustworthiness.",2019,
64,10.1109/ACCTCS53867.2022.00068,RESEARCH AND APPLICATION OF AIR QUALITY PREDICTION MODEL BASED ON URBAN BIG DATA,"The urban scale gradually expands with the development of the economy, and many problems arise in urbanization. Specifically, the urban traffic environment is gradually worsening. The traffic pressure is increasing with the urban population growth, the tail gas emissions from private cars and the factory exhaust gas produced with the industrial development around the city have caused terrible environmental influence. Therefore, with the rapid economic growth, urban air quality and residents' health have gradually become a hot research topic. In the previous research on air quality, many studies were conducted from a single time dimension, ignoring the continuation of air pollutants on the time scale and the diffusivity on the spatial scale. Therefore, on this basis, this paper proposes a prediction air quality model based on temporal and spatial distribution. By analyzing historical data, we can predict the air quality value from the time and space scales, and then the required results are obtained according to the meteorological conditions to improve forecast accuracy.",2022,
65,10.1109/BigData50022.2020.9378401,CLOSED ITEMSET BASED SENSITIVE PATTERN HIDING FOR IMPROVED DATA UTILITY AND SCALABILITY,"Frequent itemset mining is used to extract interesting associations and correlations between the itemsets present in transactional datasets. The frequently appearing patterns are used for various business decision making policies, for instance to increase co-purchase of products, price optimization, cross promotion etc. However, there are some sensitive patterns present in datasets that can reveal individual or organisation's specific confidential information that they would not prefer to be known since it can cause them huge social and monetary loss. Privacy Preserving Data Mining (PPDM) approaches are used to hide these sensitive patterns with maintaining the utility of the data. Heuristics-based PPDM approaches are widely adopted sensitive pattern hiding approaches due to their simplicity and lesser computational time as compared to the border-based and exact approaches. However, these approaches causes high side effects concerning the quality of datasets. In this paper, two heuristics-based algorithms, Removal of Closed Sensitive Itemsets with Maximum Support (MaxRCSI) and Removal of Closed Sensitive Itemsets with Minimum Support (MinRCSI), are proposed. In these algorithms, data sanitization is performed over closed sensitive itemsets to improve the utility of sanitized data. The proposed algorithms are parallelized on Spark parallel computing framework to deal with the massive amount of data i.e. big data. Experiments performed on real and synthetic datasets show that the proposed algorithms preserve the privacy of datasets with substantially better utility as compared to the traditional algorithms with less execution time.",2020,
66,10.1109/ICISE-IE53922.2021.00097,CONSTRUCTION OF BUSINESS CONTINUITY MANAGEMENT AUDIT INFORMATION MODEL BASED ON BIG DATA TECHNOLOGY,"In order to realize the improvement of business continuity management audit level in the daily work of an enterprise, this paper constructs the data and information processing model of business continuity management audit based on Bayes network structure and fuzzy set theory, and introduces the cloud data analysis technology in big data technology. In this model, the deep neural network model is mainly used for analysis and calculation to improve the work efficiency and quality of the data information processing model, and it is expected to play a greater role in future work.",2021,
67,10.1109/ICRIS52159.2020.00113,PREVENTION AND NURSING METHOD OF VASCULAR CRISIS BASED ON BIG DATA,"In order to address issues related to the prevention and care of postoperative vascular crisis in patients with multifinger detachment reimplantation, and to improve the success rate and postoperative nursing efficiency of replantation operation, the paper comes up with a novel prevention and nursing method of vascular crisis based on big data. In this paper, 248 patients with multifinger detachment injuries admitted to the Department of Hand and Foot Surgery from January 2017 to December 2019 were used and divided into a control group and an observation group. The control group adopts the traditional conventional nursing care, while the observation group adopts the total quality control concept of nursing care, which takes patients' needs and satisfaction as the ultimate goal, respects patients' personal habits, and satisfies patients' psychological and physiological needs through management and quality control from beginning to end. The experimental results show that at the p less than 0.05 level for the incidence of vascular crisis in the observation group was lower than in the control group, the success rate of finger amputation and reimplantation in the observation group was higher than in the control group, the quality of nursing care was completed better in the observation group than in the control group, and the satisfaction of patients with nurses was higher in the observation group than in the control group.",2020,
68,10.1109/BigData.2018.8622412,DATA-DRIVEN DIGITAL TWIN APPROACH FOR PROCESS OPTIMIZATION: AN INDUSTRY USE CASE,"In this paper we present a novel approach for the process improvement based on the data-driven modelling. The idea is that by performing Big data analytics on the past process data we can model what is (statistically analyzed) usual/normal for a selected period and check the variations from that model in the real-time (as Six Sigma requires). Additionally, these data-driven models can support the root- cause analysis that should provide insights what can be eliminated as a waste in the process (as Lean requires). However, due to the above mentioned variety and volume of data, the analytics must be a) robust - dealing with differences efficiently and b) scalable - realized in an extremely parallel way. We propose a novel method for process control that uses big data analytics approaches to deal with the multidimensionality and the large size of the process space. In order to realize this idea we develop a new concept of self- aware digital twins which are able to reason about own behaviour and react if needed. Indeed, we revolutionize the concept of digital twins by extending their ""virtual replica"" (of physical objects) nature into ""digital self-awareness"" of physical objects (assets, systems), leading to the new generation of digital twins, so called self-aware DTs, which can ""reasons"" about the behaviour of an object (and not only mimic it) and actively participate in its improvement. We present the outcomes from the case study related to 3D laser cutting process.",2018,
69,10.1109/TEMSCONEUROPE54743.2022.9801964,DATA VISUALIZATION AND STATISTICAL GRAPHICS IN BIG DATA ANALYSIS BY GOOGLE DATA STUDIO – SALES CASE STUDY,"Recently, there has been a surge in interest in data in all of its forms, resulting in an increase in its amount and variety, as well as the creation of the notion of big data. This study focuses on one of the most critical phases of the big data analysis life cycle, in order to apply it to a case study of a virtual corporation with a sales problem. We concentrated on meticulously recording all processes and using Google Data Studio to generate logical results that could be examined and used to assist decision-makers in making the best decisions at the right time. Data was collected, and experts were interviewed in order to build ideas, answers to hypothetical situations, and how to use the huge data available to generate graphical results. The importance of the hypotheses and qualities that were linked in the early phases of the big data analysis cycle is shown in the images.",2022,
70,10.1109/BDEIM55082.2021.00107,CURRENT SITUATION AND OPTIMIZATION OF THE COMBINED DEVELOPMENT OF E-COMMERCE AND THE RURAL CULTURAL TOURISM -- BASED ON BIG DATA AND DATA VISUALIZATION METHOD,"By using the data visualization analysis method, this paper makes an in-depth analysis of the current situation of integrated development of E-commerce and rural cultural tourism industry in China, analyzes the effect of e-commerce on the rural cultural tourism industry, and clarifies the factors restricting the development of rural e-commerce tourism in China at this stage. To further promote the development of higher quality of rural e-commerce tourism industry in China, it provides reference suggestions, that is, based on the e-commerce platform, build characteristic brands of rural tourism, provide sufficient transportation, communication, and talent support, and speed up the in-depth integration development of rural tourism industry chain and rural e-commerce. In addition, this paper uses the swot analysis tool, from the strengths, weaknesses, opportunities, threats, such four dimensions, and introduces the advantages and disadvantages of e-commerce in rural cultural tourism in detail. Finally, this paper, through structure design, finds that e-commerce can help the development of rural cultural tourism.",2021,
71,10.1109/RCIS.2018.8406658,FROM BIG DATA TO SMART DATA: A GENOMIC INFORMATION SYSTEMS PERSPECTIVE,"During the last two decades, data generated by Next Generation Sequencing Technologies have revolutionized our understanding of human biology and improved the study on how changes (variations) in the DNA are involved in the risk of suffering a certain disease. A huge amount of genomic data is publicly available and frequently used by the research community in order to extract meaningful and reliable gene-disease relationships. However, management of this exponential growth of data has become a challenge for biologists; under such a big data problem perspective, they are forced to delve into a lake of complex data spread in over thousand heterogeneous repositories, represented in multiple formats and with different levels of quality; but when data are used to solve a concrete problem only a small part of that “data lake” is really significant; this is what we call the “smart” data perspective. Using conceptual models and the principles of data quality management, adapted to the genomic domain, we propose a systematic approach to move from a big data to a smart data perspective. The aim of this approach is to populate an Information System with genomic data which must be accessible, informative and actionable enough to extract valuable knowledge.",2018,21511357
72,10.1109/BigData50022.2020.9378181,TOWARDS HIGH-QUALITY BIG DATA: LESSONS FROM FIT,"Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. Data are being generated, collected, and analyzed today at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. As the use of big data has grown, so too have concerns that poor-quality data, prevalent in large data sets, can have serious adverse consequences on data-driven decision making. Responsible data science thus requires a recognition of the importance of veracity, the fourth ""V"" of big data. In this talk, we first present a vision of high-quality big data and highlight the substantial challenges that the first three V’s, volume, velocity, and variety, bring to dealing with veracity in big data. We then present the FIT Family of adaptive, data-driven statistical tools that we have designed, developed, and deployed at AT&T for continuous data quality monitoring of a large and diverse collection of continuously evolving data. These tools monitor data movement to discover missing, partial, duplicated, and delayed data; identify changes in the content of spatiotemporal streams; and pinpoint anomaly hotspots based on persistence, pervasiveness, and priority. We conclude with lessons from FIT relevant to big data quality that are cause for optimism.",2020,
73,10.1109/ICTC52510.2021.9620761,MECHANISM OF A BIG-DATA PLATFORM FOR RESIDENTIAL HEAT ENERGY CONSUMPTION,"Although the solar energy industry is becoming widespread, it is necessary to manage the charging and generating scheduling of solar power generation according to the ever-changing climate environment. In order to do this, a judgment criterion that can give timely charge / discharge instructions is needed and it needs to be actively performed. In this paper, we define a big-data platform for residential heat energy consumption. As a technology to secure thermal energy data of apartment houses, collect thermal energy data by dividing it into supply/equipment/usage. In order to secure standardized thermal energy data from the calorimeter installed. Equipped with data classification and processing, LP storage and management, data quality measurement and analysis functions. Develop a data adapter, from several multiunit dwellings with different calorimeter types. We will collect thermal energy data with an integrated big data system.",2021,21621233
74,10.1109/TBDATA.2017.2651898,AN EXTENDED SPATIO-TEMPORAL GRANGER CAUSALITY MODEL FOR AIR QUALITY ESTIMATION WITH HETEROGENEOUS URBAN BIG DATA,"This paper deals with city-wide air quality estimation with limited air quality monitoring stations which are geographically sparse. Since air pollution is influenced by urban dynamics (e.g., meteorology and traffic) which are available throughout the city, we can infer the air quality in regions without monitoring stations based on such spatial-temporal (ST) heterogeneous urban big data. However, big data-enabled estimation poses three challenges. The first challenge is data diversity, i.e., there are many different categories of urban data, some of which may be useless for the estimation. To overcome this, we extend Granger causality to the ST space to analyze all the causality relations in a consistent manner. The second challenge is the computational complexity due to processing the massive volume of data. To overcome this, we introduce the non-causality test to rule out urban dynamics that do not “Granger” cause air pollution, and the region of influence (ROI), which enables us to only analyze data with the highest causality levels. The third challenge is to adapt our grid-based algorithm to non-grid-based applications. By developing a flexible grid-based estimation algorithm, we can decrease the inaccuracies due to grid-based algorithm while maintaining computation efficiency.",2017,23722096
75,10.1109/BigData52589.2021.9671849,A HOLISTIC DATA MODELING APPROACH FOR MULTI-DATABASE SYSTEMS,"IoT, edge-oriented systems, and the growing ubiquity of access to the Internet have driven the development of the most complex software systems to date. Designing such systems is demanding due to their distributed nature, different technologies, multi-layer, hard-to-meet quality attributes, and the integration of several databases with diverse technologies. This work proposes a data modeling method able to represent holistically these systems’ data structure, data transport, and transformation.",2021,
76,10.1109/ICVRIS.2018.00041,SELECTION MODEL OF OPTIMAL MIXED TEACHING MODE IN HIGHER VOCATIONAL COLLEGES BASED ON BIG DATA,"Under the background of big data, we should select the best hybrid teaching mode in higher vocational colleges, improve the ability of big data analysis of the mixed teaching mode in higher vocational colleges, and improve the quality of hybrid teaching mode in higher vocational colleges. A model for selecting hybrid teaching mode in the optimal higher vocational colleges is proposed based on big data. The big data analysis model of hybrid teaching in the optimal higher vocational colleges is constructed, and the information fusion of the mixed teaching mode in the optimal higher vocational colleges is carried out by using the structured big data information recombination method. The characteristic quantity of the associated information describing the optimal hybrid teaching mode in higher vocational colleges is extracted, and the big data fusion scheduling and optimization selection of the mixed teaching mode based on the piecewise information fusion is adopted. According to the characteristic clustering results, the self-regression analysis of the evaluation ability of hybrid teaching in the optimal higher vocational colleges is carried out, and the test statistic model is constructed to optimize the selection of the hybrid teaching model in higher vocational colleges. The simulation results show that this method is used to select the mixed teaching mode in higher vocational colleges, the information fusion ability of outputting big data is better, and the accuracy of model selection is high.",2018,
77,10.1109/ICBDA.2019.8713218,AN AUTOMATED BIG DATA ACCURACY ASSESSMENT TOOL,"Data analysis is the most important aspect of any business as it is critical to decision-making. Data quality assessment is a necessary function to be performed before data analysis, as the quality of data has high impact on the outcome of the analysis. Data quality is a multi-dimensional factor that affects the analysis in numerous ways. Among all the dimensions, accuracy is the most important and hardest dimension to assess. With the advent of big data, this problem becomes more complicated. There are only few studies that focus on data accuracy with minimal domain expert dependency. In this paper, we propose an extensive data accuracy assessment tool that uses machine learning to determine the accuracy of data. In addition, our model also addresses the intrinsic and contextual categories of data accuracy. Our model was developed on Apache Spark which serves as the big data environment for handling large datasets.",2019,
78,10.1109/BigData50022.2020.9377820,APPLICATION OF BIG DATA FOR IMPROVING AIR QUALITY DURING ALMOND HARVESTING PROCESS,"What was once a seasonal disturbance is becoming a more significant problem with orchard dust moving into towns and across main traffic thoroughfares. It's the act of sweeping the fruit into windrows that creates the problem by simultaneously agitating and blowing the soil along with the fruit. The soil billows up in clouds of dust, creating zero visibility. This ultimately creates a windrow that is equally filled with fruit and soil and is further exacerbated by picking up the combination of fruit and soil (within the windrow) and billowing a second round of dense dust into the surrounding air. Solutions to the dust problem focus on removing the sweepers and potentially the `pick-ups'. In this study, we are designing a new mechanism for almond sweeping and pick up with minimum dust generated by adapted sweepers in order to have minimum contact with ground using distance sensors and actively measure particulate matters (PM2.5 and PM10) using dust sensors and adjust system to reach acceptable level of generated dust according to National Ambient Air Quality Standards in almond harvesting process.",2020,
79,10.1109/ESCI50559.2021.9396813,EFFECTIVE USE OF BIG DATA IN PRECISION AGRICULTURE,"Precision Agriculture is the key terminology in agriculture Engineering. Precision agriculture can make the use of legacy data of agriculture to make the farming better in terms of quantity and quality. To enhance the production of the agriculture, technologies such as big data analytics along with data mining tool can use the legacy agricultural data to make the future prediction. This prediction can help to enhance the Agro-Economy.",2021,
80,10.1109/ISBI48211.2021.9433875,MACHINE LEARNING FRAMEWORK FOR FULLY AUTOMATIC QUALITY CHECKING OF RIGID AND AFFINE REGISTRATIONS IN BIG DATA BRAIN MRI,"Rigid and affine registrations to a common template are the essential steps during pre-processing of brain structural magnetic resonance imaging (MRI) data. Manual quality control (QC) of these registrations is quite tedious if the data contains several thousands of images. Therefore, we propose a machine learning (ML) framework for fully automatic QC of these registrations via global and local computation of the similarity functions such as normalized cross-correlation, normalized mutual-information, and correlation ratio, and using these as features for training of different ML classifiers. To facilitate supervised learning, misaligned images are generated. A structural MRI dataset consisting of 215 subjects from autism brain imaging data exchange is used for 5-fold cross-validation and testing. ML models based on local costs performed better than the models with global costs. Local cost based random forest, and AdaBoost models reached testing F1-scores and balanced accuracies of 0.98 and 0.95 respectively for QC of both rigid and affine registrations.",2021,19457928
81,10.1109/ICECCE52056.2021.9514240,INCORPORATION OF BIG DATA IN METHODOLOGY OF IDENTIFYING CORROSION FACTORS IN THE SEMICONDUCTOR PACKAGE,"The semiconductor packaging industry driven by packaging complexity and product miniaturization. Hence, the problem identification methodology in semiconductor industries is a critical interest, and a basis of continuous improvement where the lesson learned is an integral part of it. Nevertheless, the problem identification approach is stagnant at the traditional method, such as the statistical-based methodology. There are several studies on the problem identification process in semiconductor through the six-sigma methodology and statistical approach, however, the scope is limited to the inferential statistic. Therefore, the focus of this paper is proposing using big data approach which grounded on the information theory. The big data analysis approach is utilizing the algorithm and data visualization. Big data methods, such as MINE and clustering was applied to data from hundreds of variables that contain essential and undiscovered relationship. The big data analysis enables the potential factors that contributed to the root causes and provided significant input to the design of experiment and reliability analysis.",2021,
82,10.1109/ICDEW.2019.00-37,IMPLEMENTING BIG DATA LAKE FOR HETEROGENEOUS DATA SOURCES,"Modern connected cities are more and more leveraging advances in ICT to improve their services and the quality of life of their inhabitants. The data generated from different sources, such as environmental sensors, social networking platforms, traffic counters, are harnessed to achieve these end goals. However, collecting, integrating, and analyzing all the heterogeneous data sources available from the cities is a challenge. This article suggests a data lake approach built on Big Data technologies, to gather all the data together for further analysis. The platform, described here, enables data collection, storage, integration, and further analysis and visualization of the results. This solution is the first attempt to integrate a diverse set of data sources from four pilot cities as part of the CUTLER project (Coastal urban development through the lenses of resiliency). The design and implementation details, as well as usage scenarios are presented in this paper.",2019,19432895
83,10.1109/BigData.2016.7840998,DRUG TARGET PATH DISCOVERY ON SEMANTIC BIOMEDICAL BIG DATA,"Systems chemical biology integrate chemistry, biology and computation tools as a whole system, which can help researchers to deeply study the interaction and relationship among small molecules, such as genes, proteins, targets, compounds and so on. With systems chemical biology, researchers can concentrate on new way of drug discovery, including drug target path discovery, which can not only help biomedical researchers to find evidences for existing disease associate genes, but also to design new effect medicine based on targets. Network based approaches are the state-of-art solutions for drug target path discovery, however, there are still some challenges: 1) The quality of the network dominate the efficiency and accuracy of the results, therefore a well designed network is quite important on drug target path discovery mission; 2) the existing network based approaches only work on small graph, it can not handle massive data well. In the paper, we designed a novel framework of systems chemical biology based on semantic big data. In the paper, we proposed a novel drug target path discovery approach. It can identify targets associated with specific medicines (disease) and the path of relationship based on a RDF semantic D-T network. The ranking of candidate targets is performed through an improved parallel random walk with restart algorithm. The experimental studies show that the proposed approaches can efficiently discover drug target relationship path, meanwhile, the approaches have good scalability which are suitable for big data analysis.",2016,
84,10.1109/BigData50022.2020.9378231,DEVELOPMENT AND APPLICATION OF AN INTENSIVE CARE MEDICAL DATA SET FOR DEEP LEARNING,"A large number of patient healthcare data have been collected in the process of diagnosis and treatment of intensive care medicine, which provides major benefits for patient safety and quality. Unfortunately, the application of medical data is greatly limited. Key barriers to the use of the data include difficulties in data extraction and cleaning, and the construction of high-quality data sets promotes the research of medical big data analysis. In China, there is few intensive care data set built by clinicians has been used for clinical outcome prediction. This study developed and evaluated an Intensive Care Medical (ICM) data set for critically care patients that can be used for deep learning. The ICM data set contained four types of data collected routinely in Chinese hospitals, including all-cause characteristics of administrative information, vital signs, laboratory tests, and intravenous medication records. A total of 17,291 ICU admissions involving 12,815 patients aged 14 years and older were extracted from the data set. Deep learning model achieved high accuracy for tasks in hospital mortality predicting (AUROC[area under the receiver operator curve] reach 0.8941). We believe that the ICM data set can be used to create accurate predictions for a variety of clinical scenarios.",2020,
85,10.1109/TBDATA.2017.2723899,PG-CAUSALITY: IDENTIFYING SPATIOTEMPORAL CAUSAL PATHWAYS FOR AIR POLLUTANTS WITH URBAN BIG DATA,"Many countries are suffering from severe air pollution. Understanding how different air pollutants accumulate and propagate is critical to making relevant public policies. In this paper, we use urban big data (air quality data and meteorological data) to identify the spatiotemporal (ST) causal pathways for air pollutants. This problem is challenging because: (1) there are numerous noisy and low-pollution periods in the raw air quality data, which may lead to unreliable causality analysis; (2) for large-scale data in the ST space, the computational complexity of constructing a causal structure is very high; and (3) the ST causal pathways are complex due to the interactions of multiple pollutants and the influence of environmental factors. Therefore, we present pg-Causality, a novel pattern-aided graphical causality analysis approach that combines the strengths of pattern mining and Bayesian learning to efficiently identify the ST causal pathways. First, pattern mining helps suppress the noise by capturing frequent evolving patterns (FEPs) of each monitoring sensor, and greatly reduce the complexity by selecting the pattern-matched sensors as “causers”. Then, Bayesian learning carefully encodes the local and ST causal relations with a Gaussian Bayesian Network (GBN)-based graphical model, which also integrates environmental influences to minimize biases in the final results. We evaluate our approach with three real-world data sets containing 982 air quality sensors in 128 cities, in three regions of China from 01-Jun-2013 to 31-Dec-2016. Results show that our approach outperforms the traditional causal structure learning methods in time efficiency, inference accuracy and interpretability.",2018,23722096
86,10.1109/ICSGEA51094.2020.00120,TEACHING QUALITY EVALUATION MODEL FOR HUMAN RESOURCE DEVELOPMENT AND MANAGEMENT MAJOR UNDER THE BACKGROUND OF BIG DATA,"The evaluation of teaching quality in colleges relies on reliable and comprehensive evaluation data, which is also an important basis for educational decision-making. The application of big data technology can realize scientific analysis and in-depth mining of a large number of data, analyze the value and connection implied in the data on a multidimensional level, and enable the evaluation of teaching quality to transform from the previous analysis of segment information and small sample data to the whole process of all-round data decision-making. This paper proposes a learning quality evaluation model based on big data in cloud computing environment, using the structure method, AHP, fuzzy comprehensive evaluation and multi-objective optimization method. It is also combined with a variety of theories such as user perception theory and user experience theory, and the development of this model is studied. Finally, we develop and design the learning quality evaluation system under cloud computing, to test the system and prove the credibility of our scheme.",2020,
87,10.1109/INOCON50539.2020.9298381,COGNITO - INTUITIVE AUTO DATA EXPLORATORY TOOLKIT,"In any machine learning model, the eminence of discovered knowledge is directly associated with the quality of data used for analysis. In the technological world, massive growth is observed in the scale of data generation. Big data resolve many challenges of the organization by predicting the future or finding valuable insights from the data. Despite this, Big data itself is a major challenge in front of researchers because the data doesn't come in cleaned format. Data preprocessing becomes a crucial phase that handles anomalies and noise in the data. Though various data pre-processing algorithms are available to clean the dataset, data analysts spend a large amount of time to achieve it. Cognito being an open-source python library offers various features to reduce the time and efforts of the data scientist. Cognito facilitates the automatic pre-processing and fundamental data analysis by providing the cleaned dataset, summarized report about the dataset, features of each column of the dataset, and possible questions that can be asked to the dataset in an attractive format. It provides output in CSV format and a summary of the dataset in PDF format. Developers do not need to wrap their heads around or waste their time analyzing the data. It also reduces the user's effort by a considerable margin. Moreover, the cleaned data generated by Cognito is crunched, thus reducing the space required. Cognito automatically manages various data pre-processing operations such as feature selection, missing value imputation, normalization, outlier treatment, data reduction with additional features of auto-generated description insights.",2020,
88,10.1109/ICICTA51737.2020.00134,RESEARCH ON DATA PREPROCESSING AND 3D MATRIX MODEL,"In order to overcome the problems existing in the current big data mining platform, this paper proposes a novel construction method of three-dimensional matrix model based on big data mining technology. This construction method uses the massive data mining function and massive data storage function provided by big data cloud platform, starting from the needs of users, in order to solve the problems existing in big data mining platform, for example, lack of scientific and efficient data model, lack of scientific and reasonable high-dimensional data association algorithm, dealing with the thorny problems of big data link, tedious and complex association rules, and so on. The construction method takes big data extended data mining technology as the center, combining data analysis technology, data extraction technology and so on. On this basis, based on the three-dimensional matrix model, the construction method completes the scientific and efficient preprocessing of data information from different sources. The research results show that this construction method can improve the efficiency and quality of association principle mining on the basis of massive high-dimensional data, and effectively alleviate the tedious and complex problem of association rules in the process of dealing with big data.",2020,
89,10.1109/IMIS.2013.104,STUDY ON BIG DATA CENTER TRAFFIC MANAGEMENT BASED ON THE SEPARATION OF LARGE-SCALE DATA STREAM,"The network of traditional data center has been usually designed and constructed for the provision of user's equal access of data centre's resource or data. Therefore, network administrators have a strong tendency to manage user traffic from the viewpoint that the traffic has a similar size and characteristics. But, the emersion of big data begins to make data centers have to deal with 1015 byte-data transfer at once. Such a big data transfer can cause problems in network traffic management in the existed data center. And, the tiered network architecture of the legacy data center magnifies the magnitude of the problems. One of the well-known big data in science is from large hadron collider such as LHC in Swiss CERN. CERN LHC generates multi-peta byte data per year. From our experience of CERN data service, this paper showed the impact of network traffic affected by large-scale data stream using NS2 simulation, and then, suggested the evolution direction based on separating of large-scale data stream for the big data center's network architecture.",2013,
90,10.1109/ICBASE51474.2020.00017,RESEARCH ON DATA TRACEABILITY METHOD BASED ON BLOCKCHAIN TECHNOLOGY,"Energy Internet is a major innovation to deal with the environmental crisis and efficient energy management and use in the current society. The important condition to achieve this goal is to summarize, integrate, process and apply the data of various industries in the energy field, and then support the relevant management and decision-making. In this process, how to ensure the authenticity and credibility of data is one of the keys in the construction of energy Internet. Therefore, this paper will study the application scenarios of blockchain technology in data traceability. With the help of the natural characteristics of blockchain, such as decentralized, distributed storage, tamper proof, open and transparent, combined with relevant national standards and international theoretical models, based on the needs of energy Internet data integration and management, this paper will develop a data traceability method suitable for the energy industry, and build a covering energy Data life cycle model of Internet. Through the research of this paper, we can help all localities to establish data traceability mechanism in the energy Internet, to help users to accurately grasp where the data is created, what systems have been transferred, which users have carried out query and modification, and so on, so as to realize the monitoring and control of the whole process of data flow, which helps to improve the credibility of data, and also helps to ensure the safety and quality of data and promote the construction of energy Internet huge data application.",2020,
91,10.1109/BigData.2016.7840925,SCHEDULING BIG DATA WORKFLOWS IN THE CLOUD UNDER BUDGET CONSTRAINTS,"Big data is fast becoming a ubiquitous term in both academia and industry and there is a strong need for new data-centric workflow tools and techniques to process and analyze large-scale complex datasets that are growing exponentially. On the other hand, the unbound resource leasing capability foreseen in the cloud facilitates data scientists to wring actionable insights from the data in a time and cost efficient manner. In the data-centric workflow environment, scheduling data processing tasks onto appropriate resources are often driven by the constraints provided by the users. Enforcing a constraint while executing the workflow in the cloud adds a new optimization challenge on how to meet the objective while satisfying the given constraint. In this paper, we propose a new Big dAta woRkflow schEduler uNder budgeT constraint known as BARENTS that supports high-performance workflow scheduling in a heterogeneous cloud computing environment with a single objective to minimize the workflow makespan under a provided budget constraint. Our case study and experiments show the competitive advantages of our proposed scheduler. The proposed BARENTS scheduler is implemented in a new release of DATA VIEW, one of the most usable big data workflow systems in the community.",2016,
92,10.1109/SOSE.2016.63,"BIG DATA VALIDATION AND QUALITY ASSURANCE -- ISSUSES, CHALLENGES, AND NEEDS","With the fast advance of big data technology and analytics solutions, big data computing and service is becoming a very hot research and application subject in academic research, industry community, and government services. Nevertheless, there are increasing data quality problems resulting in erroneous data costs in enterprises and businesses. Current research seldom discusses how to effectively validate big data to ensure data quality. This paper provides informative discussions for big data validation and quality assurance, including the essential concepts, focuses, and validation process. Moreover, the paper presents a comparison among big data validation tools and several major players in industry are discussed. Furthermore, the primary issues, challenges, and needs are discussed.",2016,
93,10.1109/BigData.2017.8258138,A DATA-DRIVEN APPROACH TO HELP UNDERSTANDING THE PREFERENCES OF PUBLIC TRANSPORT USERS,"The maintenance of the quality of the public transport service in big cities requires constant monitoring, which may become an expensive and time-consuming practice. The perception of quality, from the users point of view is an important aspect of quality monitoring. In this sense, we proposed a methodology based on big data analysis and visualization, which allows for the structuring of estimates and assumptions of where and who seems to be having unsatisfactory experiences while making use of the public transportation in metropolitan areas. Moreover, it provides support in setting up a plan for on-site quality surveys. The proposed methodology increases the likelihood that, with the on-site visits, the interviewer finds users who suffer inconveniences, which influence their behavior. Simulation comparison and a small-scale pilot survey helped validate the proposed method.",2017,
94,10.1109/ICENCO.2018.8636154,ONTOLOGY LEARNING BASED ON WORD EMBEDDINGS FOR TEXT BIG DATA EXTRACTION,"Big Data term describes data that exists everywhere in humongous volumes, raw forms, and heterogenous types. Unstructured and uncategorized data forms 95% of big data. Text big data lacks to efficiently extract domain-relevant data in a suitable time. Thus, text big data stills a barrier for big data integration and subsequently big data analytics. Because big data integration can't consider text big data in its process of preparing data for big data analytics. On the other side, ontology represents information and knowledge in a graph schema that provides a shareable, reusing and domain-specific data. Thus, ontology fits text big data needs of extracting domain relevant data. So, this paper proposes an ontology learning (OL) methodology for text big data extraction. OL aims to provides algorithms, techniques, and tools for automatic ontology construction from the text. The proposed OL method exploits a deep learning approach i.e., word embeddings, and advanced hierarchical clustering i.e., BIRCH. The utilization of the word embeddings and the advanced hierarchical clustering improve OL quality in text big data extraction and reduce the processing time. Also, deep learning unsupervisory learns from a massive amount of unlabeled and uncategorized raw data. This great big benefit solves analytical challenge of the text big data. In evaluation, precision, recall, and f - value for the work quality and the running time for performance are measured. The quality of work is evaluated by comparing its results with gold standard datasets results. Experimental results and evaluation demonstrate that the proposed OL methodology efficiently suitable for text big data extraction.",2018,24752312
95,10.1109/BigData.2016.7840769,DATA QUALITY: EXPERIENCES AND LESSONS FROM OPERATIONALIZING BIG DATA,"Data quality issues pose a significant barrier to operationalizing big data. They pertain to the meaning of the data, the consistency of that meaning, the human interpretation of results, and the contexts in which the results are used. Data quality issues arise after organizations have moved past clear-cut technical solutions to early bottlenecks in using data. Left unaddressed, such issues can and have led to high profile missteps, and raise doubts about the data-driven world view altogether. In this paper, we share real-world case studies of tackling data quality challenges across industry verticals. We present initial ideas on how to systematically address data quality issues via technology. The success of operationalizing big data will depend on the quality of data involved, and whether such data causes uncertainty and disruptions, or delivers genuine knowledge and value.",2016,
96,10.1109/ICEEM52022.2021.9480628,SMART WATER QUALITY ANALYSIS USING IOT AND BIG DATA ANALYTICS: A REVIEW,"In recent times, water quality monitoring, observing, and testing have become a significant research study with the advancement on the Internet of Things, and Big Data analytics research ideas. The relationship between water demand and supply is very crucial for every country and is likewise a complex challenge to satisfy this requirement around the world. Water is compulsory for human survival being on the earth. Therefore, for survival, the preservation and taking care of the existing water resource are also equally substantial. Moreover, for a healthier society, access to clean and safe water resources is also imperative. To recognize the water quality effects and provide an automated water quality mentoring and a testing system can support in guaranteeing the safety of the water. This paper presents a study on water quality analysis using IoT and Big Data analytics. This can help in developing an agile environment that can handle the massive flow of water big data generated by the smart sensors spread everywhere around us.",2021,
97,10.1109/AIID51893.2021.9456537,DESIGN AND IMPLEMENTATION OF REGIONAL FOOD DISTRIBUTION PLATFORM BASED ON BIG DATA,"In recent years, the rapid development of big data has made people's daily life very convenient, and at the same time, tasting all kinds of food has become an important activity for people to travel. Due to the vast territory of China, there are many types of cuisines with great differences, it is very important for travelers to understand the special regional cuisines in the area. This project aggregates regional food data based on big data, and provides tourists with efficient, stable and professional data retrieval and analysis services through a visual data interface, and provides intuitive, accurate, and real-time data support for the decision-making of finding characteristic regional food. This thesis first conducted a relevant understanding of big data and the overall situation of regional cuisine, analyzed the distribution of food in the sub-provincial city of Xi'an, explored the research methods and implementation methods of related projects at home and abroad, based on this, summarized the research of this project the goal. At the same time, the focus of this project is to analyze the price, score and popularity of regional food data analysis in my country, and to summarize, proofread and organize the data obtained before into standard and standardized data. Through the classification of basic information, the visual analysis and display of data is realized, and the key data urgently needed by decision makers are extracted from it. To analyze the needs of decision makers, establish corresponding strategies and measures to improve the quality of data services. The establishment of this system provides a useful supplement and improvement to the existing industry data analysis system. The system is mainly divided into six modules, which are data collection, data review, data summary, and visual data display modules. Among them, data collection includes crawling relevant data from the Internet and retrieving key data. The data audit function includes classifying the crawled data and reviewing its effectiveness. Data aggregation includes summarizing the filtered data in an excel table and passing Excel generates a visual chart that you want to know, and at the same time generates a word cloud by associating certain two related items in the data in the excel table. The visual data display module can more clearly show the results of data analysis to users, so that users can make better decisions. The visualization data uses AmChart Flash charts. The implementation of this system adopts Django Python Web framework, the development language chooses Python, the development tool adopted is PyCharm, and the database tool adopts MySQL.",2021,
98,10.1109/SERVICES.2017.20,ADDING SUPPORT FOR THEORY IN OPEN SCIENCE BIG DATA,"Open Science Big Data is emerging as an important area of research and software development. Although there are several high quality frameworks for Big Data, additional capabilities are needed for Open Science Big Data. These include data provenance, citable reusable data, data sources providing links to research literature, relationships to other data and theories, transparent analysis/reproducibility, data privacy, new optimizations/advanced algorithms, data curation, data storage and transfer. An important part of science is explanation of results, ideally leading to theory formation. In this paper, we examine means for supporting the use of theory in big data analytics as well as using big data to assist in theory formation. One approach is to fit data in a way that is compatible with some theory, existing or new. Functional Data Analysis allows precise fitting of data as well as penalties for lack of smoothness or even departure from theoretical expectations. This paper discusses principal differential analysis and related techniques for fitting data where, for example, a time-based process is governed by an ordinary differential equation. Automation in theory formation is also considered. Case studies in the fields of computational economics and finance are considered.",2017,
99,10.1109/ICSSIT53264.2022.9716355,STUDY ON MISSING VALUES AND OUTLIER DETECTION IN CONCURRENCE WITH DATA QUALITY ENHANCEMENT FOR EFFICIENT DATA PROCESSING,"Data analytics is the process of analyzing raw data to make predictions and derive conclusions. This process involves collecting and organizing data to discover hidden patterns and draw insight into the data. The methods and approaches of data analytics are automated using various algorithms and mathematical formulas. Data analytics provides real-time and actionable perceptions on data that enable more accurate and prompter decision-making. During the data acquisition phase, missing values and outliers are encountered that affect the model's reliability. Missing values are the data missing in a dataset, more common on large datasets that arise due to information loss, dropout, or non-response of participants. Missing values affects the accuracy of the result and also may lead to biased results. Outliers are the abnormal values that happen to have deviated from the normal distribution pattern of data distribution. Outliers are extreme, unrealistic, extremely big, or small values in a dataset that arise due to manual errors like participant response errors and data entry errors. Outliers also affect the accuracy of the results and lead to over or underestimated resultant values. As missing values and outliers degrade the performance of the analytical data models, various research works have focused on finding and handling such values. This paper reviews the various aspects of missing values and outliers in the preprocessing phase of data analytics to enhance the accuracy of the data model.",2022,
100,10.1109/CIBDA50819.2020.00038,MAPREDUCE-BASED BP NEURAL NETWORK CLASSIFICATION OF AQUACULTURE WATER QUALITY,"As information technology develops and prevails across the globe, the informatization and data processing efficiency in the aquaculture field are exerting increasing impact on the intelligent conversion of this field. The aquaculture water quality indicators were analyzed by a feedforward error back propagation algorithm (BP neural network) with strong nonlinear mapping capacity, and the complicated nonlinear relations among the parameters of the aquaculture environment were solved. An aquaculture element analysis model was proposed, the Johnson attribute reduction algorithm based on the discernibility matrix was used to optimize the traditional algorithm, and the network convergence speed was increased under a given accuracy. The MapReduece distributed programming model was then used to perform parallel design of the BP neural network algorithm to meet the needs of massive data processing in aquaculture platforms. Also, case studies were performed to analyze the aquaculture element model and the parallel learning algorithm, and the big data framework design and data analysis method are integrated to develop an efficient, fault-tolerant aquaculture data management, mining, visualization big data system.",2020,
101,10.1109/DASC-PICom-CBDCom-CyberSciTech49142.2020.00114,APPLYING MACHINE LEARNING TO AVIATION BIG DATA FOR FLIGHT DELAY PREDICTION,"Flight delay has been a serious and widespread problem that needs to be solved. One promising solution is the flight delay prediction. Although big data analytics and machine learning have been applied successfully in many domains, their applications in aviation are limited. This paper presents a comprehensive study of flight delay spanning data pre-processing, data visualization and data mining, in which we develop several machine learning models to predict flight arrival delays. Two data sets were used, namely Airline On-Time Performance (AOTP) Data and Quality Controlled Local Climatological Data (QCLCD). This paper aims to recognize useful patterns of the flight delay from aviation data and perform accurate delay prediction. The best result for flight delay prediction (five classes) using machine learning models is 89.07% (Multilayer Perceptron). A Convolution neural network model is also built which is enlightened by the idea of pattern recognition and success of neural network method, showing a slightly better result with 89.32% prediction accuracy.",2020,
102,10.1109/TNSM.2016.2554143,A PRETREATMENT WORKFLOW SCHEDULING APPROACH FOR BIG DATA APPLICATIONS IN MULTICLOUD ENVIRONMENTS,"The rapid development of the latest distributed computing paradigm, i.e., cloud computing, generates a highly fragmented cloud market composed of numerous cloud providers and offers tremendous parallel computing ability to handle big data problems. One of the biggest challenges in multiclouds is efficient workflow scheduling. Although the workflow scheduling problem has been studied extensively, there are still very few primal works tailored for multicloud environments. Moreover, the existing research works either fail to satisfy the quality of service (QoS) requirements, or do not consider some fundamental features of cloud computing such as heterogeneity and elasticity of computing resources. In this paper, a scheduling algorithm, which is called multiclouds partial critical paths with pretreatment (MCPCPP), for big data workflows in multiclouds is presented. This algorithm incorporates the concept of partial critical paths, and aims to minimize the execution cost of workflow while satisfying the defined deadline constraint. Our approach takes into consideration the essential characteristics of multiclouds such as the charge per time interval, various instance types from different cloud providers, as well as homogeneous intrabandwidth vs. heterogeneous interbandwidth. Various types of workflows are used for evaluation purpose and our experimental results show that the MCPCPP is promising.",2016,23737379
103,10.1109/TELFOR.2016.7818902,BIG DATA AND QUALITY: A LITERATURE REVIEW,"Big Data refers to data volumes in the range of Exabyte (1018) and beyond. Such volumes exceed the capacity of current on-line storage and processing systems. With characteristics like volume, velocity and variety big data throws challenges to the traditional IT establishments. Computer assisted innovation, real time data analytics, customer-centric business intelligence, industry wide decision making and transparency are possible advantages, to mention few, of Big Data. There are many issues with Big Data that warrant quality assessment methods. The issues are pertaining to storage and transport, management, and processing. This paper throws light into the present state of quality issues related to Big Data. It provides valuable insights that can be used to leverage Big Data science activities.",2016,
104,10.1109/ACCESS.2019.2936941,AN INTEGRATED BIG AND FAST DATA ANALYTICS PLATFORM FOR SMART URBAN TRANSPORTATION MANAGEMENT,"Smart urban transportation management can be considered as a multifaceted big data challenge. It strongly relies on the information collected into multiple, widespread, and heterogeneous data sources as well as on the ability to extract actionable insights from them. Besides data, full stack (from platform to services and applications) Information and Communications Technology (ICT) solutions need to be specifically adopted to address smart cities challenges. Smart urban transportation management is one of the key use cases addressed in the context of the EUBra-BIGSEA (Europe-Brazil Collaboration of Big Data Scientific Research through Cloud-Centric Applications) project. This paper specifically focuses on the City Administration Dashboard, a public transport analytics application that has been developed on top of the EUBra-BIGSEA platform and used by the Municipality stakeholders of Curitiba, Brazil, to tackle urban traffic data analysis and planning challenges. The solution proposed in this paper joins together a scalable big and fast data analytics platform, a flexible and dynamic cloud infrastructure, data quality and entity matching algorithms as well as security and privacy techniques. By exploiting an interoperable programming framework based on Python Application Programming Interface (API), it allows an easy, rapid and transparent development of smart cities applications.",2019,21693536
105,10.1109/ITMQIS.2017.8085914,BIG DATA IN MODERN HIGHER EDUCATION. BENEFITS AND CRITICISM,"Big Data technology is very efficient in some cases, however, it has some disadvantages. It should be applied to modern universities which current quality of education is high enough. Thus the universities have to satisfy several conditions, for example, providing broad sets of different type tasks, including group discussions, oral speeches, essays with more than one possible correct opinion, developing complex skills of their students; collecting information about courses, student's activities and progress, alumni skills. Undoubtedly, Big Data are not the only opportunity to develop quality of education. Many small universities provide private educational programs for small groups. Moreover, they offer their students more direct conversations with lecturers in words of mouth format. This educational strategy definitely has its own advantages. Usage of Big Data in Russia is possible, however, it claims increasing of higher education quality.",2017,
106,10.1109/BigData.2018.8622249,"PACAS: PRIVACY-AWARE, DATA CLEANING-AS-A-SERVICE","Data cleaning consumes up to 80% of the data analysis pipeline. This is a significant overhead for organizations where data cleaning is still a manually driven process requiring domain expertise. Recent advances have fueled a new computing paradigm called Database-as-a-Service, where data management tasks are outsourced to large service providers. We propose a new Data Cleaning-as-a-Service model that allows a client to interact with a data cleaning provider who hosts curated, and sensitive data. We present PACAS: a Privacy-Aware data Cleaning-As-a-Service framework that facilitates communication between the client and the service provider via a data pricing scheme where clients issue queries, and the service provider returns clean answers for a price while protecting her data. We propose a practical privacy model in such interactive settings called (X,Y,L)-anonymity that extends existing data publishing techniques to consider the data semantics while protecting sensitive values. Our evaluation over real data shows that PACAS effectively safeguards semantically related sensitive values, and provides improved accuracy over existing privacy-aware cleaning techniques.",2018,
107,10.1109/ICBDA.2017.8078702,BAYESIAN CLASSIFICATION BASED SERVICE-AWARENESS IN SOFTWARE DEFINED OPTICAL NETWORK FOR BIG DATA SERVICES,"With emerging of multiple services brought by Big Data, the rapid evolution of Software Defined Optical Network (SDON) provides great support for big data services with high dynamic. However, newly emerging big data services have diversified characteristics and demands, which presents great problems for current SDON to match these services with high quality and matching degree. This paper proposes a Bayesian Classification based Service Awareness (BC-SA) mechanism of SDON for big data services. By using Bayesian classification, the BC-SA mechanism is able to be aware of the type of service and to cooperate with the OpenFlow protocol, with the aim to achieve high matching degree between services and SDON. Based on this BC-SA, bandwidth scheduling can be improved in SDON. Simulation results show that the BC-SA can match requirements by big data services with better performances.",2017,
108,10.1109/ICBDA.2018.8367700,VERIFICATION METHOD OF DATA QUALITY IN SCIENCE AND TECHNOLOGY CLOUD IN SHAANXI PROVINCE,This paper analyzes and summarizes the data quality problems in the Shaanxi Science and Technology Resource Coordination Center “Science and Technology Cloud” project. These two major problems about scientific and technological information data quality are verified. One is data redundancy caused by organizations' name abbreviation and the other is partial scientific and technological information data missing. This paper designs and implements solutions to the problems in “Science and Technology Cloud” project. This paper extracts 15643 data from scientific and technical talent pool and scientific literature library. The experimental results verify the effectiveness and feasibility of the solution of data redundancy and data missing.,2018,
109,10.1109/EDIS.2017.8284019,TAKE THE BEST OF BIG DATA: JUST FOCUS ON SOME OF ITS V'S,"Big data represents a new technology for managing data with high velocity, volume, variety and contributes to creating value for companies. As quoted in1, capturing all queries made on the company website or from customer support calls, emails or chat lines, regardless of their outcome, may have significant value in identifying emerging trends. The Big Data Era has largely contributed in accelerating the development strategic plans issued from governments and research organisms, coving the management, exploitation and analysis of these data by taking into account the different V's of Big Data. Among these plans, we can cite for instance the development of: (a) large-scale platforms (ex. data-clusters, distributed data clusters), (b) Software Defined Environments (SDE) (ex. IBM SDE), (c) advanced programming paradigms (ex. map-reduce, Spark, etc.), Data Analytics Tools (Rapid Miner, Google Fusion Tables, Solver), (d) Visualization tools (Google Chart, Tableau, Oracle Visual Analyzer), and (h) high quality and valuable Knowledge Bases (KB), constructed either by academicians (e.g., Cvc, DBpedia, Freebase, and YAGO) and industrials (e.g., Google Knowledge Graph, Facebook Knowledge Graph, Amazon Knowledge Graph, Credit Rating Agencies, Enterprise Knowledge Base, etc.). In this talk, we would like to foster the creation of a think tank dedicated to getting the best from Big Data V's and the efforts related to it to revisit our research activities without compromising them. In this talk, we would like to share the experience conducted with our Model and Data Engineering Team of the LIAS Laboratory at ISAE-ENSMA, which aims at the design of data warehousing applications. Based on the literature, this design is based on two main approaches: (i) a supply-driven approach (also called data-driven) that starts with an analysis of operational data sources in order to identify all the available data and (ii) a user-driven approach (also known as requirement-driven or goal-orientated) which stems from the determination of the information requirements of different business users. Several studies and experiments show that resorting to these two approaches entails a high risk for companies, since some functional requirements cannot be satisfied. This is due to the lack of relevant data in sources. In parallel, reference studies have identified the crucial role of knowledge bases (KB) for analytical tasks, by offering analysts more entities (people, places, products, etc.). The availability of a huge, high quality valuable KB is an asset for data warehousing designers and decision-makers to construct/exploit a valuable data warehouse. So, faced with this situation, we here present a value-driven approach that revisits the traditional life cycle of the design of data warehouses, by considering KB as an external resource. These different phases are illustrated via the YAGO KB.",2017,
110,10.1109/ICSPCS.2018.8631726,EXPLOITING BIG DATA ANALYTICS FOR URBAN PLANNING AND SMART CITY PERFORMANCE IMPROVEMENT,"The smart city notion facilitate interoperation among multiple disciplines to improve the Quality of Life (QoL) of urban citizens. Unceasingly growing urban networks has significantly increased the data processing complexity. In consequence, real-time data processing and analysis has become a major concern in modern smart city designing and implementation. Considering the challenges of existing smart cities, in this work we propose a smart city architecture embedded with Big Data Analytics (BDA). The utmost goal of the proposed scheme is to enhance the quality of real-time decision-making through efficient Big Data (BD) processing. The proposed architecture is in three folds to manage data collection, data processing, and data application. We evaluate the proposed BDA embedded smart city using authentic datasets on water consumption, traffic congestion, parking management, and air pollution measurements. The analysis offer useful insights for the community development, while ensuring the performance improvement of the proposed framework in terms of processing time and throughput.",2018,
111,10.1109/ICWCSG53609.2021.00085,RESEARCH ON SMART GRID BIG DATA&#X2019;S CURVE MEAN CLUSTERING ALGORITHM FOR EDGE-CLOUD COLLABORATIVE APPLICATION,"As the demand for smart grid construction increases, advanced power applications based on edge-cloud collaboration continue to increase. Among them, there are many data-driven artificial intelligence calculations and analyses, all of which are calculated and analyzed based on electric power big data. However, for the massive electric power big data, it is impossible to obtain more internally related information only by observing the data from the surface. To a certain extent, it directly affects the upper-level advanced applications. To solve this problem, this paper studies and proposes a curve-mean clustering algorithm for load big data, which is the most widely used load data in smart grid. By analyzing the advanced measurement infrastructure, the matrix low-rank property of load big data and the calculation of singular value, the curve mean clustering of load big data is realized, and the optimal determination method of cluster number is expounded. Experiments are conducted based on actual resident user load data and compared with the classic mean shift clustering algorithm. By calculating the average distance within the cluster, the average distance between clusters and the DI index, it is verified that the proposed method clustering is more accurate and the selection of cluster number is optimal. The research plays a very good role in basic analysis for improving the big data analysis capability and data quality of smart grid.",2021,
112,10.1109/CIPAE51077.2020.00044,HOW TO EFFECTIVELY INFILTRATE EMOTIONAL EDUCATION IN PRIMARY SCHOOL CHINESE TEACHING FROM PERSPECTIVE OF BIG DATA,"Cultural inheritance and education is an important part of social development, primary school is an important period for children to receive cultural education, we should not only pay attention to the study of cultural courses but also pay attention to the infiltration of emotional education. With the development of the Internet, cloud technology and big data information in the field of education, the establishment of various education platforms and learning platforms has an increasing impact on primary school Chinese teaching. Based on this background, the purpose of this study is to combine the advanced technology of big data information with the efficient Chinese teaching in primary schools to improve the teaching quality. The research idea of this paper is to use big data information analysis and calculation in the emotional infiltration education of primary school Chinese teaching, analyze and demonstrate the data through the form of questionnaire and interview, and design the research scheme of how to effectively infiltrate emotional education in primary school Chinese teaching from the perspective of big data from the perspective of theory and practice. The research results of this paper show that in the emotional penetration education of Chinese teaching in primary schools, the proportion that the learning quality is affected by big data technology accounts for 67%, and the proportion that the emotional education has a positive impact on students accounts for 63%. The research results indicate that the active application of big data technology can effectively improve the quality of education and benefit the physical and mental health development of students. However, most teachers have little understanding and application of big data, and there is also the irrational phenomenon of emphasizing cognition and ignoring emotion.",2020,
113,10.1109/BigData47090.2019.9005627,QUALIFOOD: AN INTELLIGENT QUALITY FOOD EVALUATION USING LOGICAL SATISFIABILITY REASONING ON SPARK,"There is an urgent need to address unhealthy dietary patterns for people. Therefore, having a high food quality is essential to health by adopting good eating habits. Furthermore, the evaluation of food quality is complex because it needs huge numbers of information related to food, for example its ingredients, the environmental conditions of food productions, the consumer state of health. Such information are scattered on multiple and non communicating systems including food producer systems, open data, medical data, etc. Applying the semantic to a very large collection of information from different data sources can highly contribute to enhance the food quality score. Hence, it will improve the individual diet/health by giving adequate nutritional recommendation and avoiding inconsistent food mixing by following set of rules.In this paper, we propose an intelligent and scalable approach to ensuring the food quality for meal healthiness query answering over big data related to food in a distributed way on a Spark ecosystem. For that, the cleaning inconsistent and contradictory big data approach is built by following the steps (1)modeling the consistency rules including inference and inconsistent rules (2) detecting inconsistency through rule evaluation on Apache Spark framework to discover the minimally subset of inconsistent data (3) cleaning the inconsistency through finding the cleaned meals for consistent query answering.",2019,
114,10.1109/CISCE.2019.00148,RESEARCH ON THE MANAGEMENT MODEL OF UNIVERSITY STUDENTS ACADEMIC EARLY WARNING BASED ON BIG DATA ANALYSIS,"The advancement of technology has greatly promoted education. And with the popularization of information technology construction in higher education institutions, the concept of ""Big Data"" has been paid more and more attention in the management of higher education. Data mining is used to analyze students learning situation. The design of the academic early warning management system can effectively prevent some students from being at risk of dropping out of school due to academic difficulties, improve the quality of higher education operations, and provide higher education managers with the idea of carrying out targeted risk prevention.",2019,
115,10.1109/BigData.2015.7363743,CONSIDERATIONS AND RECOMMENDATIONS FOR DATA AVAILABILITY FOR DATA ANALYTICS FOR MANUFACTURING,"Data analytics is increasingly becoming recognized as a valuable set of tools and techniques for improving performance in the manufacturing enterprise. However, data analytics requires data and a lack of useful and usable data has become an impediment to research in data analytics. In this paper, we describe issues that would help aid data availability including data quality, reliability, efficiency, and formats specific to data analytics in manufacturing. To encourage data availability, we present recommendations and requirements to guide future data contributions. We also describe the need for data for challenge problems in data analytics. A better understanding of these needs, recommendations, and requirements may improve the ability of researchers and other practitioners to improve research and more rapidly deploy data analytics in manufacturing.",2015,
116,10.1109/BigData.2018.8622349,HL7 DATA ACQUISITION & INTEGRATION: CHALLENGES AND BEST PRACTICES,"Lack of interoperability between health data systems is a leading challenge for healthcare in the United States. This paper describes the challenges and lessons learned in the process of incorporating HL7 data and integration with Electronic Health Records and Health Information Exchanges from the perspective of a midsized Health Plan (Payer). As a Health Care Payer, Premera has a unique perspective regarding how health plans can provide the necessary data to complete the picture of care. This paper shares some of the best practices and focus areas for successful implementation of healthcare data integrations. This paper also focuses on integrating claims and clinical data using a master patient index as well as challenges faced in that process.Note that going forward `Health Plan' and `Payer' will be used interchangeably. Also, `Provider(s)', `Hospitals', `Healthcare Providers', `Clinics', `Provider Organizations' will be used interchangeably and in the context of this paper may mean the same.",2018,
117,10.1109/ICAIS50930.2021.9395832,PREDICTIVE MODELS FOR RIVER WATER QUALITY USING MACHINE LEARNING AND BIG DATA TECHNIQUES - A SURVEY,"Water is an important and essential element for the life on earth. Due to the growth of population and industrialization the water resources become more polluted. Waste disposal from industry, human wastes, automobile wastes, agricultural runoff from farmlands containing chemical factors, unwanted nutrients, and other wastes from point and non-point source flow to water bodies, which affects the quality of the water resources. etc. The increase in pollution influences the quantity and quality of water, which results high risk on health and other issues for human as well as for living organisms on the planet. Hence, evaluating and monitoring the quality of water, and its prediction become crucial and applicable area for research in the current scenario. In various researchers they have used traditional approaches; Now, they are using technologies like machine learning, big data analytics for evaluation and prediction of water quality. The advanced big data implementation using sensor networks and machine learning with the data related to environment, aids in building water quality prediction models. This paper analyses various prediction models developed using machine learning and big data techniques and their experimental results of water prediction and evaluation. Various challenges and issues are reviewed and possible solutions to some research issues are proposed.",2021,
118,10.1109/ICEET53442.2021.9659660,AN IMPROVE THE QUALITY OF DATA CONSIDERING BIG DATA ASPECT BASED ON SENSITIVE OF COST TIME,"Big data is term of dataset with characteristic volume, value and veracity that lead to confrontation unable proceed using traditional techniques to extract value, project management perspective is dynamic processing that utilizes the suitable resources of organization in many stages by measuring in four-factor scope, time, cost and quality. In this research aim improve data quality from big data via project management scope consist on high trust which is bring high accuracy from confidence level in volume of data, confidence get with context and value of data which lead to determine accuracy deeply in it and finally select from data depending on veracity of it, the experiment using three main factors time, cost and scope, strongest relation organizing between them start by project scope as strongest one then cost, product and Last one time is weakest between them, in the final when select best quality use two sides mostly from quality degree and be center of quality interval and in particular from closest distance with the strongest factor.",2021,24092983
119,10.1109/BigDataCongress.2017.40,ADDING SUPPORT FOR THEORY IN OPEN SCIENCE BIG DATA,"Open Science Big Data is emerging as an important area of research and software development. Although there are several high quality frameworks for Big Data, additional capabilities are needed for Open Science Big Data. These include data provenance, citable reusable data, data sources providing links to research literature, relationships to other data and theories, transparent analysis/reproducibility, data privacy, new optimizations/advanced algorithms, data curation, data storage and transfer. An important part of science is explanation of results, ideally leading to theory formation. In this paper, we examine means for supporting the use of theory in big data analytics as well as using big data to assist in theory formation. One approach is to fit data in a way that is compatible with some theory, existing or new. Functional Data Analysis allows precise fitting of data as well as penalties for lack of smoothness or even departure from theoretical expectations. This paper discusses principal differential analysis and related techniques for fitting data where, for example, a time-based process is governed by an ordinary differential equation. Automation in theory formation is also considered. Case studies in the fields of computational economics and finance are considered.",2017,
120,10.1109/BigData.2015.7363818,DATA QUALITY ASSESSMENT AND ANOMALY DETECTION VIA MAP/REDUCE AND LINKED DATA: A CASE STUDY IN THE MEDICAL DOMAIN,"Recent technological advances in modern healthcare have lead to the ability to collect a vast wealth of patient monitoring data. This data can be utilised for patient diagnosis but it also holds the potential for use within medical research. However, these datasets often contain errors which limit their value to medical research, with one study finding error rates ranging from 2.3%-26.9% in a selection of medical databases. Previous methods for automatically assessing data quality normally rely on threshold rules, which are often unable to correctly identify errors, as further complex domain knowledge is required. To combat this, a semantic web based framework has previously been developed to assess the quality of medical data. However, early work, based solely on traditional semantic web technologies, revealed they are either unable or inefficient at scaling to the vast volumes of medical data. In this paper we present a new method for storing and querying medical RDF datasets using Hadoop Map / Reduce. This approach exploits the inherent parallelism found within RDF datasets and queries, allowing us to scale with both dataset and system size. Unlike previous solutions, this framework uses highly optimised (SPARQL) joining strategies, intelligent data caching and the use of a super-query to enable the completion of eight distinct SPARQL lookups, comprising over eighty distinct joins, in only two Map / Reduce iterations. Results are presented comparing both the Jena and a previous Hadoop implementation demonstrating the superior performance of the new methodology. The new method is shown to be five times faster than Jena and twice as fast as the previous approach.",2015,
121,10.1109/ICISE51755.2020.00019,OPTIMIZATION OF TEACHING CONTENT AND REFORM OF TEACHING METHODS ON THE COURSE OF COAL-GEOLOGY BASED ON BIG DATA ANALYSIS,"Coal-geology course is one of the core courses of resource exploration engineering major in most universities of geology, mining and petroleum. In the process of teaching, we should constantly improve the optimization design of course content, optimize course content and reform teaching methods through big data technology analysis. On the basis of the training goal, requirement, the study purpose and the emphasis difference of resource exploration engineering major, the course has relatively fewer class schedule, obviously, fewer theoretical and experimental classes, it increases the difficulty of teachers to master the teaching content of the course. Since the content of the course is the basis of ensuring the teaching goal, talent training and implementation process, therefore, the content of the course should be targeted and selected, we should grasp flexibly the big data features of systematicness, scientificalness and progressiveness of courses content, improving students' learning motivation and active participation, making full use of information technology, constantly innovating teaching methods, improving teaching quality, promoting teaching level and students' comprehensive quality, in this way, we can ensure the unity of teaching process, optimization design and teaching efficiency.",2020,
122,10.1109/APSEC53868.2021.00052,NEOMYCELIA: A SOFTWARE REFERENCE ARCHITECTUREFOR BIG DATA SYSTEMS,"The big data revolution began when the volume, velocity, and variety of data completely overwhelmed the systems used to store, manipulate and analyze that data. As a result, a new class of software systems emerged called big data systems. While many attempted to harness the power of these new systems, it is estimated that approximately 75% of the big data projects have failed within the last decade. One of the root causes of this is software engineering and architecture aspect of these systems. This paper aims to facilitate big data system development by introducing a software reference architecture. The work provides an event driven microservices architecture that addresses specific limitations in current big data reference architectures (RA). The artefact development has followed the principles of empirically grounded RAs. The RA has been evaluated by developing a prototype that solves a real-world problem in practice. At the end, succesful implementation of the reference architecture have been presented. The results displayed a good degree of applicability with respect to Quality factors.",2021,15301362
123,10.1109/TPDS.2019.2921337,EFFICIENT DATA PLACEMENT AND REPLICATION FOR QOS-AWARE APPROXIMATE QUERY EVALUATION OF BIG DATA ANALYTICS,"Enterprise users at different geographic locations generate large-volume data that is stored at different geographic datacenters. These users may also perform big data analytics on the stored data to identify valuable information in order to make strategic decisions. However, it is well known that performing big data analytics on data in geographical-located datacenters usually is time-consuming and costly. In some delay-sensitive applications, the query result may become useless if answering a query takes too long time. Instead, sometimes users may only be interested in timely approximate rather than exact query results. When such approximate query evaluation is the case, applications must sacrifice timeliness to get more accurate evaluation results or tolerate evaluation result with a guaranteed error bound obtained from analyzing the samples of the data to meet their stringent timeline. In this paper, we study quality-of-service (QoS)-aware data replication and placement for approximate query evaluation of big data analytics in a distributed cloud, where the original (source) data of a query is distributed at different geo-distributed datacenters. We focus on the problems of placing data samples of the source data at some strategic datacenters to meet stringent query delay requirements of users, by exploring a non-trivial trade-off between the cost of query evaluation and the error bound of the evaluation result. We first propose an approximation algorithm with a provable approximation ratio for a single approximate query. We then develop an efficient heuristic algorithm for evaluating a set of approximate queries with the aim to minimize the evaluation cost while meeting the delay requirements of these queries. We finally demonstrate the effectiveness and efficiency of the proposed algorithms through both experimental simulations and implementations in a real test-bed, real datasets are employed. Experimental results show that the proposed algorithms are promising.",2019,21619883
124,10.1109/BDCloud-SocialCom-SustainCom.2016.26,SURVEILLANCE SOURCE COMPRESSION WITH BACKGROUND MODELING FOR VIDEO BIG DATA,"Video source is a kind of critical component in big data. As a typical video source, the cost of storage and transmission is extremely high for surveillance source. Thus, it is the coding target to get a tradeoff between bit-rate and visual quality for surveillance video. Devoted to this subject, this work proposes a new background modeling scheme for surveillance source, which adopts the residual gradient and the block edge differences to construct background picture. The constructed background picture preserves the spatial characteristics of the source contents and then it is chosen as the long-term reference picture. This work proposes a novel background-based coding optimization algorithm (BCOA) for both picture level and the largest coding unit (LCU) level in video compression. According to the effective adjustment of quantization parameter (QP) and lagrange multiplier (λ), the proposed BCOA improves the visual quality. Compared with the background-modeling-based hierarchical prediction structure optimization, experimental results show that BCOA achieves better visual quality and BD-Rate gain up to 46.68%.",2016,
125,10.1109/ICSSE.2019.8823437,DATA VISUALIZATION FOR AIR QUALITY ANALYSIS ON BIGDATA PLATFORM,"With the advances of industry, air pollution is increasingly becoming serious, and most of governments in the world has deployed many devices to monitor daily air quality. Monitoring and forecasting of air quality has also become an important issue to improve the quality of people's lives. As far as we know, bad air quality does not only affect the health of the respiratory tract, it may but also even cause mental illness. Many researchers have investigated different approaches to work on air quality forecast, and the visualization of forecasting becomes important. In this paper, we present an architecture for visualizing forecasted air quality on a big data platform. We implemented an ETL (Extract-Transform-Load) based framework in the platform, which includes computing nodes and storage nodes. Computational nodes are used for data collection and for air quality forecasting over the next 1 to 8 hours through machine learning and deep learning. Storage nodes are used to retrieve, analyze, and preprocess of collected data. We use the RESTful Web Service as an API, and finally we use the browser to get the data by predefined API and to present the forecasted and monitored results with Google Map API and D3 JavaScript library. It reveals that the visualization on big data framework can work well for air quality analysis.",2019,23250909
126,10.1109/ICTech55460.2022.00081,A BIG DATA BASED ANALYSIS OF ACCURATE OPERATION FOR USER MULTIDIMENSIONAL VALUE IDENTIFICATION,"In the development of Internet technology, the state grid enterprises of electric power began to use big data analysis technology to identify the multi-dimensional value of users at the same time of technological innovation, and put forward more accurate marketing operation countermeasures. Because the electricity customers in the electricity market belong to a relatively large group, so the analysis service based on the actual electricity consumption of customers, power demand and other content can not only provide effective basis for the actual management decision, but also improve the operation quality and efficiency of the computer system. Therefore, on the basis of understanding the functions and technical implementation of precision marketing platform based on big data technology, this paper conducts in-depth research on the power butler service model of residential customers based on cluster analysis of user types, and finally conducts empirical analysis on the basis of constructing ADTM-AI model. The results show that the stochastic forest classification method is more suitable to identify the multi-dimensional value of users in the state grid of electric power, and can provide effective basis for the accurate operation of the actual system.",2022,
127,10.1109/ACCESS.2020.3009006,BIG DATA-DRIVEN ABNORMAL BEHAVIOR DETECTION IN HEALTHCARE BASED ON ASSOCIATION RULES,"Healthcare insurance frauds are causing millions of dollars of public healthcare fund losses around the world in various ways, which makes it very important to strengthen the management of medical insurance in order to guarantee the steady operation of medical insurance funds. Healthcare fraud detection methods can reduce the losses of healthcare insurance funds and improve medical quality. Existing fraud detection studies mostly focus on finding normal behavior patterns and treat those violating normal behavior patterns as fraudsters. However, fraudsters can often disguise themselves with some normal behaviors, such as some consistent behaviors when they seek medical treatments. To address these issues, we combined a MapReduce distributed computing model and association rule mining to propose a medical cluster behavior detection algorithm based on frequent pattern mining. It can detect certain consistent behaviors of patients in medical treatment activities. By analyzing 1.5 million medical claim records, we have verified the effectiveness of the method. Experiments show that this method has better performance than several benchmark methods.",2020,21693536
128,10.1109/ICITBS49701.2020.00149,RESEARCH ON TAX COLLECTION AND ADMINISTRATION BASED ON BIG DATA ANALYSIS,"Utilizing big data is the main trend in tax collection and administration. In the era of big data, to enhance application effect of big data in tax collection and administration, we must try our best to excavate big data. First, the work mode of tax collection and administration needs to be changed, and management force of taxation work process should be strengthened. Then, according to the current status of enterprise development, a comprehensive big data application system should be formulated, to allow enterprise personnel to grasp various tax-related information fully. Constructing a set of tax management systems that are compatible with the development of the enterprise, and paying attention to the use and research, quality and efficiency of tax administration can be steadily improved.",2020,
129,10.1109/OAJPE.2022.3197553,APPLICATION OF BIG DATA ANALYTICS AND MACHINE LEARNING TO LARGE-SCALE SYNCHROPHASOR DATASETS: EVALUATION OF DATASET ‘MACHINE LEARNING-READINESS’,"This manuscript presents a data quality analysis and holistic ‘machine learning-readiness’ evaluation of a representative set of large-scale, real-world phasor measurement unit (PMU) datasets provided under the United States Department of Energy-funded FOA 1861 research program. A major focus of this study is to understand the present-day suitability of large-scale, real-world synchrophasor datasets for application of commercially-available, off-the-shelf big data and supervised or semi-supervised machine learning (ML) tools and catalogue any major obstacles to their application. To this end, dataset quality is methodically examined through an interconnect-wide quantifications of basic bad data occurrences, a summary of several harder-to-detect data quality issues that can jeopardize successful application of machine learning, and an evaluation of the adequacy of event log labeling for supervised training of models used for online event classification. A global ‘six-point’ statistical analyses of several key dataset variables is demonstrated as a means by which to identify additional hard-to-detect data quality issues, also providing an example successful application of big data technology to extract insights regarding reasonable operational bounds of the US power system. Obstacles for application of commercial ML technologies are summarized, with a particular focus on supervised and semi-supervised ML. Lessons-learned are provided regarding challenges associated with present-day event labeling practices, large spatial scope of the dataset, and dataset anonymization. Finally, insight into efficacy of employed mitigation strategies are discussed, and recommendations for future work are made.",2022,26877910
130,10.1109/ICAICA50127.2020.9182539,BIG DATA ANALYSIS MODEL OF CUSTOMER APPEAL BASED ON POWER ENTERPRISE SERVICE PLATFORM,"Today is the era of Internet information. Affected by economic development and living standards, power users have higher and higher expectations for power supply services. Although customer complaints are inevitable, as an important part of customer feedback in high-quality services, how to use customer complaint information efficiently, reasonably, and scientifically has become a problem that every power grid company must face in the Internet + era. The purpose of this article is to study the big data analysis model of customer demands based on the electric power enterprise service platform. This article first introduces the meaning of customer satisfaction and customer demands, and then analyzes the development status of the power enterprise service platform, and proposes the need to improve customer demand management. Based on this, this article establishes a big data analysis model of customer demands. The experimental results prove that the analysis model designed in this paper can not only solve the needs of power enterprises, but also improve the economic benefits of enterprises. In this paper, the economic benefits obtained from the four indicators of failure repair service, power outage information management norms, business process norms and customer service representatives' work efficiency improvement, and the statistical results of a power company using the model one year later are 5.25 million yuan.",2020,
131,10.1109/BigData.2015.7364058,A MEMORY CAPACITY MODEL FOR HIGH PERFORMING DATA-FILTERING APPLICATIONS IN SAMZA FRAMEWORK,"Data quality is essential in big data paradigm as poor data can have serious consequences when dealing with large volumes of data. While it is trivial to spot poor data for small-scale and offline use cases, it is challenging to detect and fix data inconsistency in large-scale and online (real-time or near-real time) big data context. An example of such scenario is spotting and fixing poor data using Apache Samza, a stream processing framework that has been increasingly adopted to process near-real-time data at LinkedIn. To optimize the deployment of Samza processing and reduce business cost, in this work we propose a memory capacity model for Apache Samza to allow denser deployments of high performing data-filtering applications built on Samza. The model can be used to provision just-enough memory resource to applications by tightening the bounds on the memory allocations. We apply our memory capacity model on Linkedln's real use cases in production, which significantly increases the deployment density and saves business costs. We will share key learning in this paper.",2015,
132,10.1109/MET.2019.00019,ADDRESSING DATA QUALITY PROBLEMS WITH METAMORPHIC DATA RELATIONS,"In the era of big data, cloud computing and the Internet of Things, the quality of data has tremendous impact on our everyday life. Moreover, the increasing velocity, volume and variety of data requires new approaches for quality assessment. In this paper, a new approach for quality assessment is presented that applies metamorphic testing to data quality. The exemplary application of the approach on a big data application shows promising results for the suitability of the approach.",2019,
133,10.1109/ICACCCN.2018.8748855,ANALOGOUS EXAMINATION OF VARIOUS MACHINE LEARNING ALGORITHM APPLIED TO BIG DATA,"All domains from criminal justice to real estate to health care have adopted Big Data analytics forreaping multifold benefits. Prime need of business is actionable data. Big Data analysis business results and suggests future plans. It's application ranges from tracking everything from crime to weather to shopping to brands. Distinguishing factor here is Big Data's capacity for dealing with vast quantities of real-time unstructured data. Big Data Analytics along with machine learning helps in making big impact on service quality and customer satisfaction. In this work we explore the machine learning models for estimating the absolute quality of a model. On the basis of these properties, a candid evaluation of these models brings out the relative merits of all the models.",2018,
134,10.1109/ICRIS52159.2020.00106,COMPUTER E-COMMERCE SECURITY SYSTEM UNDER THE BACKGROUND OF BIG DATA,"In order to solve various problems of the current computer e-commerce platform, such as low security, poor interaction, low quality of service personnel, low level of information technology, this paper proposes a new computer e-commerce security system under the background of big data. The system combines big data technology, gives full play to the function of mining information and collecting information of big data technology, strengthens the combination of big data technology and Internet technology, realizes the interaction between buyers and sellers, and improves the satisfaction of users. Based on this, the system can also make full use of big data technology to solve the security problem of e-commerce platform, ensure the security of users' personal information, and reduce the risk of transaction activities. The experimental results show that the system can solve most of the current computer e-commerce platform problems, such as low security, poor interaction, and so on, so as to provide a good trading environment for buyers and sellers.",2020,
135,10.1109/ICAICT.2016.7991657,BATCH CLUSTERING ALGORITHM FOR BIG DATA SETS,"Vast spread of computing technologies has led to abundance of large data sets. Today tech companies like, Google, Facebook, Twitter and Amazon handle big data sets and log terabytes, if not petabytes, of data per day. Thus, there is a need to find similarities and define groupings among the elements of these big data sets. One of the ways to find these similarities is data clustering. Currently, there exist several data clustering algorithms which differ by their application area and efficiency. Increase in computational power and algorithmic improvements have reduced the time for clustering of big data sets. But it usually happens that big data sets can't be processed whole due to hardware and computational restrictions. In this paper, the classic k-means clustering algorithm is compared to the proposed batch clustering (BC) algorithm for the required computation time and objective function. The BC algorithm is designed to cluster large data sets in batches but maintain the efficiency and quality. Several experiments confirm that batch clustering algorithm for big data sets is more efficient in using computational power, data storage and results in better clustering compared to k-means algorithm. The experiments are conducted with the data set of 2 (two) million two-dimensional data points.",2016,24728586
136,10.1109/ICICCT.2018.8472999,A SURVEY ON BIG DATA APPLICATIONS AND CHALLENGES,"Big data defines huge, diverse and fast growing data which requires new technologies to handle. With the rapid growth of data, big data has brought attention of researchers to use it in most prominent way for decision making in various emerging applications. These huge data is extremely useful and valuable for scientific exploration, increase productivity in business and improvement in mankind. It helps from public sector to business activities, healthcare to better navigation, smart cities to national security. Though, with large opportunities to work, the challenges are to handle these data is also increased. In this paper basic of big data with its application and challenges have been discussed. These challenges are also inherent from verity, volume and velocity of data. However if we can manage this issues related to big data then there will be potential improvement in quality of our lives.",2018,
137,10.1109/ICAIBD.2019.8836982,BIG DATA DRIVEN SMART AGRICULTURE: PATHWAY FOR SUSTAINABLE DEVELOPMENT,"Increasing agricultural production is top most solution in the face of rapid population growth through digitalization of agriculture by using most developed technology like big data. There is a long debate on the application of big data in agriculture. This study is an attempt to explore the suitability of the big data technologies for increasing production and improving quality in agriculture. The study uses an extensive review of current research works and studies in agriculture for exploring the best and compatible practices which can help farmers at field level for increasing production and improving quality. This study reveals a number of available big data technologies and practices in agriculture for solving the current problems and challenges at field level. A conceptual model is developed for proper implementation of available big data technologies at farmer's field level. The study highlights data generation procedure, availability of technology, availability of hardware, software, data collection techniques, method of analysis and suitability of application of big data technologies for smart agriculture. The article explores that there are still some challenges exists in this field as a new domain in agriculture like privacy of data, data quality, availability, initial investment, infrastructure and related expertise. The study suggests that government initiatives, public-private partnership, openness of data, financial investment and regional basis research work are necessary for implementing the big data technologies in agriculture at large scale.",2019,
138,10.1109/ICDE.2014.6816764,DATA QUALITY: THE OTHER FACE OF BIG DATA,"In our Big Data era, data is being generated, collected and analyzed at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. Recent studies have shown that poor quality data is prevalent in large databases and on the Web. Since poor quality data can have serious consequences on the results of data analyses, the importance of veracity, the fourth `V' of big data is increasingly being recognized. In this tutorial, we highlight the substantial challenges that the first three `V's, volume, velocity and variety, bring to dealing with veracity in big data. Due to the sheer volume and velocity of data, one needs to understand and (possibly) repair erroneous data in a scalable and timely manner. With the variety of data, often from a diversity of sources, data quality rules cannot be specified a priori; one needs to let the “data to speak for itself” in order to discover the semantics of the data. This tutorial presents recent results that are relevant to big data quality management, focusing on the two major dimensions of (i) discovering quality issues from the data itself, and (ii) trading-off accuracy vs efficiency, and identifies a range of open problems for the community.",2014,2375026X
139,10.1109/BigData47090.2019.9006604,DECODER TRANSFER LEARNING FOR PREDICTING PERSONAL EXPOSURE TO AIR POLLUTION,"Personal air quality is an important indicator when assessing the impact of air pollution on personal health. Because personal air quality data are collected manually, it difficult to collect such data in large quantities. The main challenge facing personal air quality predictions is building an effective prediction model with a small amount of training data. Moreover, public atmospheric monitoring stations in urban areas have collected large quantities of air quality data. Therefore, we focus on using atmospheric monitoring data with a transfer-learning method to predict personal air quality. In this paper, we design a transferlearning framework based on an encoder-decoder structure. This transfer-learning framework uses the Wasserstein distance to match the heterogeneous distribution of the source domain (the data from the atmospheric monitoring stations) and the target domain (the personal air quality); we refer to this as decoder transfer learning (DTL). We use data from public atmospheric monitoring stations, collected by the Atmospheric Environmental Regional Observation System (AEROS) of Japan, as the source domain dataset and private datasets collected in Fujisawa, Japan, and Tokyo, Japan, as the target domain datasets to evaluate this approach. The experimental results demonstrate that compared with the inverse distance weighting (IDW), IDW with linear regression, and typical transfer-learning models, the proposed DTL framework demonstrates a significant improvement in prediction performance.",2019,
140,10.1109/Agro-Geoinformatics.2017.8046998,LANDQV1: A GIS CLUSTER-BASED MANAGEMENT INFORMATION SYSTEM FOR ARABLE LAND QUALITY BIG DATA,"In the era of spatial big data, geographic information system (GIS) faces many opportunities and challenges. The first challenge for future GIS is how to store and manage the spatial big data efficiently. For example, in 2013, the volume of Chinese arable land quality (ALQ) dataset is up to 2.51TB with ESRI Shapefile format, and traditional GIS development pattern with standalone version is not meeting the needs including storage, query, analysis and visualization. To solve above problems, in this paper, we present a system framework, LandQv1, based on the GIS cluster to support arable land quality big data management and analysis in geospatial domain. Firstly, it describes the design of the system architecture with three layers in details, and implemented by different technologies accordingly. Secondly, three models, data storage model, service release model, and data calling model, are developed to solve the key problems of each layer in the system framework. And then, LandQv1 is developed with the WPF, GIS cluster, Oracle database and C# language. Finally, through application and system test, the results show that LandQv1 with GIS map tools, data query and other functions can be meted the needs in high performance, which will lay the foundation for arable land big data analyzing in the future.",2017,
141,10.1109/BDC.2014.10,A SCALABLE DATA SCIENCE WORKFLOW APPROACH FOR BIG DATA BAYESIAN NETWORK LEARNING,"In the Big Data era, machine learning has more potential to discover valuable insights from the data. As an important machine learning technique, Bayesian Network (BN) has been widely used to model probabilistic relationships among variables. To deal with the challenges of Big Data PN learning, we apply the techniques in distributed data-parallelism (DDP) and scientific workflow to the BN learning process. We first propose an intelligent Big Data pre-processing approach and a data quality score to measure and ensure the data quality and data faithfulness. Then, a new weight based ensemble algorithm is proposed to learn a BN structure from an ensemble of local results. To easily integrate the algorithm with DDP engines, such as Hadoop, we employ Kepler scientific workflow to build the whole learning process. We demonstrate how Kepler can facilitate building and running our Big Data BN learning application. Our experiments show good scalability and learning accuracy when running the application in real distributed environments.",2014,
142,10.1109/BigDataCongress.2015.68,H-DRIVE: A BIG HEALTH DATA ANALYTICS PLATFORM FOR EVIDENCE-INFORMED DECISION MAKING,"Healthcare operations generates large volumes of data. Big data analytics methods are needed to derive actionable and decision-quality 'intelligence' from 'big' healthcare data in order to improve patient care. Given the technical challenges to big health data analytics, in this paper we present a specialized health analytics platform -- H-DRIVE (Health Data Reconciliation Inferencing and Visualization Environment). H-DRIVE is an integrated, end-to-end health data analytics service-oriented workbench designed to empower data analysts and researchers to design analytical experiments and then perform complex analytics on their health data. We present the high-level functional and technical architecture of H-DRIVE. As a case study, we demonstrate the application of H-DRIVE in the context of optimizing the operations of a provincial pathology lab, where we analyze province-wide lab orders to prepare scorecards outlining physician lab testing performance and offer an operational dashboard to provide an overview of lab utilization.",2015,23797703
143,10.1109/ICTech55460.2022.00079,PROBLEMS AND STRATEGIES IN THE PROCESS OF NETWORK MARKETING TOWARDS PRECISION IN THE CONTEXT OF BIG DATA,"The sudden outbreak of COVID-19 has greatly affected the development of all industries, and the development of many enterprises has been severely impacted. In the context of epidemic prevention and control, the Internet has brought new development space for enterprise marketing, so more and more enterprises begin to enter the field of online marketing. With the continuous progress of Internet technology and the deepening of informatization, China's big data industry has made qualitative progress. The traditional marketing model cannot meet the needs of the increasingly fierce market competition. By applying big data technology to network marketing, enterprises can dig deeply into user information and formulate corresponding marketing strategies based on users' preferences, behavior patterns and shopping habits, so as to realize precise marketing and improve their economic benefits by mining potential customers. However, there are also some problems in the process of using big data technology to move towards precision, such as serious homogenization, low application level, and privacy security issues. Only by solving these problems can enterprises use big data to achieve higher quality development.",2022,
144,10.1109/CLOUD.2019.00039,IMPROVING BIG DATA APPLICATION PERFORMANCE IN EDGE-CLOUD SYSTEMS,"Data analysis is widely used in all domains of the economy. While the amount of data to process grows, the time criteria and the resource consumption constraints get stricter. These phenomena call for advanced resource orchestration for the big data applications. The challenge is actually even greater at the advent of edge computing: orchestration of big data resources in a hybrid edge-cloud infrastructure is challenging. The difficulty stems from the fact that wide-area networking and all its well-known issues come into play and affect the performance of the application. In this paper we present the steps we made towards network-aware big data application design over such distributed systems. We propose a HDFS block placement algorithm for the network reliability problem we identify in geographically distributed topologies. The heuristic algorithm we propose provides better big data application performance compared to the default block placement method. We implement our solution in our simulation environment and show the improved quality of big data applications.",2019,21596182
145,10.1109/ICECE51594.2020.9352886,REGIONAL ELECTRICITY SALES FORECASTING RESEARCH BASED ON BIG DATA APPLICATION SERVICE PLATFORM,"Regional monthly electricity sales forecast is an important basis for regional power grid planning and construction, evaluation of regional economic development and operation, and protection of residents' lives. It is also an important work of regional power regulation and management, decision-making of power generation and purchase, improvement of power supply equipment utilization rate and deepening of power system reform. Based on the current situation of power supply enterprise information development, distribution network business status and characteristics, this paper analyzes the factors affecting electricity sales. According to the characteristics of annual changes in electricity sales and data quality factors, the recurrent neural network model is selected based on the big data application service platform. The long short term memory neural network model performs multi-step multivariate prediction on time series, and uses the attention mechanism to combine two independent models for prediction. Experiments conducted on the historical electricity sales data set of a power supply company show that compared with traditional machine learning methods, this method has advantages in accuracy and efficiency.",2020,
146,10.1109/ISCEIC53685.2021.00021,A MULTI-MODE LEARNING BEHAVIOR REAL-TIME DATA ACQUISITION METHOD BASED ON DATA QUALITY,"With the rapid development of new technologies such as artificial intelligence, big data, and the Internet of Things, many researchers have probed into the study of learning analysis, trying to solve the problems of teaching by analyzing the learning behavior data from learning process. And in many learning behavior research, the sensor network usually consists of a host of mutually independent data sources, which can be used to monitor measured objects from multiple dimensions thereby obtaining the multi-source multi-modal sensory data. However, there still exist false negative readings, false positive readings and environmental interference, etc. Therefore, we propose a multi-source multimode sensory data acquisition method based on Date Quality(DQ). We first define the data quality in terms of four aspects-accuracy, integrity, consistency and instantaneity. Then, by the modeling there aspects respectively, we propose metrics to estimate the comprehensive data quality method of multi-source multi-mode sensory data. Finally, a data acquisition method is presented based on data quality, which selects a part of data sources for data transmission according to the given precision. This method aims at reducing the consumption of the sensory network on the premise of the data quality guarantee. An extensive experimental evaluation demonstrates the efficiency and effectiveness of the algorithm.",2021,
147,10.1109/BigData50022.2020.9378313,APPLICATION OF DEEP LEARNING IN RECOGNIZING BATES NUMBERS AND CONFIDENTIALITY STAMPING FROM IMAGES,"In eDiscovery, it is critical to ensure that each page produced in legal proceedings conforms with the requirements of court or government agency production requests. Errors in productions could have severe consequences in a case, putting a party in an adverse position. The volume of pages produced continues to increase, and tremendous time and effort has been taken to ensure quality control of document productions. This has historically been a manual and laborious process. This paper demonstrates a novel automated production quality control application which leverages deep learning-based image recognition technology to extract Bates Number and Confidentiality Stamping from legal case production images and validate their correctness. Effectiveness of the method is verified with an experiment using a real-world production data.",2020,
148,10.1109/ICBDA.2018.8367644,THE RESEARCH ON INDUSTRIAL BIG DATA INFORMATION SECURITY RISKS,"With the industrial data digitized and across the enterprise boundaries and even across borders, industrial big data information security issues become a heated discussion. Due to the lack of mature laws and regulations, criterion and technical achievements, the industrial big data faces an increasingly severe security situation in the initial stage. Therefore it has important strategic significance for industrial big data information security research. In the view of characteristic analysis on security risks, the paper focus on relative security factors existing on life cycle of data acquisition, storage, transmission, decision making and control phases; it illustrates assessment and identification methods on industrial big data information security risk and put forward corresponding measurements and suggestions on information security risk aversion, so as to against potential security threats hidden in industrial systems and make better use of the value of industrial big data.",2018,
149,10.1109/CAC.2017.8243651,DEEP-LEVEL QUALITY MANAGEMENT BASED ON BIG DATA ANALYTICS WITH CASE STUDY,"The Big data analytics gives new chances to the enterprises to enhance their management and manufacturing levels. A solution with case study is proposed to accomplish deep-level quality management based on big data analytics. First, the implementation of big data analytics based on industrial process data is illustrated with case study illustration. Through the analysis and feature extraction of off-line data, the corresponding reference model library is constructed, which can be used for real-time processing of unlabeled data in the industrial field. The cluster, outlier and other data indicators we can get from the calculation and analysis would have a certain guiding significance for the enterprise's deep-level quality management. Then, the distributed memory computing engine based on Spark, and the implementation of web application platform based on Spring MVC framework is also described. This can help to get higher performance and unrestricted abilities for data analytics. Meanwhile, good visibility and human-data interface can be achieved.",2017,
150,10.1109/BDCloud-SocialCom-SustainCom.2016.34,A HYBRID OUTLIER DETECTION METHOD FOR HEALTH CARE BIG DATA,"Technology advancements in health care informatics, digitalizing health records, and telemedicine has resulted in rapid growth of health care data. One challenge is how to effectively discover useful and important information out of such massive amount of data through techniques such as data mining. Outlier detection is a typical technique used in many fields to analyze big data. However, for the large scale and high-dimensional heath care data, the conventional outlier detection methods are not efficient. This paper proposes a novel hybrid outlier detection method, namely, Pruning-based K-Nearest Neighbor (PB-KNN), which integrates the density-based, cluster-based methods and KNN algorithm to conduct effective outlier detection. The proposed PB-KNN adopts the case classification quality character (CCQC) as the medical quality evaluation model and uses the attribute overlapping rate (AOR) algorithm for data classification and dimensionality reduction. To evaluate the performance of the pruning operations in PB-KNN, we conduct extensive experiments. The experiment results show that the PB-KNN method outperforms the k-nearest neighbor (KNN) and local outlier factor (LOF) in terms of the accuracy and efficiency.",2016,
151,10.1109/TKDE.2018.2809747,TENSOR-BASED BIG DATA MANAGEMENT SCHEME FOR DIMENSIONALITY REDUCTION PROBLEM IN SMART GRID SYSTEMS: SDN PERSPECTIVE,"Smart grid (SG) is an integration of traditional power grid with advanced information and communication infrastructure for bidirectional energy flow between grid and end users. A huge amount of data is being generated by various smart devices deployed in SG systems. Such a massive data generation from various smart devices in SG systems may lead to various challenges for the networking infrastructure deployed between users and the grid. Hence, an efficient data transmission technique is required for providing desired QoS to the end users in this environment. Generally, the data generated by smart devices in SG has high dimensions in the form of multiple heterogeneous attributes, values of which are changed with time. The high dimensions of data may affect the performance of most of the designed solutions in this environment. Most of the existing schemes reported in the literature have complex operations for the data dimensionality reduction problem which may deteriorate the performance of any implemented solution for this problem. To address these challenges, in this paper, a tensor-based big data management scheme is proposed for dimensionality reduction problem of big data generated from various smart devices. In the proposed scheme, first the Frobenius norm is applied on high-order-tensors (used for data representation) to minimize the reconstruction error of the reduced tensors. Then, an empirical probability-based control algorithm is designed to estimate an optimal path to forward the reduced data using software-defined networks for minimization of the network load and effective bandwidth utilization. The proposed scheme minimizes the transmission delay incurred during the movement of the dimensionally reduced data between different nodes. The efficacy of the proposed scheme has been evaluated using extensive simulations carried out on the data traces using `R' programming and Matlab. The big data traces considered for evaluation consist of more than two million entries (2,075,259) collected at one minute sampling rate having hetrogenous features such as-voltage, energy, frequency, electric signals, etc. Moreover, a comparative study for different data traces and a real SG testbed is also presented to prove the efficacy of the proposed scheme. The results obtained depict the effectiveness of the proposed scheme with respect to the parameters such asnetwork delay, accuracy, and throughput.",2018,23263865
152,10.1109/IWCIM.2015.7347061,COMPUTING DATA QUALITY INDICATORS ON BIG DATA STREAMS USING A CEP,"Big Data is often referred to as the 3Vs: Volume, Velocity and Variety. A 4th V (validity) was introduced to address the quality dimension. Poor data quality can be costly, lead to breaks in processes and invalidate the company's efforts on regulatory compliance. In order to process data streams in real time, a new technology called CEP (complex event processing) was developed. In France, the current deployment of smart meters will generate massive electricity consumption data. In this work, we developed a diagnostic approach to compute generic quality indicators of smart meter data streams on the fly. This solution is based on Tibco StreamBase CEP. Visualization tools were also developed in order to give a better understanding of the inter-relation between quality issues and geographical/temporal dimensions. According to the application purpose, two visualization methods can be loaded: (1) StreamBase LiveView is used to visualize quality indicators in real time; and (2) a Web application provides a posteriori and geographical analysis of the quality indicators which are plotted on a map within a color scale (lighter colors indicate good quality and darker colors indicate poor quality). In future works, new quality indicators could be added to the solution which can be applied in an operational context in order to monitor data quality from smart meters.",2015,
153,10.23919/ICN.2021.0002,TRUE-DATA TESTBED FOR 5G/B5G INTELLIGENT NETWORK,"Future beyond fifth-generation (B5G) and sixth-generation (6G) mobile communications will shift from facilitating interpersonal communications to supporting internet of everything (IoE), where intelligent communications with full integration of big data and artificial intelligence (AI) will play an important role in improving network efficiency and provi di ng hi gh-quality servi ce. As a rapi d evolvi ng paradi gm, the AI-empowered mob i le communi cati ons demand large amounts of data acquired from real network environment for systematic test and verification. Hence, we build the world's first true-data testbed for 5G/B5G intelligent network (TTIN), which comprises 5G/B5G on-site experimental networks, data acquisition & data warehouse, and AI engine & network optimization. In the TTIN, true network data acquisition, storage, standardization, and analysis are available, which enable system-level online verification of B5G/6G-orientated key technologies and support data-driven network optimization through the closed-loop control mechanism. This paper elaborates on the system architecture and module design of TTIN. Detailed technical specifications and some of the established use cases are also showcased.",2021,27086240
154,10.1109/EISIC.2016.010,QUALITY ASSURANCE FOR SECURITY APPLICATIONS OF BIG DATA,"The quality of inferences drawn from data, big or small, is heavily dependent on the quality of the data and the quality of the processes applied to it. Big data analytics is emerging from laboratories and being applied to intelligence and security needs. To achieve confidence in the outcomes of these applications, a quality assurance framework is needed. This paper outlines the challenges, and draws attention to the consequences of misconceived and misapplied projects. It presents key aspects of the necessary risk assessment and risk management approaches, and suggests opportunities for research.",2016,
155,10.1109/ICVRIS.2018.00027,A NOVEL ARTWORK DESIGN METHOD BASED ON BIG DATA TECHNOLOGY,"How to fully the big data technology to improve the performance of artwork design is of great importance in computer vision. In this paper, we propose a novel artwork design method based on big data technology. Firstly, we provide an overview of the big data technology, which is made up of data acquisition and cloud application technology, data processing and distributed processing technology, data storage and storage technology, and data formation results and perception technology. Secondly, we discuss how to design the artwork product using the big data technology, and the proposed method is implemented by surface deformation, which is solved by an optimization problem. Finally, some examples are given to show that the proposed method can achieve realistic effects and can fully exploit the big data technology and computer vision technology to generate high quality artwork design.",2018,
156,10.1109/BigData50022.2020.9377900,SYNCHRONIZED PREPROCESSING OF SENSOR DATA,"Sensor data whether collected for machine learning, deep learning or other applications must be preprocessed to fit input requirements or improve performance and accuracy. Data preparation is an expensive, resource consuming and complex phase often performed centrally on raw data for a specific application. The dataflow between the edge and the cloud can be enhanced in terms of efficiency, reliability and lineage by preprocessing the datasets closer to their data sources. We propose a dedicated data preprocessing framework that distributes preprocessing tasks between a cloud stage and two edge stages to create a dataflow with progressively improving quality. The framework handles heterogenous data and dynamic preprocessing plans simultaneously targeting diverse applications and use cases from different domains. Each stage autonomously executes sensor specific preprocessing plans in parallel while synchronizing the progressive execution and dynamic updates of the preprocessing plans with the other stages. Our approach minimizes the workload on central infrastructures and reduces the resources used for transferring raw data from the edge. We also demonstrate that preprocessing data can be sensor specific rather than application specific and thus can be performed prior to knowing a specific application.",2020,
157,10.1109/BigData.2016.7840878,A MULTI-LAYER SOFTWARE ARCHITECTURE FRAMEWORK FOR ADAPTIVE REAL-TIME ANALYTICS,"Highly distributed applications dominate today's software industry posing new challenges for novel software architectures capable of supporting real time processing and analytics. The proposed framework, so called REAXICS, is motivated by the fact that the demand for aggregating current and past big data streams requires new software methodologies, platforms and services. The proposed framework is designed to tackle with data intensive problems in real time environments, via services built dynamically under a fully scalable and elastic Lambda based architecture. REAXICS proposes a multi-layer software platform, based on the lambda architecture paradigm, for aggregating and synchronizing real time and batch processing. The proposed software layers and adaptive components support quality of experience, along with community driven software development. Flexibility and elasticity are targeted by hiding the complexity of bootstrapping and maintaining a multi level architecture, upon which the end user can drive queries over input data streams. REAXICS proposes a flexible and extensible software architecture that can capture users preference at the front-end and adapt the appropriate distributed technologies and processes at the back-end. Such a model enables real time analytics in large-scale data driven cloud-based systems.",2016,
158,10.1109/BigData.2015.7363879,HOW VALUABLE IS YOUR DATA? A QUANTITATIVE APPROACH USING DATA MINING,"Unstructured textual data has grown rapidly in the past two decades in various domains like enterprises, web, scientific, etc. A question that arises naturally when there is such a surfeit of data is: how valuable is a certain piece of data as compared to another? In an enterprise, the answer to this question would determine how valuable said data is to the enterprise. In this paper, we build a framework using data mining that quantifies the value of data. We first identify a specific notion of ""value"" that is motivated by applications in Enterprise unstructured Information Management (EIM). Namely, we posit that for several applications in EIM, the value of unstructured data is determined by the associations it captures between concepts. The more such associations in data, the more valuable it is. Next, we build a framework using data mining that ""counts"" the number of associations in data. Our framework uses clustering and frequent itemsets. It also normalizes for data size. We demonstrate our approach on two of the most widely used text benchmark datasets: Reuters and 20 Newsgroups. Our general intuition is that a corpus of professionally written news articles are more valuable (in the sense of capturing more associations between concepts) than newsgroup postings of variable quality written by non-experts. Our quantitative approach indeed reaches the same inference.",2015,
159,10.1109/HPCC/SmartCity/DSS.2018.00251,EXPLORING METHODS FOR COMPARING SIMILARITY OF DIMENSIONALLY INCONSISTENT MULTIVARIATE NUMERICAL DATA,"When developing multivariate data classification and clustering methodologies for data mining, it is clear that most literature contributions only really consider data that contain consistently the same attributes. There are however many cases in current big data analytics applications where for same topic and even same source data sets there are differing attributes being measured, for a multitude of reasons (whether the specific design of an experiment or poor data quality and consistency). We define this class of data a dimensionally inconsistent multivariate data, a topic that can be considered a subclass of the Big Data Variety research. This paper explores some classification methodologies commonly used in multivariate classification and clustering tasks and considers how these traditional methodologies could be adapted to compare dimensionally inconsistent data sets. The study focuses on adapting two similarity measures: Robinson-Foulds tree distance metrics and Variation of Information; for comparing clustering of hierarchical cluster algorithms (such clusters are derived from the raw multivariate data). The results from experiments on engineering data highlight that adapting pairwise measures to exclude non-common attributes from the traditional distance metrics may not be the best method of classification. We suggest that more specialised metrics of similarity are required to address challenges presented by dimensionally inconsistent multivariate data, with specific applications for big engineering data analytics.",2018,
160,10.1109/ICIBA52610.2021.9687865,RESEARCH AND APPLICATION OF POWER DATA MANAGEMENT KEY TECHNOLOGY,"With the intensified application of power information systems and the advent of the “big data” era, higher requirements are put forward for power data resource management and power data security. Electric power companies have carried out research on key technologies for data management, established a three-level management system at the provincial, prefectural and county levels, built a panoramic view of data resources, a data operation management platform, a data negative list sharing mechanism, and a data security protection mechanism, which were applied to all aspects of data management and data governance. Through the support of business processes, data standards, data quality, etc., it has effectively improved the management efficiency of power data, improved the company's data management level, promoted business collaboration and efficiency.",2021,
161,10.1109/LDAV.2017.8231848,SAMPLING TECHNIQUES TO IMPROVE BIG DATA EXPLORATION,"The success of Big Data relies fundamentally on the ability of a person (the data scientist) to make sense and generate insights from this wealth of data. The process of generating actionable insights, called data exploration, is a difficult and time-consuming task. Data exploration of a big dataset usually requires first generating a small and representative data sample that can be easily plotted and viewed, managed and interpreted to generate insights. However, the literature on the topic hints at data scientists only using random sampling with regular sized datasets and it is unclear what they do with Big Data. In this work, we first show evidence from a survey that random sampling is the only technique commonly used by data scientists to quickly gain insights from a big dataset despite theoretical and empirical evidence from the active learning community that suggests benefits of using other sampling techniques. Second, to evaluate and demonstrate the benefits of other sampling techniques, we conducted an online study with 34 data scientists. These scientists performed a data exploration task to support a classification goal using data samples from more than 2 million records of editing data from Wikipedia articles, generated using different sampling techniques. The study results demonstrate that sampling techniques other than random sampling can generate insights that help to focus on different characteristics of the data, without compromising quality in a data exploration.",2017,
162,10.1109/ICBDACI.2017.8070869,IMPLEMENTATION OF DATA MINING ALGORITHMS ON STUDENT'S DATA USING RAPID MINER,"Data mining offers a new advance to data analysis using techniques based on machine learning, together with the conventional methods collectively known as educational data mining (EDM). Educational Data Mining has turned up as an interesting and useful research area for finding methods to improve quality of education and to identify various patterns in educational settings. It is useful in extracting information of students, teachers, courses, administrators from educational institutes such as schools/colleges/universities and helps to suggest interesting learning experiences to various stakeholders. This paper focuses on the applications of data mining in the field of education and implementation of three widely used data mining techniques using Rapid Miner on the data collected through a survey.",2017,
163,10.1109/IICSPI51290.2020.9332332,DISCOVER THE TAIL STRIKE RISK DURING TAKE-OFF OF AN AIRLINE BASED ON QAR BIG DATA,"In order to help airlines discover the hidden risks of tail strike during the take-off phase, this article uses the quick access record (QAR) big data of the flight operational quality assurance (FOQA) Station of CAAC, and uses the industry-wide QAR data to compare with the QAR data of individual airline to find out whether an airline has outstanding problems, and uses mathematics Statistical t test to verify whether there is a significant difference. This article analyzes the data from July to December 2019 as an example, and finds that the A321 model of a certain airline has the risk of tail strike, that is, the take-off pitch angle is too big. The t-test of mathematical statistics is used to verify that there is a significant difference between the airline's take-off pitch angle and the industry's take-off pitch angle. In addition, the related speed at rotation, the speed at liftoff, and the average pitch rate are also analyzed, and it is found that they also have significant differences. This method can be further extended to other flight quality analysis to find potential safety hazards.",2020,
164,10.1109/ICDEW.2015.7129549,BIG RDF DATA CLEANING,"Without a shadow of a doubt, data cleaning has played an important part in the history of data management and data analytics. Possessing high quality data has been proven to be crucial for businesses to do data driven decision making, especially within the information age and the era of big data. Resource Description Framework (RDF) is a standard model for data interchange on the semantic web. However, it is known that RDF data is dirty, since many of them are automatically extracted from the web. In this paper, we will first revisit data quality problems appeared in RDF data. Although many efforts have been put to clean RDF data, unfortunately, most of them are based on laborious manual evaluation. We will also describe possible solutions that shed lights on (semi-)automatically cleaning (big) RDF data.",2015,
165,10.1109/Innovate-Data.2017.14,BRINGING BIG DATA INTO THE CAR: DOES IT SCALE?,"The increasing velocity of big data captured by various sensors and processed in real-time offers support for a range of new application domains. For car information systems (CIS), data from different sources including IoT needs to be combined to offer an adequate service to the user. In this paper, we introduce a novel CIS big data-centric architecture based on a smart streaming infrastructure integrating data source in and outside of the car. We have created a prototype implementation of this architecture and run several experiments to validate the quality of our solution. Especially, we have examined the fault tolerance of the architecture by systematically introducing failures and evaluating their effects on the car information system. The experimental results show that our solution for a smart data based car information system is both scalable and fault tolerant.",2017,
166,10.26599/BDMA.2018.9020020,QOE-DRIVEN BIG DATA MANAGEMENT IN PERVASIVE EDGE COMPUTING ENVIRONMENT,"In the age of big data, services in the pervasive edge environment are expected to offer end-users better Quality-of-Experience (QoE) than that in a normal edge environment. However, the combined impact of the storage, delivery, and sensors used in various types of edge devices in this environment is producing volumes of high-dimensional big data that are increasingly pervasive and redundant. Therefore, enhancing the QoE has become a major challenge in high-dimensional big data in the pervasive edge computing environment. In this paper, to achieve high QoE, we propose a QoE model for evaluating the qualities of services in the pervasive edge computing environment. The QoE is related to the accuracy of high-dimensional big data and the transmission rate of this accurate data. To realize high accuracy of high-dimensional big data and the transmission of accurate data through out the pervasive edge computing environment, in this study we focused on the following two aspects. First, we formulate the issue as a high-dimensional big data management problem and test different transmission rates to acquire the best QoE. Then, with respect to accuracy, we propose a Tensor-Fast Convolutional Neural Network (TF-CNN) algorithm based on deep learning, which is suitable for high-dimensional big data analysis in the pervasive edge computing environment. Our simulation results reveal that our proposed algorithm can achieve high QoE performance.",2018,20960654
167,10.1002/9781119740780.ch1,BIG DATA ANALYTICS FOR THE INTERNET OF THINGS,"This introduction presents an overview of the key concepts discussed in the subsequent chapters of this book. The book contains high‐quality research articles discussing various aspects of Internet of Things (IoT) data analytics like enabling technologies of IoT data analytics, types of IoT data analytics, challenges in IoT data analytics, etc. It discusses the various supervised and unsupervised machine learning approaches and their highly significant role in the smart analysis of IoT data. The book also discusses the cloud computing framework for IoT data analytics. It unleashes the opportunities created by Deep Learning in IoT data analytics. The book focuses on the use of the concept of personalization to achieve the goal of taking the human‐computer interaction to the next level. It describes the role of IoT generated big data in environmental sustainability. The book highlights the IoT security threats and vulnerabilities.",2021,
168,10.1109/CIPAE53742.2021.00077,MATHEMATICS EDUCATION AND TEACHING BASED ON BIG DATA ANALYSIS,"The velocity of circulation of information continues to increase, the amount of data marks China's fast growth has officially entered the new age of information, the development and popularization of big data to transform human life, but also promote the transformation and upgrade of the various areas, especially large data analysis, for the school education plays an vital role in promoting transformation of education. For the sake of conform to the boom of the era of big data, from the perspective of education, teachers should reform mathematics education and teaching by combining big data analysis, realize the two gradually enter the category of deep cooperation, integrate the conventional teaching experience, and upgrade mathematics education and teaching. It is analyzed in the context of this paper the large data using computer multimedia technology to build a new sports teaching pattern, and by using the formula for calculating the efficiency and the quality is calculated, based on analysis of large data in mathematics education teaching practice in the classroom teaching efficiency and quality of data with 22% and 41% respectively than conventional mathematics education teaching, and further expounded based on analysis of large data mathematics education teaching for students to learn mathematics has a positive role.",2021,
169,10.1109/BigData52589.2021.9671538,A SECURE AND REUSABLE SOFTWARE ARCHITECTURE FOR SUPPORTING ONLINE DATA HARMONIZATION,"Retrospective data harmonization across multiple research cohorts and studies is frequently done to increase statistical power, provide comparison analysis, and create a richer data source for data mining. However, when combining disparate data sources, harmonization projects face data management and analysis challenges. These include differences in the data dictionaries and variable definitions, privacy concerns surrounding health data representing sensitive populations, and lack of properly defined data models. With the availability of mature open-source web-based database technologies, developing a complete software architecture to overcome the challenges associated with the harmonization process can alleviate many roadblocks. By leveraging state-of-the-art software engineering and database principles, we can ensure data quality and enable cross-center online access and collaboration.This paper outlines a complete software architecture developed and customized using the Django web framework, leveraged to harmonize sensitive data collected from three NIH-support birth cohorts. We describe our framework and show how we successfully overcame challenges faced when harmonizing data from these cohorts. We discuss our efforts in data cleaning, data sharing, data transformation, data visualization, and analytics, while reflecting on what we have learned to date from these harmonized datasets.",2021,
170,10.1109/FiCloud.2014.68,A BIG DATA FINANCIAL INFORMATION MANAGEMENT ARCHITECTURE FOR GLOBAL BANKING,"Global investment banks and financial institutions are facing growing data processing demands. These originate not only from increasing regulatory requirements and an expanding variety and disparity of data sources, but also from ongoing pressures in cost reduction without compromising system scalability and flexibility. In this context, the ability to apply promising state-of-the-art big data technologies to extract the maximum value from the vast amounts of the data generated is generating a lot of interest in the financial services industry. In this paper we present a Big Data architecture system design, based in open distributed computing paradigms like Hadoop map-reduce, offering horizontal scalability and no-SQL flexibility while at the same time meeting the stringent quality and resilience requirements of the banking software standards. The proposed architecture is able to consolidate, validate, enrich and process with different Big Data analytics techniques the data gathered from the different source systems as encountered in the banking practice, while at the same time supporting the different data integration, transmission and process orchestration requirements traditionally encountered in a global financial institution.",2014,
171,10.1109/EDOC.2014.21,MANAGING BIG DATA EFFECTIVELY - A CLOUD PROVIDER AND A CLOUD CONSUMER PERSPECTIVE,"Summary form only given. Instrumentation of processes and an organization's environment provides vast amounts of data that can be used to drivedecisions. Next to setting up data collection, supervising data quality, and applying proper methods of analysis, organizationsface the challenge to set up an infrastructure and architecture to do so efficiently and cost-effectively. Virtualized platformssuch as private or public clouds are the method of choice for deployment, in particular for data analyses not occurringconstantly. A cloud provider, either a commercial Cloud company or an IT organization within an enterprise, will like to set upa cloud platform such that clients can run big data workloads effectively on. Cloud customers would like to set up big dataapplications in a cost-effective and performant way on their platform.This keynote will walk through a few real life big data analysis scenarios from different industries and discuss thechallenges Cloud providers face making trade-offs. Understanding those challenges and solutions help cloud users choose theright match between their algorithm, big data system and cloud platform.",2014,15417719
172,10.1109/BigData52589.2021.9672007,PROGRAMMING LANGUAGES IN DATA SCIENCE: A COMPARISON FROM A DATABASE ANGLE,"In a typical Data Science project, the analyst uses many programming languages to explore and analyze big data coming from diverse data sources. A major challenge is managing and pre-processing so much data, with potentially inconsistent content, significant redundancy, in diverse formats, with varying data quality. Database systems research has tackled such problems for a long time, but mostly on relational databases. With such motivation in mind, this paper compares strengths and weaknesses of popular languages used nowadays from a database pespective: Python, R and SQL. We discuss the entire analytic pipeline, going from data integration, cleaning and pre-processing to model application and tuning. From a database systems perspective, we present a comprehensive survey of storage mechanisms, data processing algorithms, external algorithms, run-time memory management, consistency, optimizations and parallel processing. From a programming languages angle, we consider elegance, expressiveness, abstraction, composability, interactive behavior and automatic code optimization. We present a short experimental evaluation comparing the performance of the three languages on typical data exploration and pre-processing tasks. Our conclusion: there is no winner.",2021,
173,10.1109/IWBIS50925.2020.9255497,TOWARDS USING UNIVERSAL BIG DATA IN ARTIFICIAL INTELLIGENCE RESEARCH AND DEVELOPMENT TO GAIN MEANINGFUL INSIGHTS AND AUTOMATION SYSTEMS,"The increasing number of human activities using information and communication technology (ICT) generates a tremendous amount of data. It brings the opportunity to use universal big data for further computation that may deliver new insights and automation system. The technology that enables this work is artificial intelligence (AI). As known, the utilization of AI technology is becoming more and more pervasive in our daily life. Furthermore, discussion related to AI has played an essential role in an organization's decision-making process. Thus, this paper discusses the utilization of AI technology that penetrates end-to-end various aspects of human activities, such as in education, health, business, social life and so forth. The end-to-end process begins with data collection covering various types of data (text, picture, audio, video, animation) and various methods (survey, observation, interview, experiment). Then, it continues with the pre-process data cleansing and process to determine data features, and seeks the relationship within them, using machine learning process with appropriate algorithms. The results may then be used for applications portfolio in order to improve organization strategies and programs. The organization's performance can be visualized from time to time for continuous quality improvement. The end-to-end cycles of processes continue, from data collection, data pre-processing and processing, performance computation monitoring (identification, classification, prediction, and prescription), improving strategy and program, and implementing in the real-life activities until generating more behavioral data, finding pattern and design the systems. As a whole, this end-to-end cycle leads us to an AI automation system, with the power to generate meaningful insights suited to solve current problems, predicting trending issues, and understanding phenomena.",2020,
174,10.1109/ACCESS.2020.2995572,"A COMPREHENSIVE ANALYSIS OF HEALTHCARE BIG DATA MANAGEMENT, ANALYTICS AND SCIENTIFIC PROGRAMMING","Healthcare systems are transformed digitally with the help of medical technology, information systems, electronic medical records, wearable and smart devices, and handheld devices. The advancement in the medical big data, along with the availability of new computational models in the field of healthcare, has enabled the caretakers and researchers to extract relevant information and visualize the healthcare big data in a new spectrum. The role of medical big data becomes a challenging task in the form of storage, required information retrieval within a limited time, cost efficient solutions in terms care, and many others. Early decision making based healthcare system has massive potential for dropping the cost of care, refining quality of care, and reducing waste and error. Scientific programming play a significant role to overcome the existing issues and future problems involved in the management of large scale data in healthcare, such as by assisting in the processing of huge data volumes, complex system modelling, and sourcing derivations from healthcare data and simulations. Therefore, to address this problem efficiently a detailed study and analysis of the available literature work is required to facilitate the doctors and practitioners for making the decisions in identifying the disease and suggest treatment accordingly. The peer reviewed reputed journals are selected for the accumulated of published research work during the period ranges from 2015 - 2019 (a portion of 2020 is also included). A total of 127 relevant articles (conference papers, journal papers, book section, and survey papers) are selected for the assessment and analysis purposes. The proposed research work organizes and summarizes the existing published research work based on the research questions defined and keywords identified for the search process. This analysis on the existence research work will help the doctors and practitioners to make more authentic decisions, which ultimately will help to use the study as evidence for treating patients and suggest medicines accordingly.",2020,21693536
175,10.1109/BigDataSE53435.2021.00027,SDCCD: SPATIOTEMPORAL DATA CLEANING BASED ON COLLISION DETECTION,"In the era of big data, data resources are becoming more and more abundant, and the quality of data is getting more and more attention. Data cleaning is the process of identifying and processing dirty data in order to improve the quality of data, which is conducive to make full use of the collected data and ensure the effectiveness and accuracy of the follow-up data analysis. Spatiotemporal data is a kind of time series data, and its cleaning has been widely studied. However, existing methods based on smoothing and statistics are often only suitable for dense spatiotemporal datasets. In this paper, we propose a general spatiotemporal data cleaning method based on collision detection (SDCCD), which is suitable for both dense and sparse spatiotemporal datasets. Experiments on real spatiotemporal datasets show that SDCCD can effectively detect and process spatiotemporal collision records in spatiotemporal datasets.",2021,
176,10.1109/BigData.2018.8622613,USER-CENTERED INFORMATION RETRIEVAL USING SEMANTIC MULTIMEDIA BIG DATA,"The user is a basic component in the whole information management process and for this reason s/he has to be taken into account in the design and implementation of frameworks and systems for information retrieval. For this reason, the problem of defining efficient techniques for knowledge representation in a user's prospective is becoming a challenging topic in both academic and industrial community. Moreover, the large amount of available data induces several problems from different points of view and it is stressed in the bigdata vision. In this latter some approaches related to the semantic web could be used to improve the data models which underlie the implementation of user-based information retrieval applications. In this paper we propose the use of a semantic approach to design the structure of a multimedia BigData. In addition, the recognition of multimodal features to represent concepts and its attributes together with linguistic properties to relate them are an effective way to bridge the gap between the target semantic classes and the available low-level multimedia descriptors. Information about users (i.e. position) is also used to recognize his/her behaviour and improve the quality of the information retrieval process. Our framework has been implemented using a NoSQL graphdb populated from very large knowledge sources and mobile technologies. Extended experiments are presented to show the effectiveness of our approach.",2018,
177,10.1109/ICCN.2015.9,ANALYSIS AND PERFORMANCE IMPROVEMENT OF K-MEANS CLUSTERING IN BIG DATA ENVIRONMENT,"The big data environment is used to support the huge amount of data processing. In this environment tons (i.e. Giga bytes, Tera bytes) of data is processed. Therefore the various online applications where the huge data request are generated are treated using the big data i.e. facebook, google. In this presented work the big data environment is studied and investigated how the data is consumed using the big data and how the supporting tools are working with the Hadoop storage. Furthermore, for keen understanding and investigation, a cluster analysis technique more specifically the K-mean clustering algorithm is implemented through the Hadoop and MapReduce. The clustering is a part of big data analytics where the unlabelled data is processed and utilized to make groups of the data. In addition of that it is observed the traditional k-mean algorithm is not much suitably works with the Hadoop and MapReduce thus small amount of modification is performed on the data processing technique. In addition of that during cluster analysis various issues are found in traditional k-means i.e. fluctuating accuracy, outliers and empty cluster. Therefore a new clustering algorithm with modification on traditional approach of k-means clustering is proposed and implemented. That approach first enhances the data quality by removing the outlier points in datasets and then the bi-part method is used to perform the clustering. The proposed clustering technique implemented using the JAVA, Hadoop and MapReduce finally the performance of the proposed clustering approach is evaluated and compared with the traditional k-means clustering algorithm. The obtained performance shows the effective results and enhanced accuracy of cluster formation with the removal of the de-efficiency. Thus the proposed work is adoptable for the big data environment with improving the performance of clustering.",2015,
178,10.1109/QRS-C.2019.00026,QUALITY DRIVEN JUDICIAL DATA GOVERNANCE,"With the development of Smart Court 3.0, the amount of judicial data that can be stored and processed by the computer is increasing rapidly. People gradually realize that judicial data contains tremendous social and business value. However, we need stronger ability to handle with and apply massive, multi-source and heterogeneous judicial data. A complete data governance system should be built in order to make full use of the value of data assets. In such a data governance system, data quality control is one of the key steps of data governance, and also the bottleneck of data service development, because data quality determines the upper limit of data application. This paper proposes a judicial data quality measurement framework by analyzing some judicial business data, followed by a data governance method driven by it.",2019,
179,10.1109/ICDCECE53908.2022.9792642,ENHANCED EFFICIENT AND SECURITY IN BIG DATA USING TDES AND MACHINE LEARNING TECHNIQUE,"Big Data Analytics (BDA) is an important technology used in many industries, including banking and investments, finance, news and entertainment, transport, education, production, energy and utilities, and medical health care. Big Data in medical healthcare is described as a collection of exceptionally big and complicated sets of electronic healthcare data. Because of the huge amount and complexity of the data, it is impossible to maintain it using traditional software for practical application. The term ""Bigdata,"" created in 1990, refers to the study of huge and complicated datasets. Bigdata problems include data storage, data capture, data processing, data querying, data visualisation, and data transmission. It can only perform wonders if the most crucial information is gathered by it. Predictive analytics, user behaviour analytics, and other Bigdata analytics are being used to extract meaningful information from the massive volume of Bigdata. Bigdata may be used to prevent sickness, identify crime, and aid in commerce, financial institutions, and other fields by studying new patterns and associations. Machine Learning is an area of computer science which is used to find hidden patterns in massive amounts of complicated data. Machine learning is a technique in which a model is taught to learn from data, and it is therefore widely utilised in practically every sector in order to uncover a valuable pattern in Bigdata. This approach produces results without the need for human intervention. Businesses now understand that Bigdata is only helpful if meaningful information is extracted from it using an effective machine learning method. Furthermore, Triple Data Encryption Standard is employed for encryption to improve speed. Attribute Based Access Control is used to offer authentication and to give allowed access. TDES method is compared to current ways such as Data Encryption Standard and Advanced Encryption Standard in terms of file size and time as basis for performance evaluation. In addition, the classification method MKSVM is evaluated to Support Vector Machine and Neural Network in terms of quality, specific, and reliability. These phase evaluates the performance of Attribute Based Access Control. When compared to current approaches, the comparative study demonstrated that the suggested TDES encryption methodology achieved the shortest execution time with the most secure data.",2022,
180,10.1109/INDIS.2018.00011,"BIGDATA EXPRESS: TOWARD SCHEDULABLE, PREDICTABLE, AND HIGH-PERFORMANCE DATA TRANSFER","Big Data has emerged as a driving force for scientific discoveries. Large scientific instruments (e.g., colliders, and telescopes) generate exponentially increasing volumes of data. To enable scientific discovery, science data must be collected, indexed, archived, shared, and analyzed, typically in a widely distributed, highly collaborative manner. Data transfer is now an essential function for science discoveries, particularly within big data environments. Although significant improvements have been made in the area of bulk data transfer, the currently available data transfer tools and services can not successfully address the high-performance and time-constraint challenges of data transfer required by extreme-scale science applications for the following reasons: disjoint end-to-end data transfer loops, cross-interference between data transfers, and existing data transfer tools and services are oblivious to user requirements (deadline and QoS requirements). Fermilab has been working on the BigData Express project to address these problems. BigData Express seeks to provide a schedulable, predictable, and high-performance data transfer service for big data science. The BigData Express software is being deployed and evaluated at multiple research institutions, which include UMD, StarLight, FNAL, KISTI, KSTAR, SURFnet, Ciena, and other sites. Meanwhile, the BigData Express research team is collaborating with the StarLight International/National Communications Exchange Facility to deploy BigData Express at various research platforms, including Pacific Research Platform, National Research Platform, and Global Research Platform. It is envisioned that we are working toward building a high-performance data transfer federation for big data science.",2018,
181,10.1109/ICCWAMTIP53232.2021.9674175,RESEARCH ON THE CONSTRUCTION OF AGRICULTURAL PRODUCT QUALITY MAINTENANCE AND QUALITY TRACEABILITY SYSTEM BASED ON BIG DATA,"The quality and safety of agricultural products has been widely concerned by the whole society in recent years. Therefore, the traceability of agricultural products is a research hotspot of scholars. The quality and safety traceability system of agricultural products is an important method to monitor the quality and safety of agricultural products. The emergence and use of big data help to solve the problems of high cost, scattered information and incomplete industrial chain of quality and safety traceability of agricultural products and improve the efficiency and accuracy of the quality and safety traceability system of agricultural products. There are still some problems in the application of big data, such as weak pertinence. It is necessary to mine and use big data to realize the traceability of agricultural products.",2021,25768972
182,10.1109/ICCCE.2018.8539254,MODIFYING CLEANING METHOD IN BIG DATA ANALYTICS PROCESS USING RANDOM FOREST CLASSIFIER,"Accurate data is a key success factor influencing the performance of data analytics results, especially for the detection and prediction purpose. Nowadays, Big Data analytics (BDA) is used to analyze the sheer volume of data available in an organization. These data quality must be maintained in order to obtain correct alert and valuable insights from the rapidly changing data of high volume, velocity, variety, veracity, and value. This paper aim is to modify existing framework of big data analytics by improving an important step in pre-processing (i.e. Data Cleaning). Initially, feature selection based on Random Forest is used to extract effective features. Then, two classifier algorithms (i.e. Random Forest classifier and Linear SVM classifier) are applied to train using the dataset to classify data quality and to develop an intelligent model. In evaluation, our experimental results show a consistent accuracy of Random Forest and Linear Regression around 90%. Using this approach, we expect to provide a set of cleaned data for further processing. Besides, analysts can benefit from this system in data analytical process in cleaning stage and conclude that the data is cleaned. Finally, a comparison is presented between available functions which are used to handle missing values with the developed system.",2018,
183,10.1109/BIGCOM.2017.48,CLUSTER-BASED BEST MATCH SCANNING FOR LARGE-SCALE MISSING DATA IMPUTATION,"High-quality data are the prerequisite for analyzing and using big data to guarantee the value of the data. Missing values in data is a common yet challenging problem in data analytics and data mining, especially in the era of big data. Amount of missing values directly affects the data quality. Therefore, it is critical to properly recover missing values in the dataset. This paper presents a new imputation algorithm called Cluster-based Best Match Scanning (CBMS) designed for Big Data. It is a modification of k-NN imputation. CBMS focuses on recovering continuous numeric missing values, and aims at balancing computational complexity and accuracy. As an imputation algorithm, it can potentially reduce the time complexity of k-NN from O(n^2*d) to O(n^1.5*d), and also reduce the space/memory usage, while perform no worse than k-NN imputation. On top of that CBMS is highly parallelizable.Simulation of CBMS is conducted on smart meter reading data. Data is manually divided into training set and testing set, and testing accuracy is evaluated by computing the mean absolute deviation. Comparison with linear interpolation and k-NN imputation is made to demonstrate the power and effectiveness of our proposed CBMS algorithm.",2017,
184,10.1109/QRS54544.2021.00114,RESEARCH ON MINING OF GOVERNMENT DATA BASED ON ENHANCED-OBJECT EXCHANGE MODEL,"In order to acquire the required knowledge from the vast amounts of government affairs data, accurate data mining has important practical significance. Therefore, a precision mining method of government affairs big data based on E-OEM model is studied. The method firstly uses web crawlers to capture the vast amounts of government data. Government data then performs data cleaning, pattern normalization, format standardization, index information collection and pre-processing, and finally, according to the purpose of data mining, the Kirkpatrick model is used to complete the mining effect evaluation. Experimental comparison specifically includes correlation analysis, cluster analysis, classification analysis, exception analysis and evolution analysis as well as five others. The results show that: compared with the three methods in previous studies, the F1-score obtained by the application of the research method is higher (0.852), which proves that the quality of government affairs big data mining is better.",2021,26939185
185,10.1109/ITMQIS.2018.8524938,EVALUATION OF USING BIG DATA TECHNOLOGIES IN RUSSIAN FINANCIAL INSTITUTIONS,"Today's development of the world economy cannot be imagined without the implementation of digital technologies that transform entire economic sectors. Digital modernization of the world economy has led to the emergence of a new direction in economic development - the digital economy. It bases on the introduction of the most advanced information and communication technologies. In 2017, the state program ""Digital Economy of the Russian Federation"" was adopted in Russia. It provides the goals and tasks for implementing the state policy to develop the digital economy in Russia. The introduction and application of technologies for processing and analysis of big data are on the first place in this program. One of the leaders in this area in the Russian Federation is financial institutions. They have accumulated huge amounts of data for many years of their existence that require careful analysis and processing for further use. The paper analyzes modern technologies in the field of big data, reveals the most promising technologies that spread in the Russian market. In addition, the financial evaluation of the Russian big data market was carried out, the dynamics and growth rates were analyzed, future development was determined, the leaders in the application and implementation of these technologies were identified. The introduction and use of big data in Russian financial institutions were analyzed, the financial evaluation of the Russian big data market in the financial sphere was made, and the main directions of application of these technologies in credit institutions are defined. During the research, the reasons that hinder the more mass and rapid introduction of big data in Russian financial institutions were identified and considered.",2018,
186,10.1109/ICBAIE52039.2021.9390022,TOWARDS BIG DATA ANALYTICS IN THE ONLINE BRUSHING SYSTEM’S MODEL DESIGN,"The potential for big data analytics to improve social work students' professional qualities in a more in-depth dimension is tremendous. However, the application of big data in it is at a nascent stage. This research provides an online brushing system's model design with the data sources and methods that towards big data analytics, and describes operation interfaces, including programming brushing module and knowledge brushing module. The practice shows that big data will fulfil its potential as an important role in a quality-expanding system; also, it provides a view to dynamically extend and integrate modules considering the changing demands.",2021,
187,10.1109/BigData.2018.8622576,A SOFTWARE FRAMEWORK FOR CLUSTER LIFECYCLE ANALYSIS IN TRANSPORTATION,"Novel forms of data analysis methods have emerged as a significant research direction in the transportation domain. These methods can potentially help to improve our understanding of the dynamic flows of vehicles, people, and goods. Understanding these dynamics has economic and social consequences, which can improve the quality of life locally or worldwide. Aiming at this objective, a significant amount of research has focused on clustering moving objects to address problems in many domains, including the transportation, health and environment. However, previous research has not investigated the lifecycle of a cluster, including cluster genesis, existence, and disappearance. The representation and analysis of cluster lifecycles can create novel avenues for research, result in new insights for analyses, and allow unique forms of prediction. This paper focuses on studying the lifecycle of clusters by investigating the relations that a cluster has with moving elements and other clusters. This paper also proposes a big data framework that manages the identification and processing of a cluster lifecycle. The ongoing research approach will lead to new ways to perform cluster analysis and advance the state of the art by leading to new insights related to cluster lifecycle. These results can have a significant impact on transport industry data science applications in a wide variety of areas, including congestion management, resource optimization, and hotspot management.",2018,
188,10.1109/BigData.2018.8622101,OPTIMIZING LOSSY COMPRESSION WITH ADJACENT SNAPSHOTS FOR N-BODY SIMULATION DATA,"Today's N-body simulations are producing extremely large amounts of data. The Hardware/Hybrid Accelerated Cosmology Code (HACC), for example, may simulate trillions of particles, producing tens of petabytes of data to store in a parallel file system, according to the HACC users. In this paper, we design and implement an efficient, in situ error-bounded lossy compressor to significantly reduce the data size for N-body simulations. Not only can our compressor save significant storage space for N-body simulation researchers, but it can also improve the I/O performance considerably with limited memory and computation overhead. Our contribution is threefold. (1) We propose an efficient data compression model by leveraging the consecutiveness of the cosmological data in both space and time dimensions as well as the physical correlation across different fields. (2) We propose a lightweight, efficient alignment mechanism to align the disordered particles across adjacent snapshots in the simulation, which is a fundamental step in the whole compression procedure. We also optimize the compression quality by exploring best-fit data prediction strategies and optimizing the frequencies of the space-based compression vs. time-based compression. (3) We evaluate our compressor using both a cosmological simulation package and molecular dynamics simulation data-two major categories in the N-body simulation domain. Experiments show that under the same distortion of data, our solution produces up to 43% higher compression ratios on the velocity field and up to 300% higher on the position field than do other state-of-the-art compressors (including SZ, ZFP, NUMARCK, and decimation). With our compressor, the overall I/O time on HACC data is reduced by up to 20% compared with the second-best compressor.",2018,
189,10.1109/ICIRCA51532.2021.9544814,BIG DATA ARTIFICIAL INTELLIGENCE IN THE DIRECTION OF TOURISM SOCIAL MEDIA: A SYSTEMATIC STUDY,"This paper conducts the systematic study on big data artificial intelligence in the direction of tourism social media. At present, the focus of smart city should be the central city and big city. At the same time, the overall quality of citizens should be high, there should be a suitable living environment, including good culture and public services, convenient transportation services, reasonable living costs, and good city management. For the intelligent analysis, firstly it is required to design the big data model, which is combined for the AI frameworks as the theoretical basis, and then, the social network is modeled with the topology analysis and the data connection analysis. Furthermore, the application scenario of the smart tourism is tested. The designed platform is implemented based on software structure optimization.",2021,
190,10.1109/ICMTMA50254.2020.00168,RESEARCH ON NURSING MANAGEMENT BASED ON BIG DATA,"With the rapid development of the Internet, the era of science and technology has arrived, and big data in information technology has become the driving force behind scientific and technological progress. Applying big data and intelligence to the clinical nursing quality management system, constructing a nursing management and control platform that integrates nursing quality indicators, nursing event reporting, and nursing risk management, and realizes the dynamic and intelligent management and control of nursing quality throughout the process. After applying big data to the nursing quality management system, the time for entering and analyzing nursing quality problems is significantly shortened, the quality of nursing services is improved, and patient satisfaction is effectively improved. The establishment and application of mobile nursing quality management system reduces time cost, improves nursing work efficiency, promotes the full participation of nursing quality management, and improves the effectiveness of nursing quality management and patient satisfaction.",2020,21571473
191,10.1109/ICICS49469.2020.239526,AN APPROACH TO IMPROVE DATA QUALITY FROM BIG DATA ASPECT BY SENSITIVE COST AND TIME,"Big data is term of dataset with characteristic volume, value and veracity that lead to challenges unable proceed using traditional techniques to extract value, project management perspective is dynamic processing that utilizes the appropriate resources of organization in many phases by measuring in four-factor scope, time, cost and quality. In this research aim improve data quality from big data via project management scope depends on high trust which is getting high accuracy from confidence level in volume of data, confidence get with context and value of data which lead to determine accuracy deeply in it and finally choose from data depending on veracity of it, the experiment using three main factors time, cost and scope, strongest relation arranging between them start by project scope as strongest one then cost, product and finally time is weakest between them, in the final when select best quality use two sides generally from quality degree and be middle-quality interval and especially from relative distance with the strongest factor.",2020,2471125X
192,10.1109/IICSPI.2018.8690369,THE RESEARCH AND ANALYSIS OF BIG DATA APPLICATION ON DISTRIBUTION NETWORK,"Big data used in intelligent power grid operation is gradually getting more and more widely acknowledged and emphasized. Due to the fast development of smart grid and distribution network, big data application on the traditional power industry is undergoing profound changes, which promotes the decentralization, digitalization and intellectualization of distribution system devices, where power users could significantly make use of the information amount of system running state. Through the analysis of big data derived from distribution network, this paper discusses the concept of big data characteristics in modern intelligent distribution network, clarifies data source of distributed network system and illustrates the significance of big data application. Finally, effective big data analysis methods are next employed to demonstrate the special implement of big data technologies in distribution network environment.",2018,
193,10.1109/ICBDSC.2016.7460348,UTILIZING BIG DATA ANALYTICS AS A SOLUTION FOR SMART CITIES,"Cities generate huge volumes of data that are diverse in nature and are generated on a daily basis. These data types involve environment, energy, transport and economic data. This includes structured and unstructured data which needs to be processed and understood to become valuable. By managing and analyzing big data, smart cities will be able to tackle formidable problems and ascend to a state where better and more informed decisions will be taken resulting in enhanced economic and environment outcomes leading to better quality of life. This paper reviews utilizing big data in cities by using big data analytics as a solution for smart cities and conceiving related issues being faced.",2016,
194,10.1109/ICBDACI.2017.8070878,ADVANCED QUERY MODEL DESIGN CONCEPT TO SUPPORT MULTI-DIMENSIONAL DATA ANALYTICS FOR RELATIONAL DATABASE MANAGEMENT SYSTEMS,"Industries use information for decision making to help process changers and business expansions. They use historical data to produce the required information. Preparation of information varies with the way of arranging, retrieving and processing data accurately. Decision support systems use many different approaches to design and data manipulation. Existing small to medium systems still use relational databases and basic query models as primary data analysis technique. But large scaled enterprise systems use different analytical, data warehousing and business intelligent systems rather than using databases due to the numerous RDBMS limitations. As described in (Ballard, et al., 1998, February) data warehousing concept evolve the easy access method of quality data in a structured store for decision making. Objective of this paper is to introduce an advanced database and query model design to support data analysis, business intelligence and data visualization over relational database management systems.",2017,
195,10.1109/MCC.2015.36,TRUSTWORTHY PROCESSING OF HEALTHCARE BIG DATA IN HYBRID CLOUDS,"A 2015 Gartner report noted that data processing technologies haven't kept pace with the significant increase in the volume of digital healthcare data, and an integrated and trustworthy healthcare analytics solution can facilitate more effective decision making in patient care and risk management, improving quality of life, optimizing performance of services, and so on. The challenge is how to ensure data confidentiality and integrity when storing such data but still make it highly available, process it to extract actionable information for decision makers, including medical professionals, and share it with collaborators, while preserving the privacy of individual patients and giving them the full control of their data at all times. This challenge calls for a trustworthy big data processing platform.",2015,23722568
196,10.1109/ICEKIM52309.2021.00181,RESEARCH ON THE AUDIT OF NATURAL RESOURCES ASSETS FROM THE PERSPECTIVE OF BIG DATA CLOUD COMPUTING,"With the rapid development of information technology in the era of big data, cloud computing technology is applied to all aspects of social life. There are many kinds of natural resource assets, and the formulation of audit standards and the selection of audit methods are difficult. This paper studies the application of big data cloud computing in the audit of natural resource assets, which plays an important role in mining the useful information carried by audit data, in order to solve various practical problems faced by natural asset audit, improve audit efficiency, reduce audit risk and ensure the quality of audit At the same time, it analyzes and discusses the problems of data and information security in the current cloud audit. The application of big data cloud computing technology is the development trend of the times, which has a huge role in promoting the audit of natural resources assets, and will also provide experience for the historical change of full audit coverage.",2021,
197,10.23919/FRUCT.2019.8711912,DISTRIBUTED BIG DATA DRIVEN FRAMEWORK FOR CELLULAR NETWORK MONITORING DATA,"The smart monitoring system (SMS) vision relies on the use of ICT to efficiently manage and maximize the utility of network infrastructures and services in order to improve the quality of service and network performance. Many aspects of SMS projects are dynamic data driven application system where data from sensors monitoring the system state are used to drive computations that in turn can dynamically adapt and improve the monitoring process as the complex system evolves. In this context, a research and development of new paradigm of Distributed Big Data Driven Framework (DBDF) for monitoring data in mobile network infrastructures entails the ability to dynamically incorporate more accurate information for network monitoring and controlling purposes through obtaining real-time measurements from the base stations, user demands and claims, and other sensors (for weather conditions, etc.). The proposed framework consists of network probes, data parsing application, Message-Oriented Middleware, real-time and offline data models, Big Data storage and Decision layers., and Other data sources. Each Big Data layer might be implemented using comparative analysis of the most effective Big Data solutions. In addition, as a proof of concept, the roaming users detection model was created based on Apache Spark application. The model filters streaming protocols data, deserializes it into Json format and finally sends it to Kafka application. The experiments with the model demonstrated and acknowledged the capacities of the Apache Spark in building foundation for Big Data hub as a basic application for online mobile network data processing.",2019,23057254
198,10.1109/ICBDIE52740.2021.00013,INNOVATIVE RESEARCH AND EXPLORATION OF AUDITING UNDER THE BIG DATA,"With the advent of the information age, the development of big data has been accelerating, and the application of cloud computing in the auditing industry has also been increasing. This has had a huge impact on the development of our country's auditing industry and presented new challenges. How to use big data technology to conduct cloud audits, adapt to changes in the current environment, and reduce audit risks has become an urgent issue in the auditing community. Based on the background of big data, this paper studies the main difficulties faced by audit development, analyzes the causes of the problems, and finally proposes to use big data technology for cloud audi, innovate audit organization methods, focus on the training of audit talents, strengthen audit business training, hire expert consultants to improve audit quality, and optimize the staff structure, etc.",2021,
199,10.1109/ICAML51583.2020.00045,ANALYSIS OF BIG DATA CLEANING ALGORITHM RESEARCH AND SYSTEM PLATFORM CONSTRUCTION,"In recent years., due to the rapid development of industry., science and technology., the sources of big data with important industrial significance are increasingly diversified., showing an exponential growth trend. In today&#x0027;s social production., if we want to obtain useful quality data about fast-growing and complex data., we need to clean up these data. In the era of Industry 4.0., the global manufacturing industry is developing rapidly., and the data in the production process has increased., showing an exponential trend. Pay more attention to the use of these data in production. How to make effective use of public industrial big data from networked and intelligent manufacturing is the main solution of manufacturing industry. To this end, in order to better use the big data cleaning algorithm in production and production., this paper studies the design., system construction and implementation of the big data platform.",2020,
0,10.1109/ICESIT53460.2021.9696607,ENTERPRISE INTERNAL AUDIT SYSTEM IN THE CONTEXT OF BIG DATA,"In order to overcome the problems existing in the traditional enterprise internal audit system under the background of big data, this paper proposes a novel enterprise internal audit system under the background of big data. From the perspective of corporate governance, the system fully discusses the scientific and usefulness of big data technology, and emphasizes the importance of the combination of big data technology and enterprise internal audit. Based on this, the system also fully combines the characteristics of big data technology and emphasizes the importance of big data thinking in the big data era and the importance of enterprise internal audit for enterprise governance. The research results show that the system can not only overcome the problems existing in the traditional internal audit system of enterprises under the background of big data, but also improve the efficiency and quality of internal audit.",2021,
1,10.1109/BigData47090.2019.9005600,THE SERUMS TOOL-CHAIN: ENSURING SECURITY AND PRIVACY OF MEDICAL DATA IN SMART PATIENT-CENTRIC HEALTHCARE SYSTEMS,"Future-generation healthcare systems will be highly distributed, combining centralised hospital systems with decentralised home-, work-and environment-based monitoring and diagnostics systems. These will reduce costs and injury-related risks whilst both improving quality of service, and reducing the response time for diagnostics and treatments made available to patients. To make this vision possible, medical data must be accessed and shared over a variety of mediums including untrusted networks. In this paper, we present the design and initial implementation of the SERUMS tool-chain for accessing, storing, communicating and analysing highly confidential medical data in a safe, secure and privacy-preserving way. In addition, we describe a data fabrication framework for generating large volumes of synthetic but realistic data, that is used in the design and evaluation of the tool-chain. We demonstrate the present version of our technique on a use case derived from the Edinburgh Cancer Centre, NHS Lothian, where information about the effects of chemotherapy treatments on cancer patients is collected from different distributed databases, analysed and adapted to improve ongoing treatments.",2019,
2,10.1109/PMIS52742.2021.00010,RESEARCH ON INTELLIGENT MODE OF PUBLIC SERVICE DRIVEN BY BIG DATA,"Big data is the key to building a intelligent public service mode and a technical means to promote the modernization of national governance. However, the public service mode driven by big data has problems such as inefficient government purchases and failure to achieve multi-level and cross-departmental information interaction, which directly affects the quality of public service work. This article takes the “5V” characteristics of big data as the background, discusses the “5N” feature of optimizing public services, and proposes a intelligent mode of public services driven by big data, thereby enhancing the awareness of big data at all levels of public services.",2021,
3,10.1109/BigData47090.2019.9005596,TIME SERIES CLASSIFICATION: LESSONS LEARNED IN THE (LITERAL) FIELD WHILE STUDYING CHICKEN BEHAVIOR,"Poultry farms are a major contributor to the human food chain. However, around the world, there have been growing concerns about the quality of life for the livestock in poultry farms; and increasingly vocal demands for improved standards of animal welfare. Recent advances in sensing technologies and machine learning allow the possibility of monitoring birds, and employing the lessons learned to improve the welfare for all birds. This task superficially appears to be easy, yet, studying behavioral patterns involves collecting enormous amounts of data, justifying the term Big Data. Before the big data can be used for analytical purposes to tease out meaningful, well-conserved behavioral patterns, the collected data needs to be preprocessed. The pre-processing refers to processes for cleansing and preparing data so that it is in the format ready to be analyzed by downstream algorithms, such as classification and clustering algorithms. However, as we shall demonstrate, efficient preprocessing of chicken big data is both non-trivial and crucial towards success of further analytics.",2019,
4,10.1109/ICAIBD55127.2022.9820032,RESEARCH ON THE APPLICATION OF DATA MINING IN TEACHING SYSTEM,"Using data mining technology, based on relevant scientific theories, optimize the school teaching system. Taking full advantage of the huge data stored in the teaching system of colleges and universities for many years, through decision analysis and decision tree pruning method to carefully analyze the data, discover the messages and useful knowledge to promote the teaching quality from a great many data in teaching system, and be used in teaching practice. This paper establishes data mining system to monitor and control teaching quality. Among them, data mining algorithms such as association rules and decision trees are applied. From the perspective of data analysis, the main body, process and other factors affecting teaching quality and their mutual relations are studied, and a rule discovery method suitable for teaching evaluation is proposed.",2022,
5,10.1109/ICSGEA51094.2020.00102,QUALITY-CENTERED MOBILE TERMINAL TEACHING ASSESSMENT SYSTEM UNDER THE BACKGROUND OF BIG DATA,"The teaching system of mobile terminals is a teaching environment that has emerged with the popularity of computer technology and mobile devices. It is the main research object in this article. It is different from the traditional classroom teaching environment. It completely uses mobile terminals instead of traditional paper textbooks. In this classroom environment, students and teachers each have a handheld mobile device that connects to the wireless network through a server. The information is processed dynamically and statically through the data acquisition layer, and the processed data is analyzed dynamically and statically, so that the data storage layer stores the information. Then the data storage layer feeds back the data information to students, service organizations, operators and teachers through the precise teaching engine, so as to meet the needs of teaching. Students acquire personalized teaching needs, and establish a precise large data analysis and data mining model.",2020,
6,10.1109/BigData.2015.7363909,HIGH QUALITY CLUSTERING OF BIG DATA AND SOLVING EMPTY-CLUSTERING PROBLEM WITH AN EVOLUTIONARY HYBRID ALGORITHM,"Achieving high quality clustering is one of the most well-known problems in data mining. k-means is by far the most commonly used clustering algorithm. It converges fairly quickly, but achieving a good solution is not guaranteed. The clustering quality is highly dependent on the selection of the initial centroid selections. Moreover, when the number of clusters increases, it starts to suffer from ""empty clustering"". The motivation in this study is two-fold. We not only aim at improving the k-means clustering quality, but at the same time not being effected by the empty cluster issue. For achieving this purpose, we developed a hybrid model, H(EC)2S, Hybrid Evolutionary Clustering with Empty Clustering Solution. Firstly, it selects representative points to eliminate Empty Clustering problem. Then, the hybrid algorithm uses only these points during centroid selection. The proposed model combines Fireworks and Cuckoo-search based evolutionary algorithm with some centroid-calculation heuristics. The model is implemented using a Hadoop Mapreduce algorithm for achieving scalability when faced with a Big Data clustering problem. The advantages of the developed model is particularly attractive when the amount, dimensionality and number of cluster parameters tend to increase. The results indicate that considerable clustering quality performance improvement is achieved using the proposed model.",2015,
7,10.1109/ICDE48307.2020.00140,DATA SENTINEL: A DECLARATIVE PRODUCTION-SCALE DATA VALIDATION PLATFORM,"Many organizations process big data for important business operations and decisions. Hence, data quality greatly affects their success. Data quality problems continue to be widespread, costing US businesses an estimated $600 billion annually. To date, addressing data quality in production environments still poses many challenges: easily defining properties of high-quality data; validating production-scale data in a timely manner; debugging poor quality data; designing data quality solutions to be easy to use, understand, and operate; and designing data quality solutions to easily integrate with other systems. Current data validation solutions do not comprehensively address these challenges. To address data quality in production environments at LinkedIn, we developed Data Sentinel, a declarative production-scale data validation platform. In a simple and well-structured configuration, users declaratively specify the desired data checks. Then, Data Sentinel performs these data checks and writes the results to an easily understandable report. Furthermore, Data Sentinel provides well-defined schemas for the configuration and report. This makes it easy for other systems to interface or integrate with Data Sentinel. To make Data Sentinel even easier to use, understand, and operate in production environments, we provide Data Sentinel Service (DSS), a complementary system to help specify data checks, schedule, deploy, and tune data validation jobs, and understand data checking results. The contributions of this paper include the following: 1) Data Sentinel, a declarative production-scale data validation platform successfully deployed at LinkedIn 2) A generic design to build and deploy similar systems for production environments 3) Experiences and lessons learned that can benefit practitioners with similar objectives.",2020,10636382
8,10.1109/BigData.2018.8622178,A SKETCH-BASED NAIVE BAYES ALGORITHMS FOR EVOLVING DATA STREAMS,"A well-known learning task in big data stream mining is classification. Extensively studied in the offline setting, in the streaming setting - where data are evolving and even infinite - it is still a challenge. In the offline setting, training needs to store all the data in memory for the learning task; yet, in the streaming setting, this is impossible to do due to the massive amount of data that is generated in real-time. To cope with these resource issues, this paper proposes and analyzes several evolving naive Bayes classification algorithms, based on the well-known count-min sketch, in order to minimize the space needed to store the training data. The proposed algorithms also adapt concept drift approaches, such as ADWIN, to deal with the fact that streaming data may be evolving and change over time. However, handling sparse, very high-dimensional data in such framework is highly challenging. Therefore, we include the hashing trick, a technique for dimensionality reduction, to compress that down to a lower dimensional space, which leads to a large memory saving.We give a theoretical analysis which demonstrates that our proposed algorithms provide a similar accuracy quality to the classical big data stream mining algorithms using a reasonable amount of resources. We validate these theoretical results by an extensive evaluation on both synthetic and real-world datasets.",2018,
9,10.1109/ICRMS.2018.00012,BIG DATA ORIENTED ROOT CAUSE HEURISTIC IDENTIFICATION APPROACH BASED ON FWARM FOR QUALITY ACCIDENT,"With the advent of big data era, data driven quality management decision-making has become an important means to seek new quality improvement opportunity for manufacturers. Quality accidents (QAs) can cause severely economic and reputational damage to manufacturer, accurate identification of root causes for severe quality accident is the primary task and always a big challenge for quality managers. Because of the fuzzy nature of incomplete and noisy data for big data, the fuzzy concept is proposed, and the fuzzy weighted association rule mining (FWARM) method is adopted into the root cause identification of quality accident novelly. Firstly, Quality Accident formation mechanism and modeling of big data for quality accident are defined; Secondly, root cause identification framework of quality accident based on the relevance tree is established; Then, the FWARM method is used to identify product functional defects and physical defects through big data for quality accident. Finally, a case study of root cause identification of quality accident is adopted to validate the proposed approach.",2018,23340878
10,10.1109/COMPSACW.2014.52,MANAGING HETEROGENEOUS SENSOR DATA ON A BIG DATA PLATFORM: IOT SERVICES FOR DATA-INTENSIVE SCIENCE,"Big data has emerged as a key connecting point between things and objects on the internet. In this cyber-physical space, different types of sensors interact over wireless networks, collecting data and delivering services ranging from environmental pollution monitoring, disaster management and recovery, improving the quality of life in homes, to enabling smart cities to function. However, despite the perceived benefits we are realizing from these sensors, the dawn of the Internet of Things (IoT) brings fresh challenges. Some of these have to do with designing the appropriate infrastructure to capture and store the huge amount of heterogeneous sensor data, finding practical use of the collected sensor data, and managing IoT communities in such a way that users can seamlessly search, find, and utilize their sensor data. In order to address these challenges, this paper describes an integrated IoT architecture that combines the functionalities of Service-Controlled Networking (SCN) with cloud computing. The resulting community-driven big data platform helps environmental scientists easily discover and manage data from various sensors, and share their knowledge and experience relating to air pollution impacts. Our experience in managing the platform and communities provides a proof of concept and best practice guidelines on how to manage IoT services in a data-intensive research environment.",2014,
11,10.1109/EIConRus.2019.8657115,PROSPECTS FOR USING BIG DATA TO IMPROVE THE EFFECTIVENESS OF AN EDUCATION ORGANIZATION,"Today, teaching methods have changed significantly. With new innovations in the field of information technology, work in the classroom becomes virtual, and cloud knowledge. Although these new technologies have been adopted in the field of education, the informative organizations still face the challenge of improving the quality of education and reducing dropout rates. This article will examine the analysis of big data that can be used in education and how it affects the improvement of the quality of education.",2019,23766557
12,10.1109/ICTER.2015.7377714,QUALITY OF INFORMATION FOR QUALITY OF LIFE: HEALTHCARE BIG DATA ANALYTICS,"Business intelligence and analytics, and big data analytics have become increasingly important in describing the data sets and analytical techniques in software applications that are so large and complex. These two techniques have been used as analytics by several e-commerce communities. For example, vendors such as Amazon and eBay have adapted these techniques to significantly advance in innovative and highly recommended scalable e-commerce platforms and product systems to target potential customers thus increasing business revenues. In a similar context, the health community have experienced not only more complex and large data content, but also information systems that contain a large number of data sources with interrelated data. Furthermore, due to the increasing diversity and differentiation of expansions by service providers in the form of primary or nursing care, a variety of service organisations in the public and private hospital networks including new medical specialist facilities have resulted in challenging, and highly dynamic environments resulting in the creation of big data with its enumerate complexities, for instance sharing information with expected security requirements of stakeholders. Therefore, the health community will have to adapt the concept of big data analytics in order to solve major issues that have occurred due to complex shared information.",2015,
13,10.1109/HPCC/SmartCity/DSS.2019.00100,EDGE COMPUTING FOR USER-CENTRIC SECURE SEARCH ON CLOUD-BASED ENCRYPTED BIG DATA,"Cloud service providers offer a low-cost and convenient solution to host unstructured data. However, cloud services act as third-party solutions and do not provide control of the data to users. This has raised security and privacy concerns for many organizations (users) with sensitive data to utilize cloud-based solutions. User-side encryption can potentially address these concerns by establishing user-centric cloud services and granting data control to the user. Nonetheless, user-side encryption limits the ability to process (e.g., search) encrypted data on the cloud. Accordingly, in this research, we provide a framework that enables processing (in particular, searching) of encrypted multiorganizational (i.e., multi-source) big data without revealing the data to cloud provider. Our framework leverages locality feature of edge computing to offer a user-centric search ability in a realtime manner. In particular, the edge system intelligently predicts the user's search pattern and prunes the multi-source big data search space to reduce the search time. The pruning system is based on efficient sampling from the clustered big dataset on the cloud. For each cluster, the pruning system dynamically samples appropriate number of terms based on the user's search tendency, so that the cluster is optimally represented. We developed a prototype of a user-centric search system and evaluated it against multiple datasets. Experimental results demonstrate 27% improvement in the pruning quality and search accuracy.",2019,
14,10.1109/BigData47090.2019.9006422,A POLICY-BASED APPROACH FOR MEASURING DATA QUALITY,"With the growing importance of data in all aspects of the functioning of an enterprise, having good quality of data is crucial in support of business processes. However, there do not exist good metrics to measure the quality of data that is available within an enterprise. While there are several data quality standards, their complexity and their required customization makes them difficult to use in real-world industrial scenarios. In this paper, we discuss the challenges encountered in measuring data quality within asset management systems. We propose a policy-based approach for measuring data quality, and show how such an approach can be customized and interpreted easily by practitioners in the field.",2019,
15,10.1109/ICMEIM51375.2020.00051,THE CONSTRUCTION OF “DUAL-QUALIFIED” TEACHERS IN APPLIED COLLEGES BASED ON BIG DATA,"Big data mainly refers to large-scale or ultra-large-scale data sets, which are called “massive data” or “massive data”. Nowadays, with the development of information technology, such as mobile Internet, cloud computing, Internet of Things, and data mining, a variety of information is gradually being widely used in various industries. Moreover, they can provide important forces to promote national innovation, scientific development, economic take-off, and educational reform. The characteristics of big data are mainly manifested as massive, fast, diversified and high value. Based on the quality requirements of “double-qualified” teachers in applied colleges, schools need to strengthen the construction of the teaching team and create good conditions to actively cultivate high-quality talents.",2020,
16,10.1109/BigData.2017.8258220,AUGMENTATION AND EVALUATION OF TRAINING DATA FOR DEEP LEARNING,"Deep learning is an important technique for extracting value from big data. However, the effectiveness of deep learning requires large volumes of high quality training data. In many cases, the size of training data is not large enough for effectively training a deep learning classifier. Data augmentation is a widely adopted approach for increasing the amount of training data. But the quality of the augmented data may be questionable. Therefore, a systematic evaluation of training data is critical. Furthermore, if the training data is noisy, it is necessary to separate out the noise data automatically. In this paper, we propose a deep learning classifier for automatically separating good training data from noisy data. To effectively train the deep learning classifier, the original training data need to be transformed to suit the input format of the classifier. Moreover, we investigate different data augmentation approaches to generate sufficient volume of training data from limited size original training data. We evaluated the quality of the training data through cross validation of the classification accuracy with different classification algorithms. We also check the pattern of each data item and compare the distributions of datasets. We demonstrate the effectiveness of the proposed approach through an experimental investigation of automated classification of massive biomedical images. Our approach is generic and is easily adaptable to other big data domains.",2017,
17,10.1109/IISA50023.2020.9284399,BIG DATA MINING FOR SMART CITIES: PREDICTING TRAFFIC CONGESTION USING CLASSIFICATION,"This paper provides an analysis and proposes a methodology for predicting traffic congestion. Several machine learning algorithms and approaches are compared to select the most appropriate one. The methodology was implemented using Data Mining and Big Data techniques along with Python, SQL, and GIS technologies and was tested on data originating from one of the most problematic, regarding traffic congestion, streets in Thessaloniki, the 2nd most populated city in Greece. Evaluation and results have shown that data quality and size were the most critical factors towards algorithmic accuracy. Result comparison showed that Decision Trees were more accurate than Logistic Regression.",2020,
18,10.1109/BigDIA56350.2022.9874081,THE INTEGRATION OF BIG DATA AND PHARMACEUTICAL STANDARDS IMPROVE THE LEVEL OF HOSPITAL PHARMACEUTICAL MANAGEMENT,"Objective: Through the practice discussion and exchange of the integration of big data and pharmaceutical standards, to provide experience reference for the practice of information technology in the management of rational drug use. Methods: The application of data information in the practice of hospital pharmacy management is demonstrated through the introduction of practical cases and data achievements at different levels. Results: The integration of medical big data and pharmaceutical standards effectively improved the hospital's quantitative quality management indicators and the informatization of many pharmaceutical management methods in the hospital is realized. Conclusions: The integration of medical big data and pharmaceutical standards can effectively help improve the management level of rational drug use in hospitals, and there are still many possibilities for future exploration.",2022,27716910
19,10.1109/CICED.2014.6991919,TRANSIENT POWER QUALITY ASSESSMENT BASED ON BIG DATA ANALYSIS,"A transient power quality assessment method is proposed in this paper, using Naive Bayes classification method which is based on big data processing architecture, in this architecture, data sources will be extended to the aspects of power grid monitoring data, the power customer data and the public data, and the assessment severity will be classified into the normal state, the abnormal state, the critical state, and the failed state, according to the Naive Bayes classification results. Based on the data type of transient power quality assessment, big data processing architecture used in this paper can be able to process distributed data and streaming data, so that it can ensure not only updates classifier rules regularly, but also the real-time condition assessment. In the classifier training phase, we use the massive historical data as the distributed learning object, and generate assessment rules periodically. In the state assessment phase, each assessment node will update the assessment rules generated by training phase, generate real- time evaluation of samples from stream processing framework, and evaluate the power quality state according to the current rule. On this basis, this paper designs a Naive Bayes classification method based on MapReduce processing, and realizes the map and reduce process method to compute the priori probability and the conditional probability in distributed way. Experiments show that the transient power quality evaluation method based on the big data analysis presented in this paper is feasible, and achieve good results both in classification accuracy and processing speed.",2014,21617481
20,10.1109/BigDataCongress.2018.00030,A FAST AND INCREMENTAL DEVELOPMENT LIFE CYCLE FOR DATA ANALYTICS AS A SERVICE,"Big Data does not only refer to a huge amount of diverse and heterogeneous data. It also points to the management of procedures, technologies, and competencies associated with the analysis of such data, with the aim of supporting high-quality decision making. There are, however, several obstacles to the effective management of a Big Data computation, such as data velocity, variety, and veracity, and technological complexity, which represent the main barriers towards the full adoption of the Big Data paradigm. The goal of this work is to define a new software Development Life Cycle for the design and implementation of a Big Data computation. Our proposal integrates two model-driven methods: a first method based on pre-configured services that reduces the cost of deployment and a second method based on custom component development that provides an incremental process of refinement and customization. The proposal is experimentally evaluated by clustering a data set of the distribution of the population in the United States based on contextual criteria.",2018,
21,10.1109/TPDS.2015.2473174,COLLABORATION- AND FAIRNESS-AWARE BIG DATA MANAGEMENT IN DISTRIBUTED CLOUDS,"With the advancement of information and communication technology, data are being generated at an exponential rate via various instruments and collected at an unprecedented scale. Such large volume of data generated is referred to as big data, which now are revolutionizing all aspects of our life ranging from enterprises to individuals, from science communities to governments, as they exhibit great potentials to improve efficiency of enterprises and the quality of life. To obtain nontrivial patterns and derive valuable information from big data, a fundamental problem is how to properly place the collected data by different users to distributed clouds and to efficiently analyze the collected data to save user costs in data storage and processing, particularly the cost savings of users who share data. By doing so, it needs the close collaborations among the users, by sharing and utilizing the big data in distributed clouds due to the complexity and volume of big data. Since computing, storage and bandwidth resources in a distributed cloud usually are limited, and such resource provisioning typically is expensive, the collaborative users require to make use of the resources fairly. In this paper, we study a novel collaboration- and fairness-aware big data management problem in distributed cloud environments that aims to maximize the system throughout, while minimizing the operational cost of service providers to achieve the system throughput, subject to resource capacity and user fairness constraints. We first propose a novel optimization framework for the problem. We then devise a fast yet scalable approximation algorithm based on the built optimization framework. We also analyze the time complexity and approximation ratio of the proposed algorithm. We finally conduct experiments by simulations to evaluate the performance of the proposed algorithm. Experimental results demonstrate that the proposed algorithm is promising, and outperforms other heuristics.",2016,21619883
22,10.1109/HPCS.2017.16,LINKED THESAURI QUALITY ASSESSMENT AND DOCUMENTATION FOR BIG DATA DISCOVERY,"Thesauri are knowledge systems which may ease Big Data access, fostering their integration and re-use. Currently several Linked Data thesauri covering multi-disciplines are available. They provide a semantic foundation to effectively support cross-organization and cross-disciplinary management and usage of Big Data. Thesauri effectiveness is affected by their quality. Diverse quality measures are available taking into account different facets. However, an overall measure is needed to compare several thesauri and to identify those more qualified for a proper reuse. In this paper, we propose a Multi Criteria Decision Making based methodology for the documentation of the quality assessment of linked thesauri as a whole. We present a proof of concept of the Analytic Hierarchy Process adoption to the set of Linked Data thesauri for the Environment deployed in LusTRE. We discuss the step-by-step practice to document the overall quality measurements, generated by the quality assessment, with the W3C promoted Data Quality Vocabulary.",2017,
23,10.1109/ISPA-BDCloud-SustainCom-SocialCom48970.2019.00037,MULTI-GRANULARITY POWER PREDICTION FOR DATA CENTER OPERATIONS VIA LONG SHORT-TERM MEMORY NETWORK,"The increasing numbers of the applications and requirement of cloud computing have made huge power consumption in data centers, which brings the problems of the high cost and resource waste. This problem attracts significant attention from academia and industry. A critical approach to solve this problem is constructing an intelligent energy management system for data centers. Furthermore, an efficient assessment and prediction module of power consumption in data centers is an essential part of the management system. It facilitates cloud service providers to perform workflow scheduling at the minimal cost and energy efficiency management with the requirement of QoS. Since the assessment and prediction of power consumption correlate, this paper presents a multi-granularity approach for power consumption prediction in data centers, which combines multi-task learning with the LSTM network. We first transfer a multi-granularity power prediction problem into a multi-task regression problem to assess and predict the power consumption of data center system maintenance and scheduling operations. Due to the time requirement for workflow and container scheduling, the prediction interval is 30 seconds. Then we propose an efficient long short-term memory network for the multigranularity prediction. The experimental results show our model outperforms other prediction models on the real datasets.",2019,
24,10.1109/BDC.2014.11,A PRACTICAL APPROACH TO SCALABLE BIG DATA COMPUTING FOR THE PERSONALIZATION OF SERVICES AT SAMSUNG,"We observe that the recent advances in big data computing have empowered the personalization of service including model-based services such as speech recognition, face recognition, and context-aware service. Various sources of user's logs can be utilized in remodeling, adapting, and personalizing pretrained models to improve the quality of service. We propose a system that can support store/retrieve data and process them in a scalable manner on top of Samsung' big data infrastructure. An automatic speech recognition (ASR) service such as Samsung's S-Voice, Apple's SIRI is one of the representative examples. Recently advances in ASR married with big data technologies drive more personalized services in many areas of services. A speaker adaptation is now a well-accepted technology that requires huge computation cost in creating a personalized acoustic model and corresponding language model over several billions of Samsung product users. We implement a personalized and scalable ASR system powered by the big data infrastructure which brings data-driven personalized opportunities to voice-enabled services such as voice-to-text transcriber, voice-enabled web search in a peta bytes scale. We verify the feasibility of speaker adaptation based on 107 testers' recordings and obtain about 10% of recognition accuracy. An optimal set of performance optimization is suggested to have the best performance such as workflow compaction, file compression, best file system selection among several distributed file systems.",2014,
25,10.1109/TBDATA.2018.2869165,CORNAC: TACKLING HUGE GRAPH VISUALIZATION WITH BIG DATA INFRASTRUCTURE,"The size of available graphs has drastically increased in recent years. The real-time visualization of graphs with millions of edges is a challenge but is necessary to grasp information hidden in huge datasets. This article presents an end-to-end technique to visualize huge graphs using an established Big Data ecosystem and a lightweight client running in a Web browser. For that purpose, levels of abstraction and graph tiles are generated by a batch layer and the interactive visualization is provided using a serving layer and client-side real-time computation of edge bundling and graph splatting. A major challenge is to create techniques that work without moving data to an ad hoc system and that take advantage of the horizontal scalability of these infrastructures. We introduce two novel scalable algorithms that enable to generate a canopy clustering and to aggregate graph edges. These two algorithms are both used to produce levels of abstraction and graph tiles. We prove that our technique guarantee a quality of visualization by controlling both the necessary bandwidth required for data transfer and the quality of the produced visualization. Furthermore, we demonstrate the usability of our technique by providing a complete prototype. We present benchmarks on graphs with millions of elements and we compare our results to those obtained by state of the art techniques. Our results show that new Big Data technologies can be incorporated into visualization pipeline to push out the size limits of graphs one can visually analyze.",2020,23722096
26,10.1109/ACIT54803.2022.9912749,"TOWARDS AN EFFECTIVE MONITORING, EVALUATION AND LEARNING (MEL) SYSTEM: CHALLENGES AND SOLUTIONS IN A DATA SCIENCE PERSPECTIVE","This paper aims at addressing some challenges that prevent monitoring, evaluation and learning systems from effectively supporting the adaptive management. Using a case study of monitoring the drinking water quality compliance in Rwanda, a proposed four-step approach illustrated how big data can be integrated and leveraged to guarantee accurate and timely data insights as well as reliable predictions and user-friendly reporting. That approach capitalized on Hadoop data lake and R/RStudio to ingest and store structured and unstructured data from different sources, process them, and finally to consume them into interactive dashboards created in Microsoft Power Business Intelligence. Compared to the traditional practice of considering only measurable indicators, more quality key performance indicators were introduced to support the acquisition of additional and triangulated evidence on performance, which should satisfy the information needs of stakeholders.",2022,27705218
27,10.1109/MC.2013.195,BIG DATA'S BIG UNINTENDED CONSEQUENCES,"Businesses and governments exploit big data without regard for issues of legality, data quality, disparate data meanings, and process quality. This often results in poor decisions, with individuals bearing the greatest risk. The threats harbored by big data extend far beyond the individual, however, and call for new legal structures, business processes, and concepts such as a Private Data Commons. The Web extra at http://youtu.be/TvXoQhrrGzg is a video in which author Marcus Wigan expands on his article ""Big Data's Big Unintended Consequences"" and discusses how businesses and governments exploit big data without regard for issues of legality, data quality, disparate data meanings, and process quality. This often results in poor decisions, with individuals bearing the greatest risk. The threats harbored by big data extend far beyond the individual, however, and call for new legal structures, business processes, and concepts such as a Private Data Commons.",2013,15580814
28,10.1109/BigData.2018.8622362,SERVERLESS BIG DATA PROCESSING USING MATRIX MULTIPLICATION AS EXAMPLE,"Serverless computing, or Function-as-a-Service (FaaS), is emerging as a popular alternative model to on-demand cloud computing. Function services are executed by a FaaS provider; a client no longer uses cloud infrastructure directly as in traditional cloud consumption. Is serverless computing a feasible and beneficial approach to big data processing, regarding performance, scalability, and cost effectiveness? In this paper, we explore this research question using matrix multiplication as example. We define requirements for the design of serverless big data applications, present a prototype for matrix multiplication using FaaS, and discuss and synthesize insights from results of extensive experimentation. We show that serverless big data processing can lower operational and infrastructure costs without compromising system qualities; serverless computing can even outperform cluster-based distributed compute frameworks regarding performance and scalability.",2018,
29,10.1109/ACCESS.2019.2936133,BIG DATA VISUALIZATION IN CARDIOLOGY—A SYSTEMATIC REVIEW AND FUTURE DIRECTIONS,"The digital transformations and use of healthcare information system, electronic medical records, wearable technology, and smart devices are increasing with the passage of time. A variety of sources of big data in healthcare are available, such as biometric data, registration data, electronic health record, medical imaging, patient reported data, biomarker data, clinical data, and administrative data. Visualization of data is a key tool for producing images, diagrams, or animations to convey messages from the viewed insight. The role of cardiology in healthcare is obvious for living and life. The function of heart is the control of blood supply to the entire parts of the body. Recent speedy growth in healthcare and the development of computation in the field of cardiology enable researchers and practitioners to mine and visualize new insights from patient data. The role of visualization is to capture the important information from the data and to visualize it for the easiness of doctors and practitioners. To help the doctors and practitioners, the proposed study presents a detailed report of the existing literature on visualization of data in the field of cardiology. This report will support the doctors and practitioners in decision-making process and to make it easier. This detailed study will eventually summarize the results of the existing literature published related to visualization of data in the cardiology. This research uses the systematic literature protocol and the data was collected from the studies published during the year 2009 to 2018 (10 years). The proposed study selected 53 primary studies from different repositories according to the defined exclusion, inclusion, and quality criteria. The proposed study focused mainly on the research work been done on visualization of big data in the field of cardiology, presented a summary of the techniques used for visualization of data in cardiology, and highlight the benefits of visualizations in cardiology. The current research summarizes and organizes the available literature in the form of published materials related to big data visualization in cardiology. The proposed research will help the researchers to view the available research studies on the subject of medical big data in cardiology and then can ultimately be used as evidence in future research. The results of the proposed research show that there is an increase in articles published yearly wise and several studies exist related to medical big data in cardiology. The derivations from the studies are presented in the paper.",2019,21693536
30,10.1109/EMR.2019.2900208,"BIG DATA TECHNOLOGY: CHALLENGES, PROSPECTS, AND REALITIES","We attempt to demonstrate the value of big data to enterprises by interweaving the perceptions, challenges, and opportunities of big data for businesses. While enterprises are aware of the value of big data to their businesses, there are challenges of exploiting big data in terms of data quality and usage. Executives might lack knowledge on how applications are related to one another in the big data ecosystem and the business benefits to reap. We summarize the results of a research study that explores emerging business perceptions of big data. We examine the current practice in 20 large enterprises, each having an annual revenue of more than USD 0.5 billion and present our findings related to executive perceptions. We provide insights on how firms can develop their big data expertise along various dimensions and identify critical ideas to be further investigated to better understand the issues that practitioners and researchers might be equally grappling with.",2019,19374178
31,10.1109/BigData50022.2020.9378483,LARGE-SCALE DATA-DRIVEN SEGMENTATION OF BANKING CUSTOMERS,"This paper presents a novel big data analytics framework for creating explainable personas for retail and business banking customers. These personas are essential to better tailor financial products and improve customer retention. This framework is comprised of several components including anomaly detection, binning and aggregation of contextual data, clustering of transaction time series, and mining association rules that map contextual data to cluster identifiers. Leveraging rich transaction and contextual data available from nearly 60,000 retail and 90,000 business customers of a financial institution, we empirically evaluate this framework and describe how the identified association rules can be used to explain and refine existing customer classes, and identify new customer classes and various data quality issues. We also analyze the performance of the proposed framework and show that it can easily scale to millions of banking customers.",2020,
32,10.1109/ICMTMA.2019.00132,RESEARCH ON INTELLIGENT EVALUATION METHOD OF BUILDING CONSTRUCTION QUALITY BASED ON BIG DATA UNDER THE INFLUENCE OF MULTIPLE FACTORS,"In order to solve the problems of large errors and low consistency in traditional evaluation methods, the intelligent evaluation method of building construction quality is studied in a multi-factor environment combined with big data method. By calculating the index weight coefficient of building construction quality evaluation elements, the consistency index parameters, diversity index and randomness index of building construction quality are obtained. According to the index, the evaluation grade of building construction quality evaluation index is divided, the building construction quality evaluation index system is established, and the evaluation steps of the evaluation index system are optimized. Finally, the research requirement of accurately evaluating the construction quality under the influence of multiple factors is met. Experiments prove that the intelligent evaluation method of building construction quality based on big data under the influence of multiple factors has relatively higher consistency and smaller error compared with the evaluation results of traditional methods, which fully meets the research requirements.",2019,21571473
33,10.1109/ICAML51583.2020.00054,"IN THE ERA OF BIG DATA, TRAINING THE PRACTICAL INNOVATION ABILITY OF APPLIED TALENTS WITH THE CARRIER OF MATHEMATICAL MODELING COMPETITION","In recent years, with the gradual development of innovative technology, innovative technology has become the main driving force for the development of mathematics major in China. With China gradually stepping into the era of big data, it cultivates a large number of mathematics professionals for China through the use of innovative technology. This paper aims to train a large number of outstanding talents for Our country by adopting mathematics competition mode, and improve students&#x0027; data processing ability under the background of big data, so as to comprehensively improve students&#x0027; mathematical innovation ability. This paper mainly takes data modeling contest as the carrier and conducts analysis and research from three aspects: competition platform, teacher formulation and student management, so as to cultivate innovative talents in mathematics. This paper also builds a mathematical innovation practice platform by cultivating students&#x0027; innovative thinking ability and practical ability, so as to improve students&#x0027; mathematical innovation ability and cultivate high-quality mathematical talents.",2020,
34,10.1109/INFRKM.2018.8464790,INFORMATION SYSTEM SUCCESS AND KNOWLEDGE GRID INTEGRATION IN FACILITATING KNOWLEDGE SHARING AMONG BIG DATA COMMUNITY,"Nowadays many domains interested to use big data to improve their decision making, strategic planning, and productivity while communication infrastructure supports various applications for users to share their resources, ideas or experiences. However, accessing, managing, analyzing, and using the rapidly expanding big data had raised challenges especially because of dispersed and heterogeneous nature of data. This review aimed to identify the factors to influence facilitating knowledge sharing among big data community from user satisfaction aspect and the way knowledge grid effect big data sharing by Delone and Maclean model for information system success. The research analysis and results revealed three features of knowledge grid which may influence information and system quality which defined as main factors affect knowledge sharing among the community from member's aspect. It also demonstrates seven factors to measure the facilitating knowledge sharing from community member's satisfaction view and quantity of knowledge sharing. Finally, the review established an initial conceptual model for facilitating knowledge sharing among big data which in follow up research will review by domain experts.",2018,
35,10.1109/SCC.2017.25,BUILDING A DEEP LEARNING CLASSIFIER FOR ENHANCING A BIOMEDICAL BIG DATA SERVICE,"Providing an easily accessible data service with high quality data is important for building big data applications. In this paper, we introduce a big data service for managing and accessing massive-scale biomedical image data. The service includes three major components: a NoSQL database for storing images and data analytics results, a client consisting of a group of query scripts for data access and management, and a data quality enhancement component for improving the performance of data analytics. Low-quality data can result in incorrect analytics results and may lead to no value even harmful conclusions. Therefore, it is important to provide an effective mechanism for ensuring data quality improvement in a big data service. We describe the implement ion of a deep learning classifier to automatically filter low quality data in datasets. To improve the effectiveness of data separation, the classifier is rigorously validated with synthetic data generated by a collection of scientific tools. Design of big data services with data quality improvement as an integral component, along with the best practices collected from this experimental study, will help researchers and practitioners to develop strategies for improving the quality of big data services, building big data applications, and designing machine learning classifiers.",2017,24742473
36,10.1109/CSCI.2017.219,SECURE PRIVACY PRESERVING ACROSS PERSONAL HEALTH DATA AND SINGLE CELL GENOMICS RESEARCH INSPIRE ACADEMIC PEDAGOGY — MERGING BIG DATA MULTIPLATFORM WITH DEEP LEARNING,"Enhancing student academic performance and transdisciplinary ability is challenging, but the time and effort put into accomplishing this ambitious feat is priceless. We develop secure privacy preserving across Personal Health Data (PHD) repository and single-cell genomics research for building an Innovative Systematic Pedagogy for Integrated Research - Education (INSPIRE) (http://americancse.org/events/csce2017/csce17_awards). In this paper we further build a novel, eclectic, and insightful framework based on classical and popular machine learning approaches to help us meet the educational challenge. Our framework focuses on using integrative research technologies to help solve “Education's Performance Prediction Data Mining Crisis” (EPPDMC), by putting to rest issues associated with mining and making best use of big data for educational enhancement, such as multi-source education acquisition, data fusion, and unstructured data analysis. We exploit the uses of deep learning, text classification, and semi-supervised learning approaches to solve challenging problems that educators face when analyzing multiplatform big data involved in education, research and training students. Based on new machine learning approached we developed for genomic big-data research and in combination with machine learning methods (http://americancse.org/events/csce2017/keynotes_lectures/yang_talk) and the vast availability of education data available to us, not only can we utilize structured, unstructured, and even multi-media data, but while engaging in leaning intelligent thinking along the way, we can also maximize the utilization of big data by studying the motion and performance of these data. Hence we build the INSPIRE model that can further incorporate Student Face Expression in Class (SFEiC) to help educators and managers make further improvements as they become involved in the teaching-learning process. This research further facilitates the effectiveness of the INSPIRE model.",2017,
37,10.1109/ACCESS.2021.3089100,DEVELOPMENT OF USABILITY ENHANCEMENT MODEL FOR UNSTRUCTURED BIG DATA USING SLR,"Unstructured text contains valuable information for a range of enterprise applications and informed decision making. Text analytics is used to extract valuable insights from unstructured big data. Among the most significant challenges of text analytics, quality and usability are critical in affecting the outcome of the analytical process. The enhancement in usability is important for the exploitation of unstructured data. Most of the existing literature focuses on the usability of structured data as compared to unstructured data whereas big data usability has been discussed merely in the context of its assessment. The existing approaches do not provide proper guidelines on the usability enhancement of unstructured data. In this study, a rigorous systematic literature review using PRISMA framework has been conducted to develop a model enhancing the usability of unstructured data bridging the research gap. The recent approaches and solutions for text analytics have been investigated thoroughly. The usability issues of unstructured text data and their consequences on data preparation for analytics have been identified. Defining the usability dimensions for unstructured big data, identification of the usability determinants, and developing a relationship between usability dimension and determinants to derive usability rules are the significant contributions of this research and are integrated to formulate the usability enhancement model. The proposed model is the major outcome of the research. It contributes to make unstructured data usable and facilitates the data preparation activities with more valuable data that eventually improve the analytical process.",2021,21693536
38,10.1109/ICVRIS.2019.00103,RESEARCH ON CONSTRUCTION OF EVALUATION INDEX SYSTEM OF AGRICULTURAL PROFESSIONAL MANAGERS FOR BIG DATA,"The application of big data technology in agriculture is the core development direction to realize the maximum utilization of agricultural data information. It is an important task to cultivate high-quality agricultural talents to promote the sustainable and rapid development of intelligent agriculture. Based on the demand side and supply side of intelligent agricultural education system, the big data platform of evaluation index system for professional agricultural managers is constructed. Firstly, this paper studies the application of big data technology in the evaluation index system. Then a rational and effective evaluation index system of agricultural professional managers is constructed by using the network analytic hierarchy process and the grey theory model.",2019,
39,10.1109/ICABCD.2018.8465422,ENHANCEMENT OF TASK SCHEDULING TECHNIQUE OF BIG DATA CLOUD COMPUTING,"Big Data refers to the large size chunks of data that traditional computing approaches can handle. Despite the truth of having huge cloud systems to manage this data nowadays, there are many challenges related to performing the tasks in the cloud within the expected timeframe using the minimum number of resources possible. The necessity to fulfil user requirements is the main reason of having studies for optimizing the cloud computing of big data in terms of latency, bandwidth, execution time and resource utilization. Therefore, we proposed an efficient task scheduling technique capable to manage big data processing and storage in the cloud in an efficient way that meets user expectations. We provide a solution that involves multiple number of metrics necessary to optimize the solution of big data cloud computing. Our designed model consists of multiple control nodes that control the work done on multiple compute nodes. We used a load balancing algorithm to manage task scheduling on the compute nodes so we make sure that all nodes have equal balance of loads at all times. We simulate different scenarios to prove the concept of the study including latency, task execution time, bandwidth and resource utilization. This study achieved 31.4% as an average decrease percentage in task execution time and this has led to 11.36% utilization of resources.",2018,
40,10.1109/TNSE.2020.2997376,SERVICE FUNCTION CHAIN DEPLOYMENT AND NETWORK FLOW SCHEDULING IN GEO-DISTRIBUTED DATA CENTERS,"Network Function Virtualization (NFV), as an emerging solution to virtualize network services traditionally running on proprietary, dedicated devices, can effectively reduce the cost of big data processing service providers and improve Quality of Service (QoS) by running a chain of ordered Virtual Network Functions (VNFs) on commodity hardware. One fundamental and critical problem of big data processing with NFV is how to deploy the chained VNFs and dispatch corresponding network flows to process the big data traffics so that the service cost can be minimized with guaranteed QoS. In this paper, we study the problem of VNF deployment and flow scheduling in distributed data centers with joint consideration of the service requirements and the resource capacity, and prove its NP-hardness through reduction from the k-level uncapacitated facility location problem. A two-phased algorithm is also devised by first balancing VNF resource requirements and then selecting VNF locations with a low complexity of O(M2log2M) and an approximation ratio of K + ρ. Finally, with extensive simulation experiments, the result shows that our algorithm can efficiently reduce the total cost of VNF deployment and flow communication in various scenarios.",2020,2334329X
41,10.1109/HPCC-SmartCity-DSS50907.2020.00027,AGE-AWARE QUERY EVALUATION FOR BIG DATA ANALYTICS IN MOBILE EDGE CLOUDS,"The advancement of 5G technology and extensive usage of smart devices have led to a deluge of data. It is invaluable to analyze such big data by issuing queries to derive real-time business insights for better, smarter and fact-based decisions. Timely big data analytics are crucial in many service domains, thus the age of data (AoD) required by queries is emerging as a novel metric that measures the freshness of data and evaluates the quality of data analytics. Traditional big data analytics that are performed in remote clouds cannot satisfy the AoD requirements of queries, due to the congested core networks and long transmission latency between users and clouds. The technique of mobile edge computing (MEC) is expected to reduce the age and guarantee the timeliness of queries for big data analytics, by processing data at edge cloudlets close to users. In this paper, we investigate a problem of age-aware query evaluation for big data analytics in a mobile edge cloud network. We first formulate the problem with certain data processing latency as an Integer Linear Program (ILP). We then develop an online learning algorithm for the problem with uncertain data processing latency. We finally evaluate the performance of proposed algorithms against existing studies by simulations and testbed implementations. Evaluation results show that the proposed algorithms outperform existing works by achieving 20% lower AoD.",2020,
42,10.1109/BigData.2018.8622624,"TECHNOLOGY ENABLERS FOR BIG DATA, MULTI-STAGE ANALYSIS IN MEDICAL IMAGE PROCESSING","Big data medical image processing applications involving multi-stage analysis often exhibit significant variability in processing times ranging from a few seconds to several days. Moreover, due to the sequential nature of executing the analysis stages enforced by traditional software technologies and platforms, any errors in the pipeline are only detected at the later stages despite the sources of errors predominantly being the highly compute-intensive first stage. This wastes precious computing resources and incurs prohibitively higher costs for re-executing the application. The medical image processing community to date remains largely unaware of these issues and continues to use traditional high-performance computing clusters, which incur a high operating cost due to the use of dedicated resources and expensive centralized file systems. To overcome these challenges, this paper proposes an alternative approach for multi-stage analysis in medical image processing by using the Apache Hadoop ecosystem and offering it as a service in the cloud. We make the following contributions. First, we propose a concurrent pipeline execution framework and an associated semi-automatic, real-time monitoring and checkpointing framework that can detect outliers and achieve quality assurance without having to completely execute the expensive first stage of processing thereby expediting the entire multi-stage analysis. Second, we present a simulator to rapidly estimate the execution time for a given multi-stage analysis, which can aid the users in deciding the appropriate approach for their use cases. We conduct empirical evaluation of our framework and show that it requires 76.75% lesser wall time and 29.22% lesser resource time compared to the traditional approach that lacks such a quality assurance mechanism.",2018,
43,10.1109/ICAIE53562.2021.00126,INSIGHTS.WISDOM.LEADERSHIP: A STUDY OF TALENT CULTIVATION OF BIG DATA,"With the continuous progress and innovation of information technology, Big Data has become one of the most discussed topics in recent years. It alters how the new knowledge is learned and provides opportunities and unprecedented challenges for big data talent training in the educational field. Traditional talent training objectives are not clear and lack curriculum resources and practical teaching insufficiency. This paper analyzes how to cultivate innovative big data talents under the information environment. Our approach is based on three tenets: (1) INSIGHTS: Combine international theories and China's social development characteristics to build a big data technology curriculum system, focusing on the cultivation of insight skills. (2) WISDOM: Establish a ""WISDOM"" co-sharing learning space that focuses on Ubiquitous learning, which combines online + offline (3) LEADERSHIP: Cultivate students' innovation ability, leadership, and teamwork through project-based learning. It has been proved that the innovation of the big data talent training model has improved the quality of talent training and cultivate integrated talents with excellent teaching effects to meet the needs of modern society.",2021,
44,10.1109/ICMEIM51375.2020.00056,TO COMPREHENSIVELY IMPROVE STUDENTS' PROFESSIONAL QUALITY UNDER THE GUIDING ROLE OF THE COMMUNIST PARTY IN THE CONTEXT OF BIG DATA,"Big data can summarize and discover massive information. Under the background of modernization, vocational colleges play the leading role of the communist party and strengthen the participation of big data in student management, which is conducive to improving the scientific and effective management of students, it can better help relevant personnel use big data for student learning and life.",2020,
45,10.1109/BigData50022.2020.9378094,EVANET: AN EXTREME VALUE ATTENTION NETWORK FOR LONG-TERM AIR QUALITY PREDICTION,"Air quality affects social activities and human health. Air quality prediction, especially for extreme events such as severe haze pollution, plays an essential guiding role in government decision-making and outdoor activity scheduling. Established prediction models face the challenges of forecasting extreme values and long-term tendency. In this paper, we propose an extreme value attention network (EvaNet) based on encoder and decoder framework to achieve long-term air quality prediction. This model designs an extreme value attention mechanism to alleviate the impact of sudden changes on prediction. In addition, to capture long-term dependence relationships, EvaNet introduces a temporal attention mechanism. Integrating the dual attention mechanisms, the extracted features are fed into a decoder to yield the final prediction. The experiments evaluated on two real-world air quality datasets show the superiority of our method against other state-of-the-art baselines.",2020,
46,10.1109/BigData.2017.8258090,A DATA-DRIVEN APPROACH FOR MULTIVARIATE CONTEXTUALIZED ANOMALY DETECTION: INDUSTRY USE CASE,"Anomaly detection is the process of discovering some anomalous behaviour in the real-time operation of a system. It is a difficult task, since in a general case (multivariate anomaly detection) an anomaly can be related to the behaviour of several parameters which are not necessarily behaving anomalously per se, but their (complex) relation is anomalous (not usual/normal). This implies the need for a very efficient modeling of the normal behaviour in order to know what should be treated as anomalous/outlier/unusual. Consequently, classical model-driven approaches, due to their focus on the selected parameters for creating models, are not able to model the behaviour of the whole system. This is why data-driven approaches for anomaly detection are getting even more important for the industry use cases where hundreds (thousands) of parameters should be taken into account. However, current approaches are usually focused on the univariate anomaly detection (or some variations of it), so without observing the entire space of relations since the computation is very complex. In this paper we present a novel approach for the multivariate anomaly detection that is based on modeling and managing the streams of variations in a multidimensional space. The main advantage of this approach is the possibility to observe the relations between variations in a large set of parameters and create clusters of “normal/usual” variations. In order to ensure scaling, which is one of the most challenging requirements, the approach is based on the usage of the big data technologies for realizing data analytics tasks/calculations. The approach is realized as a part of D2Lab (Data Diagnostics Laboratory) framework and has been applied in several industrial use cases. In this paper we present an interesting usage for the anomaly detection in the process of functional testing of home appliances (in particular case refrigerators) after manufacturing/assembling process. It has been done for a big vendor (Whirlpool), who expects huge saving in testing and improved customer satisfaction from this approach.",2017,
47,10.1109/ACCESS.2020.3015016,SEQUAL: BIG DATA TOOL TO PERFORM QUALITY CONTROL AND DATA PREPROCESSING OF LARGE NGS DATASETS,"This paper presents SeQual, a scalable tool to efficiently perform quality control of large genomic datasets. Our tool currently supports more than 30 different operations (e.g., filtering, trimming, formatting) that can be applied to DNA/RNA reads in FASTQ/FASTA formats to improve subsequent downstream analyses, while providing a simple and user-friendly graphical interface for non-expert users. Furthermore, SeQual takes full advantage of Big Data technologies to process massive datasets on distributed-memory systems such as clusters by relying on the open-source Apache Spark cluster computing framework. Our scalable Spark-based implementation allows to reduce the runtime from more than three hours to less than 20 minutes when processing a paired-end dataset with 251 million reads per input file on an 8-node multi-core cluster.",2020,21693536
48,10.1109/ICIRCA.2018.8597313,CHALLENGES ASSOCIATED WITH QUALITY AND BIG DATA TOOL BASED STUDY IN BLENDED LEARNING MODELS,"The main aim of this study focus on blended learning and its ways of maintaining the quality studies based on various blended models. Most of the issues faced by students and teachers and the ways of resolving forms the discussion of the paper. As blended learning is based on traditional and face-to -face method, teaching and learning components such as assessment including prior knowledge of the topic, evaluation methods of assessment and appeals, identification of barriers such as language, Blended learning (BL) literacy & needs, fees structure of the course, balanced workload, recognition towards achievement, monitoring continuous improvement of individual student etc., forms the quality along with different challenges and the ways of reaching the solution. It is more evident that big data which is an innovative driving tool in education supports teaching quality with various impacts on individual student long term achievement.",2018,
49,10.1109/BigComp.2018.00116,CORRELATION ANALYSIS BETWEEN VEHICULAR TRAFFIC AND PM USING SENSOR BIG DATA,"Air pollution has a negative impact on human life around the world. Particularly, researches on PM, which is a harmful air pollution source, are being actively carried out these days. To help these environmental problems, we propose the system through this paper. The proposed system is a useful system for analyzing environmental problems related to air pollution. It uses various information collectively to generate high quality data for analysis. In addition, we analyzed the correlation between PM and vehicular traffic in the Daejeon area. As a result, we found that the speed of the vehicle has a greater effect on the road PM value than the number of vehicles.",2018,23759356
50,10.1109/ICCEAI55464.2022.00157,RESEARCH AND APPLICATION OF IMPROVED K-MEANS ALGORITHM BASED ON EDUCATIONAL BIG DATA,"This paper aims to conduct in-depth mining of student performance data. It is helpful to provide teachers with teaching strategies and improve teaching quality. Compared with the previous research on the k-means algorithm, the main innovation of this paper lies in the selection of the initial cluster center and providing teaching strategies for universities. First, we collect the students' transcripts of three subjects and process the original data including cleaning data, extracting key fields, and integrated processing of data. Then, we use the k-means algorithm, to get the cluster center and the category. Finally, we visualized the clustering results.",2022,
51,10.1109/IWSM-Mensura.2016.026,QUALITY EVALUATION FOR BIG DATA: A SCALABLE ASSESSMENT APPROACH AND FIRST EVALUATION RESULTS,"High-quality data is a prerequisite for most types of analysis provided by software systems. However, since data quality does not come for free, it has to be assessed and managed continuously. The increasing quantity, diversity, and velocity that characterize big data today make these tasks even more challenging. We identified challenges that are specific for big data quality assessments with particular emphasis on their usage in smart ecosystems and make a proposal for a scalable cross-organizational approach that addresses these challenges. We developed an initial prototype to investigate scalability in a multi-node test environment using big data technologies. Based on the observed horizontal scalability behavior, there is an indication that the proposed approach also allows dealing with increasing volumes of heterogeneous data.",2016,
52,10.1109/BigData52589.2021.9672004,A SOCIO-TECHNICAL FRAMEWORK FOR MEASURING ORGANIZATIONAL CAPACITY DURING CYBERINFRASTRUCTURE DIFFUSION,"This paper presents a socio-technical framework for measuring organizational capacity for cyberinfrastructure (CI) implementation, adoption, and diffusion at the team’s level. CI implementation is an example of big data science project in data-intensive projects funded by the US National Science Foundation (NSF), providing a unique case for understanding big data science teams from an important field that is academic and scientific in nature. We argue that organizational capacity can be defined by the three dimensions of foundational technical expertise, daily social interactions, and enduring organizational qualities. We provide scale items for measuring these three dimensions, using a questionnaire in a self-reported and self-reflexive fashion. The overall average score and the individual composite scores of the three dimensions (and their sub-dimensions) can be used as feedback and capacity building activities as intervention strategies. Future research will statistically validate the framework using exploratory and confirmatory factor analyses.",2021,
53,10.1109/BigData.2015.7363827,ROBUST CROWD BIAS CORRECTION VIA DUAL KNOWLEDGE TRANSFER FROM MULTIPLE OVERLAPPING SOURCES,"One of the largest constituents of big data is the crowdsourced or user-generated data which contain a wide range of valuable information. However, they are inherently biased and possibly spammed, making trustworthy information extraction an imperative task. As a special case, we study reviewer-posted ratings for products. The biased ratings can lead to disappointed customers due to overrated products, and reduced revenues of business owners caused by undeserved negative ratings. To distill objective product quality measurements, most existing methods try to infer unbiased ratings from the raw ratings alone, and may not overcome the inherent bias to recover the underlying true ratings. Though improved bias corrections have been achieved with domain expert helps, the overhead of expert efforts can be rather expensive in practice. We exploit the variety of big data and adopt a multiple source mining approach, which finds trustworthy measurements without domain expert, but with knowledge crowdsourced and transferred from external domains. We address the challenges that the multiple data sources are 1) inherently heterogeneous, 2) at most only partially overlapping and 3) biased by themselves. We explore and analyze the strengths and weaknesses of various knowledge transfer strategies. We then propose Consensus Ranking Dual Transfer (CRDT) to handle the above challenges by identifying ""anchor reviewers"" as a bridge for robust ""dual transfer"", and removing bias in individual sources via consensus ranking aggregation. Experiments on real-world rating datasets demonstrate that the proposed approach can deliver more robust bias correcting effects than the baselines and can identify abnormal reviewers.",2015,
54,10.1109/BigData.2016.7840729,WHEN REMOTE SENSING DATA MEET UBIQUITOUS URBAN DATA: FINE-GRAINED AIR QUALITY INFERENCE,"With the growth of the economy, the air quality is becoming a serious issue, especially for those developing countries, such as China. Therefore, it is very important for the public and the government to access real-time air quality information. Unfortunately, the limited number of air quality monitoring stations is unable to provide fine-grained air quality information in a huge city, such as Beijing. One cost-effective approach for obtaining fine-grained air quality information is to infer air quality with those measured data at the monitoring stations. However, existing inference techniques have poor performance because of the extreme data sparsity problem (e.g., only 0.2% data are known). We observe that remote sensing has been a high-quality data source about urban dynamics. In this paper, we propose to integrate remote sensing data and ubiquitous urban data for air quality inference. There are two main challenges, i.e., data heterogeneity and incomplete remote sensing data. In response to the challenges, we propose a two-stage inference approach. In the first stage, we use the AOT remote sensing data and the meteorological data to infer the air quality values with an Artificial Neural Network (ANN). After this stage, we significantly reduce the percentage of empty cells in the tensor representing the spatio-temporal air quality values. In the second stage, we propose a tensor decomposition method to infer the complete set of air quality values. We use the spatial features (i.e., road features and POI features) and the temporal features (i.e., meteorological features) as the constraints in the tensor decomposition process. Experiments with real data sets show that our approach has profound performance advantage over the state-of-the-art methods, such as U-Air.",2016,
55,10.1109/IGARSS.2019.8900190,PRACTICES AND EXPERIENCES IN HIGH VOLUMES OF SATELLITE DATA MANAGEMENT,"High volumes of satellite data management within an organization is still challenging and daunting in the era of big data. The increasing information technology costs and limited budgets, growing satellite data needs, data availability across multiple teams and projects, strategic goals of organization, and expected project outcomes require better satellite data management mechanism and system to facilitate research and development activities. An organization level centralized satellite data repository is a practical solution to satisfy these requirements. This paper describes the best practices and experiences from building such a central satellite data repository within our organization, including data management strategy and policy, scalable and extensible system infrastructure, comprehensive data management system, and technical support and user assistance. These practices can be borrowed and applied in other organizations with similar requirements.",2019,21536996
56,10.1109/ICSTW.2015.7107424,TESTING BIG DATA (ASSURING THE QUALITY OF LARGE DATABASES),"The volume and variety of modern day databases presents a particular challenge to the system testing community. The question is how to go about testing such large collections of various data types ranging from tables to texts and images. To test those applications which use them, these conglomerations of multiple data object types have to be automatically generated and validated. There is no other way but to automate the test process. This contribution outlines the challenge and presents an automated approach to setting up and testing big data bases. At the end a case study of a large data warehouse is discussed with lessons learned from that industrial test project.",2015,
57,10.1109/TSUSC.2018.2881466,A TENSOR-BASED OPTIMIZATION MODEL FOR SECURE SUSTAINABLE CYBER-PHYSICAL-SOCIAL BIG DATA COMPUTATIONS,"Secure cyber-physical-social big data computations are being increasingly used to protect the users' data security in cyber-physical-social systems (CPSS). Despite the increasing popularity, how to process the tasks of the secure cyber-physical-social big data computations, while taking care of the energy consumption and meeting the users' requirements, remains challenging. To address the problem, in this work, we propose a novel tensor-based optimization model for the secure sustainable cyber-physical-social big data computations. The proposed model is a general and fine-grained model, which can jointly optimize the execution time, energy consumption, reliability, and quality of experience, and can comprehensively take into account step, task, time slot, type, node, core, cryptosystem, and security level. To our knowledge, this is the first study to holistically optimize the tasks in the secure cyber-physical-social big data computations. To illustrate the proposed model, the case study of the secure high-order Lanczos in cloud-assisted CPSS is presented. Finally, the proposed model is empirically evaluated by using multi-objective optimization, and the extensive results demonstrate that from the users' perspective our proposed tensor-based optimization model is preferable for the secure sustainable cyber-physical-social big data computations.",2020,23773790
58,10.1109/ICCSMT51754.2020.00049,BLOCKCHAIN-BASED POWER GRID DATA ASSET MANAGEMENT ARCHITECTURE,"Although the continuous construction of smart grid and power Internet of Things has brought massive data to grid enterprises, the cost of data storage, operation and maintenance is gradually increasing, on the contrary, the value of data has not been effectively mined and utilized, the main reason is that the current data asset management of State Grid Corporation of China is still an infancy stage, and still faces the problems of data aggregation quality, safety and compliance control, shared application scope, and management mechanism efficiency and so on. In order to solve the above problems, Big Data Center of State Grid Corporation of China takes advantage of the blockchain with distributed consensus autonomy and data storage, non-tampering and traceability, and business intelligence contract script, explores the application of blockchain in power grid data asset management, and then, tries to construct the architecture of power grid data asset management based on blockchain, which is to promote security compliance and open sharing of power grid data operation, and realize the efficient management and operation of power grid data assets.",2020,
59,10.1109/ISAIEE55071.2021.00094,APPLICATION OF BIG DATA INFORMATION PROCESSING TECHNOLOGY IN THE IMPROVEMENT OF PRACTICAL ENGLISH TEACHING MODE,"Under the background of strengthening quality-oriented education, the English teaching mode in many schools is still the traditional knowledge-based teaching mode. This kind of teaching mode is far from practical application for language learning with strong application. This article analyzes the existing problems of traditional English teaching, and proposes the basic ways and methods to use big data technology to improve the practical English teaching mode.",2021,
60,10.1109/ICPICS50287.2020.9202267,RESEARCH ON COLLECTION AND PREPROCESSING OF MULTISOURCE HETEROGENEOUS ELEVATOR DATA,"With the rapid development of China's economy, the elevator has become a basic tool in our daily life, and the safety risks brought by the elevator are also increasingly serious. It is necessary to use the technology of big data to mine the existing multi-source Heterogeneous elevator data, so as to find out the various rules of fault. Data collection and preprocessing are the important parts of big data mining and analysis. In this paper, through the research of collection and preprocessing of multi-source heterogeneous elevator data, the quality of inputting data which is needed for the data modeling later on is improved, so as to provide better service for elevator fault prediction.",2020,
61,10.1109/ACCESS.2020.3005268,CHALLENGES AND SOLUTIONS FOR PROCESSING REAL-TIME BIG DATA STREAM: A SYSTEMATIC LITERATURE REVIEW,"Contribution: Recently, real-time data warehousing (DWH) and big data streaming have become ubiquitous due to the fact that a number of business organizations are gearing up to gain competitive advantage. The capability of organizing big data in efficient manner to reach a business decision empowers data warehousing in terms of real-time stream processing. A systematic literature review for real-time stream processing systems is presented in this paper which rigorously look at the recent developments and challenges of real-time stream processing systems and can serve as a guide for the implementation of real-time stream processing framework for all shapes of data streams. Background: Published surveys and reviews either cover papers focusing on stream analysis in applications other than real-time DWH or focusing on extraction, transformation, loading (ETL) challenges for traditional DWH. This systematic review attempts to answer four specific research questions. Research Questions: 1)Which are the relevant publication channels for real-time stream processing research? 2) Which challenges have been faced during implementation of real-time stream processing? 3) Which approaches/tools have been reported to address challenges introduced at ETL stage while processing real-time stream for real-time DWH? 4) What evidence have been reported while addressing different challenges for processing real-time stream? Methodology: A systematic literature was conducted to compile studies related to publication channels targeting real-time stream processing/joins challenges and developments. Following a formal protocol, semi-automatic and manual searches were performed for work from 2011 to 2020 excluding research in traditional data warehousing. Of 679,547 papers selected for data extraction, 74 were retained after quality assessment. Findings: This systematic literature highlights implementation challenges along with developed approaches for real-time DWH and big data stream processing systems and provides their comparisons. This study found that there exists various algorithms for implementing real-time join processing at ETL stage for structured data whereas less work for un-structured data is found in this subject matter.",2020,21693536
62,10.1109/ICISCE50968.2020.00458,CONTROLLED SOURCE ELECTROMAGNETIC DATA DENOISING BASED ON CEEMD AND CORRELATION ANALYSIS,"Controlled-source electromagnetic method (CSEM) signals are inevitably contaminated by man-made noises. For this reason, a new CSEM data processing method was proposed. First, the 50 Hz powerline interference was removed by FFT method, followed by the CEEMD method to remove baseline drift, and finally the correlation analysis method was used to select the high-quality signal. The method was applied to the processing of measured data in Huidong, Sichuan. As a conclusion, the presented method can effectively remove the strong cultural noise from raw CSEM data and preserve the useful signal completely; the apparent resistivity curves acquired by using the filtered data are improved significantly upon the previous.",2020,
63,10.1109/DSA.2019.00025,CONTROLLABLE CORRELATION BIG DATA DYNAMIC PREDICTION MODEL FOR MOBILE COMMUNICATION,"At present, there were many mobile communication deviees in China. In order to improve the prediction effect of mobile communication and realize the scalability and dynamic evolution of the prediction system, a dynamic prediction model of controllable correlation big data for mobile communication was proposed. The application experiment was carried out, and the experimental results of various models were compared and analyzed. The experimental results showed that the model fully integrated the openness, extensibility and big data dynamic prediction advantages of mobile communication. The communication quality, total duration and quantitative qualitative prediction of con trollable correlation big data based on mobile communication in big data environment were realizedÇ4bstract).",2020,
64,10.1109/IPEC49694.2020.9115188,ANALYSIS OF UNIVERSITY STUDENTS EMPLOYMENT RECOMMENDATION SYSTEM BASED ON APRIORI ALGORITHM,"“Employment Recommendation System for College Students Based on Apriori Algorithm”, which has taken the big data of the web recruitment platform in China as its technology and service foundation, has adopted Apriori recommendation algorithm to restructure relevant quality information on employment intelligently by data analyzing information on employment of all large recruitment platforms. The system has achieved the accurate employment recommendation for college students, which has improved not only the job satisfaction and employment rate of college students, but the work efficiency of employment service for college students.",2020,
65,10.1109/TKDE.2019.2936565,A PRIVACY-PRESERVING DISTRIBUTED CONTEXTUAL FEDERATED ONLINE LEARNING FRAMEWORK WITH BIG DATA SUPPORT IN SOCIAL RECOMMENDER SYSTEMS,"Nowadays, the booming demand of big data analytics and the constraints of computational ability and network bandwidth have made it difficult for a stand-alone agent/service provider to provide suitable information for every user from the large volume online data within the limited time. To handle this challenge, a recommender system (RS) can call in a group of agents to collaborate to learn users' preference and taste, which is known as a distributed recommender system (DRS). DRSs can improve the accuracy of a traditional RS by requesting agents to share information with each other. However, it is challenging for DRSs to make personalized recommendations for each user due to the large amount of candidates. In addition, information sharing among agents raises a privacy concern. Thus, we propose a privacy-preserving DRS in this paper, and then model each service provider as a distributed online learner with context-awareness. Service providers collaborate to make personalized recommendations by learning users' preferences according to the user context and users' history behaviors. We adopt the federated learning framework to help train a high quality privacy- preserving centralized model over a large number of distributed agents which is probably unreliable with relatively slow network connections. To handle big data scenario, we build an item-cluster tree to deal with online and increasing datasets from top to the bottom. We further consider the structure of social network and present an efficient algorithm to avoid more performance loss adaptively. Theoretical proofs show that our proposed algorithm can achieve sublinear regret and differential privacy protection simultaneously for service providers and users. Numerical results confirm that our novel framework can handle increasing big datasets and strike a trade-off between privacy-preserving level and the prediction accuracy.",2021,23263865
66,10.1109/ICBDA.2018.8367655,A BRIEF ANALYSIS OF THE KEY TECHNOLOGIES AND APPLICATIONS OF EDUCATIONAL DATA MINING ON ONLINE LEARNING PLATFORM,"With the rapid development of the Internet and communication technology, online education has drawn more and more attention, online learning platforms, on the other hand, store massive learner behavioral data and educational data. How to effectively analyze and utilize the data to improve the quality of online education has become a key issue urgently needed to be solved in the field of big data in education(BDE), educational data mining(EDM) is exactly an effective and practical method and means of applying BDE. Therefore, EDM is an important academic research hotspot in the field of EDM. Firstly, the paper introduces the basic concepts of BDE, EDM and online learning platform, and then elaborates on the process of how educational data mining transforms raw data into knowledge. Finally, the key technologies of data mining are classified according to their uses, and gives its application in online education scene. The paper can provide some guidance for the research and application of educational data mining based on online education.",2018,
67,10.1109/CompComm.2018.8780724,DESIGN AND IMPLEMENTATION OF AIR QUALITY DATA PROCESSING SYSTEM BASED ON BIG DATA TECHNOLOGY,"In this article, we design and implement an air quality data processing system based on big data technology, which leverages Flume to acquire data, Kafka to cache data, and Storm to process data, and finally save the results to the in-memory database called Redis and distributed database called HBase. In Redis, only the latest data of each monitoring point is saved, and the original data is deleted periodically to reduce the memory load. An application is implemented by which people can get real-time data from Redis and query historical data from HBase. Through experiments, it is proved that querying real-time data from Redis is faster than relational database. The system solves the issues of poor storage capacity and slow query speed when using relational database in the big data environment, and improves the efficiency of data processing significantly.",2018,
68,10.1109/SERVICES.2019.00113,TOWARDS A METHODOLOGY FOR EVALUATING BIG DATA PLATFORMS,"In recent years, several new multipurpose Big Data platforms have emerged. They are used in various application domains with diverse requirements. Evaluating complex Big Data solutions is not a trivial task, due to the need to assess their utility in both quantitative and qualitative terms based on existing use cases. In this short paper, we discuss the requirements and the methodology for such an evaluation. We also discuss how benchmarking could be part of such an evaluation methodology.",2019,23783818
69,10.1109/ACCESS.2018.2857845,VPL-BASED BIG DATA ANALYSIS SYSTEM: UDAS,"Over the past five years, research on big data analysis has been actively conducted, and many services have been developed to find valuable data. However, low quality of raw data and data loss problem during data analysis make it difficult to perform accurate data analysis. With the enormous generation of both unstructured and structured data, refinement of data is becoming increasingly difficult. As a result, data refinement plays an important role in data analysis. In addition, as part of efforts to ensure research reproducibility, the importance of reuse of researcher data and research methods is increasing; however, the research on systems supporting such roles has not been conducted sufficiently. Therefore, in this paper, we propose a big data analysis system named the unified data analytics suite (UDAS) that focuses on data refinement. UDAS performs data refinement based on the big data platform and ensures the reusability and reproducibility of refinement and analysis through the visual programming language interface. It also recommends open source and visualization libraries to users for statistical analysis. The qualitative evaluation of UDAS using the functional evaluation factor of the big data analysis platform demonstrated that the average satisfaction of the users is significantly high.",2018,21693536
70,10.1109/ICEMIS.2016.7745338,BIG DATA APPLICATIONS IN SMART CITIES,"A “Smart City” generally means a technologically advanced city, that is able to understand its environment through analyzing its data so that it immediately makes changes to solve issues and to improve the residents' quality of life. The huge volume, high velocity and wide variety of city's data require the utilization of “Big Data” technologies to gain valuable insights from it. This paper reviews the applications and, hence, the potentials where Big Data technology can drive a city to be smart. Starting from investigating the visibility of the city, which means collecting data from all networks, devices and sensors that embedded in its infrastructure. Continuing to explain how can this data become valuable by passing different processing stages, and by applying advanced analyzing Big Data platforms on data. The smartness of the data-driven city is achieved by visualizing the data in useful shape in order to improve any city's system application. The review has also included few real world examples that shows the practical applications of Big Data in a Smart City in the domains of smart energy, smart public safety and smart traffic systems.",2016,
71,10.1109/TrustCom/BigDataSE.2018.00235,OPTIMIZED DATA DE-IDENTIFICATION USING MULTIDIMENSIONAL K-ANONYMITY,"In the globalized knowledge economy, big data analytics have been widely applied in diverse areas. A critical issue in big data analysis on personal information is the possible leak of personal privacy. Therefore, it is necessary to have an anonymization-based de-identification method to avoid undesirable privacy leak. Such method can prevent published data form being traced back to personal privacy. Prior empirical researches have provided approaches to reduce privacy leak risk, e.g. Maximum Distance to Average Vector (MDAV), Condensation Approach and Differential Privacy. However, previous methods inevitably generate synthetic data of different sizes and is thus unsuitable for general use. To satisfy the need of general use, k-anonymity can be chosen as a privacy protection mechanism in the de-identification process to ensure the data not to be distorted, because k-anonymity is strong in both protecting privacy and preserving data authenticity. Accordingly, this study proposes an optimized multidimensional method for anonymizing data based on both the priority weight-adjusted method and the mean difference recommending tree method (MDR tree method). The results of this study reveal that this new method generate more reliable anonymous data and reduce the information loss rate.",2018,23249013
72,10.1109/AIE57029.2022.00065,THE APPLICATION OF LDA MODEL IN THE ANALYSIS OF JOB TALENT DEMAND UNDER BIG DATA TECHNOLOGY,"With the arrival of the data era, data analysis job has leaped to become one of the most popular jobs. This paper aims to provide reference for the development direction of job seekers through the study of recruitment information. This paper crawl 1023 data analysis job recruitment information of Guangzhou City from the ""future without worry"" website. After cleaning the data, firstly, descriptive statistical analysis is made on the basic recruitment information and Chinese word segmentation is processed on the text data, secondly, based on the results of word segmentation, the clustering analysis of vocational skills is carried out, finally, the positions are classified based on the LDA theme model. It is found that the data job requirements for job seekers' technical ability are focused on the mastery of SQL language, python, R software and other programming software, as well as statistical analysis software such as SAS and spss. Requirements for basic quality include the ability to collaborate with other departments, management ability, team cooperation ability and other personal soft strength. Finally, the classification results show that data analysis positions can be roughly divided into three types: partial technology, partial business and commercial data analysis.",2022,
73,10.1109/BigData.2015.7363850,SMOG DISASTER FORECASTING USING SOCIAL WEB DATA AND PHYSICAL SENSOR DATA,"Smog disaster is a type of air pollution event that negatively affects people's life and health. Forecasting smog disasters may largely reduce potential loss that they may cause. However, it is a great challenge since smog disasters are often caused by many complex factors. With the availability of huge amounts of data from the social web and physical sensors, covering information of air quality, meteorology, social event, human mobility, people's opinion, etc., it becomes possible to utilize such big data to forecast smog disasters. Especially, we can investigate the effect of social activities in smog disaster forecasting with the help of social web, which is ignored in traditional studies. In this paper, we propose a big data approach named B-Smog for smog disaster forecasting. It mainly has two components: 1) features extraction from multiple data sources to model the factors that indicate the appearance or disappearance of a smog disaster like traffic condition, human mobility, weather condition and air pollution transportation; 2) learning and predicting with heterogeneous features in multiple views. For the second component, we propose a prediction model based on an ensemble learning framework and artificial neural networks (ANNs), which achieves high accuracy in this application and can also be applied to other similar problems. We present the effectiveness of B-Smog through two cases studies in Beijing and Shanghai, and evaluate the accuracy of the prediction model through comparing it with some baselines. Moreover, the empirical findings of our study can also support decision making in smog disaster management.",2015,
74,10.1109/ICOSEC49089.2020.9215250,AN ENSEMBLE LEARNING APPROACH FOR PRIVACY–QUALITY–EFFICIENCY TRADE-OFF IN DATA ANALYTICS,"Privacy is an issue of concern in the electronic era where data has become a primary source of investment for businesses and organizations. The value generated from data is put to use in a number of ways for economic benefit. Customer profiling is one such instance, where data collected is used for targeted marketing, personalized purchase recommendations and customized product deliveries. In such applications, the risk of individual sensitive information disclosure always prevails, affecting the privacy of individuals involved. Hence privacy preserving analysis demands suppressing or transforming data before it is published for analysis, thus curbing data leak. Subsequently, data quality degrades, and operative analytics is affected. With Big data, algorithms that offer a reasonable qualityprivacy trade off need enhancements in terms of efficiency and scalability. In this paper, the work proposed uses a privacy based composite classifier model to analyze the accuracy of classification. The diverse characteristics of algorithms in the composite classifier are found to balance the classification accuracy that is likely to get affected by privacy model. Further, the model's performance with respect to execution time is then evaluated using the parallel computing framework Spark.",2020,
75,10.1109/SERVICES.2019.00086,IMPROVING HEARING HEALTHCARE WITH BIG DATA ANALYTICS OF REAL-TIME HEARING AID DATA,"Modern hearing aids are not simple passive sound enhancers, but rather complex devices that can log (via smartphones) multivariate real-time data from the acoustic environment of a user. In the evotion project (http://h2020evotion.eu) such hearing aids are integrated with a Big Data analytics platform to bring about ecologically valid evidence to support the hearing healthcare sector. Here, we present the background of the Big Data analytics platform and demonstrate that modeling of longitudinally sampled data from hearing aids can support clinical investigations with hypotheses about hearing aid usage prognosis, and support public health decision-making within the hearing healthcare sector by simulation techniques. We found, that distinct characteristics of the acoustic environment significantly modulate how hearing impaired individuals use their hearing aids. Higher sound levels and an increased sound diversity but degraded signal quality all predicts more minutes of use per hour. By simulation, we show that a projected increase in the overall sound levels by 10dB followed by a 4dB increase in noise exposure will increase the need for hearing aid use by an additional 1 hour/day across a population of hearing impaired hearing aid users.",2019,23783818
76,10.1109/CIS.2016.0063,ANALYSIS OF INFORMATION SECURITY OF ELECTRIC POWER BIG DATA AND ITS COUNTERMEASURES,"In recent years, big data technology has become the representative of new information technology. It is also playing an increasing important role in the electric power industry. With big data technology, electric power company introduces new business, like accurate load forecasting, user behavior analysis, etc. It helps make the service more quality and the upper level decisions more reasonable. This paper expounds the concept of big data. The characteristics of electric power big data and data processing steps are introduced as well. On this basis, the opportunities and challenges faced by big data are analyzed. The problems and countermeasures of big data are focused in this paper. The future development of big data security is summarized in the end. With the guarantee of technology and management, big data technology will have a bright future in the big data industry. It will provide strong support for improving enterprise efficiency.",2016,
77,10.1109/MLBDBI51377.2020.00066,RESEARCH ON REAL ESTATE MARKETING INNOVATION SYSTEM IN THE ERA OF BIG DATA,"This article analyzes the importance of real estate marketing in the era of big data. The content of this article includes how to enhance the value of data application, the important means of reconstructing marketing strategies, and the inevitable trend of changes in the marketing system. Combining the content of real estate marketing strategy, the author studies how to analyze customer needs, do a good job of customer segmentation and clustering, optimize marketing strategy distribution models, improve the real estate marketing system, strengthen marketing effect evaluation, and improve the overall quality of personnel. The purpose of this article is to improve the applicability of the content of the system and accelerate the development of the real estate economy.",2020,
78,10.1109/BigDataCongress.2017.41,DE-CENTRALIZED REPUTATION-BASED TRUST MODEL TO DISCRIMINATE BETWEEN CLOUD PROVIDERS CAPABLE OF PROCESSING BIG DATA,"Trust and reputation systems represent a significant trend in decision support including selection of best match cloud providers to process Big Data. Reputation is often considered as a collective measure of trustworthiness based on the referrals or ratings from members in a community. Reputation systems have been applied in various applications such as online service provision. However, reputation models do not reflect user's quality of service (QoS) preferences and thus they might not be satisfied with the recommendations from others. In this paper, we propose a de-centralized reputation-based trust model that incorporates the user QoS preferences to select the best match Cloud Service Provider to process Big Data. Our trust model relies on three multi-attribute decision-making (MADM) methods including Simple Additive Weighting (SAW), Weighted Product Method (WPM), and Technique for Order Preference by Similarity to Ideal Solution (TOPSIS). We conducted several experiments using simulated cloud environment to validate our trust model and assess the three MADM methods. The results show that the proposed model is pliable to users' requirements and efficiently evaluate trust of cloud providers.",2017,
79,10.1109/TKDE.2020.3015777,A SURVEY ON LARGE-SCALE MACHINE LEARNING,"Machine learning can provide deep insights into data, allowing machines to make high-quality predictions and having been widely used in real-world applications, such as text mining, visual classification, and recommender systems. However, most sophisticated machine learning approaches suffer from huge time costs when operating on large-scale data. This issue calls for the need of Large-scale Machine Learning (LML), which aims to learn patterns from big data with comparable performance efficiently. In this paper, we offer a systematic survey on existing LML methods to provide a blueprint for the future developments of this area. We first divide these LML methods according to the ways of improving the scalability: 1) model simplification on computational complexities, 2) optimization approximation on computational efficiency, and 3) computation parallelism on computational capabilities. Then we categorize the methods in each perspective according to their targeted scenarios and introduce representative methods in line with intrinsic strategies. Lastly, we analyze their limitations and discuss potential directions as well as open issues that are promising to address in the future.",2022,23263865
80,10.1109/BigData.2017.8258174,BUILDING NEW KNOWLEDGE FROM DISTRIBUTED SCIENTIFIC CORPUS: HERBADROP & EUROPEANA: TWO CONCRETE CASE STUDIES FOR EXPLORING BIG ARCHIVAL DATA,"This paper presents approaches for building new knowledge using emerging methods and big data technologies together with archival practices. Two cases studies have been considered. The first one called HERBADROP is concerned with preservation and analysis of herbarium images. The second one called EUROPEANA investigates how to facilitate the re-use of cultural heritage language resources for research purposes. The common point between these two case studies is that they are both concerned with the use of valuable heritage resources within the EUDAT (European Data) infrastructure. HERBADROP leverages on the data services provided by EUDAT for long-term preservation, while EUROPEANA leverages on EUDAT to achieve citability and persistent identification of cultural heritage datasets. EUDAT1 is an initiative of some of the main European data centers and together with community research infrastructure organisations, to build a common eInfrastructure for general research data management. In this paper, we show how technologcal trends may offer some new research potential in the domain of computational archival science in particular appraising the challenges of producing quality, meaning, knowledge and value from quantity, tracing data and analytic provenance across complex big data platforms and knowledge production ecosystems.",2017,
81,10.1109/MLBDBI51377.2020.00067,KINDERGARTEN BIG DATA SYSTEM SOLUTION ARCHITECTURE,"The kindergarten big data system is designed based on the practical problems of insufficient pre-school education resources, inadequate regulatory systems and mechanisms, and the need to improve the quality of childcare, and the management needs of the kindergarten urgently need to improve the science, flexibility, efficiency, and safety, developing. It includes three major aspects of network system architecture, environment system and management system. The kindergarten big data system is based on modern information technology and can eventually achieve ""intelligent connection with government authorities"" to facilitate supervision. The kindergarten big data system can ""integrate intelligently the various affairs of the kindergarten"" for effective management; ""monitor children's health information at all times"" to ensure safety. It also can ""real-time interact with parents and share children's information"" to achieve home-school co-education; ""develop games, learning environment and resources"" to provide personalized services and other functions for kindergarten teachers.",2020,
82,10.1109/DDCLS.2018.8516121,A NOVEL SCALABLE SEMI-SUPERVISED GMM AND ITS APPLICATION FOR MULTIMODE PROCESS QUALITY PREDICTION WITH BIG DATA,"In this paper, a novel variational inference semi-supervised GMM (VI-S2GMM) model is firstly proposed for multimode process predictive modeling with semi-supervised data. Since all the labeled and unlabeled data samples are involved in each iteration of parameter updating, an intractable computing problem occurs when facing a high-dimension and large-scale dataset. To tack this problem, a scalable Stochastic Variational Inference semi-supervised GMM (SVI-S2GMM) is further proposed for massive semi-supervised data. Through taking advantage of stochastic gradient optimization algorithm to maximize the Evidence of Lower Bound (ELBO), the VI-based algorithm becomes scalable. In SVI-S2GMM, only one or a mini-batch of samples is randomly selected to update parameters in each iteration, which is more efficient than VI-S2GMM. In this way, a large number of unlabeled process data can be useful in the modeling, which will benefit the parameter identification. The SVI-S2GMM is then exploited for the prediction of quality-related key performance index (KPI). Two modeling cases with large scale of semi-supervised datasets demonstrate the feasibility and effectiveness of the proposed algorithms.",2018,
83,10.1109/CIMPS52057.2020.9390151,SMART UNIVERSITY: BIG DATA ADOPTION MODEL,"New technologies foster a variety of smart solutions in university settings to improve the quality of life and performance for both teachers and students. Research on information governance shows the importance of the alignment between information and communication technologies (ICTs) and strategic objectives. From this perspective, the adoption of smart technologies is the result of strategic management deliberations that address the application, the risk, the use of resources and the feasibility of technology. The main challenge is to predict how this technology is adopted through overcoming the barriers that affect, for example, your perception of usefulness or your intention to use it. This has led to the concept of an Intelligent University where Big Data has proven to be very important. This article reviews technology adoption models and proposes a specific model for Big Data based on three factors: Individual perception; security and risk; and organizational support.",2020,
84,10.1109/ICMCCE51767.2020.00338,"MANAGEMENT INNOVATION OF PARTY BUILDING WORK IN HIGHER VOCATIONAL COLLEGES UNDER THE BACKGROUND OF ""INTERNET +"" AND BIG DATA","In the process of increasing the level of informatization technology, the development speed of Internet technology is also getting faster and faster. Big data technology and Internet technology have become important types of technology used in various industries. In the context of ""Internet +"" and big data, innovative management of the current party building work in higher vocational colleges can improve the efficiency and quality of party building work. In the management of party building in higher vocational colleges, we need to study the actual application of network technology and the actual requirements of party building work. The author of this article puts forward the key points of the application of Internet + and big data technology in campus party building innovation management.",2020,
85,10.1109/ICSGEA51094.2020.00117,RESEARCH ON PERSONALIZED SERVICE STRATEGY OF UNIVERSITY LIBRARY BASED ON BIG DATA MINING SYSTEM,"In order to improve the level and efficiency of personalized service of university library, and to meet the personalized needs of college teachers and students for borrowing books, this paper puts forward a novel personalized service strategy of university library based on big data mining system. This strategy is based on big data mining technology, and comprehensively adopts high and new technologies such as association data analysis, clustering data analysis, classification data analysis and so on. The research shows that this strategy can not only really improve the level and efficiency of personalized service of university library, but also meet the personalized needs of college teachers and students to borrow books to a certain extent.",2020,
86,10.1109/ICCWAMTIP.2017.8301496,INTERNET OF THINGS — SMART TRAFFIC MANAGEMENT SYSTEM FOR SMART CITIES USING BIG DATA ANALYTICS,Smart Traffic System (STS) is a one of the important aspect for future smart city. STS is more expensive and highly configurable to provide better quality of service for public traffic management. This paper proposes a low cost future STS to provide better service by deploying traffic update instantly. Low cost vehicle detecting sensors are fixed in the middle of road for every 500 meters. Internet of Things (IoT) is being used to attain public traffic data quickly and send it for data processing. The Real time streaming data is sent for Big Data analytics. There are several analytical scriptures to analyze the traffic density and provide solution through predictive analytics.,2017,
87,10.1109/ICCCE.2014.46,BIG DATA ANALYSIS SOLUTIONS USING MAPREDUCE FRAMEWORK,"Recently, data that generated from variety of sources with massive volumes, high rates, and different data structure, data with these characteristics is called Big Data. Big Data processing and analyzing is a challenge for the current systems because they were designed without Big Data requirements in mind and most of them were built on centralized architecture, which is not suitable for Big Data processing because it results on high processing cost and low processing performance and quality. MapReduce framework was built as a parallel distributed programming model to process such large-scale datasets effectively and efficiently. This paper presents six successful Big Data software analysis solutions implemented on MapReduce framework, describing their datasets structures and how they were implemented, so that it can guide and help other researchers in their own Big Data solutions.",2014,
88,10.1109/CRC51253.2020.9253463,QUERY OPTIMIZATION FOR AIR QUALITY BIG DATA BASED ON HIVE-ORC,"To improve the efficiency of analyzing the massive amount of monitoring data collected from an air quality monitoring system, a method based on Hive data warehouse to store data as ORC file format then create Row Group Index and Bloom Filter Index is proposed to optimize the query of air quality big data. The air quality monitoring data from the environmental monitoring center of Hubei province was taken as a research object. After the data was transferred to Hive, Spark was used to query and analyze data. Five queries were carried out on three datasets to conduct the comparison experiment between the method proposed in this paper and the method of storing data as TextFile by default on Hive. The results show that the optimization method based on Hive-ORC and its indexes reduces the storage space of big data by 90% and reduces its query time by 86%. What's more, better optimization effect can be achieved with the increase of data volume.",2020,
89,10.1109/ICITBS.2015.134,MULTIDIMENSIONAL ASSESSMENT RESULTS DATA MINING AND ANALYSIS FOR COURSES OF SCIENCE,"This system adopts the decision tree ID3 algorithm and the teachers' classroom teaching quality evaluation attribute are classified and evaluation attribute characteristic of each category is obtained. The result of the rules of data mining are analyzed and the analysis results are applied to improve the level of teaching quality, which has very strong practical significance. By using data mining technique, multidimensional assessment results are deeply analyzed, so that we can more clearly find out correlation between factors influencing the quality of classroom teaching of science course. Then we can help the teaching management and teachers to solve teaching problem and improve the quality of classroom teaching of science course.",2015,
90,10.1109/SERVICES51467.2021.00047,A DATA-DRIVEN EXPLORATORY SERVICE COMPOSITION TOOL FOR DATA SCIENTISTS,"In the era of big data, data scientists gain insights from data and make decisions. However, when constructing a data analysis pipeline, data scientists are often required to be proficient in multiple algorithm models and theoretical foundations, and they have to select and combine multiple algorithm models, and repeatedly adjust algorithms and parameters to construct a high-performance data analysis pipeline. In response to the above problems, this paper proposes an exploratory service composition tool, which can perform real-time service recommendations according to users’ needs and data features, and assists users in constructing data analysis pipelines in an exploratory manner. This method can reduce the difficulty of data analysis, effectively save labor costs, and improve the quality and efficiency of data analysis.",2021,23783818
91,10.1109/ICAC.2016.13,AN ENERGY-AWARE ADAPTATION MODEL FOR BIG DATA PLATFORMS,"Platforms for big data includes mechanisms and tools to model, organize, store and access big data (e.g. Apache Cassandra, Hbase, Amazon SimpleDB, Dynamo, Google BigTable). The resource management for those platforms is a complex task and must account also for multi-tenancy and infrastructure scalability. Human assisted control of Big data platform is unrealistic and there is a growing demand for autonomic solutions. In this paper we propose a QoS and energy-aware adaptation model designed to cope with the real case of a Cassandra-as-a-Service provider.",2016,
92,10.1109/ICCNEA53019.2021.00076,DEVELOPMENT AND APPLICATION OF PROCESS MONITORING EQUIPMENT FOR POLE-HOLDING AND TOWER ASSEMBLY BASED ON BIG DATA TECHNOLOGY,"Big data technology plays a very important role in the monitoring process of pole-mounted towers. Based on big data technology, this article discusses the impact of big data on pole-holding towers and the development and application of pole-holding tower monitoring equipment, expounds the key technologies of big data in process monitoring, and real-time monitoring of the operating status of process equipment. It will analyze and process according to different types of test results. The final test results show that, compared with the traditional MSPCA technology, the improved MSPCA has significantly improved the missed detection rate and detection delay of monitoring equipment, achieved the expected effect, and met the requirements of project construction.",2021,
93,10.1109/BigData.2013.6691571,GPU ACCELERATED ITEM-BASED COLLABORATIVE FILTERING FOR BIG-DATA APPLICATIONS,"Recommendation systems are a popular marketing strategy for online service providers. These systems predict a customer's future preferences from the past behaviors of that customer and the other customers. Most of the popular online stores process millions of transactions per day; therefore, providing quick and quality recommendations using the large amount of data collected from past transactions can be challenging. Parallel processing power of GPUs can be used to accelerate the recommendation process. However, the amount of memory available on a GPU card is limited; thus, a number of passes may be required to completely process a large-scale dataset. This paper proposes two parallel, item-based recommendation algorithms implemented using the CUDA platform. Considering the high sparsity of the user-item data, we utilize two compression techniques to reduce the required number of passes and increase the speedup. The experimental results on synthetic and real-world datasets show that our algorithms outperform the respective CPU implementations and also the naïve GPU implementation which does not use compression.",2013,
94,10.1109/FCCM.2016.59,CS-BASED SECURED BIG DATA PROCESSING ON FPGA,"The four V's in Big data sets, Volume, Velocity, Variety, and Veracity, provides challenges in many different aspects of real-time systems. Out of these areas securing big data sets, reduction in processing time and communication bandwidth are of utmost importance. In this paper we adopt Compressive Sensing (CS) based framework to address all three issues. We implement compressive Sensing using Deterministic Random Matrix (DRM) on Artix-7 FPGA, and CS reconstruction using Orthogonal Matching Pursuit (OMP) algorithm on Virtex-7 FPGA. The results show that our implementations for CS sampling and reconstruction are 183x and 2.7x respectively faster when compared to previously published work. We also perform case study of two different applications i.e. multi-channel Seizure Detection and Image processing to demonstrate the efficiency of our proposed CS-based framework. CS-based framework allows us to reduce communication transfers up to 75% while achieving satisfactory range of quality. The results show that our proposed framework is 290x faster and has 7.9x less resource utilization as compared to previously published AES based encryption.",2016,
95,10.1109/CECIT53797.2021.00159,REAL-TIME TRANSMISSION AND CONFIGURATION STRATEGY OF MEASUREMENT DATA BASED ON NEWTON'S LAW OF COOLING,"At present, with the deepening construction of the two-level data center of State Grid, there are more and more supporting applications. This paper mainly studies and solves the problems of rough data classification and poor transmission timeliness in the current real-time access transmission of measurement data, and puts forward a method of obtaining data priority by Newton's Law of Cooling, and using dynamic configuration strategy to carry out real-time transmission and dynamic configuration according to data priority. In order to effectively improve the data access quality of two-level data, give full play to the basic role of data as a factor of production, better serve business application and innovative development, and improve the accuracy of data classification and real-time transmission in real-time access and transmission of measurement data. Simulation experiment results show that this method can reduce the average waiting time of the data during the data transmission, and complete the data transmission early, which can effectively improve the timeliness of the data transmission.",2021,
96,10.1109/ICSGEA.2017.127,STRATEGY ANALYSIS OF PSYCHOLOGICAL QUALITY EDUCATION IN THE ENVIRONMENT OF BIG DATA,"College psychological health education is very important for college psychological discipline construction, lodging the important role of college psychology in the reform and the change, emphasizes two related themes of the psychological health education-the empirical application and the overall adjustment for prevention. With the development of modern network technology and the increasing information, society begins to gradually into the era of big data. Under the environment of Big data, the campuses how to implement the psychological quality education strategy with the help of big data, and how to strengthen the utilization of big data to make construction of informational psychological quality education strategy become the focus of current topic which people should think. In view of the foregoing, in this paper, starting from the definition of big data, the meaning of big data in psychological quality education strategy and the specific informational psychological quality education strategy application are analyzed, and psychological quality education weighting model based on AHP was built, so as to provide reference for the application of big data psychological quality education strategy in the future.",2017,
97,10.1109/ICMLC.2016.7872993,A DYNAMIC DATA CORRECTION ALGORITHM BASED ON POLYNOMIAL SMOOTH SUPPORT VECTOR MACHINE,"Data quality plays an important role in modern intelligent information system and is crucial to any data analysis task. Many imperfection-handling techniques avoid overfitting or simply remove offending portions of the data. Data correction can help to retain and recover as much information as possible from the original data resources. In this paper, we proposed a novel technique based on polynomial smooth support vector machine. The quadratic polynomial and the first degree of polynomial as the support vector machine smooth functions are investigated. At the same time, the function was used as smooth function to calculate compensation values. In order to show the procedures of our algorithm, some necessary steps need to be considered. Firstly, the original data are normalized, so as to eliminate experimental effects of dimensional problems. Secondly, the three different kinds of smooth functions need to be analysed mathematically. The difference measure are calculated to make sure the results of correction through different data correction models. The results of given noised data sets can show that the proposed the data correction method based on polynomial smooth support vector machine is effectiveness.",2016,21601348
98,10.1109/ICDE.2017.151,FAST AND SCALABLE DISTRIBUTED SET SIMILARITY JOINS FOR BIG DATA ANALYTICS,"Set similarity join is an essential operation in big data analytics, e.g., data integration and data cleaning, that finds similar pairs from two collections of sets. To cope with the increasing scale of the data, distributed algorithms are called for to support large-scale set similarity joins. Multiple techniques have been proposed to perform similarity joins using MapReduce in recent years. These techniques, however, usually produce huge amounts of duplicates in order to perform parallel processing successfully as MapReduce is a shared-nothing framework. The large number of duplicates incurs on both large shuffle cost and unnecessary computation cost, which significantly decrease the performance. Moreover, these approaches do not provide a load balancing guarantee, which results in a skewness problem and negatively affects the scalability properties of these techniques. To address these problems, in this paper, we propose a duplicatefree framework, called FS-Join, to perform set similarity joins efficiently by utilizing an innovative vertical partitioning technique. FS-Join employs three powerful filtering methods to prune dissimilar string pairs without computing their similarity scores. To further improve the performance and scalability, FS-Join integrates horizontal partitioning. Experimental results on three real datasets show that FS-Join outperforms the state-of-theart methods by one order of magnitude on average, which demonstrates the good scalability and performance qualities of the proposed technique.",2017,2375026X
99,10.1109/iThings/GreenCom/CPSCom/SmartData.2019.00100,PRECISE SUBSIDIZATION GRANTS FOR COLLEGE STUDENTS OVER BIG DATA OPTIMIZED RANDOM FOREST,"It is difficult to accurate identify and ascertain college students' grants. Open ""poorer than poor"" funding mode lacks humanistic care. How to subsidize has always been a difficult problem in the education of University grants. In the past, due to the lack of data and the high cost of acquisition data, how to introduce big data processing technology and machine learning method into the accurate prediction of college student grants has been not achieved good results. We establish a prediction model in this paper, by optimizing the characteristic selection and running parameters for random forest model, also, we can precise subsidization grants for college students' based on balanced corpus strategy and big data analysis. The experiments on real data sets show that the accuracy can reach 91%, although not the most final determinative result, to improve the precision and quality of college student grant work, has the very vital significance.",2019,
100,10.1109/ICSCSE.2018.00156,COMPETITION DECISION FOR BOTTLENECK TRAVELING SALESMAN PROBLEM BASED ON BIG DATA MINING ALGORITHM WITH MULTI-SEGMENT SUPPORT,"Due to the existence of multiple constraints and multiple optimization objectives, the competition decision for bottleneck traveling salesman problem is very difficult. The paper proposes the competition decision for bottleneck traveling salesman based on big data mining algorithm with multi-segment support. The big data mining algorithm involving multi-segment support covers the initial division of the tour area of travelling salesman, competition decision on the tourism regional boundary, and creation of the initial solution. The competition decision for bottleneck traveling salesman problem defines the local search scope within nearest neighbor of network K, and only to search the most likely spatial neighborhood, and iteratively improves the quality of the solution. Test the performance of the algorithm through the competition decision for bottleneck traveling salesman problem. The test results show that the proposed method in this paper can solve the bottleneck travelling salesman competition decision problem of 6,400 customer points within 15 minutes. The quality of the solution is about 10.8% better than ArcGIS, and the calculation time is about 21.2% of ArcGIS.",2018,
101,10.1109/FiCloud.2015.121,CHALLENGES AND SOLUTIONS IN BIG DATA MANAGEMENT -- AN OVERVIEW,"Currently, a huge explosion of data is observed in many organizations in the world. Industry analysts and businesses are looking towards Big Data As the next big thing to provide opportunities, insights, solutions and a new way to increase profits in business. From social networking sites to records in a hospital, Big Data has played an important role to improve businesses and innovation. Businesses strive to get quality information and retrieve them for data analysis and business purposes. Though big data is obtained from numerous resources, there are many issues and challenges that companies face while storing and handling Big Data. Proper data management practices, techniques, technology and infrastructure can help overcome these challenges, problems and issues. This paper gives an overview about the issues and challenges of Big Data management, also their solutions and practices being used to handle them.",2015,
102,10.1109/BigData50022.2020.9378296,DQLEARN : A TOOLKIT FOR STRUCTURED DATA QUALITY LEARNING,"Data Quality (DQ) has been one of the key focuses as Data Analytics and Artificial Intelligence (AI) fields continue to grow. Yet, data quality analysis has mostly been a disjointed, ad-hoc, and cumbersome process in the overall data analysis workflow. There have been ongoing attempts to formalize this process, but the solutions that have come out are not universally applicable. Most of the proposed solutions try to address the problem of data quality from a limited perspective and suc-cessfully address only a subset of all challenges. These solutions fail to translate to other domains due to a lack of structure. In this paper, we present DQLearn, a toolkit for structured data quality learning. We start by presenting the core principle on which we build our library and introduce the four components that provide a solid base to address the needs of the data quality problem. Then, we showcase our automation structure - ""Workflows"", and the two optimization techniques equipped with it, that help the users to structure their learning problem very easily. Next, we discuss four important scenarios of the DQ Workflows in the overall life-cycle. Finally, we demonstrate the utility of the proposed toolkit with public datasets and show benchmark results from optimization experiments.",2020,
103,10.1109/PACRIM.2017.8121916,MARINE MAMMAL SOUND ANOMALY AND QUALITY DETECTION USING MULTITAPER SPECTROGRAM AND HYDROPHONE BIG DATA,"This paper proposes a novel method for anomaly and quality detection of marine mammal sounds using multitaper spectrogram and hydrophone big data. The proposed method is aimed to automatically detect anomaly, such as high-frequency vessel noise, Doppler noise, in sperm whale (SPW) sound as well as the quality of the sound. A new signature function derived from a multi-taper spectrogram is able to detect the anomaly in the data and a new anomaly distortion measure can detect the sound quality into good/bad. The proposed method, is tested with 1905 minutes of data spanning a single year, and using a human operator's annotations. The experimental results reveal that the proposed multitaper spectrogram based approach is efficient in detecting anomaly as well as sperm whale sound quality for hydrophone big data and high detection accuracy (>85%) is achieved for raw input hydrophone data.",2017,
104,10.1109/INDIS.2018.00009,BANDWIDTH SCHEDULING FOR BIG DATA TRANSFER WITH DEADLINE CONSTRAINT BETWEEN DATA CENTERS,"An increasing number of applications in scientific and other domains have moved or are in active transition to clouds, and the demand for the movement of big data between geographically distributed cloud-based data centers is rapidly growing. Many modern backbone networks leverage logically centralized controllers based on software-defined networking (SDN) to provide advance bandwidth reservation for data transfer requests. How to fully utilize the bandwidth resources of the links connecting data centers with guaranteed QoS for each user request is an important problem for cloud service providers. Most existing work focuses on bandwidth scheduling for a single request for data transfer or multiple requests using the same service model. In this work, we construct rigorous cost models to quantify user satisfaction degree and formulate a generic problem of bandwidth scheduling for multiple deadline-constrained data transfer requests of different types to maximize the request scheduling success ratio while minimizing the data transfer completion time of each request. We prove this problem to be NP-complete and design a heuristic solution. Extensive simulation results show that our scheduling scheme significantly outperforms existing methods in terms of user satisfaction degree and scheduling success ratio.",2018,
105,10.1109/DCOSS.2017.35,BIG SENSED DATA CHALLENGES IN THE INTERNET OF THINGS,"Internet of Things (IoT) systems are inherently built on data gathered from heterogeneous sources. In the quest to gather more data for better analytics, many IoT systems are instigating significant challenges. First, the sheer volume and velocity of data generated by IoT systems are burdening our networking infrastructure, especially at the edge. The mobility and intermittent connectivity of edge IoT nodes are further hampering real-time access and reporting of IoT data. As we attempt to synergize IoT systems to leverage resource discovery and remedy some of these challenges, the rising challenges of Quality of Information (QoI) and Quality of Resource (QoR) calibration, render many IoT interoperability attempts far-fetched. We survey a number of challenges in realizing IoT interoperability, and advocate for a uniform view of data management in IoT systems. We delve into three planes that encompass Big Sensed Data (BSD) research directions, presenting a building block for future research efforts in IoT data management.",2017,23252944
106,10.1109/CIT.2016.86,A NOVEL STORAGE ARCHITECTURE FOR FACILITATING EFFICIENT ANALYTICS OF HEALTH INFORMATICS BIG DATA IN CLOUD,"Analytics of health big data are very crucial for providing cost effective quality health care. Over recent years, the analytics on healthcare big data has evolved into a challenging task for getting insights into a very large data set for improving the health services. This enormous amount of data, which is being generated incessantly over a long period of time, has put a great deal of stress on the write performance as well as on scalability. Moreover, there is a requirement of efficient storage and meaningful processing of these data which is an another challenging issue. The traditional relational databases, which were used in the storage of health data, are now unable to handle due to its massive and varied nature. Besides, these databases have some inherent weakness in terms of scalability, storing varied data format, etc. So there is a necessity for a new kind of data storage management system. This paper proposes a new big data storage architecture consisting of application cluster and a storage cluster to facilitate read/write/update speedup as well as data optimization. The application cluster is used to provide efficient storage and retrieval functions from the users. The storage services will be provided through the storage cluster.",2016,
107,10.1109/ICVRIS.2018.00028,AN ORAL ENGLISH TRAINING METHOD BASED ON BIG DATA ANALYSIS,"Oral English training is a crucial part in English teaching. How to fully exploit information technology and big data technology to promote the quality of oral English training has attracted more and more attention. In this paper, we aim to propose a novel oral English training method based on big data analysis. In particular, there are two main considerations in this paper, that is, 1) what information services and support for a specific user and which page content is arranged, and 2) the identity worker with the unique identity of different ID. Afterwards, we discuss how to utilize the hidden Markov model to achieve speech recognition, which is a key module in this proposed system. Finally, experimental results prove that the proposed solution can obtain high accuracy of speech recognition, and then enhance users' scores on English course.",2018,
108,10.1109/CSIT49958.2020.9321955,METHODS OF BIG VECTOR DATA PROCESSING UNDER TOROIDAL COORDINATE SYSTEMS,"Methods of big vector data (BVD) processing under toroidal coordinate systems for development of high performance vector information technologies with improving the quality indices of the technologies presents in this paper. These methods involve novel mathematical principle relating to the minimizing of structural elements in toroidal coordinate systems, including the appropriate algebraic constructions such as vector different sets of cyclic groups and ""GlorytoUkraineStar"" (GUS) combinatorial configurations, namely the concept of Perfect Toroidal Codes (PTC)s. These codes form binary coding system under two- or multidimensional ring axes of toroidal reference grid, which provides big data processing of multidimensional arrays, using the smallest possible basis.",2020,27663639
109,10.1109/SBEC.2016.88,APPLICATION OF ANALYTICS TO BIG DATA IN HEALTHCARE,"In the current age of smart phones and wearable devices, vast amounts of patient health data files forming Big Data are being placed into large databases where they can be accessed by multiple users including doctors, caregivers and patients. The estimated spending on healthcare in 2015 in the U.S. is around $3.2 trillion, which triggers the question of improvement of patient care while containing the costs. The objective of the present study is to review a few applications of analytics of Big Data in the healthcare field and the associated outcomes. Big Data is generally characterized by the volume, velocity, variety and veracity of complex data. Many hospitals have applied analytics to big data from various sources including patient health records to achieve overall improvement in healthcare. Operationally, most of the pertinent data of patients are made available on demand so doctors can see how other treatments have worked globally and apply relevant results to facilitate better decision making and interventions. Making proper use of big data analytics in healthcare can lead to improvement in care delivery coupled with significant cost savings. Concurrent challenges to be addressed include accessibility, privacy, security, usability, implementation costs, transportability, interoperability, and standardization. In conclusion, employing efficient and streamlined analytics to big data will contribute to quick and accurate diagnosis, appropriate treatment, reduced costs and improved overall healthcare quality.",2016,
110,10.1109/BDIDM53834.2021.00021,BIG DATA-BASED DYNAMIC DECISION-MAKING ALGORITHM FOR POWER ENTERPRISE OPERATION RISK MANAGEMENT,"In the 21st century, with the continuous development of science and technology in our country, the explosive growth of data, big data has become an important resource acquisition channel at home and abroad with its massive data resources. De-importance can not only help small shops, but also The level of economic management of government organizations has spawned many unprecedented business models, which are more capable of promoting and promoting the sustainable development of our country's economy. For our country's power companies, each node such as the distribution network will generate massive amounts of data, and these cumbersome data in the past are now becoming a rare asset for the company. In the era of big data, the data generated by electricity can bring earth-shaking changes to enterprises. With the in-depth reform of all aspects of power companies, power companies in various places can only maintain a firm position in corporate competition if they closely follow the trend of big data, with lower costs, higher efficiency, and better power quality. Using the core technology of big data to seamlessly integrate it with enterprise risk management and realize the top-down application of big data mining and other information technologies within the enterprise will be an important way to build an information enterprise in an all-round way. Making full use of information technology in power production and management and actively promoting the application of big data in the power system will help improve the level of refined operation and management of power companies at all levels, improve operational efficiency, and promote the transformation and development of enterprises. The purpose of this paper is to research on the dynamic decision-making algorithm of power enterprise's operational risk management based on big data. In this paper, through in-depth research on big data, analyzing the current big data mining technology and combining with the operating enviro nment of electric power enterprises in our society, this paper discusses the dynamic decision-making algorithm of electric power enterprise operating risk management under big data. This article will take the management of electric power enterprises under big data as the research object, and conduct research and analysis through literature method, questionnaire survey method and mathematical statistics method. Studies have shown that some small probability of unqualified power equipment is put into the production and operation of the power industry, which has a great negative effect on the management and control of equipment risks, and the probability of accidents is greatly increased.",2021,
111,10.1109/CSE.2013.164,PUBLIC AUDITING FOR BIG DATA STORAGE IN CLOUD COMPUTING -- A SURVEY,"Data integrity is an important factor to ensure in almost any data and computation related context. It serves not only as one of the qualities of service, but also an important part of data security and privacy. With the proliferation of cloud computing and the increasing needs in big data analytics, verification of data integrity becomes increasingly important, especially on outsourced data. Therefore, research topics related to data integrity verification have attracted tremendous research interest. Among all the metrics, efficiency and security are two of the most concerned measurements. In this paper, we provide an analysis on authenticator-based efficient data integrity verification. we will analyze and provide a survey on the main aspects of this research problem, summarize the research motivations, methodologies as well as main achievements of several of the representative approaches, then try to bring forth a blueprint for possible future developments.",2013,
112,10.1109/BigData.2017.8258329,OPTIMAL VIEWPOINT FINDING FOR 3D VISUALIZATION OF SPATIO-TEMPORAL VEHICLE TRAJECTORIES ON CAUTION CROSSROADS DETECTED FROM VEHICLE RECORDER BIG DATA,"Traffic accidents are still troubling our society. The number of drive recorders sold has increased, and therefore we can collect large-scale vehicle recorder data to be used to support traffic safety. We have developed a system for detecting potentially risky crossroads on the basis of vehicle recorder data, road shapes, and weather information. Visualization combining space and time in a single display called a “space time cube (STC)” helps us to understand and analyze spatio-temporal mobility data on caution crossroads. The STC enables us to simultaneously explore not only shapes and positions of vehicle trajectories but also their temporal distributions. However, it is difficult for users to manually find good viewpoints for understanding such characteristics of trajectories. In this paper, we propose an optimal viewpoint selection method for visualizing spatio-temporal characteristics of vehicle trajectories on a large set of crossroads using an STC. Major contributions of this paper are as follows: (1) We provide an algorithm based on viewpoint entropy weighted by angles of trajectories with a horizontal line as a measure of a viewpoint quality on a projected 2D image. (2) We demonstrate our solution can be adapted to crossroads with different trajectory shapes. We also extend the proposed method to find an optimal viewpoint for multiple crossroads. (3) We verify the proposed method through users' evaluations. (4) We construct an overviewing catalog of potentially risky crossroads detected from real vehicle recorder big data to discuss and analyze them with stakeholders.",2017,
113,10.1109/PESGM.2015.7285809,SOME DISCUSSIONS ABOUT DATA IN THE NEW ENVIRONMENT OF POWER SYSTEMS,"The objective of this paper is to present some discussions about data treatment in the new environment existing in the power systems and smart-grids. The paper presents some aspects of the data acquisition, communication, and security, and data amount, management, and mining. In this paper, some ideas are proposed to inspire discussions about the above themes. The goal of these discussions is to provide energy with a better quality to the consumers without disruption and mitigating all sorts of contingencies involving the distribution and transmission systems. Also some discussions about data treatment in power control center are also made involved aspects of big data.",2015,19325517
114,10.1109/ICSSIT53264.2022.9716554,FUSION MODELING AND HETEROGENEOUS SIMULATION OF BIG DATA TECHNOLOGY IN 5G COMMUNICATION NETWORK,"In order to continuously improve the service capabilities of the 5G communication network and promote the formation of a high-quality and efficient service structure, the R&D team and technical personnel have gradually tried to apply big data technology to the 5G communication network and achieved certain results. In this context, the article takes the application of big data technology as a starting point, systematically discusses the application method of big data technology in 5G communication network, and studies the integration modeling and heterogeneous simulation of big data technology in 5G communication network based on deep learning. The simulation results show that the heterogeneous network improves big data by 6.6%.",2022,
115,10.1109/TVT.2020.2991372,TRADING: TRAFFIC AWARE DATA OFFLOADING FOR BIG DATA ENABLED INTELLIGENT TRANSPORTATION SYSTEM,"Todays' Intelligent Transportation System (ITS) applications majorly depend on either limited neighbouring traffic data or crowd sourced stale traffic data. Enabling big traffic data analytics in ITS environments is a step closer towards utilizing significant traffic patterns and trends for making more precise and intelligent decisions particularly in connected autonomous vehicular environments. Towards this end, this paper presents a Traffic Aware Data Offloading (TRADING) approach for big traffic data centric ITS applications in connected autonomous vehicular environments. Specifically, TRADING balances offloading data traffic among gateways focusing on vehicular traffic and network status in the vicinity of gateways. In addition, TRADING mitigates the effect of gateway advertisement overhead to liberate the transmission channels for traffic big data transmission. The performance of TRADING is comparatively evaluated in a realistic simulation environment by considering gateway access overhead, load distribution among gateways, data offloading delay, and data offloading success ratio. The comparative performance evaluation results show some significant developments towards enabling big traffic data centric ITS.",2020,19399359
116,10.1109/ICCC49264.2020.9257251,STATISTICAL PROCESS CONTROL IN BIG DATA ENVIRONMENT,"Big data analysis tools are an inevitable part of instruments and methods for monitoring and predicting the longitudinal performance of the processes in the production systems of the future, based on the deep automatization and overall digitalization. From this point of view statistical process control (SPC) will continue to be very effective method for meeting these goals. But there must be done some modifications. This paper deals with such possible modifications of SPC. In the first part of the paper the stress is put on various methods that can be integrated into SPC to meet new challenges in collecting, analysing and interpreting data (control charts for high yield processes, multivariable approaches, profile monitoring, data mining tools including machine learning methods, nonparametric control charts). SW for the selected discussed methods is also mentioned. The second part of the paper is devoted to the nonparametric methods of SPC and to the methodology of their practical application.",2020,
117,10.1109/BigData50022.2020.9378457,COMBINING SPATIAL AND TEMPORAL PROPERTIES FOR IMPROVEMENTS IN DATA REDUCTION,"Due to I/O bandwidth limitations, intelligent in situ data reduction methods are needed to enable post-hoc workflows. Current state-of-the-art sampling methods save data points if they deem them spatially or temporally important. By analyzing the properties of the data values at each time-step, two consecutive steps may be very similar. This research follows the notion that if neighboring time-steps are very similar, samples from both are unnecessary, which leaves storage for adding more useful samples. Here, we present an investigation of the combination of spatial and temporal sampling to drastically reduce data size without the loss of valuable information. We demonstrate that, by reusing samples, our reconstructed data set reduces the overall data size while achieving a higher post-reconstruction quality over other reduction methods.",2020,
118,10.1109/BigData.2014.7004392,BIG DATA IN GENOMICS: AN OVERVIEW,"Studies show that healthcare industry in U.S. alone could save billions of dollars by utilizing big data and analytics technologies. Big Data can improve operational efficiencies, help predict and plan responses to disease epidemics, improve the quality of monitoring of clinical trials, and optimize healthcare spending at all levels from patients to hospital systems to governments. Another key area is genomics sequencing which is expected to be the future of healthcare. In this paper the authors look at the opportunities, work in progress and challenges of genomics with emerging big data and analytics.",2014,
119,10.1109/BigDataCongress.2017.12,BUILDING AN SVM CLASSIFIER FOR AUTOMATED SELECTION OF BIG DATA,"The quality of big data could great impact the value extracted from the data. Automated filtering of noisy data from big data is an ideal approach for improving the quality of big data. However, due to large volume and variety of big data, automated filtering of noisy data from big data is a grand challenging task. In this paper, we propose a support vector machine based approach for automated classification of big data so that the noisy data are classified as separated categories from the regular data. In order to improve the classification accuracy and training performance, we design an experiment for improving the classification model through finding the optimized learning feature set and an approach for iteratively improving the quality of the training data set. We conducted a thorough experimental study of automated classification of massive image data of biology cells to explain the approach of automated selection of big data and demonstrate its effectiveness. Finally, we compare the performance of the SVM based classification and a deep learning based classification of the same data set. The proposed approach and experience collected from the experimental study can help big data researchers and practitioners to design strategies for improving the quality of big data, designing high performance classifier, and building tools for automated selection of big data.",2017,
120,10.1109/ICEMME51517.2020.00185,CFO’S WORKING AS THE BOARD SECRETARY CONCURRENTLY AND CORPORATE DISCLOSURE QUALITY: BASED ON BIG DATA SAMPLES AND ECONOMIC MODEL,"This paper took the selected data listed companies in Shenzhen Stock Exchange in 2008-2015 as big data samples to study the relationship between the CFO's working as the Board Secretary concurrently and corporate disclosure quality. Through statistical analysis and economic model, it transforms qualitative questions into quantitative questions. The results indicate that the CFO's doubling as the Board Secretary can distinctly improve the quality of corporate disclosure in listed companies.",2020,
121,10.1109/TBDATA.2022.3164916,COMMUNITY DETECTION BASED ON MULTIOBJECTIVE PARTICLE SWARM OPTIMIZATION AND GRAPH ATTENTION VARIATIONAL AUTOENCODER,"Community detection is an important research direction in complex network analysis that can help us discover valuable network structures. The community detection algorithms based on multiobjective particle swarm optimization encode community membership of nodes in particles and employ evolutionary strategies to search for the optimal community division. Existing algorithms face two challenges: (1) they are inapplicable to large networks because the evolution process is time-consuming; (2) they are easy to fall into local optima. In this paper, we propose a novel algorithm that combines a multiobjective particle swarm optimization algorithm based on label propagation with a graph attention variational autoencoder to realize community detection. On the one hand, the label propagation strategy is involved in the update of a swarm's particles to speed up its evolution. The optimal solutions found by the particle swarm optimization algorithm are embedded into the objective of the autoencoder to improve the embedding vectors quality. On the other hand, the embedding vectors are used to improve the solutions of the particle swarm optimization algorithm to avoid its early convergence. The experiments on artificial and real-world networks demonstrate the feasibility and effectiveness of our algorithm compared with the state-of-the-art algorithms.",2022,23722096
122,10.1109/CSCWD.2014.6846857,A BIG DATA CLEANSING APPROACH FOR N-DIMENSIONAL RFID-CUBOIDS,"Radio Frequency Identification (RFID) technology has been widely used in manufacturing sites for supporting the shopfloor management. Huge amount of RFID-enabled production data has been generated. In order to discover invaluable information and knowledge from the RFID big data, it is necessary to cleanse such dataset since there is large number of noises. This paper uses n-dimensional RFID-Cuboids to establish the data warehouse. A big data cleansing approach is proposed to detect, remove and tidy the RFID-Cuboids so that the reliability and quality of dataset could be ensured before knowledge discovery. Experiments and discussions are carried out for validating the proposed approach. It is observed that the proposed big data cleansing approach outperforms other methods like statistics analysis in terms of finding incomplete and missing cuboids.",2014,
123,10.1109/ICACSIS51025.2020.9263243,HASSO: A HIGHLY-AUTOMATED SOURCE SELECTION AND ORDERING SYSTEM BASED ON DATA QUALITY FACTORS,"Big data integration gives access to a large number of data sources through a unified user interface. Answers include high-quality data, medium-quality data, and low-quality data. Selecting a subset of high accurate and consistent data sources and ordering them appropriately is critical to obtain as many high-quality answers as possible right after querying few data sources. However, the process of selecting and ordering data sources can be quite complicated and can present several challenges. The main challenge faced during that process is identifying the most adequate data quality factors to consider. In this paper, we present HASSO, a Highly-Automated Source Selection and Ordering System based on data quality factors. To produce consistent and high accurate answers, HASSO identifies, for each data source, its domain, data consistency and data accuracy using the schema matches. To maximize the total number of complete and non-redundant answers returned right after querying a small number of data sources, HASSO orders data sources in terms of their data overlap and in a decreasing order of their overall coverage. Experimental results in real-world domains show that HASSO produces high-quality answers at high speed.",2020,23304588
124,10.1109/CAIS.2019.8769451,MINIMIZING PREDICTION TIME FOR CATHETER-ASSOCIATED URINARY TRACT INFECTION USING BIG DATA MINING MODEL,"This paper focuses on minimizing prediction time for Catheter-Associated Urinary Tract Infection (CAUTI) as one of the main types of Healthcare Associated Infections (HAIs) through a big data analytics model. Big data raises the bar as a result of additional features. It is mainly characterized by tremendous amount of data that is composed of different forms. It also deals with the rapid data flow rate that is generated from multiple sources, and to top it off the quality of the data is questionable. Data mining (DM) approach consumes significantly less time, provides higher accuracy, and prevents personal subjective decisions. The paper evaluates seven data mining algorithms with real patients dataset. It includes more than 28,000 cases for a period of five years. The modeling process considers the latest definition of the Centers for Disease Control and Prevention (CDC), published in January 2019. The model is evaluated through calculations of different assessment factors such as accuracy, computation speed, and precision (true positive and true negative). The research results show the best suitable algorithm is Naïve Bayes. It overcomes the other data mining techniques utilized in similar works.",2019,
125,10.1109/CICED.2018.8592556,DATA-DRIVEN LEAN MANAGEMENT FOR DISTRIBUTION NETWORK,"This paper proposes a concept of “data-driven, lean-oriented and closed-loop” management for distribution network and explain how to implement this kind of management, as shown in fig.1 Firstly, a big data platform is constructed to integrate and combine the multi-source data. Secondly, big data analysis technologies such as data mining, machine learning and data visualization are applied to solve problems in distribution network production. For example, accurate location of the fault can be found with help of multisource information from different devices and systems. And we can also be aware of the risk points in distribution network through history data analysis. Finally, this Paper explains how to promote lean management of distribution network in the fields of asset, operation, maintenance and investment based on the big data platform and big data analysis methods. In addition, the feedback procedure sets up a bridge between application and data collecting, which further improve the data quality. Those management measure have been piloted in several cities in Jiangsu. The result proves that they can improve power supply reliability and reduce operating costs significantly. Two practical cases are given to show how they work.",2018,21617481
126,10.1109/ICESIT53460.2021.9696685,DESIGN OF UIMPROVE KNOWLEDGE PAYMENT PLATFORM USING ARTIFICIAL INTELLIGENCE AND BIG DATA ANALYSIS,"In recent years, the in-depth development of the Internet has given birth to the learning mode of knowledge payment. Since 2016 to date, knowledge payment has gone through the process of fumbling to maturity from its infancy to emergence, and its market scale has expanded, and is expected to reach $50 billion after 2020, with college students expected to account for up to $10 billion in the interim. In this explosion of the knowledge payment industry, countless platforms have emerged and grown. However, among the existing platforms, most of them target at the whole society, and the products are many and miscellaneous, with insufficient quality control ability and low product repurchase rate. Today, with the rapid development of distributed ledger and big data mining technologies, this situation can be precisely solved.",2021,
127,10.1109/ITOEC.2018.8740576,MEDICAL DATA QUALITY ASSESSMENT MODEL BASED ON CREDIBILITY ANALYSIS,"The current main problem of medical data is low accuracy. Although existing data quality evaluation and audit can meet the needs, network security problem will be more serious in the future. As a result, the data quality evaluation model must include data credibility. This paper proposes the medical big data quality evaluation model based on credibility analysis and Analytic Hierarchy Process(AHP). Firstly, analyze the data credibility. After eliminating the unreliable data, calculate the data quality dimensions respectively. Then obtain the data quality evaluation result by integrating all dimensions with AHP. Through simulation, the data quality evaluation model can effectively identify the data that affects the credibility. The data quality evaluation results are improved after removing the untrusted data, besides the amount of data is reduced, which is applicable to the big data scenario.",2018,
128,10.1109/BigData.2015.7364133,IMPROVING THE QUALITY OF SEMANTIC RELATIONSHIPS EXTRACTED FROM MASSIVE USER BEHAVIORAL DATA,"As the ability to store and process massive amounts of user behavioral data increases, new approaches continue to arise for leveraging the wisdom of the crowds to gain insights that were previously very challenging to discover by text mining alone. For example, through collaborative filtering, we can learn previously hidden relationships between items based upon users' interactions with them, and we can also perform ontology mining to learn which keywords are semantically-related to other keywords based upon how they are used together by similar users as recorded in search engine query logs. The biggest challenge to this collaborative filtering approach is the variety of noise and outliers present in the underlying user behavioral data. In this paper we propose a novel approach to improve the quality of semantic relationships extracted from user behavioral data. Our approach utilizes millions of documents indexed into an inverted index in order to detect and remove noise and outliers.",2015,
129,10.1109/BigDataService49289.2020.00013,CONTENT-BASED ANALYTICS: MOVING BEYOND DATA SIZE,"Efforts on Big Data technologies have been highly directed towards the amount of data a task can access or crunch. Yet, for content-driven decision making, it is not (only) about the size, but about the ""right"" data: The number of available datasets (a different type of volume) can reach astronomical sizes, making a thorough evaluation of each input prohibitively expensive. The problem is exacerbated as data sources regularly exhibit varying levels of uncertainty and velocity/churn. To date, there exists no efficient method to quantify the impact of numerous available datasets over different analytics tasks and workflows. This visionary work puts the spotlight on data content rather than size. It proposes a novel modeling, planning and processing research bundle that assesses data quality in terms of analytics performance. The main expected outcome is to provide efficient, continuous and intelligent management and execution of content-driven data analytics. Intelligent dataset selection can achieve massive gains on both accuracy and time required to reach a desired level of performance. This work introduces the notion of utilizing dataset similarity to infer operator behavior and, consequently, be able to build scalable, operator-agnostic performance models for Big Data tasks over different domains. We present an overview of the promising results from our initial work with numerical and graph data and respective operators. We then describe a reference architecture with specific areas of research that need to be tackled in order to provide a data-centric analytics ecosystem.",2020,
130,10.1109/IMCEC.2018.8469275,RESEARCH ON SOFTWARE TESTING TECHNOLOGY UNDER THE BACKGROUND OF BIG DATA,"With the progress of science and technology, computer technology continues to improve, the Internet has produced a large amount of data information, mankind has entered the era of “big data”, has great influence in this era of complex software industry development, the software product has penetrated into various industries in various fields of society. Under such a background, one of the problems that people need to solve is the quality, stability and reliability of software products. The key to solve this problem is software testing. This paper describes the current situation of the software testing technology under the background of big data, the purpose of the test, the testing principles, the testing methods, the development process of software testing technology based on the UML model, and the prospect of the development.",2018,
131,10.1109/RTEICT42901.2018.9012549,A NOVEL APPROACH USING BIG DATA ANALYTICS TO IMPROVE THE CROP YIELD IN PRECISION AGRICULTURE,Agriculture is the main work field in India. Farming industry adopts less innovative technology compared to other industries. Information and Communication Technologies provides simple and cost effective techniques for farmers to enable precision agriculture. The work propose a state of the art model in agriculture field which will guide the rural farmers to use Information and Communication technologies (ICT) in agriculture fields. Big data analytics is used to improve the crop yield. It can be customized for precision agriculture to improve the quality of crops which improves the overall production rate.,2018,
132,10.1109/IWQOS52092.2021.9521286,EUNOMIA: EFFICIENTLY ELIMINATING ABNORMAL RESULTS IN DISTRIBUTED STREAM JOIN SYSTEMS,"With the emergence of big data applications, stream join systems are widely used in extracting valuable information among multi-source streams. However, providing completeness of processing results in a large-scale distributed stream join system is challenging because it is hard to guarantee the consistency among all instances. We show through experiments that the abnormal result can make the quality of achieved data unacceptable in practice.In this paper, we propose Eunomia, a novel distributed stream join system which leverages an ordered propagation model for efficiently eliminating abnormal results. We design a light-weighted self-adaptive strategy to adjust the structure in the model according to the dynamic stream input rates and workloads. It can improve the scalability and performance significantly. We implement Eunomia and conduct comprehensive experiments to evaluate its performance. The results show that Eunomia eliminates abnormal results to guarantee the completeness, improves the system throughput by 25% and reduces the processing latency by 74% compared to state-of-the-art designs.",2021,1548615X
133,10.1109/CSTIC49141.2020.9282471,QUALITY CONTROL IN SAPPHIRE GROWING: FROM AUTOMATED DEFECT DETECTION TO BIG DATA APPROACH,"We illustrate how automated scanners visualise internal defects in raw sapphire prior to its processing, and present some defect statistics that Scientific Visual has collected over five years of serving key sapphire suppliers in Europe and Asia. The article illustrates use of defect location and morphology data to reveal trends in sapphire quality, compare production modes, and to find out the optimal parameters for sapphire growth.",2020,
134,10.1109/TCC.2018.2889956,SLA-BASED PROFIT OPTIMIZATION RESOURCE SCHEDULING FOR BIG DATA ANALYTICS-AS-A-SERVICE PLATFORMS IN CLOUD COMPUTING ENVIRONMENTS,"The value that can be extracted from big data greatly motivates users to explore data analytics technologies for better decision making and problem solving in various application domains. Analytical solutions can be expensive due to the demand for large-scale and high-performance computing resources. To provision online big data Analytics-as-a-Service (AaaS) to users in various domains, a general purpose AaaS platform is required to deliver on-demand services at low cost and in an easy to use manner. Our research focuses on proposing efficient and automatic admission control and resource scheduling algorithms for AaaS platforms in cloud environments. In this paper, we propose scalable and automatic admission control and profit optimization resource scheduling algorithms, which effectively admit data analytics requests, dynamically provision resources, and maximize profit for AaaS providers, while satisfying QoS requirements of queries with Service Level Agreement (SLA) guarantees. Moreover, the proposed algorithms enable users to trade-off accuracy for faster response times and less resource costs for query processing on large datasets. We evaluate the algorithm performance by adopting a data splitting method to process smaller data samples as representatives of the original big datasets. We conduct extensive experiments to evaluate the proposed admission control and profit optimization scheduling algorithms. Experimental evaluation shows the algorithms perform significantly better compared to the state-of-the-art algorithms in enhancing profits, reducing resource costs, increasing query admission rates, and decreasing query response times.",2021,23720018
135,10.1109/I-SMAC52330.2021.9640645,HIGH-FREQUENCY CONCURRENCY MODELING OF REGIONAL ENVIRONMENTAL ECONOMIC EFFICIENCY BASED ON BIG DATA AND IMPROVED NEURAL NETWORK,"The existence and development of human society are affected by environmental issues. With the continuous in-depth understanding of concepts such as sustainable development, simple economic indicators such as GDP can no longer accurately measure the quality of economic development based on resource and environmental consumption. With the development of Internet technology, the government proposes new infrastructure construction plans and system theories. Under this background, this article studies the high-frequency concurrency modeling of regional environmental economic efficiency based on big data technology and improved neural networks, in order to solve the regional environment economic ills",2021,27680665
136,10.1109/GCRAIT55928.2022.00172,ESTABLISHMENT OF TEACHING DIAGNOSIS AND IMPROVEMENT SYSTEM BASED ON BIG DATA,"The development of the education industry has been greatly boosted by the age of Big Data. Big Data with a large amount of information operation has been proposed and applied in the teaching of higher vocational colleges, which is both an opportunity and a challenge for teaching diagnosis and improvement of higher vocational colleges. To improve the quality of teaching and talent cultivation, Beijing Polytechnic established a big data-based teaching diagnosis and improvement system, extracted data from the school information system and diagnosed and analyzed the data on five levels, i.e., levels of the college, the major, the curriculum, the teaching staff, and the students. At the same time, a complete education mechanism and guarantee sub-sytem was set up to ensure the school-level and relevant departments handle the problems properly and promptly. Now, the system has been working for 2 years and and has achieved remarkable results.",2022,
137,10.1109/TIE.2019.2903774,AN INCORRECT DATA DETECTION METHOD FOR BIG DATA CLEANING OF MACHINERY CONDITION MONITORING,"The presence of incorrect data leads to the decrease of condition-monitoring big data quality. As a result, unreliable or misleading results are probably obtained by analyzing these poor-quality data. In this paper, to improve the data quality, an incorrect data detection method based on an improved local outlier factor (LOF) is proposed for data cleaning. First, a sliding window technique is used to divide data into different segments. These segments are considered as different objects and their attributes consist of time-domain statistical features extracted from each segment, such as mean, maximum and peak-to-peak value. Second, a kernel-based LOF (KLOF) is calculated using these attributes to evaluate the degree of each segment being incorrect data. Third, according to these KLOF values and a threshold value, incorrect data are detected. Finally, a simulation of vibration data generated by a defective rolling element bearing and three real cases concerning a fixed-axle gearbox, a wind turbine, and a planetary gearbox are used to verify the effectiveness of the proposed method, respectively. The results demonstrate that the proposed method is able to detect both missing segments and abnormal segments, which are two typical incorrect data, effectively, and thus is helpful for big data cleaning of machinery condition monitoring.",2020,15579948
138,10.1109/ICENCO.2017.8289792,RECORD LINKAGE APPROACHES IN BIG DATA: A STATE OF ART STUDY,"Record Linkage aims to find records in a dataset that represent the same real-world entity across many different data sources. It is a crucial task for data quality. With the evolution of Big Data, new difficulties appeared to deal mainly with the 5Vs of Big Data properties; i.e. Volume, Variety, Velocity, Value, and Veracity. Therefore Record Linkage in Big Data is more challenging. This paper investigates ways to apply Record Linkage algorithms that handle the Volume property of Big Data. Our investigation revealed four major issues. First, the techniques used to resolve the Volume property of Big Data mainly depend on partitioning the data into a number of blocks. The processing of those blocks is parallelly distributed among many executers. Second, MapReduce is the most famous programming model that is designed for parallel processing of Big Data. Third, a blocking key is usually used for partitioning the big dataset into smaller blocks; it is often created by the concatenation of the prefixes of chosen attributes. Partitioning using a blocking key may lead to unbalancing blocks, which is known as data skew, where data is not evenly distributed among blocks. An uneven distribution of data degrades the performance of the overall execution of the MapReduce model. Fourth, to the best of our knowledge, a small number of studies has been done so far to balance the load between data blocks in a MapReduce framework. Hence more work should be dedicated to balancing the load between the distributed blocks.",2017,24752320
139,10.1109/CISCE50729.2020.00062,THE VOCATIONAL SKILLS COMPETITION BASED ON BIG DATA ANALYSIS PROMOTES THE RESEARCH OF STUDENTS' VOCATIONAL ABILITY,"With the progress of information technology, big data gradually shows its extraordinary value. Big data is used in all walks of life, including education and teaching. The big data on the vocational Skills Competition can analyze the students' mastery of vocational ability. Through big data analysis, I was informed of the projects of China Vocational Skills Competition and the awards of various provinces and cities. It is concluded that the competition of vocational skills is difficult and can train students' various abilities. Therefore, it is proposed to take the vocational skill contest as an opportunity to improve students' vocational ability and teaching quality and promote the development of vocational education.",2020,
140,10.1109/BigData.2013.6691553,ELASTIC ALGORITHMS FOR GUARANTEEING QUALITY MONOTONICITY IN BIG DATA MINING,"When mining large data volumes in big data applications users are typically willing to use algorithms that produce acceptable approximate results satisfying the given resource and time constraints. Two key challenges arise when designing such algorithms. The first relates to reasoning about tradeoffs between the quality of data mining output, e.g. prediction accuracy for classification tasks and available resource and time budgets. The second is organizing the computation of the algorithm to guarantee producing better quality of results as more budget is used. Little work has addressed these two challenges together in a generic way. In this paper, we propose a novel framework for developing elastic big data mining algorithms. Based on Shannon's entropy, an information-theoretic approach is introduced to reason about how result quality is affected by the allocated budget. This is then used to guide the development of algorithms that adapt to the available time budgets while guaranteeing producing better quality results as more budgets are used. We demonstrate the application of the framework by developing elastic k-Nearest Neighbour (kNN) classification and collaborative filtering (CF) recommendation algorithms as two examples. The core of both elastic algorithms is to use a naïve kNN classification or CF algorithm over R-tree data structures that successively approximate the entire datasets. Experimental evaluation was performed using prediction accuracy as quality metric on real datasets. The results show that elastic mining algorithms indeed produce results with consistent increase in observable qualities, i.e., prediction accuracy, in practice.",2013,
141,10.1109/BigData.2017.8257990,HOLISTIC AND SCALABLE RANKING OF RDF DATA,"The volume and number of data sources published using Semantic Web standards such as RDF grows continuously. The largest of these data sources now contain billions of facts and are updated periodically. A large number of applications driven by such data sources requires the ranking of entities and facts contained in such knowledge graphs. Hence, there is a need for time-efficient approaches that can compute ranks for entities and facts simultaneously. In this paper, we present the first holistic ranking approach for RDF data. Our approach, dubbed HARE, allows the simultaneous computation of ranks for RDF triples, resources, properties and literals. To this end, HARE relies on the representation of RDF graphs as bi-partite graphs. It then employs a time-efficient extension of the random walk paradigm to bi-partite graphs. We show that by virtue of this extension, the worst-case complexity of HARE is O(n5) while that of PageRank is O(n6). In addition, we evaluate the practical efficiency of our approach by comparing it with PageRank on 6 real and 6 synthetic datasets with sizes up to 108 triples. Our results show that HARE is up to 2 orders of magnitude faster than PageRank. We also present a brief evaluation of HARE's ranking accuracy by comparing it with that of PageRank applied directly to RDF graphs. Our evaluation on 19 classes of DBpedia demonstrates that there is no statistical difference between HARE and PageRank. We hence conclude that our approach goes beyond the state of the art by allowing the ranking of all RDF entities and of RDF triples without being worse w.r.t. the ranking quality it achieves on resources. HARE is open-source and is available at http://github.com/dice-group/hare.",2017,
142,10.1109/CITISIA50690.2020.9371793,IMPROVING THE QUALITY OF EDUCATION SYSTEM USING DATA SCIENCE TECHNOLOGIES: SURVEY,"Education is the most important and silent weapon in a country for both individual and country's economy. However lower level of adoption in the education system, poor decision making with less accuracy levels, adoption to new curriculums or subjects, teaching and learning styles are the main issues in education systems. These factors also have further long-term consequences for a country such as unemployment rates rises high, lack of suitable workforce for the demanding fields, individual dissatisfaction while being unemployed as well as in the community and socially. Unemployment rates are risen in Australia from past few years and this as a factor will be an ongoing issue if the government does not take any further actions to overcome these issues will definitely be direct hit to their economy in relation to work force in the present and future. Therefore the right technology should be implemented in order to obtain extract insights, obtain accurate decisions and high level adoption in education sector, as an example technologies such as data warehousing, big data, data mining, business intelligence and data analytics are in the peak of other industries such as aviation, retail, banking and other financial institutions. The main objective of this project is to facilitate a guide or a review for having data science technologies implemented in education sector in order to accomplish better education, as well as emphasis potential advantages of data technologies if it has been implemented in and around education systems.",2020,
143,10.1109/DSIT55514.2022.9943854,RESEARCH ON THE CONSTRUCTION AND APPLICATION OF KNOWLEDGE GRAPH FOR PROCESS QUALITY OPTIMIZATION,"Process quality optimization is a necessary means for enterprises to improve product quality. However, the current process quality optimization of enterprises is highly dependent on experienced senior technicians. If the experience of these people cannot be effectively converted into knowledge, the value of massive industrial data cannot be effectively obtained. It has become the main bottleneck faced by the intelligent production of enterprises. To this end, this paper proposes a process quality optimization method based on knowledge graph, which uses process quality optimization rules and artificial experience knowledge to construct a process quality optimization knowledge graph including process knowledge, product defect knowledge, and process optimization knowledge. Associate process defects and defect handling cases in the form of event clusters, combine artificial intelligence and graph algorithms, and assist process quality optimization decision-making in various ways such as knowledge search, knowledge question and answer, and knowledge recommendation.",2022,
144,10.1109/ICISCT52966.2021.9670387,INCREASING DATA RELIABILITY BY USING BIGDATA PARALLELIZATION MECHANISMS,"Nowadays, process management based on digital technologies leads to the minimization of human labor and costs, and a sharp increase in work efficiency. In the current information age and globalization, the rapid increase in the flow of data, the development of storage, processing and transmission of data in network technologies remains relevant. Consequently, the increase in data flow on computer networks creates big data. Big data processing and increasing the reliability of big data serve to create a factor of high-quality data processing. Implementing parallelism and distribution mechanisms to improve data reliability in big data will determine the solution to the problem. This study looked at implementing a parallelism mechanism to improve data reliability in big data improving data reliability in networked systems.",2021,
145,10.1109/BigData.2015.7363784,LABBOOK: METADATA-DRIVEN SOCIAL COLLABORATIVE DATA ANALYSIS,"Open data analysis platforms are being adopted to support collaboration in science and business. Studies suggest that analytic work in an enterprise occurs in a complex ecosystem of people, data, and software working in a coordinated manner. These studies also point to friction between the elements of this ecosystem that reduces user productivity and quality of work. LabBook is an open, social, and collaborative data analysis platform designed explicitly to reduce this friction and accelerate discovery. Its goal is to help users leverage each other's knowledge and experience to find the data, tools and collaborators they need to integrate, visualize, and analyze data. The key insight is to collect and use more metadata about all elements of the analytic ecosystem by means of an architecture and user experience that reduce the cost of contributing such metadata. We demonstrate how metadata can be exploited to improve the collaborative user experience and facilitate collaborative data integration and recommendations. We describe a specific use case and discuss several design issues concerning the capture, representation, querying and use of metadata.",2015,
146,10.1109/BigData.2014.7004210,REPRESENTATIVE SUBSETS FOR BIG DATA LEARNING USING K-NN GRAPHS,"In this paper we propose a deterministic method to obtain subsets from big data which are a good representative of the inherent structure in the data. We first convert the large scale dataset into a sparse undirected k-NN graph using a distributed network generation framework that we propose in this paper. After obtaining the k-NN graph we exploit the fast and unique representative subset (FURS) selection method [1], [2] to deterministically obtain a subset for this big data network. The FURS selection technique selects nodes from different dense regions in the graph retaining the natural community structure. We then locate the points in the original big data corresponding to the selected nodes and compare the obtained subset with subsets acquired from state-of-the-art subset selection techniques. We evaluate the quality of the selected subset on several synthetic and real-life datasets for different learning tasks including big data classification and big data clustering.",2014,
147,10.1109/ICESIT53460.2021.9696498,RESEARCH ON THE APPLICATION OF ARTIFICIAL INTELLIGENCE IN COMPUTER NETWORK TECHNOLOGY IN BIG DATA ERA,"The 21st century is an information age, and it is also a data age. The importance of data information resources is highlighted under the background of big data age. While the development of computer and its derivative technology promotes the growth of computer science, it also makes the artificial intelligence technology gradually mature. A lot of problems need to be considered, mainly including the efficiency, real-time. However, the computer network structure is becoming more and more complex, resulting in the low efficiency of computer network communication. With the development of artificial intelligence technology, it provides an effective solution to this problem. Artificial intelligence algorithm can optimize the whole network according to the scale of the network and the deployment of equipment, so as to improve the communication quality and efficiency of computer network.",2021,
148,10.1109/BigData.2017.8258196,TOWARDS A UNIFIED STORAGE AND INGESTION ARCHITECTURE FOR STREAM PROCESSING,"Big Data applications are rapidly moving from a batch-oriented execution model to a streaming execution model in order to extract value from the data in real-time. However, processing live data alone is often not enough: in many cases, such applications need to combine the live data with previously archived data to increase the quality of the extracted insights. Current streaming-oriented runtimes and middlewares are not flexible enough to deal with this trend, as they address ingestion (collection and pre-processing of data streams) and persistent storage (archival of intermediate results) using separate services. This separation often leads to I/O redundancy (e.g., write data twice to disk or transfer data twice over the network) and interference (e.g., I/O bottlenecks when collecting data streams and writing archival data simultaneously). In this position paper, we argue for a unified ingestion and storage architecture for streaming data that addresses the aforementioned challenge. We identify a set of constraints and benefits for such a unified model, while highlighting the important architectural aspects required to implement it in real life. Based on these aspects, we briefly sketch our plan for future work that develops the position defended in this paper.",2017,
149,10.1109/BigData50022.2020.9377972,"MAKING ""MAGIC"" WITH ENGINEERED DECISIONS, DATA, AND PROCESSES: A HOSPITAL OPERATIONS CENTER","Healthcare in the U.S. is the most expensive in the world by any measure, but not the best. The reasons are many and apply worldwide. The short summary is that technology in healthcare is Space Age, but processes echo medieval artisan work (for a variety of reasons). Big Data is the latest technology to promise vast improvement, but may follow many other technologies in making little or no significant improvement and possibly making matters worse. Decision analysis supplies a philosophy, theory, methodology, and tool set for making better decisions in novel, complex, or uncertain situations. It can be applied to individual or group decisions, including Big Data team projects. Hospital operations centers may achieve the sought-after ""Quadruple Aim,"" with simultaneous improvements in healthcare quality, patient and provider satisfaction, and reduction in cost. Decision analysis offers a framework for the system engineering efforts required, which may include harnessing Big Data. Current thoughts for a development and implementation pathway are described.",2020,
150,10.1109/ICSGEA51094.2020.00094,ANALYSIS OF COMPUTER NETWORK INFORMATION SECURITY UNDER THE BACKGROUND OF BIG DATA,"In today's society, under the comprehensive arrival of the Internet era, the rapid development of technology has facilitated people's production and life, but it is also a “double-edged sword”, making people's personal information and other data subject to a greater threat of abuse. The unique features of big data technology, such as massive storage, parallel computing and efficient query, have created a breakthrough opportunity for the key technologies of large-scale network security situational awareness. On the basis of big data acquisition, preprocessing, distributed computing and mining and analysis, the big data analysis platform provides information security assurance services to the information system. This paper will discuss the security situational awareness in large-scale network environment and the promotion of big data technology in security perception.",2020,
151,10.1109/TBME.2016.2573285,–OMIC AND ELECTRONIC HEALTH RECORD BIG DATA ANALYTICS FOR PRECISION MEDICINE,"Objective: Rapid advances of high-throughput technologies and wide adoption of electronic health records (EHRs) have led to fast accumulation of -omic and EHR data. These voluminous complex data contain abundant information for precision medicine, and big data analytics can extract such knowledge to improve the quality of healthcare. Methods: In this paper, we present -omic and EHR data characteristics, associated challenges, and data analytics including data preprocessing, mining, and modeling. Results: To demonstrate how big data analytics enables precision medicine, we provide two case studies, including identifying disease biomarkers from multi-omic data and incorporating -omic information into EHR. Conclusion: Big data analytics is able to address -omic and EHR data challenges for paradigm shift toward precision medicine. Significance: Big data analytics makes sense of -omic and EHR data to improve healthcare outcome. It has long lasting societal impact.",2017,15582531
152,10.1109/COMPSAC.2015.343,BIOMEDICAL BIG DATA ANALYTICS FOR PATIENT-CENTRIC AND OUTCOME-DRIVEN PRECISION HEALTH,"Rapid advancements in biotechnologies such as -omic (genomics, proteomics, metabolomics, lipidomics etc.), next generation sequencing, bio-nanotechnologies, molecular imaging, and mobile sensors etc. accelerate the data explosion in biomedicine and health wellness. Multiple nations around the world have been seeking novel effective ways to make sense of ""big data"" for evidence-based, outcome-driven, and affordable 5P (Patient-centric, Predictive, Preventive, Personalized, and Precise) healthcare. My main research focus is on multi-modal and multi-scale (i.e. molecular, cellular, whole body, individual, and population) biomedical data analytics for discovery, development, and delivery, including translational bioinformatics in biomarker discovery for personalized care; imaging informatics in histopathology for clinical diagnosis decision support; bionanoinformatics for minimally-invasive image-guided surgery; critical care informatics in ICU for real-time evidence-based decision making; and chronic care informatics for patient-centric health. In this talk, first, I will highlight major challenges in biomedical and health informatics pipeline consisting of data quality control, information feature extraction, advanced knowledge modeling, decision making, and proper action taking through feedback. Second, I will present informatics methodological research in (i) data integrity and integration; (ii) case-based reasoning for individualized care; and (iii) streaming data analytics for real-time decision support using a few mobile health case studies (e.g. Sickle Cell Disease, asthma, pain management, rehabilitation, diabetes etc.). Last, there is big shortage of data scientists and engineers who are capable of handling Big Data. In addition, there is an urgent need to educate healthcare stakeholders (i.e. patients, physicians, payers, and hospitals) on how to tackle these grant challenges. I will discuss efforts such as patient-centric educational intervention, community-based crowd sourcing, and Biomedical Data Analytics MOOC development. Our research has been supported by NIH, NSF, Georgia Research Alliance, Georgia Cancer Coalition, Emory-Georgia Tech Cancer Nanotechnology Center, Children's Health Care of Atlanta, Atlanta Clinical and Translational Science Institute, and industrial partners such as Microsoft Research and HP.",2015,07303157
153,10.1109/BIBM47256.2019.8983192,GSDCREATOR: AN EFFICIENT AND COMPREHENSIVE SIMULATOR FOR GENARATING NGS DATA WITH POPULATION GENETIC INFORMATION,"In recent decades, NGS data analysis has become a major research field in bioinformatics, which presents great advantages in many application scenarios. Many algorithms and software were designed for analyzing the NGS data, while simulation datasets are urgently needed for testing software and optimizing their parameter configurations. Thus, a series of NGS data simulators have been published. However, the existing simulators cannot satisfy the requirements from many specific scenarios. First, they do not support many newly discovered variations. Second, complex structural variations are difficult to generate. In addition, along with the increase of population data, it is urgent to increase population information simulation. In this paper, we propose GSDcreator, a comprehensive NGS simulator that overcome the three weaknesses mentioned above. It can produce all known types of variation, where the complex of variations are also supported. Furthermore, it can capture many important real data features including population polymorphism, insert size distribution, adjacent site depth distribution, overall depth distribution, quality score distribution, amplification bias, sequencing errors and so on. It's highlighted that 1000 Genomes Project Database is taken as a reference and integrates population genetic information to simulate population polymorphism. To test the performance, we did a lot of experiments and found that simulated data produced by GSDcreator are quit mimic to the real sequencing data.",2019,
154,10.1109/INFOCOM48880.2022.9796718,SCHEDULE OR WAIT: AGE-MINIMIZATION FOR IOT BIG DATA PROCESSING IN MEC VIA ONLINE LEARNING,"The age of data (AoD) is identified as one of the most novel and important metrics to measure the quality of big data analytics for Internet-of-Things (IoT) applications. Meanwhile, mobile edge computing (MEC) is envisioned as an enabling technology to minimize the AoD of IoT applications by processing the data in edge servers close to IoT devices. In this paper, we study the AoD minimization problem for IoT big data processing in MEC networks. We first propose an exact solution for the problem by formulating it as an Integer Linear Program (ILP). We then propose an efficient heuristic for the offline AoD minimization problem. We also devise an approximation algorithm with a provable approximation ratio for a special case of the problem, by leveraging the parametric rounding technique. We thirdly develop an online learning algorithm with a bounded regret for the online AoD minimization problem under dynamic arrivals of IoT requests and uncertain network delay assumptions, by adopting the Multi-Armed Bandit (MAB) technique. We finally evaluate the performance of the proposed algorithms by extensive simulations and implementations in a real test-bed. Results show that the proposed algorithms outperform existing approaches by reducing the AoD around 10%.",2022,0743166X
155,10.1109/ICCES51350.2021.9489156,APPLICATION OF DATA MINING TECHNOLOGY IN INTELLIGENT PUSHING URBAN TOURISM INFORMATION SYSTEM,"With the advent of the era of big data and information, the smart tourism industry has achieved rapid development. At present, tourism authorities and enterprises should change their development concepts. Hence, this paper considers the application of the data mining technology in intelligent pushing urban tourism information system. Firstly, the intelligent system has been designed from the data perspective, where the fuzzy data mining and the pushing information collection systems are both studied. Then, the data analytic model is applied. The proposed model has been tested and compared with the latest models. The pushing efficiency and QoS are guaranteed.",2021,
156,10.1109/BigData.2014.7004349,SCALABLE BIG DATA COMPUTING FOR THE PERSONALIZATION OF MACHINE LEARNED MODELS AND ITS APPLICATION TO AUTOMATIC SPEECH RECOGNITION SERVICE,"We observe that the recent advances in big data computing have empowered model-based services such as speech recognition, face recognition, context-aware service, and many other services. Various sources of user's logs can be utilized in remodeling or adapting existing models to improve the quality of service. We propose a system that can support store/retrieve data and process them in a scalable manner. Recently advances in ASR and big data technologies drive more personalized services in many areas of services. A speaker adaptation is one good example which requires huge computation cost in creating a personalized acoustic model and corresponding language model over 100s millions of Samsung product users. We propose a personalized and scalable ASR system powered by the big data infrastructure which brings data-driven personalized opportunities to voice-enabled services such as voice-to-text transcriber, voice-enabled web search in a peta bytes scale. We verify the feasibility of speaker adaptation based on 107 testers' recordings and obtain about 10% of recognition accuracy. We study an optimal set of execution environments by executing jobs running either on Hadoop 1 or Hadoop 2 cluster, and move forward performance optimization strategies: workflow compaction, file compression, best file system selection among several distributed file systems. We devise a metric for the cost of personalized model creation to compare the efficiency of one cluster with the other cluster, and it provides the estimated total execution time for the given number of machines. We finally introduce our in-house object storage and data storage design, and their high performance compared to state-of-the art systems, optimized for voice-enabled services to effectively support small and large files.",2014,
157,10.1109/MSP.2014.2327238,MODELING AND OPTIMIZATION FOR BIG DATA ANALYTICS: (STATISTICAL) LEARNING TOOLS FOR OUR ERA OF DATA DELUGE,"With pervasive sensors continuously collecting and storing massive amounts of information, there is no doubt this is an era of data deluge. Learning from these large volumes of data is expected to bring significant science and engineering advances along with improvements in quality of life. However, with such a big blessing come big challenges. Running analytics on voluminous data sets by central processors and storage units seems infeasible, and with the advent of streaming data sources, learning must often be performed in real time, typically without a chance to revisit past entries. Workhorse signal processing (SP) and statistical learning tools have to be re-examined in todays high-dimensional data regimes. This article contributes to the ongoing cross-disciplinary efforts in data science by putting forth encompassing models capturing a wide range of SP-relevant data analytic tasks, such as principal component analysis (PCA), dictionary learning (DL), compressive sampling (CS), and subspace clustering. It offers scalable architectures and optimization algorithms for decentralized and online learning problems, while revealing fundamental insights into the various analytic and implementation tradeoffs involved. Extensions of the encompassing models to timely data-sketching, tensor- and kernel-based learning tasks are also provided. Finally, the close connections of the presented framework with several big data tasks, such as network visualization, decentralized and dynamic estimation, prediction, and imputation of network link load traffic, as well as imputation in tensor-based medical imaging are highlighted.",2014,15580792
158,10.1109/ICCSE51940.2021.9569727,TEACHING PATH TO IMPROVE THE PRACTICAL ABILITY OF POSTGRADUATES MAJORED IN DATA ENGINEERING IN MINORITY NATIONALITY REGIONS,"To alleviate the shortage of high-level big data experts available in minority nationality regions and improve the quality of relevant postgraduate education, the current problems of postgraduates training in the minority nationality regions are analyzed and explored. The training of practical ability of postgraduates will significantly affect whether postgraduate students can really meet the needs of the related industries in minority nationality regions, and thus it has become the top priority in the process of postgraduate education. Considering the significantly diversity in different areas of minority nationality regions, the unbalanced development and the rough supporting of IT industries, based on modern education theories, this paper tries to find a training path of practical ability of postgraduates. More specifically, we build a collaborative teaching team, explore the novel training modes to improve the education quality of postgraduates in minority nationality regions. After this training path is applied at our university, the practice ability of postgraduates in big data field is improved significantly.",2021,24716146
159,10.1109/BigData.Congress.2014.81,FSBD: A FRAMEWORK FOR SCHEDULING OF BIG DATA MINING IN CLOUD COMPUTING,"Cloud computing is seen as an emerging technology for big data mining and analytics. Cloud computing can provide data mining results in the form of a Software As a Service (SAS). Both performance and quality of mining are fundamentals criteria for the use of a data mining application provided by a Cloud computing environment. In this paper, we propose a Cloud computing framework, which is responsible to distribute and schedule a Cluster-Based data mining application and its data set. The main goal of our proposed framework for scheduling of Big Data Mining (FSBD) is to decrease the overall execution time of the application with minimum loss in mining quality. We consider the Cluster-based data mining technique as a pilot application for our framework. The results show an important speedup with a minimum loss in quality of mining. We obtained a ratio of 2 of the normalized actual makespan vis-a-vis the ideal makespan. The quality of mining scales well with the number of clusters and the increasing size of the dataset. The results are promising, encouraging the adoption of the framework by Cloud providers.",2014,23797703
160,10.1109/ACCESS.2021.3100287,ANALYZING AND EVALUATING CRITICAL CHALLENGES AND PRACTICES FOR SOFTWARE VENDOR ORGANIZATIONS TO SECURE BIG DATA ON CLOUD COMPUTING: AN AHP-BASED SYSTEMATIC APPROACH,"Recently, its becomes easy to track down the data due to its availability in a large number. Although for data management, processing, and obtainability, cloud computing is considered a well-known approach for organizational development on the internet. Despite many advantages, cloud computing has still numerous security challenges that can affect the big-data usage on cloud computing. To find the security issues/challenges that are faced by software vendors’ organizations we conducted a systematic literature review (SLR) through which we have find out 103 relevant research publications by developing a search string that is inspired by the research questions. This relevant data was comprised from different databases e.g. Google Scholar, IEEE Explore, ScienceDirect, ACM Digital Library, and SpringerLink. Furthermore, for the detailed literature review, we have accomplished all the steps in SLR, for example, development of SLR protocol, Initials and final assortment of the relevant data, data extraction, data quality assessment, and data synthesis. We identified fifteen (15) critical security challenges which are: data secrecy, geographical data location, unauthorized data access, lack of control, lack of data management, network-level issues, data integrity, data recovery, lack of trust, data sharing, data availability, asset issues, legal amenabilities, lack of quality, and lack of consistency. Furthermore, sixty four (64) standard practices are identified for these critical security challenges using the proposed SLR that could help vendor organizations to overcome the security challenges for big data. The findings of our research study demonstrate the resemblances and divergences in the identified security challenges in different periods, continents, databases, and methods. The proposed SLR will also support software vendor organizations for securing big data on the cloud computing platforms. This paper has the following content: in Section II, we have describe the Literature review; in Section III, research methodology is specified; in Section IV, the findings of the SLR and the analysis of result are discussed; in Section V, the limitations of this research are given; in Section VI, we discussed our conclusions and future work.",2021,21693536
161,10.1109/BigDataService.2016.16,POLICY-BASED QOS ENFORCEMENT FOR ADAPTIVE BIG DATA DISTRIBUTION ON THE CLOUD,"Big Data distribution has benefited from the Cloud resources to accommodate application's QoS requirements. In this paper, we propose Big Data distribution scheme that matches the Cloud available resources to guarantee application's QoS given the continuously dynamic and varying resources of the Cloud infrastructure. We developed Two-Level QoS Policies (TLPS) for selecting clusters and nodes while satisfying the client's application QoS. We also proposed an adaptive data distribution algorithm to cope with changing QoS requirements. Experiments have been conducted to evaluate both the effectiveness and the communication overhead of our proposed distribution scheme and the results we have reported are convincing. Other experiments evaluated our TLPS algorithm against other single-based QoS data distribution algorithms and the results show that TLPS algorithm adapts to the customer QoS requirements.",2016,
162,10.1109/MELECON48756.2020.9140534,THE QUALITY CONCERNS IN HEALTH CARE BIG DATA,"Health information technology is showing an impressive growing interest towards Big Data. Big Data Analytics is expected to bring important achievements for building sophisticated models, methods and tools that are expected to improve healthcare services and citizen health and wellbeing. In spite of these expectations data quality and analytics methods are not getting the attention they deserve. In this short paper, we aimed to highlight the issues of data quality in the context of Big Data Healthcare Analytics. The common sources of errors, the consequence of these errors, and potential solutions that should be considered to mitigate errors and pitfalls are discussed in the healthcare context.",2020,21588473
163,10.1109/BigData47090.2019.9005636,ASSOCIATION MODEL BETWEEN VISUAL FEATURE AND AQI RANK USING LIFELOG DATA,"Air Quality Index (AQI) is an indicator of the rank of air pollution that is very vital for the environmental impacts to the public health. In this paper, we propose an association model between visual feature and AQI rank of lifelog data. Visual data (i.e., environmental pictures) and numerical data (i.e., environmental AQI measurements) of lifelog are utilized for the data training stage. The features of the visual data are extracted using a CNN-based method, where the latter are calculated using the standard AQI ranking. The extracted visual features and ranked AQI are combined as the input data for a deep neural network MLP (Multi-layer Perception) to study the association relationship between visual feature and AQI rank. The experimental results show that the proposed method can provide accurate predictions of good or unhealthy AQI ranks from lifelog visual data.",2019,
164,10.1109/COMST.2018.2844341,DEEP LEARNING FOR IOT BIG DATA AND STREAMING ANALYTICS: A SURVEY,"In the era of the Internet of Things (IoT), an enormous amount of sensing devices collect and/or generate various sensory data over time for a wide range of fields and applications. Based on the nature of the application, these devices will result in big or fast/real-time data streams. Applying analytics over such data streams to discover new information, predict future insights, and make control decisions is a crucial process that makes IoT a worthy paradigm for businesses and a quality-of-life improving technology. In this paper, we provide a thorough overview on using a class of advanced machine learning techniques, namely deep learning (DL), to facilitate the analytics and learning in the IoT domain. We start by articulating IoT data characteristics and identifying two major treatments for IoT data from a machine learning perspective, namely IoT big data analytics and IoT streaming data analytics. We also discuss why DL is a promising approach to achieve the desired analytics in these types of data and applications. The potential of using emerging DL techniques for IoT data analytics are then discussed, and its promises and challenges are introduced. We present a comprehensive background on different DL architectures and algorithms. We also analyze and summarize major reported research attempts that leveraged DL in the IoT domain. The smart IoT devices that have incorporated DL in their intelligence background are also discussed. DL implementation approaches on the fog and cloud centers in support of IoT applications are also surveyed. Finally, we shed light on some challenges and potential directions for future research. At the end of each section, we highlight the lessons learned based on our experiments and review of the recent literature.",2018,2373745X
165,10.1109/BigDataService.2016.41,CLEANING FRAMEWORK FOR BIGDATA: AN INTERACTIVE APPROACH FOR DATA CLEANING,"Data is a valuable resource. Proper use of high-quality data can help people make better predictions, analyses and decisions. However, no matter how much effort we put into collecting a good dataset, errors will inevitably creep into the data, making it necessary for data cleaning. This becomes a concern particularly when large-scale heterogeneous data from multiple sources are integrated for other purposes. Data cleaning can be complicated, time-consuming, and expensive, but it is a necessary step in any data-related system since poor-quality data may not be suitable to achieve the intended purposes. The core of our data cleaning system is data association and repairing. Association aims to identify the same object and link with the most associated objects, and repairing is to make a database reliable by fixing errors in the data. For big data applications, we don't necessarily need to use all the data. In most situations, we only need a small subset of the most relevant data. So the goal of association is to convert big raw data into a small subset of the most relevant data that are most useful for a particular application. After we obtain a small amount of relevant data, we also need to further analyze the data to help people digest the data and turn the data into knowledge. We use a number of techniques to associate the data to get useful knowledge for data repairing. Our research shows that data association can effectively help with data repairing. To capture the interaction, we provide a uniform framework that unifies the association and repairing process seamlessly based on context patterns, usage patterns, metadata, and repairing rules.",2016,
166,10.1109/EEI48997.2019.00115,RELAY PROTECTION DATA INTEGRITY CHECK METHOD BASED ON BIG DATA ASSOCIATION ALGORITHM,"Relay protection big data creates good conditions for the improvement of professional applications, and data integrity is an important aspect that reflects data quality. The association of relay protection big data is intense. This paper applies Apriori algorithm to mine data relevance and generate association rules. Based on this, the integrity of relay protection data is checked, and the incomplete data is predicted. Taking the relay protection defect data as an example, the paper explores the correlation among 251 items of the six dimensions of protection relay defect data such as type of protection, the severity of the defect, whether the protection is out of operation, the defect location, the cause of the defect, and the equipment manufacturer, completing the processing of incomplete data with great application results.",2019,
167,10.1109/ICACCS.2016.7586377,SURVEY ON INCREMENTAL AND ITERATIVE MODELS IN BIG DATA MINING ENVIRONMENT,"It has become increasingly popular to mine big data in order to gain insights to help business decisions or to provide more desirable personalized, higher quality services. They usually include data sets with sizes beyond the ability of commonly used software tools to retrieve, manage, and process data within an adequate elapsed time. So there is big demand for distributed computing framework. As new data and updates are constantly arriving, the results of data mining applications become incomplete over time. In such situations it is desirable to periodically refresh the mined data in order to keep it up-to-date. This paper describes the existing approaches to big data mining which uses these frameworks in an incremental approach that saves and reuses the previous states of computations. It also explores several enhancements introduced in this same framework with iterative mapping characteristics. Gaps in the current methods are identified in this literature review.",2016,
168,10.1109/BigDataCongress.2015.66,BIG DATA ANALYTICS FRAMEWORK FOR SYSTEM HEALTH MONITORING,"In this paper, we present our Machine Learning (ML) based big data analytics framework that we tested to improve the quality and performance of Auxiliary Power Units (APU) health monitoring services. We are motivated to develop and apply practical and useful big data analytics technologies for industrial applications in aerospace and aviation. Key contributions of our work include the development and use of our ML algorithms that have been tested and used to analyze multiple data sources and to provide useful insights and increase the ability to predict (1) APU wear from 39%to 56% and (2) APU shutdown events from 19% to 60%. Such system health monitoring can be integrated with the widely used condition based maintenance (CBM) services. Users can use this cloud based analytic toolset and access the big data through any devices (PCs, Tablets, smart phones) anytime and anywhere.",2015,23797703
169,10.1109/ICACCI.2017.8126163,ANALYSIS OF FEATURE SELECTION AND EXTRACTION ALGORITHM FOR LOAN DATA: A BIG DATA APPROACH,"Fraudulent activities in financial institutes can break the economic system of the country. These activities can be identified using clustering and classification algorithms. Effectiveness of these algorithms depend on quality of the input data. Moreover, financial data comes from various sources and forms such as financial statements, stakeholders activities and others. This data from various sources is very vast and unstructured big data. Hence, parallel distributed pre-processing is very significant to improve the quality of the data. Objective of this work is dimensionality reduction considering feature selection and extraction algorithm for large volume of financial data. In this paper an attempt is made to understand the implications of feature extraction and transformation algorithm using Principal Feature Analysis on the financial data. Effect of reduced dimension is studied on various classification algorithms for financial loan data. Parallel and distributed implementation is carried out on IBM Bluemix cloud platform with spark notebook. The results show that reduction of features has significantly improved execution time without compromising the accuracy.",2017,
170,10.1109/SERVICES.2019.00082,A BIG DATA ARCHITECTURE FOR THE EXTRACTION AND ANALYSIS OF EHR DATA,"In the current Italian eHealth scenario, a national IT platform has been designed and developed with the purpose of ensuring the interoperability between the various Electronic Health Record (EHR) systems that have been adopted in the different regions of the country, according to the requirements provided by Italian Laws. In this way, the healthcare providers and the policy makers can acquire and process the data of a patient despite its initial format and source, allowing an improved quality of patient care and optimizing the management of the financial resources. To further exploit this huge resource of health and social data, it is very important to allow the extraction of the complex information buried under the Big Data source enabled by the EHRs, providing the physicians, the researchers and public health policy makers with innovative instruments. Meeting this need is not a trivial task, due to the difficulties of processing different document formats and processing Natural Language text, alongside to the problems related to the data size. In this paper we propose a Big Data architecture that is able to extract information from the documents acquired by the EHRs, integrate and process them, providing a set of valuable data for both physicians and patients, as well as decision makers.",2019,23783818
171,10.1109/BigData47090.2019.9006596,CYBERCRIME INVESTIGATIONS IN THE ERA OF SMART APPLICATIONS: WAY FORWARD THROUGH BIG DATA,"The omnipresence of smart devices in many aspects of modern everyday life has helped to achieve an enormous level of automation, has ensured sustainable development, and improved quality of life. Over the last decade, such small and portable devices became cheap and easy to deploy in any kind of application. With the full range of versatile connectivity, such technological development also brings multiple challenges related to the security of infrastructure and data. Many individuals, companies, and states worldwide experience the previously unseen scale and scope of the attacks using novel approaches. All these smart applications have also increased the overall attack surface leading to multiple attack vectors available through vulnerabilities. Lack of standards, insufficient security awareness, and new technological landscape does not help either. Considering this, one needs to enhance forensics investigation methodologies, employ novel tools, combine threat intelligence, and integrate forensic readiness. Such measures will help to reduce the total cyber risk through a high level of preparedness for anticipated data-driven crimes in smart applications. We believe that this paper will help in bringing novel focus to existing digital forensics methodologies with a focus on smart applications.",2019,
172,10.1109/IPEC54454.2022.9777433,THE DESIGN OF WEARABLE SMART PRODUCTS BASED ON BIG DATA,"With the continuous development of my country’s economic level, people’s living standards have improved significantly, and wearable devices have gradually entered people’s sight. More and more people have begun to use wearable products such as smart watches and smart bracelets. In recent years, with the continuous development of big data technology, big data technology has also begun to be applied in the design of wearable smart products. This article analyzes and studies the design of wearable smart products. This article uses literature research, questionnaire surveys and other methods to study the design of wearable products based on big data research, and investigates what types and functions of wearable products are more popular with consumers. The survey results show that consumers know very little about the category of wearable design. About 50% of the people choose bracelets. This shows that the promotion of wearable products in bracelets is relatively good at the beginning. One reason is that its price is reasonably designed and can be accepted by most people. About 30% of people pay more attention to wearable products with healthy functions. This is also a modern trend. With the improvement of people’s quality of life, more people are choosing products that focus on health issues.",2022,
173,10.1109/SCC.2017.65,SCALABLE PROVENANCE STORAGE AND QUERYING USING PIG LATIN FOR BIG DATA WORKFLOWS,"Provenance refers to the information about the derivation history of a data product. It is important for evaluating the quality and trustworthiness of a data product and ensuring the reproducibility of scientific discoveries. Much research has been done on storing and querying scientific workflow provenance - provenance that is produced in the execution of data-centric scientific workflows. To address the challenges of big data in increasing volume, velocity and variety, a new generation of scientific workflows, called big data workflows are under active research. As both data and workflows increase in their scale, the scale of provenance naturally increases, calling for a new scalable storage and querying infrastructure. This paper leverages Pig Latin, a high-level platform for creating programs that run on Apache Hadoop, and OPQL, a graph-level provenance query language, to build a scalable provenance storage and querying system for big data workflows. Our main contributions are: i) we propose algorithms to translate OPQL constructs to equivalent Pig Latin programs, ii) we extend OPQL, to support the W3C PROV-DM standard provenance model, iii) we develop and evaluate our system on provenance datasets from the UTPB benchmark, and (iv) we create some visual OPQL constructs in the DATAVIEW big data workflow system to facilitate the easy creation of complex OPQL queries in a visual workflow style. Our preliminary experimental study shows the feasibility of our framework for big-data-scale provenance storage and querying.",2017,24742473
174,10.1109/ICAAIC53929.2022.9793035,INTELLIGENT SYSTEM DESIGN OF HUMAN RESOURCE MANAGEMENT FOR SMALL AND MEDIUM-SIZED ENTERPRISES BASED ON BIG DATA,"This paper focuses on the application of big data theory in three human resource management fields: talent recruitment and selection, talent assessment and incentives, talent assessment and development. Human resource management is very important, and big data provides new opportunities for human resource management in small and medium-sized enterprises. Direction, this paper will explore the human resource management of small and medium-sized enterprises in the context of big data. In order to ensure the improvement of the quality of human resource management, the development of this management work can provide assistance for small and medium-sized enterprises to improve their competitiveness.",2022,
175,10.1109/ICAIBD55127.2022.9820258,AN OPTIMIZED GRAPH EMBEDDING BASED KNOWLEDGE GRAPH CLEANING ALGORITHM,"Data quality of knowledge graph is a one of the most important guareentees for many knowledge-based applications. We investigate the konwledge graph cleaning problem. We propose a knowledge graph error detection framework and design an optimized embedding based clean algorithm. The framework maps the knowledge graph into an numerical space and keeps the relationship between different nodes. With this framework, both miss data error and errous relationship can be cleaned. Extensive experimental study over different data sets validate the effectiveness of the method.",2022,
176,10.1109/BigData50022.2020.9378498,TASK QUALITY OPTIMIZATION IN COLLABORATIVE ROBOTICS,"This paper addresses commonsense knowledge (CSK) to enhance human-robot collaboration (HRC) in large scale smart manufacturing. As big data in collaborative robotics grows, CSK in useful to achieve task optimization as depicted in our simulation studies and laboratory experiments, extendable to real-world applications.",2020,
177,10.1109/BigData.2016.7840916,SYMMETRIC REPOSITIONING OF BISECTING K-MEANS CENTERS FOR INCREASED REDUCTION OF DISTANCE CALCULATIONS FOR BIG DATA CLUSTERING,"Clustering is one of the fundamental data mining procedures. Bisecting K-means (BKM) clustering has been studied to have higher computing efficiency and better clustering quality when compared with the basic Lloyd version of the K-means clustering. Elkan's method of utilizing triangle inequality significantly reduces distance calculations, and is applicable to each K-means iteration without affecting the clustering result of the iteration. Thus, it is natural to think of using the Elkan method to further improve the efficiency of the already efficient bisecting K-means. In this paper, we find that the bisecting K-means allows the repositioning of the two centers of a to-be-bisected cluster to further reduce distance calculations. Based on our heuristic analysis we investigate a repositioning strategy with a set of repositioning parameters, and incorporated the repositioning techniques into the Elkan method for bisecting K-means algorithms. We tested these new algorithms on three big datasets with millions of data points and compared with the straightforwardly combined Elkan-BKM without center repositioning. The experimental results show that the center-repositioned algorithms have fewer distance calculations than the Elkan-BKM algorithms without center repositioning in almost all cases. While our repositioning parameters produced good results for the tested datasets, these parameter sets are derived based on our intuitive thinking and hence they are by no means optimal. The experimental data presented in this paper suggest the potential of achieving higher efficiency if better repositioning parameters can be discovered.",2016,
178,10.1109/TPDS.2013.191,AUTHORIZED PUBLIC AUDITING OF DYNAMIC BIG DATA STORAGE ON CLOUD WITH EFFICIENT VERIFIABLE FINE-GRAINED UPDATES,"Cloud computing opens a new era in IT as it can provide various elastic and scalable IT services in a pay-as-you-go fashion, where its users can reduce the huge capital investments in their own IT infrastructure. In this philosophy, users of cloud storage services no longer physically maintain direct control over their data, which makes data security one of the major concerns of using cloud. Existing research work already allows data integrity to be verified without possession of the actual data file. When the verification is done by a trusted third party, this verification process is also called data auditing, and this third party is called an auditor. However, such schemes in existence suffer from several common drawbacks. First, a necessary authorization/authentication process is missing between the auditor and cloud service provider, i.e., anyone can challenge the cloud service provider for a proof of integrity of certain file, which potentially puts the quality of the so-called ‘auditing-as-a-service’ at risk; Second, although some of the recent work based on BLS signature can already support fully dynamic data updates over fixed-size data blocks, they only support updates with fixed-sized blocks as basic unit, which we call coarse-grained updates. As a result, every small update will cause re-computation and updating of the authenticator for an entire file block, which in turn causes higher storage and communication overheads. In this paper, we provide a formal analysis for possible types of fine-grained data updates and propose a scheme that can fully support authorized auditing and fine-grained update requests. Based on our scheme, we also propose an enhancement that can dramatically reduce communication overheads for verifying small updates. Theoretical analysis and experimental results demonstrate that our scheme can offer not only enhanced security and flexibility, but also significantly lower overhead for big data applications with a large number of frequent small updates, such as applications in social media and business transactions.",2014,21619883
179,10.1109/ICMCCE51767.2020.00280,QUALITY ANALYSIS AND EVALUATION METHOD FOR MULTISOURCE AGGREGATION DATA BASED ON STRUCTURAL EQUATION MODEL,"In the era of big data, how to evaluate the data quality of multi-source aggregation data is very important. The reason is that uneven data quality will directly lead to inaccurate or ambiguous data in the database, and indirectly lead to the deviation of subsequent data mining and decision-making. In this paper, structural equation model(SEM) is introduced to explore the effectiveness of various data quality evaluation indicators in data aggregation and finding out internal relationship between them. A new quality evaluation method of multi-source aggregation data is proposed, based on the regression's significance analysis and factor loads of each observation index in the SEM model. The case analysis shows that the proposed method is feasible and can be used to evaluate the quality of multi-source aggregation data adaptively for a long time.",2020,
180,10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00341,THE IMPACT OF BIKE-SHARING RIDERSHIP ON AIR QUALITY: A SCALABLE DATA SCIENCE FRAMEWORK,"This research explores the relationship between daily air quality indicator (AQI) values and the daily intensity of bike-share ridership in New York City. The authors designed and deployed a distributed data science framework on which to process and run Elastic Net, Random Forest Regression, and Gradient Boosted Regression Trees. Nine gigabytes of CitiBike ridership data, along with one gigabyte of air quality indicator (AQI) data were employed. All machine learning algorithms identified bike-share ridership intensity as either the most important or the second most important feature in predicting future daily AQIs. The authors also empirically demonstrated that although a distributed platform was necessary to ingest and pre-process the raw 10 gigabytes of data, the actual execution time of all three machine learning algorithms on cleaned, joined, and aggregated data was far faster on a local, commodity computer than on its distributed counterpart.",2019,
181,10.1109/SmartCity.2015.46,OPC: A DISTRIBUTED COMPUTING AND MEMORY COMPUTING-BASED EFFECTIVE SOLUTION OF BIG DATA,"The Big Data computing is one of hot spots of the internet of things and cloud computing. How to compute efficiently on the Big Data is the key of improving performance. By means of distributed computing or memory computing, many companies and institutions provide some technologies and produces. But they are invalid in the scene in which there are real-time demands in the low-configure cluster. To deal with the problem, this paper provides a distributed computing and memory computing-based effective solution (Objectification Parallel Computing, OPC). In the solution, the data can be formatted into object. Then the objects are distributed stored in the computer memories and parallel compute to complete tasks. The OPC is applied to the Electric Asset Quality Supervision Manage System (EAQSMS) of State Grid of China, the result shows that with PCs the system is efficiently available, reliable, and flexible expansible.",2015,
182,10.1109/ICCNEA53019.2021.00053,RESEARCH ON THE CONSTRUCTION OF NETWORK COURSE PLATFORM BASED ON BIG DATA,"As an important part of education, curriculum construction is always the focus of teaching quality in colleges and universities. This study mainly discusses the construction of cloud platform for online courses of college pop singing majors in the era of big data. The user center module mainly involves user WeChat login, user view personal information, user view their study record and consumption record. Data mining model management provides users with simple and interactive operation interface to create mining model, which enables users to create mining simply and clearly and manage it conveniently. Users can use offline caching to download videos of the courses they need to study, instead of using up network or mobile data multiple times. In the final results, 26 students in the experimental group scored in the excellent range, compared with 22 students in the control group. This study is helpful to promote the development of popular singing major in colleges and universities.",2021,
183,10.1109/ICIEA49774.2020.9102068,DATA QUALITY ANALYSIS FRAMEWORK AND EVALUATION METHODS FOR POWER SYSTEM OPERATION WITH HIGH PROPORTION OF RENEWABLE ENERGY PENETRATION,"Global climate crisis in 21st century pushed countries to move towards energy transformation in generation and consumption. To achieve green and low-carbon energy transformation goals, it is necessary that a large number of renewable energy resources such as wind and solar to be consumed. Renewable energy with intermittent fluctuations in time dimension and agglomerations in spatial dimension increases the complexity of green energy consumption friendly. Therefore, comprehensive data and advanced predictive analysis methods are required to guarantee safety of operation and transactions for renewable energy plants and stations. We can even say that quality of renewable energy data determines the accuracy of prediction and analysis. Firstly, this article analyzes the operation and transaction characteristics of distributed renewable energy plants, and data quality analysis framework for distributed renewable energy operations and transactions was built on the new energy cloud platform. Data information were classified into model parameter and status instance, which are related to dispatching and energy power transaction businesses such as equipment model management, operation monitoring and security analysis, measurement statistics etc. The importance between them is determined according to pairwise comparison. Finally, analytic hierarchy process (AHP) theory was applied to calculate weights for data integrity, accuracy, consistency and timeliness, data quality assessment process and calculation methods were designed, and load series data was used to verify its correctness.",2020,
184,10.1109/ICEKIM52309.2021.00104,BASED ON BIG DATA TO ANALYSE THE INFLUENCE OF EDUCATION ON THE RESIDENCE WILLINGNESS OF FLOATING POPULATION,"With the development of information technology and the expansion of the floating population in China, the level of urbanization is constantly improving. Big data will set off a new wave, affecting the life and production mode of floating population, and changing their thoughts and concepts in the new era. In the wave of big data, as the main body of urbanization, the citizenization of floating population is an important embodiment of high-quality urban development. Based on the perspective of urban adaptation, this paper constructs a model of the effect of the education level of floating population on residence willingness, and analyzes how the education level of floating population affects their residence willingness by using the dynamic monitoring survey data of China's floating population in 2017 and Logit model.",2021,
185,10.1109/ICAICA52286.2021.9498101,RESEARCH ON THE APPLICATION OF ELECTRONIC TECHNOLOGY IN COMMUNICATION ENGINEERING UNDER THE BACKGROUND OF BIG DATA,"Electronic communication technology is very critical to improve the quality of communication engineering. This paper discusses the application of electronic technology in communication engineering under the background of big data. The use of electronic technology in the allocation of communication engineering resources is mainly reflected in that the electronic communication resources can only be allocated, and the communication engineering resources can only be allocated through the model to ensure the efficient transmission of mass data in communication engineering. The application of electronic technology in communication engineering information transmission is mainly shown in the electronic control system. First, the noise reduction of communication engineering is processed by filtering device. Then, the controller is used to control the information transmission path of communication engineering to ensure the efficient transmission of communication engineering. In order to ensure the safe and stable operation of the integrated circuit in the communication engineering, the application of electronic technology in the communication engineering under the background of big data is discussed.",2021,
186,10.1109/CONFLUENCE.2016.7508137,EMPIRICAL ANALYSIS OF DATA ACQUISITION TECHNIQUES: PAPI VS. CAPI,"Data acquisition refers to the act of collecting data on a large scale. This paper presents a qualitative and quantitative analysis of two data acquisition techniques, namely, Pen and Paper Interviewing (PAPI) and Computer-Assisted Personal Interviewing (CAPI). It cites two case studies to clearly define the difference between the two techniques. They have been compared on four factors, namely, cost, time consumed during the whole process, productivity of each interviewer and quality assessment of each techniques (average number of errors occurred per interview). It emphasizes on providing absolute numbers to clearly show the difference between the two techniques.",2016,
187,10.1109/BigData50022.2020.9378192,AN AUTOCORRELATION-BASED LSTM-AUTOENCODER FOR ANOMALY DETECTION ON TIME-SERIES DATA,"Data quality significantly impacts the results of data analytics. Researchers have proposed machine learning based anomaly detection techniques to identify incorrect data. Existing approaches fail to (1) identify the underlying domain constraints violated by the anomalous data, and (2) generate explanations of these violations in a form comprehensible to domain experts. We propose IDEAL, which is an LSTM-Autoencoder based approach that detects anomalies in multivariate time-series data, generates domain constraints, and reports subsequences that violate the constraints as anomalies. We propose an automated autocorrelation-based windowing approach to adjust the network input size, thereby improving the correctness and performance of constraint discovery over manual and brute-force approaches. The anomalies are visualized in a manner comprehensible to domain experts in the form of decision trees extracted from a random forest classifier. Domain experts can then provide feedback to retrain the learning model and improve the accuracy of the process. We evaluate the effectiveness of IDEAL using datasets from Yahoo servers, NASA Shuttle, and Colorado State University Energy Institute. We demonstrate that IDEAL can detect previously known anomalies from these datasets. Using mutation analysis, we show that IDEAL can detect different types of injected faults. We also demonstrate that the accuracy improves after incorporating domain expert feedback.",2020,
188,10.1109/BigData.2017.8258543,THE INFLUENCES OF DEEP-SEA VISION DATA QUALITY ON OBSERVATIONAL ANALYSIS,"Deep-sea study by optical observation method is an interdisciplinary subject and faces plenty of difficulties. To find out the influences of vision data quality, characteristic of vision data for deep-sea observation is analyzed, and a deep-sea landing experiment has been implemented. Data quality analyzing based on real deep-sea vision data that collected in the in-situ observation platform is actualized. It is expected that the research on influence mechanism of deep-sea vision quality is beneficial to the detection of region of interest, the judging of animal existence, the classification of species, and the trajectories labeling. Further analyzing on unsupervised deep-sea vision data quality control is necessary.",2017,
189,10.1109/IGARSS.2017.8126976,LATENCY ANALYSIS OF LARGE VOLUME SATELLITE DATA TRANSMISSIONS,"A wide array of time-sensitive satellite data is required in the research and development activities for natural hazard assessment, storms and weather prediction, hurricane tracking, disaster and emergency response, and so on. Identifying and analyzing the latencies of large volumes of real-time and near real-time satellite data is very useful and helpful for detecting transmission issues, managing IT resources, and configuring and optimizing data management systems. This paper introduces how to monitor and collect important timestamps of data transmissions, organize them in a NoSQL database, and explore data latency via a user-friendly dashboard. Taking Sentinel series satellite data as an example, data transmission issues are illustrated and investigated further. Latency analysis and explorations help data providers and managers improve data transmission and enhance data management.",2017,21537003
190,10.1109/INDIN.2016.7819372,REQUIREMENTS FOR A BIG DATA CAPTURING AND INTEGRATION ARCHITECTURE IN A DISTRIBUTED MANUFACTURING SCENARIO,"Big Data is one of the key enabling technologies in smart manufacturing, where manufacturing companies aim at leveraging the data generated throughout their processes. The potential of Big Data Analytics is particularly significant in the context of manufacturing companies distributed worldwide. These companies own several manufacturing plants operating the same process in different environments and conditions. This generates massive amounts of data that could be analyzed in order to improve process efficiency and product quality. This paper presents the requirements for an architecture to capture, integrate and analyze the large-scale volumes of data generated in a real-world manufacturing business scenario - a chemical manufacturing sector distributed worldwide-. This scenario serves as a case study for an applied research project on Big Data Analytics. The business nature of this scenario provides those real-life requirements the architecture has to deal with. Existing approaches can be extended to fulfill these requirements, in order to be effectively applied in similar manufacturing business contexts.",2016,2378363X
191,10.1109/BigData.2017.8258222,DATA QUALITY CHALLENGES WITH MISSING VALUES AND MIXED TYPES IN JOINT SEQUENCE ANALYSIS,"The goal of this paper is to investigate the impact of missing values in categorical time series sequences on common data analysis tasks. Being able to more effectively identify patterns in socio-demographic longitudinal data is an important component in a number of social science settings. However, performing fundamental analytical operations, such as clustering for grouping these data based on similarity patterns, is challenging due to the categorical and multi-dimensional nature of the data, and their corruption by missing and inconsistent values. To study these data quality issues, we employ longitudinal sequence data representations, a similarity measure designed for categorical and longitudinal data, together with state-of-the art clustering methodologies reliant on hierarchical algorithms. The key to quantifying the similarity and difference among data records is a distance metric. Given the categorical nature of our data, we employ an “edit” type distance using Optimal Matching (OM). Because each data record has multiple variables of different types, we investigate the impact of mixing these variables in a single similarity measure. Between variables with binary values and those with multiple nominal values, we find that the ability to overcome missing data problems is harder in the nominal domain versus the binary domain. Additionally, artificial clusters introduced by the alignment of leading missing values can be resolved by tuning the missing value substitution cost parameter.",2017,
192,10.1109/ITQMIS51053.2020.9322890,THE PROSPECTS FOR THE CREATION OF A DIGITAL QUALITY MANAGEMENT SYSTEM DQMS,"The development of digital technologies can give a new impetus to the development of quality management (QM). The development of new approaches based on the integration of quality management methods and digital technologies creates prerequisites for the digital transformation of the entire product lifecycle. The difficulty of creating an effective quality management system using digital technologies is not only in the absence of specialists in two areas of knowledge simultaneously, but also in the lack of integration of modern quality management methods with existing software products. In most ready-made solutions, quality management is limited to controlling process parameters and product quality. Automatic registration of process parameters with real-time data analysis should be additionally enabled in DQMS. This will allow you to organize monitoring and control of processes at each automated workplace. The accumulated analysis results will help you make decisions in difficult situations. A set of processes with digital control and analysis ensures quality assurance at all stages of the product lifecycle.",2020,
193,10.1109/WHISPERS.2015.8075416,MULTIPLE STRATIFIED SAMPLING STRATEGY FOR ASSESSING THE BIG REMOTE SENSING PRODUCTS,"The number and volume of remote sensing data and its derived products, which are regarded as typical “big data”, have grown exponentially. How to assess the quality of these big remote sensing products become a challenge. As an importance technique, spatial sampling is regarded to be necessary for the quality assessment of remote sensing derived products. This paper proposes an approach of multiple stratified spatial sampling for assessing the remote sensing products, with the aim of resolving the issue of the quality inspection of big remote sensing products. The proposed method improves the sampling accuracy without increasing the sampling size, and the whole procedure is repeatable and easily adopted for the quality inspection of remote sensing derived products.",2015,21586276
194,10.1109/ICCSE.2019.8845059,REFORM OF UNIVERSITY COMPUTER FOUNDATION COURSE BASED ON MOBILE TERMINALS IN BIG DATA ERA,"Under the background of big data era, the rise of mobile terminals has also put forward new requirements for the teaching of university computer foundation course. This thesis puts forward some common problems in the teaching practice, and discusses how to promote informatization education under the background of big data based on mobile terminals, thus improving teaching quality and cultivating students into individuals with independent learning, cooperative spirit and innovative ability.",2019,24716146
195,10.1109/ICCSE51940.2021.9569497,RESEARCH ON THE OPTIMIZATION OF THE INNOVATION AND ENTREPRENEURSHIP EDUCATION ECOSYSTEM OF PRIVATE UNIVERSITIES FROM THE PERSPECTIVE OF BIG DATA,"In the government work report delivered by Premier Li Keqiang at the session of NPC 2015, encourage mass entrepreneurship and innovation was noted as one of the twin engines to drive the Chinese economy maintains high speed of growth and upgraded to a national strategy. To further improve the reform of innovation and entrepreneurship education in Private Universities; to integrate innovation and entrepreneurship education with professional education throughout the whole process of education, teaching and talent training; to enhance the quality of training, promote the well-rounded development of students, cultivate the innovative spirit, entrepreneurial awareness and innovative entrepreneurial abilities of the students, this essay will discuss the status and problems of innovation and entrepreneurship education in Private Universities from the perspective of big data including policy formulation, curriculum system, faculty, competition activities, practice platform, cultural environment, etc. Suggestions will be provided combining the advantages of big data platforms to further improve the reform of innovation and entrepreneurship education in Private Universities.",2021,24716146
196,10.1109/AEECA55500.2022.9919060,RESEARCH ON THE APPLICATION OF COMPUTER BIG DATA TECHNOLOGY IN THE AUXILIARY SYSTEM FOR TEACHING AFTER-SCHOOL EXERCISES,"College English teaching after-school problem assistance system is a new stage of deep integration of English teaching and modern information technology, and it is also a hot spot in current education informatization research. The college English teaching after-school problem assistance system is guided by the English classroom teaching theory in colleges and universities. This system is a quality-effective dynamic intelligent classroom that is applied in all stages and processes before, during and after class. Based on the review of the current situation of college English smart teaching in the era of big data, this paper proposes that the design of college English classroom teaching under the background of big data should be guided by the concept of smart teaching, and the online teaching of college English should be fully combined with offline teaching. As for the teaching strategy of this key learning link after class.",2022,
197,10.1109/CIT/IUCC/DASC/PICOM.2015.29,SMART: AN APPLICATION FRAMEWORK FOR REAL TIME BIG DATA ANALYSIS ON HETEROGENEOUS CLOUD ENVIRONMENTS,"The amount of data that human activities generate poses a challenge to current computer systems. Big data processing techniques are evolving to address this challenge, with analysis increasingly being performed using cloud-based systems. Emerging services, however, require additional enhancements in order to ensure their applicability to highly dynamic and heterogeneous environments and facilitate their use by Small & Medium-sized Enterprises (SMEs). Observing this landscape in emerging computing system development, this work presents Small & Medium-sized Enterprise Data Analytic in Real Time (SMART) for addressing some of the issues in providing compute service solutions for SMEs. SMART offers a framework for efficient development of Big Data analysis services suitable to small and medium-sized organizations, considering very heterogeneous data sources, from wireless sensor networks to data warehouses, focusing on service composability for a number of domains. This paper presents the basis of this proposal and preliminary results on exploring application deployment on hybrid infrastructure.",2015,
198,10.1109/TC.2015.2417566,A GENERAL COMMUNICATION COST OPTIMIZATION FRAMEWORK FOR BIG DATA STREAM PROCESSING IN GEO-DISTRIBUTED DATA CENTERS,"With the explosion of big data, processing large numbers of continuous data streams, i.e., big data stream processing (BDSP), has become a crucial requirement for many scientific and industrial applications in recent years. By offering a pool of computation, communication and storage resources, public clouds, like Amazon's EC2, are undoubtedly the most efficient platforms to meet the ever-growing needs of BDSP. Public cloud service providers usually operate a number of geo-distributed datacenters across the globe. Different datacenter pairs are with different inter-datacenter network costs charged by Internet Service Providers (ISPs). While, inter-datacenter traffic in BDSP constitutes a large portion of a cloud provider's traffic demand over the Internet and incurs substantial communication cost, which may even become the dominant operational expenditure factor. As the datacenter resources are provided in a virtualized way, the virtual machines (VMs) for stream processing tasks can be freely deployed onto any datacenters, provided that the Service Level Agreement (SLA, e.g., quality-of-information) is obeyed. This raises the opportunity, but also a challenge, to explore the inter-datacenter network cost diversities to optimize both VM placement and load balancing towards network cost minimization with guaranteed SLA. In this paper, we first propose a general modeling framework that describes all representative inter-task relationship semantics in BDSP. Based on our novel framework, we then formulate the communication cost minimization problem for BDSP into a mixed-integer linear programming (MILP) problem and prove it to be NP-hard. We then propose a computation-efficient solution based on MILP. The high efficiency of our proposal is validated by extensive simulation based studies.",2016,23263814
199,10.1109/LSENS.2022.3192620,QUALITY-DRIVEN ENERGY-EFFICIENT BIG DATA AGGREGATION IN WBANS,"In the Internet of Things (IoT) era, the development of wireless body area networks (WBANs) and their applications in big data infrastructure has gotten a lot of attention from the medical research community. Since sensor nodes are low-powered devices that require heterogeneous quality of service, managing large amounts of medical data is critical in WBANs. Therefore, effectively aggregating a large volume of medical data is important. In this context, we propose a quality-driven and energy-efficient big data aggregation approach for cloud-assisted WBANs. For both the intra-BAN (Phase I) and inter-BAN (Phase II) communications, the aggregation approach is cost effective. Extensive simulation results show that quality-driven energy-efficient big data aggregation for WBANs improves network efficiency in terms of traffic served and energy consumption by 5–7 and 7–8% as compared to the existing schemes.",2022,24751472
0,10.1109/ICTKE47035.2019.8966806,BIG DATA MINING: MANAGING THE COSTS OF DATA MINING,"The amount of data collected and stored in various industries has grown exponentially in the last decade. Data is collected and stored from industries consisting of large consumers such as telecommunications, banking or financial sectors. Further, given the advent of cloud computing and software availability in the cloud being cheaper, smaller industries are utilizing data storage for competitive advantage. Companies increasingly rely on analysis of huge amounts of data to gain a strategic advantage, improving on product quality and providing better services to their end users be it the employee, consumer or customer. A combination of statistical techniques and file management tools once sufficed for analyzing mounds of data. The costs of analysis are often charged out at very high rates for companies that require data analysis and the output is dependent very much on analyzing the correct attributes within large databases to ensure the data analyzed provides the relevant result. The most known technique or tools are the subject of the growing field of knowledge discovery in databases (KDD) [1]. Using business process data mapping (BPDM) to define the targeted data along with the process of knowledge discovery mapping in the database may provide a more targeted approach with much lest costs expended.",2019,21570981
1,10.1109/POWERCON.2018.8601548,DATA ANALYSIS AND APPLICATIONS OF THE POWER QUALITY MONITORING,"This paper presents three applications to transform the power quality (PQ) monitoring data into the useful information. With the increasing volume of the PQ monitoring data, mining the values of the data is very important for the power system operations. Three applications are introduced with the PQ monitoring data in Guangzhou grid, China. Firstly, the cumulative probability of PQ monitoring data is applied to certificate the PQ limits according to the national standards. Secondly, three types of voltage sags are counted by the PQ monitoring data to show the severity of voltage sags in local grid. Thirdly, the correlation analysis is applied to show the impact of PQ problem on the device malfunctions. The correlation coefficients between the PQ monitoring data and the device malfunction data can show the impacts of PQ problems on the devices directly. The malfunctions of capacitors/inductors are relevant to the voltage deviation and harmonic distortion obviously which is shown by the correlation coefficients. It is a good attempt to translate the PQ monitoring data into the useful information, which can help the operators decide.",2018,
2,10.1109/AIEA53260.2021.00066,RESEARCH ON THE TRAINING MODEL OF MARKETING PROFESSIONALS UNDER THE BACKGROUND OF INTERNET + BIG DATA,"Marketing is a specialty that closely fits the practice of social development. In order to effectively cultivate high-quality and innovative talents for social needs, colleges and universities should fully and comprehensively grasp the development trend of Internet + big data, and it is effective construct a scientific training system to effectively improve the overall effectiveness of talent training. In the process of cultivating marketing professionals, colleges and universities should fully cater to the development needs of the Internet + big data, effectively construct a refined talent training program, and consolidate the actual results of training.",2021,
3,10.1109/BigData50022.2020.9377812,LOCATION YARDSTICK: CALCULATION OF THE LOCATION DATA VALUE DEPENDING ON THE USERS’ CONTEXT,"These days, many apps acquire location data as a way of estimating the user's behavior. As such, there are privacy concerns in using location data. In particular, users who are concerned about privacy may reduce the frequency of location acquisition or turn off the function, even though it degrades the quality of service. On the other hand, the only options available to users are yes-no or either-or ones such as ""Always permit background acquisition"" or ""Permit only while using the app"". For example, users who give permission to ""Permit only while using the app"" are themselves unable to understand how far their own veil of privacy will be lifted. That is, there are no metrics that can help users to understand the value of their own location data. How should the value of location data be determined? This study attempts to answer that question. The difficulty is that the value of a single point of location data depends on the context, such as how much other location data the app holds or when the location data was obtained. We propose a ""Location YardStick"" (LYS) that calculates the value of location information fairly in context. We confirmed that the LYS score is close to the user's expectations by comparing its results with those of a large online survey of 1300 people, and we conducted case studies in which we calculated LYS on location data acquired in various actual contexts.",2020,
4,10.1109/ACCESS.2018.2881759,BUSINESS PROCESS ANALYTICS AND BIG DATA SYSTEMS: A ROADMAP TO BRIDGE THE GAP,"Business processes represent a cornerstone to the operation of any enterprise. They are the operational means for such organizations to fulfill their goals. Nowadays, enterprises are able to gather massive amounts of event data. These are generated as business processes are executed and stored in transaction logs, databases, e-mail correspondences, free form text on (enterprise) social media, and so on. Taping into these data, enterprises would like to weave data analytic techniques into their decision making capabilities. In recent years, the IT industry has witnessed significant advancements in the domain of Big Data analytics. Unfortunately, the business process management (BPM) community has not kept up to speed with such developments and often rely merely on traditional modeling-based approaches. New ways of effectively exploiting such data are not sufficiently used. In this paper, we advocate that a good understanding of the business process and Big Data worlds can play an effective role in improving the efficiency and the quality of various data-intensive business operations using a wide spectrum of emerging Big Data systems. Moreover, we coin the term process footprint as a wider notion of process data than that is currently perceived in the BPM community. A roadmap towards taking business process data intensive operations to the next level is shaped in this paper.",2018,21693536
5,10.1109/TBDATA.2016.2601934,DISTRIBUTED FEATURE SELECTION FOR EFFICIENT ECONOMIC BIG DATA ANALYSIS,"With the rapidly increasing popularity of economic activities, a large amount of economic data is being collected. Although such data offers super opportunities for economic analysis, its low-quality, high-dimensionality and huge-volume pose great challenges on efficient analysis of economic big data. The existing methods have primarily analyzed economic data from the perspective of econometrics, which involves limited indicators and demands prior knowledge of economists. When embracing large varieties of economic factors, these methods tend to yield unsatisfactory performance. To address the challenges, this paper presents a new framework for efficient analysis of high-dimensional economic big data based on innovative distributed feature selection. Specifically, the framework combines the methods of economic feature selection and econometric model construction to reveal the hidden patterns for economic development. The functionality rests on three pillars: (i) novel data pre-processing techniques to prepare high-quality economic data, (ii) an innovative distributed feature identification solution to locate important and representative economic indicators from multidimensional data sets, and (iii) new econometric models to capture the hidden patterns for economic development. The experimental results on the economic data collected in Dalian, China, demonstrate that our proposed framework and methods have superior performance in analyzing enormous economic data.",2018,23722096
6,10.1109/ICMEIM51375.2020.00088,APPLICATION RESEARCH ON BIG DATA OF MILITARY TRAINING IN MILITARY ACADEMY TEACHING,"As an important driving force for future social development, big data technology is promoting the development of science, technology, industry, military, education and other fields. In the era of big data, military education faces new opportunities and challenges. How to combine big data of military training with the military education has become a new and important topic. At present, the military education is still confronted with such problems as decoupling of teaching content from the military training, single teaching method and low teaching effect. In order to improve the training quality of military talents, and make education closer to the reality of military training, this paper applies big data of military training to military academy education. By using teaching methods such as battle plan making, case teaching, tactic discussion and training evaluation, it can promote the transformation of military training achievements in military academy teaching, realize the comprehensive and deep integration of military training and military academy teaching, improve the cultivating level of military talents in military academy, and achieve modernization of military education based on big data of military training.",2020,
7,10.1109/ICIEA.2016.7603605,THE DATA MINING APPLICATION IN THE POWER QUALITY MONITORING DATA ANALYSIS,"It is a main issue to find valuable information from the power quality data because of its big volume, heterogeneity and low value density in the power quality monitoring system of the grid. An analysis system of the power quality analysis based on the data mining technologies is presented in this paper, consisting of the technologies of data cleaning, data fusion, cluster analysis, correlation analysis, and etc. The proposed analysis system is applied in the power quality data analysis of a certain city power quality monitoring system. The meaningful variation laws of the power quality indices are obtained, which can provide valuable reference to the grid planning, dispatch and operation.",2016,21582297
8,10.1109/ITNAC50341.2020.9315078,INDIGENOUS BIG DATA IMPLICATIONS IN NEW ZEALAND,"Our world is dynamic and digital. It is irrefutable; big data are reshaping commerce, healthcare and governmental decisions. Individuals forgo awareness or even desire to uncover or appreciate how big data are affecting our society. This raises distinct data issues of huge effect, especially for indigenous communities. The novel proposition of this research, is that by considering advances in big data optimisation and issues from an indigenous perspective, a broad appreciation of decision-making significances will result. The microcosm in Aotearoa, New Zealand is relatively simple. It permits both a clear and leading view, especially of complex indigenous effects, due to its foundations in Maoridom. As big data advancements impact the quality of life for us all, there is an enormous responsibility, opportunity and incentive to focus research and awareness into nascent data design fields. Through an indigenous lens, unique perspectives and insights are shaped and presented in this paper, implicating feature selection in high-dimensional data.",2020,24741531
9,10.1109/ICRIS52159.2020.00110,LOCATION SELECTION METHOD OF REAL ESTATE DEVELOPERS BASED ON BIG DATA TECHNOLOGY,"In order to provide consumers with high-quality real estate products and meet their purchase needs, this paper proposes a novel location selection method of real estate developers based on big data technology. This method combines the big data technology, and gives full play to the characteristics and advantages of big data technology, especially data acquisition and data mining. In addition, based on literature research and questionnaire survey, this paper constructs structural equation model from the influencing factors of consumers' purchase choice, and then provides feasible path for real estate open development site selection. Based on this, the model constructed by this method is also combined with quantitative analysis method, and then the specific factor structure is obtained, and the relationship between variables is fitted. The experimental results show that the method can help real estate developers to choose a reasonable development location, and better meet the purchase preferences of consumers.",2020,
10,,A PRACTICAL STUDY ON OPTIMIZATION OF BIG DATA STREAMING AND DATA ANALYTICS INFRASTRUCTURE FOR EFFICIENT AI-BASED PROCESSING,"In today’s world, the potential of gathering insights from the data increases, proportional to the capabilities of data acquisition hardware being produced. Data has become more valuable than ever in every domain, including Internet of Things(IoT) and especially Industrial Internet of Things (IIoT). An important factor to maximize the production quality is to enable predictive maintenance in production. Manufacturing domain is one of the key sectors where IIoT solutions have become a necessity rather than an option to compete and shine out in the market. In this paper, a reference architectural model is proposed and applied for a manufacturing use case in practice which visualizes the immense effect of making use of big data frameworks and technologies in order to improve the predictive maintenance during the production process, resulting in the improvement of reliability.",2022,
11,10.1109/Trustcom/BigDataSE/ICESS.2017.332,A DATA SCIENCE AND ENGINEERING SOLUTION FOR FAST K-MEANS CLUSTERING OF BIG DATA,"With advances in technology, high volumes of a wide variety of valuable data of different veracity can be easily collected or generated at a high velocity in the current era of big data. Embedded in these big data are implicit, previously unknown and potentially useful information. Hence, fast and scalable big data science and engineering solutions that mine and discover knowledge from these big data are in demand. A popular and practical data mining task is to group similar data into clusters (i.e., clustering). To cluster very large data or big data, k-means based algorithms have been widely used. Although many existing k-means algorithms give quality results, they also suffer from some problems. For instance, there are risks associated with randomly selecting the k centroids, there is a tendency to produce roughly equal circular clusters, and the runtime complexity is very high. To deal with these problems, we present in this paper a big data science and engineering solution that applies heuristic prototype-based algorithm. Evaluation results show the efficiency and scalability of this solution.",2017,23249013
12,10.1109/BigData.2015.7364123,30 DAY HOSPITAL READMISSION ANALYSIS,"Readmissions to a hospital after procedures are costly and considered to be an indication of poor quality. As Per the Affordable Care Act of 2010, hospitals may be reimbursed at a reduced rate for patients readmitted to a hospital within 30 days of discharge. In this project, we used statistical and machine-learning methods to analyze the Nationwide Inpatient Sample dataset provided by HCUP (Healthcare Cost and Utilization Project) to identify various clinical, demographic and socio-economic factors that play crucial roles in predicting the revenue loss due to readmissions. Three medical conditions, namely chronic obstructive pulmonary disorder (COPD), total hip arthroplasty (THA), and total knee arthroplasty (TKA) have been primarily used for this purpose. Our analysis builds on both non-parametric and parametric statistical models and machine learning techniques such as Decision Tree, Gradient Boosting, Logistic Regression and Neural Networks. We evaluated and compared these models based on Area under ROC (AUC) and misclassification rate. By including visual analytics, this analysis not only enables the hospitals to compute the loss of revenue but also monitors their quality of service in a real-time fashion.",2015,
13,10.1109/INDIN41052.2019.8972078,DATA PREPARATION FOR DATA MINING IN CHEMICAL PLANTS USING BIG DATA,"Data preparation for data mining in industrial applications is a key success factor which requires considerable repeated efforts. Although the required activities need to be repeated in very similar fashion across many projects, details of their implementation differ and require both application understanding and experience. As a result, data preparation is done by data mining experts with a strong domain background and a good understanding of the characteristics of the data to be analyzed. Experts with these profiles usually have an engineering background and no strong expertise in distributed programming or big data technology. Unfortunately, the amount of data can be so large that distributed algorithms are required to allow for inspection of results and iteration of preparation steps. This contribution introduces an interactive data preparation workflow for signal data from chemical plants enabling domain experts without background in distributed computing and extensive programming experience to leverage the power of big data technologies.",2019,19354576
14,10.1109/ICMEAE.2017.29,EFFECTIVE DATA QUALITY DIAGNOSTIC SCHEMA FOR BIG DATA,"Big Data environment is a computing area with a great growth. Today it is common that we hear about databases with huge volumes of information and also we hear about Data Mining and Business Intelligence projects related with these huge databases. However, in general, little attention has been given to the quality of the data. Here we propose and present innovative metrics and schema designed to perform a basic task related to the Data Quality issue, this is, the diagnostic. The preliminary results that we obtained when we apply our approaches to Big Data encourage us to continue this work.",2017,
15,10.1109/ICACCCN.2018.8748630,BIG DATA AND INTERNET OF THINGS: A SURVEY,"In this digital era, here the sum of data is generate with store has prolonged inside a less period of time. The data in this era generated high speed leads to many challenges. The size is primarily and periodically, just the measurement that bounces in the big data position. In this survey, we have attempt to give a broad explanation of big data that captures its other unique and important features. We have discussed 4V's model. Also, the latest technologies uses in big data, like Hadoop, HDFS, MapReduce and different type of methods used by big data. Finally various benefits of using big data analysis and include some feature of cloud computing.",2018,
16,10.1109/COMPSAC48688.2020.00042,DATA LINKING AS A SERVICE: AN INFRASTRUCTURE FOR GENERATING AND PUBLISHING LINKED DATA ON THE WEB,"Companies, government, and even ordinary people have been producing and publishing huge amounts of data. This phenomena, known as big data, leveraged the interest in advanced analytics and data science. Many observers, though, are pointing out that extracting knowledge from such datasets requires suitable tools for handling and integrating data. Research in the last years has shown that taking into account the semantics of data is crucial for fostering data integration solutions. However, there is a lack of solutions for data publishing that follow the best practices for exposing and connecting data. With this regard, this work proposed DLaaS, an infrastructure for generating and publishing linked data on the Web. It aims at facilitating the execution of necessary processes to properly publish high quality linked data. Its main goal is to improve the reuse of data by connecting entities based on heterogeneous datasets that share a certain level of data intersection or semantic relationship.",2020,07303157
17,10.1109/SYNCHROINFO.2018.8456931,BIG DATA APPLICATION ON SIGNAL PROCESSING SYSTEMS,"One of the tasks of system identification within the modern signal processing system is the task of identification of unknown error-correction code (ECC). System identification can be done using various methods. Anyway, the more knowledge one has about the possible system structure provides better solution for system identification. Here we take a look into digital communication systems uses ECC, which are the major part of modern communication systems are. We create a new practical approach for identification of a priori unknown ECC being used over the transmission channel of the communication system which is examined for identification purposes. The approach is based as Big Data application which can provide new source for the study. Shown that usage of new approach provides better speed for system identification making the process of identification faster, more reliable, with better quality for solution found and with lower requirements for quality of the transmission channel.",2018,
18,10.1109/ICMEIM51375.2020.00053,STUDY ON CONSTRUCTION PATH OF CURRICULUM IDEOLOGICAL AND POLITICAL EDUCATION IN HIGHER VOCATIONAL COLLEGES UNDER THE BACKGROUND OF BIG DATA,"Ideological and political education in colleges and universities is carried by curriculum ideological and political education in the development process of the new period, which is an important means to improve the effect of teaching and educating in order to fully reflect the core idea of ideological and political course. It is necessary to pay attention to improving the level of ideological and political construction in the process of carrying out ideological and political education in higher vocational colleges, and meanwhile to innovate the teaching methods and means of ideological and political course. Only in this way can the teaching quality of ideological and political courses be improved. In the course of curriculum ideological and political construction in higher vocational colleges, it is necessary to analyze the existing problems, understand the influence of big data technology on curriculum ideological and political teaching, and put forward relevant strategies for curriculum ideological and political construction in higher vocational colleges, which can really improve the quality of curriculum ideological and political construction and ensure the smooth progress of ideological and political education in higher vocational colleges.",2020,
19,10.1109/CLOUDCOM-ASIA.2013.75,MOBILE AGENT BASED NEW FRAMEWORK FOR IMPROVING BIG DATA ANALYSIS,"The rising number of applications serving millions of users and dealing with terabytes of data need to a faster processing paradigms. Recently, there is growing enthusiasm for the notion of big data analysis. Big data analysis becomes a very important aspect for growth productivity, reliability and quality of services (QoS). Processing of big data using a powerful machine is not efficient solution. So, companies focused on using Hadoop software for big data analysis. This is because Hadoop designed to support parallel and distributed data processing. However, Hadoop has several drawbacks effect on its performance and reliability against big data analysis. In this paper, a new framework is proposed to improve big data analysis and overcome the drawbacks of Hadoop. The proposed framework is called MapReduce Agent Mobility (MRAM). MRAM is developed by using mobile agent and MapReduce paradigm under Java Agent Development Framework (JADE).",2013,
20,10.1109/ICCC47050.2019.9064371,A STUDY OF REAL TIME RASTER GRAPHIC VISUALIZATION BASED ON BIG DATA TECHNOLOGY,"The grid sizes were designed to provide best quality for raster graphics. The grid data was regionally grouped into blocks and stored by partition, providing a data structure suitable for distributed parallel processing. The whole procedure from data query to grid block sub-graphic generation was fully parallel processed by an independently developed cluster, achieving high performance visualization of pixel-level raster graphics under full-HD screen resolution. The solution provides reference for the construction of similar systems.",2019,
21,10.1109/CISP-BMEI51763.2020.9263667,EVALUATION OF FLIGHT TEST DATA QUALITY BASED ON ROUGH SET THEORY,"With the continuous development of flight test technology, the test system is filled with massive, multi-structured, and multi-dimensional data resources. The value of big data has been fully recognized by the society. How to tap the value of data has become an application in various research fields and industries. The most concerned issue of the field. Whether the data is rubbish or treasure, the most important question is whether the data to be analyzed and mined is of high quality. A low-quality data source will not only fail to reflect the value of the data, but may also run counter to the actual situation, which has side effects. In order to effectively evaluate the quality of flight test data, according to the characteristics of test data in flight test, a test data quality evaluation method based on rough set theory is proposed, standard test methods and test indicators for flight test data quality are proposed, and data quality evaluation is given. The method of rule extraction realizes the quality evaluation of flight test data.",2020,
22,10.1109/TrustCom/BigDataSE.2018.00240,THE CHALLENGES OF BIG DATA GOVERNANCE IN HEALTHCARE,"Big data starts to be employed in some industries but not yet widely or properly adopted in healthcare industry. This research paper aims at studying the usages and challenges of big data in healthcare sector. Governance of big data will include the domains of strategy, process, people, policy and technology and automation. Among the challenges identified in the healthcare sector, reliability and integrity are especially important because it is related to life and death. Big data governance for policy maker, authentication for data integrity, and future development of healthcare big data governance are discussed here. Moreover, some future development questions are raised in this paper for further study, which will improve the quality of life and lead to a better and healthier world under the proper and adequate big data governance environment.",2018,23249013
23,10.1109/ACCESS.2019.2916912,INTELLIGENT DATA ENGINEERING FOR MIGRATION TO NOSQL BASED SECURE ENVIRONMENTS,"In an era of super computing, data is increasing exponentially requiring more proficiency from the available technologies of data storage, data processing, and analysis. Such continuous massive growth of structured and unstructured data is referred to as a “Big data”. The processing and storage of big data through a conventional technique is not possible. Due to improved proficiency of Big Data solution in handling data, such as NoSQL caused the developers in the previous decade to start preferring big data databases, such as Apache Cassandra, Oracle, and NoSQL. NoSQL is a modern database technology that is designed to provide scalability to support voluminous data, leading to the rise of NoSQL as the most viable database solution. These modern databases aim to overcome the limitations of relational databases such as unlimited scalability, high performance, data modeling, data distribution, and continuous availability. These days, the larger enterprises need to shift NoSQL databases due to their more flexible models. It is a great challenge for business organizations and enterprises to transform their existing databases to NoSQL databases considering heterogeneity and complexity in relational data. In addition, with the emergence of big data, data cleansing has become a great challenge. In this paper, we proposed an approach that has two modules: data transformation and data cleansing module. The first phase is the transformation of a relational database to Oracle NoSQL database through model transformation. The second phase provides data cleansing ability to improve data quality and prepare it for big data analytics. The experiments show the proposed approach successfully transforms the relational database to a big data database and improve data quality.",2019,21693536
24,10.1109/BigData.2015.7363904,PERFORMANCE ASSESSMENT AND UNCERTAINTY QUANTIFICATION OF PREDICTIVE MODELS FOR SMART MANUFACTURING SYSTEMS,We review in this paper several methods from Statistical Learning Theory (SLT) for the performance assessment and uncertainty quantification of predictive models. Computational issues are addressed so to allow the scaling to large datasets and the application of SLT to Big Data analytics. The effectiveness of the application of SLT to manufacturing systems is exemplified by targeting the derivation of a predictive model for quality forecasting of products on an assembly line.,2015,
25,10.1109/ACCESS.2020.3033068,ECO-ENVIRONMENT CONSTRUCTION OF ENGLISH TEACHING USING ARTIFICIAL INTELLIGENCE UNDER BIG DATA ENVIRONMENT,"Application of big data and artificial intelligence has become one influence factor of English teaching, which have broken the balance of the teaching Eco-environment for English. In this article, the artificial intelligence and big data are introduced into English teaching to propose a new teaching Eco-environment construction method to meet the needs of the social development and international communication in English. In the proposed method, the characteristics of English teaching under big data environment are analyzed in detail. Then the big data technology is used to construct a new Eco-environment of English teaching to improve the teaching and learning quality. The data mining method is one of artificial intelligence methods, which is used to analyze the relationship of interdependence and mutual restriction among various factors in English teaching in order to build and implement a new Eco-environment with the information sharing, quality teaching and personalized learning of English. Finally, through the practical application of the constructed Eco-environment, the experiment results show that the proposed method can help students update their learning concepts, methods and contents of English, inspire their interest and initiative by comparing with some existed teaching methods, so as to improve their learning effects and application ability of English. Therefore, the constructed Eco-environment provides a new idea and direction for English teaching reform by application of big data and artificial intelligence.",2020,21693536
26,10.1109/ICEKIM52309.2021.00106,RESEARCH INTO ONLINE TEACHING IN PRIVATE COLLEGES UNDER THE BACKGROUND OF INTERNET BIG DATA,"With the development of modern information technology, online teaching is a new educational method. Teachers and students can use high-quality information resources, and break the limitations of time and space, to achieve personalized teaching.[1] During the current epidemic prevention and control period, online teaching can ensure “no suspension of classes, no suspension of teaching”, which is also a measure of not affecting students' papers, employment and other work, by making a good using of Internet big data. This paper first analyzes the current problems of online teaching in Colleges and universities, and then puts forward effective measures for the existing problems in order to maximize the effect of online teaching.",2021,
27,10.1109/ICCT.2017.8359833,A NOVEL TEMPORAL-SPATIAL ANALYSIS SYSTEM FOR QAR BIG DATA,"The current analysis restricts to the statistics of different exceedance events in flight operational quality assurance (FOQA). Statistical methods lack effective correlation to flight data at different time and space, and lack deep-level mining and application of flight quality monitoring information. For questions raised above, this paper presents a novel approach for FOQA based on temporal geography information system (T-GIS). We construct a time-snapshot-> time series-> space-time evolution model, and design a dynamic spatiotemporal statistical analysis algorithm. The results show that the system can be used to deal with large flight data to find the unsafety events spatial-temporal distribution of the whole civil aviation industry. It also provides a new research idea to raise the level of FOQA.",2017,25767828
28,10.1109/ICCCBDA.2018.8386595,AN INFRASTRUCTURE OF MULTI-POLLUTANT AIR QUALITY DETERIORATION EARLY WARNING SYSTEM IN SPARK PLATFORM,"In recent years, the increase of multi-pollutant air pollution index has a direct impact on people's health. Relevant studies have pointed out that the exposure of air pollutants has a strong connection with asthma and other unhealthy diseases such as the lungs. The air pollution has brought health issue, but also caused a huge medical costs each year derived from the disease. Although there are many different approaches to forecasting air pollution, most of them are based on historical data and the environment of local server devices. Some of the data records may differ from those of real-time situations and those limitation of traditional hardware devices. Therefore, this project tries to adopt Azure, a cloud computing platform with advantages of scalability, reliability and agility, to set up a multi-pollutant air quality deterioration warning system. This project is divided into three parts. The first part is the collection of multi-pollutant data and the prediction of multi-pollutant air pollution quality. After the government and private data are collected and integrated into the database, the air quality of multi-pollutant are predicted. The second part is big data analysis of air quality indicators for pollutants indicator AQI, AAQI, and HAQI AIDE, a real-time deterioration warning system for deteriorating, was popularized in Spark for big data analytics framework and various reports were generated. The third part will be a combination of blockchain and perpetual system performance assessment. The overall system will integrate with the blockchain and conduct a continuous evaluation of the system.",2018,
29,10.1109/ACCESS.2019.2951364,DYNAMIC RELEASE OF BIG LOCATION DATA BASED ON ADAPTIVE SAMPLING AND DIFFERENTIAL PRIVACY,"Data releasing is a key part bridging between the collection of big data and their applications. Traditional methods release the static version of dataset or publish the snapshot with a fixed sampling interval, which cannot meet the dynamic query requirements and query precision for big data. Moreover, the quality of published data cannot reflect the characteristics of the dynamic changes of big data, which often leads to subsequent data analysis and mining errors. This paper proposes an adaptive sampling mechanism and privacy protection method for the release of big location data. In order to reflect the dynamic change of data in time, we design an adaptive sampling mechanism based on the proportional-integral-derivative (PID) controller according to the temporal and spatial correlation of the location data. To ensure the privacy of published data, we propose a heuristic quad-tree partitioning method as well as a corresponding privacy budget allocation strategy. Experiments and analysis prove that the adaptive sampling mechanism proposed in this paper can effectively track the trend of dynamic changes of data, and the designed differential privacy method can improve the accuracy of counting query and enhance the availability of published data under the premise of certain privacy intensity. The proposed methods can also be readily extended to other areas of big data release applications.",2019,21693536
30,10.1109/ICITECH.2017.8079923,MOBILE NETWORK QUALITY OF EXPERIENCE USING BIG DATA ANALYTICS APPROACH,"Traditionally, Quality of experience is mostly examined in a laboratory experiments to enable a fixed contextual factor. While the results present an estimated mean opinion score representing perceived QoE. It is imperative to estimate mean opinion score employing large data (big data) gathered from the mobile network comprising of different user's location and time for a specific service. Because time and location can have a huge influence on the user perceived quality of experience. Therefore, this paper proposed a framework for modelling perceived QoE through big data analytics. The proposed framework describes the process of estimating perceived quality of experience to assist the mobile network operators effectively manage the network performance and aid satisfactory provision of mobile internet services.",2017,
31,10.1109/ICEEOT.2016.7755397,A FILTER BASED FEATURE SET SELECTION APPROACH FOR BIG DATA CLASSIFICATION OF PATIENT RECORDS,"The flourishing fame and development of big data in recent years made researchers to have a detailed study. Of the all entire emerging big data research topics, classification of data from big data is identified as a great challenge to address as of our analysis. The Classification is the process of categorizing data for its most effective and efficient use. While analyzing large scale patient records, hierarchical learning approach which is tree structured that train max-margin classifier will give better classification results and also it is computationally efficient. The quality of features has an effect on the performance of hierarchical learning approach for classification of patient records. So we have to extract discriminative features for training hierarchical classifier. In this paper Highly Correlated Feature Set Selection (HCFS) algorithm is proposed to combine with the hierarchical leaning approach to improve its performance. This algorithm identifies the good feature subsets which will improve the classification accuracy.",2016,
32,10.1109/BDEIM55082.2021.00089,RESEARCH ON BIG DATA TECHNOLOGY TO REDUCE THE COST OF COMMUNITY GROUP BUYING SUPPLY CHAIN,"In the era of big data, applying big data technology to the community group buying platform can provide a brand new data information processing model for the community group buying supply chain and effectively manage the supply chain costs. This article analyzes and explores the current community group buying supply chain model, and uses data analysis theory to find out the shortcomings of the current model. From the perspective of big data analysis and mining, this article uses big data technology to analyze users' evaluation, products' quality feedback, and warehousing distribution to provide solutions for the improvement of supply chain models, and help community group buying companies to better manage costs.",2021,
33,10.1109/ICMTMA.2018.00123,"RESEARCH ON ENGINEERING TEACHING MODE OF ""INTRODUCTION"" COURSE IN ENGINEERING COLLEGES UNDER THE BACKGROUND OF BIG DATA","The development of information technology, such as Internet, Internet of things and cloud computing, has brought the storage of massive information resources for human beings, and human beings have entered the era of big data.Big data has changed people's values, lifestyles and ways of thinking.The big data brings challenges to the teaching of ""Introduction""course, but also brings opportunities to the teaching of ""Introduction""course.For the teaching of ""Introduction""course of engineering colleges, we must deepen teaching reform, actively meet the challenges and opportunities brought by big data, and innovate engineering teaching mode, so as to continuously improve teaching quality.",2018,21571481
34,10.1109/ISCON52037.2021.9702517,"PACKET LOSS PREDICTION USING ARTIFICIAL INTELLIGENCE UNIFIED WITH BIG DATA ANALYTICS, INTERNET OF THINGS AND CLOUD COMPUTING TECHNOLOGIES","Big Data Analytics, Artificial Intelligence and cloud computing all together has emerged with an ultimate goal of automating and changing human life by providing their services. These incredibly strong technologies have huge potential by working together, making human life simpler and advanced. To increase the popularity of any of these services, Quality of Service metrics are needed to be defined clear. One of those quality metrics is packet loss or packet delivery, which is the main research idea of this paper. With advancement in Intelligent Network there exists a scope to predict packet loss, by analyzing the recorded network traffic and processing said data under certain machine learning algorithms to create a model to either predict packet loss or tell which variable is responsible for packet loss. This paper includes the study of packet loss behavior of networks. The Analytics techniques applied successfully by analyzing big network traffic data, processing of data, using AI and Machine Learning classifier “XGBoost” and hence designed a model to predict Packet loss which is a QoS metric with an accuracy of 90 percent. The model is personalized to work on WireShark data.",2021,
35,10.1109/BigData.2014.7004306,USING GEOMETRIC STRUCTURES TO IMPROVE THE ERROR CORRECTION ALGORITHM OF HIGH-THROUGHPUT SEQUENCING DATA ON MAPREDUCE FRAMEWORK,"Next-generation sequencing (NGS) data are a rapidly growing example of big data and a source of new knowledge in science. However, sequencing errors remain unavoidable and reduce the quality of NGS data. Error correction, therefore, is a critical step in the successful utilization of NGS data, including de novo genome assembly and DNA resequencing. Since NGS throughput doubles approximately every five months and the length of NGS records (i.e., reads) is increasing, improvements in efficiency and effectiveness of computational strategies are needed. In this study, we aim to improve the performance of CloudRS, an open-source MapReduce application designed to correct sequencing errors in NGS data. We introduce the readmessage (RM) diagram to represent the set of messages, i.e., the key-value pairs generated on each read. We also present the Gradient-number Votes (GNV) scheme in order to trim off portions of the RM diagram, thereby reducing the total size of messages associated with each read. Experimental results show that the GNV scheme successfully reduce execution time and improve the quality of the de novo genome assembly.",2014,
36,10.1109/ICISE-IE53922.2021.00217,APPLICATION OF BIG DATA TECHNOLOGY IN CURRICULUM PRACTICE AND RESEARCH WORK,"With the advancement of information technology, most of the current research work has no longer used the traditional written research methods, but turned to research on the Internet. Internet public opinion analysis based on big data analysis technology is gradually becoming the main data source for research work. Big data technology is aimed at a large number of Internet users, and can provide a large number of experimental samples and survey objects for ideological and political research work. Big data technology can collect data through sensors, logs, and crawlers, store data in the form of distributed storage, perform data calculations based on MapReduce, stream computing, graph computing and other computing models, and use various data mining and machine learning algorithms. Data analysis and big data technology can make ideological and political practice activities more space for development, and make up for the shortcomings of traditional research work. There are now a large number of various types of data on the Internet, which can accurately reflect the current ideological and political situation in society and bring inspiration to the practice of ideological and political courses. The practical research work of ideological and political courses should make good use of Internet resources and big data technology to improve the teaching quality of practical courses.",2021,
37,10.1109/ICoICT.2015.7231398,BIG DATA ANALYTICS ON LARGE-SCALE SOCIO-TECHNICAL SOFTWARE ENGINEERING ARCHIVES,"Given the fast growing nature of software engineering data in online software repositories and open source communities, it would be helpful to analyse these assets to discover valuable information about the software engineering development process and other related data. Big Data Analytics (BDA) techniques and frameworks can be applied on these data resources to achieve a high-performance and relevant data collection and analysis. Software engineering is a socio-technical process which needs development team collaboration and technical knowledge to develop a high-quality application. GitHub, as an online social coding foundation, contains valuable information about the software engineers' communications and project life cycles. In this paper, unsupervised data mining techniques are applied on the data collected by general Big Data approaches to analyse GitHub projects, source codes and interactions. Source codes and projects are clustered using features and metrics derived from historical data in repositories, object oriented programming metrics and the influences of developers on source codes.",2015,
38,10.1109/CLOUD.2014.130,SLA-GUIDED DATA INTEGRATION ON CLOUD ENVIRONMENTS,"Existing data integration techniques have to be revisited to query big data collections on the Cloud. Service Level Agreements implement the contracts between the cloud provider and the users, and between the cloud and service providers. Given SLA heterogeneity and data integration scalability problems, we propose an SLA guided data integration for querying data on multiple clouds.",2014,21596182
39,10.1109/IIAI-AAI.2015.178,BIG DATA AND ITS RESEARCH IMPLICATIONS FOR HIGHER EDUCATION: CASES FROM UK HIGHER EDUCATION INSTITUTIONS,"With the rise of big data, its impact has becoming apparent even in the Higher Education sector. The strategic use and applications of big data in higher education would lead to higher educational quality and better student and staff experience. Using the output from UK JISC's BI projects, this paper reviews and outlines some cases of big data analytics in the UK Higher Education institutions, followed by some research implications for future big data research in Higher Education.",2015,
40,10.1109/DSAA.2016.70,EFFICIENT LARGE SCALE CLUSTERING BASED ON DATA PARTITIONING,"Clustering techniques are very attractive for extracting and identifying patterns in datasets. However, their application to very large spatial datasets presents numerous challenges such as high-dimensionality data, heterogeneity, and high complexity of some algorithms. For instance, some algorithms may have linear complexity but they require the domain knowledge in order to determine their input parameters. Distributed clustering techniques constitute a very good alternative to the big data challenges (e.g.,Volume, Variety, Veracity, and Velocity). Usually these techniques consist of two phases. The first phase generates local models or patterns and the second one tends to aggregate the local results to obtain global models. While the first phase can be executed in parallel on each site and, therefore, efficient, the aggregation phase is complex, time consuming and may produce incorrect and ambiguous global clusters and therefore incorrect models. In this paper we propose a new distributed clustering approach to deal efficiently with both phases, generation of local results and generation of global models by aggregation. For the first phase, our approach is capable of analysing the datasets located in each site using different clustering techniques. The aggregation phase is designed in such a way that the final clusters are compact and accurate while the overall process is efficient in time and memory allocation. For the evaluation, we use two well-known clustering algorithms, K-Means and DBSCAN. One of the key outputs of this distributed clustering technique is that the number of global clusters is dynamic, no need to be fixed in advance. Experimental results show that the approach is scalable and produces high quality results.",2016,
41,10.1109/INFRKM.2018.8464770,EXPERT REVIEW ON BIG DATA ANALYTICS IMPLEMENTATION MODEL IN DATA-DRIVEN DECISION-MAKING,"Data-driven decision-making can offer improved insights for information value and create new business opportunity. The purpose of this paper is to present the findings of expert opinion in verifying the influencing factors in big data analytics (BDA) implementation that are beneficial in developing the BDA implementation assessment model. The study was carried out by conducting face-to-face approach sessions with three academicians and four industry experts who have vast experience in big data research and its implementation. The findings from these exercises has confirmed and verified the content of the ten factors that include organization dimension (such as big data strategy, top management support, resource commitment, organizational relationship), people dimension (such as analytics skills, managerial skills and analytics culture) and technology dimension (includes data infrastructures, information processing and quality) were appropriate for the research model. It was described using descriptive analysis such as frequency, mean and standard deviation. Once the verification process is complete, the research model will be validated through survey in the future work.",2018,
42,10.1109/ITQMIS51053.2020.9322987,DIGITALIZATION OF QUALITY MANAGEMENT OF THE STRATEGIC DECISION-MAKING PROCESS,"Strategic decisions are linked to long-term goals and priorities. At the same time, current trends point to an increasing dynamics of ongoing events that affect these priorities. To improve the quality of the decisions made, we propose to add a tactical loop that is able to track changes in events in a relatively short period of time to a system with a strategic management loop. To improve the quality of the relevant strategic decisions, we propose to use the block for analyzing big data, reflecting the state of the control object. The dynamics of the process is provided by intelligent analysis of the digital twin of the control object. Application of the approach under consideration relates to the field of strategic decision making as an additional solver.",2020,
43,10.1109/ICDM.2019.00132,CONSTRUCTING EDUCATIONAL CONCEPT MAPS WITH MULTIPLE RELATIONSHIPS FROM MULTI-SOURCE DATA,"Concept map is an useful tool to help people organize and improve knowledge. Particularly in educational domain, it is beneficial for students and teachers to improve the learning and teaching quality. Traditionally, manual educational concept maps, provided by teachers, are quite time-consuming and limited to teachers' experience. Thus, it is meaningful to automatically construct high-quality concept maps. However, existing data-driven solutions only focus on either separate data source or single pedagogic relationship, which are not sufficient to satisfy actual demands. To this end, we propose a novel framework, named Extracting Multiple Relationships Concept Map (EMRCM), to construct multiple relations concept maps from Multi-source Data. Specifically, we design various targeted evidences to explore diverse information of multi-source data from different perspectives. Then, we employ three classic classifiers to bulid the predictive model for extracting key concepts and multiple concept relationships using the proposed evidences. We create a real dataset for empirically studying this problem. Extensive experiments on a real-world dataset show the effectiveness of our method.",2019,15504786
44,10.1109/BDEIM55082.2021.00105,PROMOTE INTERNET ECONOMIC DEVELOPMENT AND SMART CITY CONSTRUCTION WITH DATA SHARING,"Internet economy is a new economic form based on the Internet, with information technology as the core, a smart city is the use of a new generation of information technology, in order to achieve urban management services, enterprise survival development, and residents' production lifestyle intelligence, further enhance the core competitiveness of urban development, can provide residents with a high-quality life. Taking China as an example, this paper puts forward the path of smart city development according to the current situation of the smart city construction: use Internet technology to build public big data, strengthen the construction of digital infrastructure projects, fully mine data, lay a good technical foundation for the construction of the smart city, integrate urban development with the Internet, and gradually promote harmonious development of the city.",2021,
45,10.1109/DSA.2019.00035,RESEARCH ON THE SAFETY PREDICTION METHOD OF LONG JUMP IN BIG DATA,"The long jump has a great influence on sports events, but the long jump athletes are vulnerable in training and their safety cannot be guaranteed. Therefore, this paper puts forward the research on the safety prediction method of long jump movement under big data. Through the research on the sport form characteristics of long jump athletes, this paper analyzes the athletes' physical quality and training mode, so as to develop the method to guarantee the safety prediction of long jump athletes. This paper makes a comparative analysis on the safety prediction methods of big data long jumpers. The experimental data shows that the safety prediction methods of big data long jumpers are 16.8% higher than the traditional methods. The prediction accuracy is better, the prediction time is shortened, and the safety of athletes can be better guaranteed(Abstract).",2020,
46,10.1109/ICEIEC.2019.8784484,CONSTRUCTION OF ELEVATOR INSPECTION QUALITY EVALUATION SYSTEM BASED ON BIG DATA,"Elevator inspection information has typical big data characteristics. This paper points out that the elevator inspection data introduces the method of elevator inspection big data analysis. Taking elevator inspection as an example, it lists several kinds of big data analysis methods for inspection data, including the risk points describing the basic information of the elevator, the scanning inspection process and the inspection quality. Based on frequency analysis of active factors, outlier test, quality assessment, correlation analysis. Using big data technology, it can make statistical analysis on the data obtained by elevator inspection, make the inspection situation more intuitive, help the management organization to understand the overall elevator quality and elevator inspection, and build an elevator inspection quality evaluation system to make the work more transparent and management more precise. Find more accurate questions, deeper supervision, and more scientific government decisions.",2019,23778431
47,10.1109/COMPSAC.2019.10285,AN EMPERICAL STUDY ON APPLICATION OF BIG DATA ANALYTICS TO AUTOMATE SERVICE DESK BUSINESS PROCESS,"The maturity of the Big Data analytics allows enterprises to enhance their business processes using a data-driven approach. From retailers, E-commerce, risk analysis to pharmaceuticals, Bioinformatics, healthcare informatics, and others, business owners are thinking about mining business data into information for making better decisions to drive business value. However, the lack of reverse engineers and data analysts makes it harder for enterprises to apply Big Data analytics in their business data with a goal to enhance their operational processes. Ultimately, these processes can be automated to reduce human errors. In this paper, we study an empirical dataset containing real world service desk tickets using machine learning with a goal to automate and improve business processes, which will benefit to enterprises and customers in increasing quality of service.",2019,07303157
48,,IDENTIFICATION OF QUALITY PARAMETERS ASSOCIATED WITH 3V'S OF BIG DATA,"Big Data approach uses an empirical process that does not lie on the understanding of underlying mechanisms, but lies on the observation of facts. Achieving high quality in Big Data is a critical issue for both the database researchers and practitioners. More explicit consideration must be given to data quality since data increasingly outlives the application for which it was initially designed. In this paper, identification of quality parameters is done which are compatible to the 3V's of big data which will further provide enhancement in achieving quality data to be stored in the repository. Good utilization of Big Data strengthens the performance and competitiveness of the firms by enabling better and faster results to its customer needs. In this paper GQM (Goal Question Metric) methodology is proposed to measure quality using metrics. It describes how to include data quality metrics to project, progress and maintain levels of quality in an organization. It helps to make a decision whether or not our current data satisfies our quality prospects.",2016,
49,10.1109/TSG.2019.2916570,UPS: UNIFIED PMU-DATA STORAGE SYSTEM TO ENHANCE T+D PMU DATA USABILITY,"The emerging distribution-level phasor measurement unit (D-PMU) is expected to play an important role in enhancing distribution system observability and situational-awareness. It is a demanding yet challenging task to develop advanced D-PMU data management and analytics tools to improve D-PMU data usability and further promote D-PMU projects deployment. This paper focuses on D-PMU data processing and storage. It presents a brief review of existing D-PMU data storage systems and points out their limitations on high-performance, flexibility, and scalability. To overcome the limitations, a unified PMU-data storage system (UPS) is proposed. Specifically, a unified I/O interface between storage servers and computing jobs is developed to effectively reduce the overhead of managing various computing jobs and data analytics over multiple storage infrastructures; and PMUCache with PMUCache partitioning and PMUCache replacement algorithms are designed to support in-situ data processing and shared distributed data storage and further serve the computing jobs/queries with different quality of service (QoS) requirements. Through a series of experiments, it is demonstrated that UPS achieves high performance on fast and big data processing and storage, and efficiently increases the flexibility and scalability of PMU data management systems.",2020,19493061
50,10.1109/QRS-C.2017.136,SECURITY ANALYTICS IN THE BIG DATA ERA,"This paper discusses the reality of the state-of-theartof existing information security systems that often providesenseless functions based on “buzz-words”. It points out real-liferequirements that these systems tend to ignore. It proposes thatdynamically changing understandable use cases should becreated in collaboration with corporate management to addressobjectives that are essential for the businesses. Big data analyticmethodologies should be utilized to assist the design of efficientimplementations of these use cases. Big data approaches shouldalso be used for heuristic detection of unknown attacks andanomalies, for data enrichment, and for post-hoc forensicanalysis. The main goal for information security systems that areusable in real-life should be that corporate decision makers areonly provided trustworthy and actionable insights that arerelevant to the business and provided services.",2017,
51,10.1109/ACCESS.2021.3074559,FROM BIG DATA TO DEEP DATA TO SUPPORT PEOPLE ANALYTICS FOR EMPLOYEE ATTRITION PREDICTION,"In the era of data science and big data analytics, people analytics help organizations and their human resources (HR) managers to reduce attrition by changing the way of attracting and retaining talent. In this context, employee attrition presents a critical problem and a big risk for organizations as it affects not only their productivity but also their planning continuity. In this context, the salient contributions of this research are as follows. Firstly, we propose a people analytics approach to predict employee attrition that shifts from a big data to a deep data context by focusing on data quality instead of its quantity. In fact, this deep data-driven approach is based on a mixed method to construct a relevant employee attrition model in order to identify key employee features influencing his/her attrition. In this method, we started thinking `big' by collecting most of the common features from the literature (an exploratory research) then we tried thinking `deep' by filtering and selecting the most important features using survey and feature selection algorithms (a quantitative method). Secondly, this attrition prediction approach is based on machine, deep and ensemble learning models and is experimented on a large-sized and a medium-sized simulated human resources datasets and then a real small-sized dataset from a total of 450 responses. Our approach achieves higher accuracy (0.96, 0.98 and 0.99 respectively) for the three datasets when compared previous solutions. Finally, while rewards and payments are generally considered as the most important keys to retention, our findings indicate that `business travel', which is less common in the literature, is the leading motivator for employees and must be considered within HR policies to retention.",2021,21693536
52,10.1109/BigComp48618.2020.00019,KISTI VEHICLE-BASED URBAN SENSING DATASET,"Recent smart city projects are rapidly expanding their technical boundary up to the Internet of Things, Big Data, and AI technology to realize the data-oriented urban situation awareness and making lots of sophisticated decision makings for various social issues such as traffic, air quality, boosting local economy, etc. In terms of cyber-physical systems, sensing data would be the first step to enable whole urban ecology to live and derive numerous benefits. In this work, in order to satisfy growing demands on urban environments by researchers and government officials, we have developed a vehicle-based urban sensing system to construct the basis to create urban sensing datasets. In this paper, we introduce our effort to collect urban sensing data collected in Daejeon City, Korea with two exclusive vehicles for three months (Sept.-Nov., 2019). We also describe what we can obtain from the collected datasets in terms of public benefits for establishing data-oriented smart cities.",2020,2375933X
53,10.1109/ICICTA51737.2020.00096,RESEARCH ON PARALLEL CLUSTERING ALGORITHM OF FEATURE HIDING BIG DATA IN HETEROGENEOUS NETWORKS,"Under the background of big data, network data presents a complex and multi structural feature. In the computer network, clustering is the key feature of big data hiding in heterogeneous network. In a word, the data chain in the network is more compact, and the data link between structures is more evacuation. As an important network data application method and traditional data development tool, clustering algorithm has been widely used in academia and society. Based on this, a parallel clustering algorithm for feature hiding big data in heterogeneous networks is proposed. On the premise of establishing the fuzzy equivalent constraint Association of hidden data, the heterogeneous measurement of mixed data is calculated, and online clustering is realized by reconstructing data structure. The experimental results show that the parallel clustering algorithm designed in this paper can achieve good clustering results in data sets, and can measure the differences between data and classes more accurately and reasonably. The new algorithm overcomes the shortcomings of traditional clustering algorithm which classifies attributes according to the overall size of data set or the dispersion degree within the cluster. Compared with other data clustering algorithms, the algorithm proposed in this paper has higher practicability and higher clustering quality.",2020,
54,10.1109/CINTI.2016.7846371,BIG DATA-BASED CLOUDS HEALTH-CARE AND RISK PREDICTIONS BASED ON ENSEMBLE CLASSIFIERS AND SUBJECTIVE PROJECTION,"Discovering patterns from big data attracts a lot of attention due to its importance in discovering accurate patterns and features that are used in predictions of decision making. The challenges in big data analytics are the high dimensionality and complexity in data representation. Granular computing and feature selection are among the challenge to deal with big data analytics that is used for Decision making. We will discuss these challenges in this talk and provide new projection on ensemble learning for health care risk prediction. In decision making most approaches are taking into account objective criteria, however the subjective correlation among different ensembles provided as preference utility is necessary to be presented to provide confidence preference additive among it reducing ambiguity and produce better utility preferences measurement for good quality predictions. Most models in Decision support systems are assuming criteria as independent. Different type of data (time series, linguistic values, interval data, etc.) imposes some difficulties to data analytics due to preprocessing and normalization processes which are expensive and difficult when data sets are raw and imbalanced. We will highlight these issues though project applied to health-care for elderly, by merging heterogeneous metrics for providing health care predictions for elderly at home. We have utilized ensemble learning as multi-classification techniques on multi-data streams that collected from multi-sensing devices. Subjectivity (i.e., service personalization) would be examined based on correlations between different contextual structures that are reflecting the framework of personal context, for example in nearest neighbor based correlation analysis fashion. Some of the attributes incompleteness also may lead to affect the approximation accuracy. Attributes with preference-ordered domain relations properties become one aspect in ordering properties in rough approximations. We outline issues on Virtual Doctor Systems, and highlights its innovation in interactions with elderly patients, also discuss these challenges in granular computing and decision support systems research domains. In this talk I will present the current state of art and focus it on health care risk analysis with examples from our experiments.",2016,24719269
55,10.1109/IDAACS53288.2021.9661000,AIR QUALITY SENSOR DATA COLLECTION AND ANALYTICS WITH IOT FOR AN APARTMENT WITH MECHANICAL VENTILATION,"The aim of the research was to develop a concept of a remote measurement system for air quality management using IoT for an apartment with a mechanical exhaust system. The constant monitoring system with the possibility of manual control by the occupants allows teaching the occupants how to chose optimal settings with the biggest impact on the air quality. Additionally, using cloud data analysis, the measurements can be compared with simulations performed before the building construction. This allows choosing the proper apartment depending on the foreseen occupancy schedule and the family size.",2021,27704262
56,10.1109/IGARSS.2018.8517556,ADVANCED VISUALISATION OF BIG DATA FOR AGRICULTURE AS PART OF DATABIO DEVELOPMENT,"There is an increasing tension in agriculture between the requirements to assure full safety on the one hand and keep costs under control on the other hand, both with respect to (inter)national strategies. Farmers need to measure and understand the impact of huge amount and variety of data which drive overall quality and yield in their fields. Among others, those are local weather data, Global Navigation System of Systems data, orthophotos and satellite imagery, data on soil specifics etc. A strong need to secure Big Data arises due to various repositories and heterogeneous sources. Data storage and visualisation requirements are in some cases competing as they are a common interest as well as a threat that helps one part of a value chain to gain a higher profit. As demonstrated in this paper, handling (Big) data is therefore a sensitive topic, where trust of producers on data security is essential.",2018,21536996
57,10.1109/BigData47090.2019.9006288,DISCOVERING HIGH DEMANDING BUS ROUTES USING FARECARD DATA,"Having an effective public transport system is one of the most important factors to improve the quality of urban residents' life and to bring a sustainable development in urban areas. In this paper, we detect high demanding region pairs with inconvenient bus route design, such as taking circuitous routes or having too many stops, etc., to improve the utilization efficiency of public transportation services, according to people's real demands. The detected results consist of 1) region pairs with significant bus route design problems, and 2) the linking structure as well as the correlation among these region pairs. We compare these results to some existing and future urban planning, such as MRT lines, and study whether this planning reduces the current problems.",2019,
58,10.1109/ACCESS.2019.2898707,ONE-PASS INCONSISTENCY DETECTION ALGORITHMS FOR BIG DATA,"Data in the real world is often dirty. Inconsistency is an important kind of dirty data; before repairing inconsistency, we need to detect them first. The time complexities of the current inconsistency detection algorithms are super-linear to the size of data and not suitable for the big data. For the inconsistency detection of big data, we develop an algorithm that detects inconsistency within the one-pass scan of the data according to both the functional dependency (FD) and the conditional functional dependency (CFD) in our previous work. In this paper, we propose inconsistency detection algorithms in terms of FD, CFD, and Denial Constraint (DC). DCs are more expressive than FDs and CFDs. Developing the algorithm to detect the violation of DCs increases the applicability of our inconsistency detection algorithms. We compare the performance of our algorithm with the performance of implementing SQL queries in MySQL and BigQuery. The experimental results indicate the high efficiency of our algorithms.",2019,21693536
59,10.1109/TCYB.2020.2970198,GENETIC PROGRAMMING FOR EVOLVING A FRONT OF INTERPRETABLE MODELS FOR DATA VISUALIZATION,"Data visualization is a key tool in data mining for understanding big datasets. Many visualization methods have been proposed, including the well-regarded state-of-the-art method t-distributed stochastic neighbor embedding. However, the most powerful visualization methods have a significant limitation: the manner in which they create their visualization from the original features of the dataset is completely opaque. Many domains require an understanding of the data in terms of the original features; there is hence a need for powerful visualization methods which use understandable models. In this article, we propose a genetic programming (GP) approach called GP-tSNE for evolving interpretable mappings from the dataset to high-quality visualizations. A multiobjective approach is designed that produces a variety of visualizations in a single run which gives different tradeoffs between visual quality and model complexity. Testing against baseline methods on a variety of datasets shows the clear potential of GP-tSNE to allow deeper insight into data than that provided by existing visualization methods. We further highlight the benefits of a multiobjective approach through an in-depth analysis of a candidate front, which shows how multiple models can be analyzed jointly to give increased insight into the dataset.",2021,21682275
60,10.1109/BigData.Congress.2013.21,TOWARDS A QUALITY-CENTRIC BIG DATA ARCHITECTURE FOR FEDERATED SENSOR SERVICES,"As the Internet of Things (IoT) paradigm gains popularity, the next few years will likely witness 'servitization' of domain sensing functionalities. We envision a cloud-based eco-system in which high quality data from large numbers of independently-managed sensors is shared or even traded in real-time. Such an eco-system will necessarily have multiple stakeholders such as sensor data providers, domain applications that utilize sensor data (data consumers), and cloud infrastructure providers who may collaborate as well as compete. While there has been considerable research on wireless sensor networks, the challenges involved in building cloud-based platforms for hosting sensor services are largely unexplored. In this paper, we present our vision for data quality (DQ)-centric big data infrastructure for federated sensor service clouds. We first motivate our work by providing real-world examples. We outline the key features that federated sensor service clouds need to possess. This paper proposes a big data architecture in which DQ is pervasive throughout the platform. Our architecture includes a markup language called SDQ-ML for describing sensor services as well as for domain applications to express their sensor feed requirements. The paper explores the advantages and limitations of current big data technologies in building various components of the platform. We also outline our initial ideas towards addressing the limitations.",2013,23797703
61,10.1109/CIS52066.2020.00060,DATA PREPROCESSING METHOD FOR THE ANALYSIS OF INCOMPLETE DATA ON STUDENTS IN POVERTY,"Data mining is the focus of big data applications in various fields. Data pre-processing is a crucial step in the data mining process. With the development of the information society and the application of databases, the educational data has seen explosive growth, and the data on poor students has become informative. However, the actual student financial aid management system collects the data on poor students which generally has problems such as missing values, attributes redundancy, and noise. To solve this problem, we proposed a novel method called DPBP to preprocess data. The proposed DPBP approach consists of four stages: the preparation of data, the scoping of characteristics, the combination of characteristics, and the filtering of missing number. Firstly, we prepare the dataset by extracting data. Next, the characteristic range is limited by choosing experimental results of feature selection algorithm. Then, third stage performs feature combination to obtain the feature decomposition sets. Finally, based on accuracy and missing number, we gain the optimal dataset. Series of experiments result show that our proposed method significantly improves the data quality and stability.",2020,
62,10.1109/WIECON-ECE52138.2020.9397990,PRE-PROCESSING DATA IN WEATHER MONITORING APPLICATION BY USING BIG DATA QUALITY FRAMEWORK,"In this research, we are working with Big Data for obtaining, preparing and analyzing data-based information to make use of the data retrieved which will benefit any organization. It is a progressing part of all divisions of industry and business. All organizations in any field, for example, oil, money, fabricating hardware and so forth produce big data, which can show incredibly helpful designs to business directors to make and develop their organizations, when the information is gathered and analyzed accurately. It permits us to gather, store, and decipher immense measures of big data to produce useful outcomes. Data quality is affected by the information that is gathered to be analyzed as that data will make sure whether in the long run a specific method of conducting the ongoing process is useful or not. Consequently, the consistency of big data very important. Here, we propose that the various types of raw information should be analyzed to expand its precision in the pre-handling stage, as those pieces of information are not utilized later in the process. During investments, we break down and model the big data to decrease overhead expenses to create and add to a solid understanding of results to improve information consistency.",2020,
63,10.1109/ICCOINS.2016.7783229,ANALYSIS OF BIG DATA AND QUALITY-OF-EXPERIENCE IN HIGH-DENSITY WIRELESS NETWORK,"The proliferation of smart devices, along with the availability of bandwidth-intensive applications are generating huge volumes of data that create challenges to IT industries. Data handling becomes more troublesome when mobile user's gather in tens and thousands of quantity at confine locations and generates Big Data. Analysis and storing of this huge, varied and complex data put great challenges to the Network Service Providers (NSP). Thus, service providers are facing problems in managing big data in dense environment and maintaining user's Quality of Experience (QoE). However, big data also provide great opportunities to NSP. The accurate analysis of big data in real-time reveals the user experience of network services which helps the service providers to take timely action to improve user QoE. Thus, this paper presents an overview of Big Data and QoE in High-Density Wireless Network (HDWN) environment.",2016,
64,10.1109/ACCESS.2018.2885142,AIR QUALITY FORECAST MONITORING AND ITS IMPACT ON BRAIN HEALTH BASED ON BIG DATA AND THE INTERNET OF THINGS,"Brain health quality pre-monitoring has become an urgent need, and this is a system of complex engineering. From the perspective of intelligent decision-making based on big data, the intelligent air index prediction is introduced, the popular classification algorithm is introduced, the hidden information of historical data is mined, and the brain health quality prediction is realized. The brain health quality monitoring system based on the Internet of Things is constructed, and the classification algorithm is used to realize real-time acquisition, intelligent processing of data. In order to improve the data processing speed and enhance the real-time performance of brain health quality prediction, this paper introduces cloud computing technology to accelerate data processing. In order to enable users to understand the air index, anytime and anywhere, it is also designed based on the problem of large historical data of air index and real-time data collection. The Android platform develops an air index forecast client.",2018,21693536
65,10.1109/I-SMAC49090.2020.9243611,INFORMATION RECONSTRUCTION AND DATA MODELING AND THE HISTORICAL DILEMMA OF CONTEMPORARY LITERATURE WITH INFORMATION MINING,"Information reconstruction and data modeling and the historical dilemma of contemporary literature with information mining are studied in this paper. From the technology level, the novelties are summarized as follows. (1) Big data basic information, functional information, and collaborative linkage information to then establish the meta-description of big data bodies that need to be pre-processed. (2) There is a core certain difference in the amplitude value between the reconstructed data and the original data, but it is a systematic difference and will not affect the quality of the reconstructed data and imaging effect. Then, the proposed methodology is applied to the application and the performance is evaluated. The experimental results have shown that the proposed model has better robustness.",2020,
66,10.1109/ICEKIM52309.2021.00127,RESEARCH ON THE COUNTERMEASURES OF IMPROVING TEACHERS' INFORMATION TEACHING ABILITY IN HIGHER VOCATIONAL COLLEGES BASED ON BIG DATA,"Under the modern education system, the teacher's informatization teaching ability plays a key role, directly related to the level of teaching quality. Using big data can provide high quality teaching service. Teachers should establish student data analysis model according to the actual situation of teaching, and give full play to the role of open teaching platform with information technology, build an integrated teaching model, promote the implementation of talent training plans, and deeply integrate large-scale and personalized education to guarantee the quality of education and teaching.",2021,
67,10.1109/BID.2017.8336582,ROADMAP TO PREPARE DISTRIBUTION GRID-TIED PHOTOVOLTAIC SITE DATA FOR PERFORMANCE MONITORING,"One of the key analytics conducted on a gridtied Photovoltaic (PV) system is the periodic monitoring of its performance. It is expected that with increased PV penetration into the distribution smart grid in the future, quality and integrity of the data required to conduct such analytics will be crucial. While data processing and management tools for smart grid in the literature use cloud, distributed file management and parallel processing, the latency and computation requirements specific to performance monitoring need more lightweight, descriptive methods. This paper provides a systematic roadmap to analyze data collected from a real distribution grid-tied 1.4MW PV power plant for completeness, consistency and integrity, with the objective of using it for performance monitoring. To ensure the data's integrity is not compromised, the distribution of processed data is compared with that of the raw data. This paper makes one of the first few attempts to provide a comprehensive approach for data scientists to clean and prepare grid-tied PV data for site-level performance monitoring.",2017,
68,10.1109/CBD.2019.00011,COMMUNICATION OPTIMIZATION OF COMPUTE-INTENSIVE CLUSTERS BASED ON SOFTWARE-DEFINED NETWORKS,"Data transmission among nodes in compute-intensive cluster with MapReduce is a major performance bottleneck. In order to optimize network communication performance, a distributed data mapping model is constructed for big data computing jobs. Firstly, the mapping model extracts the spatio-temporal features of the input data of computing jobs, and optimizes job data layout by replacing storage locality with communication locality. Secondly, decision-space transformation is used to classify calculation jobs, and big data calculation jobs are divided into two categories: communication-intensive and non-intensive. Finally, the data communication of computing cluster is managed by software-defined network, and communication-intensive jobs are mapped to nodes of high-quality link by using global sensing capability of software-defined network to improve the communication performance of intermediate data. Experiments show that the model has better communication optimization effect for data communication-intensive jobs, and data transmission delay is reduced from 4.2% to 5.7%. Therefore, this communication optimization scheme is suitable for data center or large-scale cluster communication optimization, and adapts to various big data scheduling strategies and multiple network topologies.",2019,
69,10.1109/IGARSS.2015.7325916,THIN CLOUD REMOVAL FOR LANDSAT 8 OLI DATA USING INDEPENDENT COMPONENT ANALYSIS,"Using independent component analysis (ICA) coupled with the quality assessment (QA) band of Landsat 8, an approach for thin cloud removal in Landsat 8 operational land imager (OLI) data was developed. After the ICA transformation of the visible, near infrared, short-wavelength and cirrus bands of OLI data, cloud component was identified by the mixing matrix. Then, a cloud mask derived from the analysis of the QA band was formed such that an image pixel with and without cloud cover was delineated. The cloud component and cloud mask were used to remove the thin clouds. Thin clouds disappeared visually within the OLI data. Using another cloud-free image acquired in the previous overflight as the reference, we assessed the accuracy level of the cloud removal. Before and after the cloud removal, the spatial correlation coefficients increased from 0.69 to 0.83 in band 1, 0.75 to 0.86 in band 2, 0.81 to 0.88 in band 3, 0.87 to 0.91 in band 4, and no change in bands 5, 6, and 7 for pixels identified with cloud cover.",2015,21537003
70,10.1109/ICBDIE52740.2021.00022,THE EXPLORATION OF COLLEGE STUDENTS’ INNOVATION AND ENTREPRENEURSHIP EDUCATION IN THE NEW ERA FROM THE PERSPECTIVE OF BIG DATA,"In this innovative life, big data, as an important new force of innovative entrepreneurship education in the new era, always promotes the progress of information, makes education form a personalized way, and promotes the reform of teaching platform. In the era of big data, colleges and universities also ushered in a new challenge to the cultivation of talents, innovative entrepreneurship education conforms to the development of the times, social development with the progress of students also form a market demand. In the process of practical exploration of innovative entrepreneurship education, colleges and universities have many problems. The lack of specialization of educational teachers will lead to the lack of students' educational consciousness. It can effectively improve the quality of innovative entrepreneurship education in colleges and universities. On the basis of the original, it is proposed that training practice as the guide, ability innovation as the optimization of top-level design, reform of the education system, strengthen the construction of teachers, explore the big data teaching model, can effectively solve the problem. At the same time, it can promote the upgrading of innovative entrepreneurship education, cultivate students' unremitting ability, let students master the ability of independent thinking, dare to explore the spirit of a series of abilities.",2021,
71,10.1109/ICET.2018.8603594,DATA QUALITY TECHNIQUES IN THE INTERNET OF THINGS: RANDOM FOREST REGRESSION,"Internet of Things (IoTs) is one of the most promising fields in computer science. It consists of physical devices, automobiles, home appliances, embedded hardware, sensors and actuators which empowers these objects to interface and share information with other devices over the network. The data gathered from these devices is used to make intelligent decisions. If the data quality is poor, decisions are likely to be flawed. A little work has been carried out regarding data quality in the Internet of Things, but there is no scheme which is experimentally proved. In this paper we will identify data quality challenges in the Internet of Things domain and propose a model which ensure data quality standards provided by ISO 8000. We evaluated our model on the weather dataset and used the random forest prediction method to calculate the accuracy of our data. Results show that when compared with the baseline model the proposed system improves accuracy by 38.88%.",2018,
72,10.1109/ACCESS.2018.2856623,A MULTI-DIMENSIONAL TRUST MODEL FOR PROCESSING BIG DATA OVER COMPETING CLOUDS,"Cloud computing has emerged as a powerful paradigm for delivering data-intensive services over the Internet. Cloud computing has enabled the implementation and success of big data, a recent phenomenon handling huge data being generated from different sources. Competing clouds have made it challenging to select a cloud provider that guarantees quality of cloud service (QoCS). Also, cloud providers' claims of guaranteeing QoCS are exaggerated for marketing purposes; hence, they cannot often be trusted. Therefore, a comprehensive trust model is necessary to evaluate the QoCS prior to making any selection decision. In this paper, we propose a multi-dimensional trust model for big data workflow processing over different clouds. It evaluates the trustworthiness of cloud providers based on: the most up-to-date cloud resource capabilities, the reputation evidence measured by neighboring users, and a recorded personal history of experiences with the cloud provider. The ultimate goal is to ensure an efficient selection of trustworthiness cloud provider who eventually will guarantee high QoCS and fulfills key big data workflow requirements. Various experiments were conducted to validate our proposed model. The results show that our model captures the different components of trust, ensures high QoCS, and effectively adapts to the dynamic nature of the cloud.",2018,21693536
73,10.1109/CICED50259.2021.9556785,THE FAST SIMULATION ARCHITECTURE CONSTRUCTION FOR INTEGRATED ELECTRIC TRANSMISSION AND DISTRIBUTION POWER GRID BASED ON BIG DATA PLATFORM,"This paper studies constructing methods and key technologies of the fast simulation architecture for integrated electric transmission and distribution power grid based on big data platform, aiming at the problems about difficult communication, poor collaboration, low reusability of services and applications, and time-consuming preparation in the fast integrated simulation system. Specifically, this paper focuses on the creation technology of simulating scenarios, the design of dispatcher storage system for distributed tasks, the management of service based on micro service framework, and the simulation data mart, etc. Besides, a business flow and data flow of fast simulation are realized. Comparing with the traditional DTS process, this method makes full use of simulation resource, also improves the speed and quality of simulation process and achieves a better human-computer interaction experience. In addition, this method enables the reusability for services and applications. The feasibility and effectiveness is validated.",2021,21617481
74,10.1109/ICBDA.2017.8078853,BIG DATA IN ENTERPRISE MANAGEMENT: TRANSFORMATION OF TRADITIONAL RECRUITMENT STRATEGY,"Technological advancements and growth of social media prompted organizations to deploy better ways of doing business. Social big data analytics provide a platform to facilitate the job seekers and job providers. The study highlights the significance of social networking sites and its role in transforming traditional recruitment strategy. The transitioning to e-recruitment signifies the importance of social networking sites as a source of recruitment mix. Primary data was collected through questionnaire from 110 employees of commercial banks operating in the city of Lahore, Pakistan. Simple random sampling technique and willingness to participate on the part of respondents generated productive findings. Cronbach's alpha, descriptive statistics, correlation matrix, and regression analysis were used as data analysis tools to endorse reliability of data collection and test hypotheses. Information quality, popularity, and security of social media found to have significant impact on the effectiveness of e-recruitment. The findings of the research suggest that big data applications not only results in better business performance management, but also enhances accessibility and reputation of the enterprise through cyber-social media. Management issues of social network big data with respect to HR managers, IT professionals and academia are also discussed.",2017,
75,10.1109/BIGDSE.2015.11,"MINING BIG DATA FOR DETECTING, EXTRACTING AND RECOMMENDING ARCHITECTURAL DESIGN CONCEPTS","An architecture recommender system can help programmers make better design choices to address their architectural quality attribute concerns while doing their daily programming tasks. We mine big data to detect and extract a large set of architectural design concepts, such as design patterns, design tactics, architecture styles, etc., to be used in our architecture recommender system called ARS. However, mining big data poses many practical challenges for system implementation. The volume, velocity and variety of our data set, like all other big data systems, requires careful planning. This first challenge is to select appropriate technologies from the large number of available products for our system implementation. Building on these technologies our greatest challenge is to custom-fit our algorithms to the parallel processing platform we have selected for ARS, to meet our performance goals.",2015,
76,10.1109/KSE.2015.56,TOWARDS AN OPEN DATA DEVELOPMENT MODEL FOR LINKING HETEROGENEOUS DATA SOURCES,"Open data is providing opportunities as well as challenges for research and development in the field of knowledge and systems engineering. The prospects for both researchers and application developers accessing and integrating publicly available data from various sources are unprecedented. However, many open data platforms are built in such a way that it is difficult or, sometimes, impossible to extract value from the data, integrate data from various sources, transform or improve the quality of the data through continuous use and reuse, track provenance, and share research findings. In this paper we present open data development model (ODDM) that addresses some these difficulties. The model shows how various stakeholders can check-out data from an open data repository, use and reuse the data, add new functionalities, appraise, transform, and publish or check-in the improved data for other researchers to benefit. We discuss how we intend to apply the model in practice to build a new generation of open data repository for interdisciplinary research, and highlight key issues for debate and discussion.",2015,
77,10.1109/FoNeS-AIoT54873.2021.00053,"THE STUDY ON HOW TO MAKE SCIENTIFIC USE OF ""POKA YOKE TECHNOLOGY"" IN THE PROCESS OF CIGARETTE PRODUCTION AND MANAGEMENT UNDER THE BACKGROUND OF BIG DATA ANALYSIS","In order to effectively control the quality, special and unique Poka-Yoke technology should be used to play a positive role in the manufacturing process with high precision. Because of the particularity of tobacco enterprises, the Poka-Yoke technology has been popularized in the tobacco production process to a certain extent, and has gradually become an effective measure to improve the level of tobacco production technology. In view of this, this paper conducts discussion on the risk factors of quality errors in cigarette technology from multiple perspectives, and actively explores the specific application and strategy of Poka-Yoke technology in cigarette production management.",2021,
78,10.1109/FiCloud.2014.65,SIMULATION AND BIG DATA: A WAY TO DISCOVER UNUSUAL KNOWLEDGE IN EMERGENCY DEPARTMENTS: WORK-IN-PROGRESS PAPER,"Here a work in progress is reported on within research that aims to obtain knowledge about variables which may influence a hospital emergency department's performance and quality of service. Knowledge discovery will be achieved through the analysis of intensive data generated by the simulation of any possible scenario in the real system. The challenge is to provide knowledge of critical, non-usual or extreme situations. Simulation is the only way to obtain information about these kinds of situations, as it is not possible to test such scenarios in the real system. We show how simulation of the real system through advanced computing is a source of big data, as it allows rapid and massive data generation. The potential of high performance computing makes it possible to generate a very large amount of data within a reasonable time, store this data, then process and analyze it to obtain knowledge. We describe the methodology proposed for this goal, which is based on the use of the simulator as a sensor of the real system, and so as the main source of data. The application of data mining techniques will open the doors to knowledge. To verify that the proposed methodology works, we propose a case study in which the aim is to obtain knowledge from a set of data already available, obtained from the simulation of a reduced set of scenarios of the real system.",2014,
79,10.1109/ICCMC48092.2020.ICCMC-00050,FRAUD DETECTION AND PREVENTION BY USING BIG DATA ANALYTICS,"A retail sector is a group of organization or people who sell goods or services for gaining income. Fraud is wrongful or criminal activities for the economic and personal benefits. Fraud detection is finding actual or expected fraud which takes place in an organization and in the retail market is one of the challenging aspects. Fraud is mischievous activities occur in retail sector includes shoplifting, skimming, replicating cards from skimmed data, counterfeiting, bar code or POS(Point-of-Sale) manipulation, contamination, mislabeling, substitute cheaper ingredients instead of high-quality ingredients. Fraudulent activities occur in the retail sector by both consumer and supplier. Analyzing financial crimes related to fraudulent activities is difficult where traditional data mining techniques fail to address all of them. Big data analytics is used to identify an unusual pattern to detect and prevent fraud in the retail sector. Various predictive analytics tools are used to handle massive data and their pattern.",2020,
80,10.1109/TKDE.2020.3046443,DATA DEPENDENCIES EXTENDED FOR VARIETY AND VERACITY: A FAMILY TREE,"Besides the conventional schema-oriented tasks, data dependencies are recently revisited for data quality applications, such as violation detection, data repairing and record matching. To address the variety and veracity issues of big data, data dependencies have been extended as data quality rules to adapt to various data types, ranging from (1) categorical data with equality relationships to (2) heterogeneous data with similarity relationships, and (3) numerical data with order relationships. In this survey, we briefly review the recent proposals on data dependencies categorized into the aforesaid types of data. In addition to (a) the concepts of these data dependency notations, we investigate (b) the extension relationships between data dependencies, e.g., conditional functional dependencies (CFDs) extend the conventional functional dependencies (FDs). It forms a family tree of extensions, mostly rooted in FDs, helping us understand the expressive power of various data dependencies. Moreover, we summarize (c) the discovery of dependencies from data, since data dependencies are often unlikely to be manually specified in a traditional way, given the huge volume and high variety of big data. We further outline (d) the applications of the extended data dependencies, in particular in data quality practice. It guides users to select proper data dependencies with sufficient expressive power and reasonable discovery cost. Finally, we conclude with several directions of future studies on the emerging data.",2022,23263865
81,10.1109/ICBDA49040.2020.9101191,THE RESEARCH AND DESIGN OF TRUST BUSINESS MANAGEMENT AND ANALYSIS SYSTEM BASED ON BIG DATA TECHNOLOGY,"China's trust companies are gradually moving towards the international market and directly participating in international competition. Facing the increasingly complex macroeconomic environment and more prudent external regulatory requirements, the trust industry has both opportunities and challenges. On the one hand, the number of high-end customers and financial needs have increased dramatically, providing many high-quality customers for trust companies. On the other hand, trust product allocation lacks flexibility. How to balance the balance between competition and customers' changing needs, give full play to the asset management and allocation functions of trust companies, improve product innovation ability, become the top priority of trust companies' operation, Informatization and risk prevention become the key to the success of the new business model and management model.In the era of information revolution, big data has become an important driving force for financial innovation. Trust companies should actively seize the new opportunities for the vigorous development of financial market, accelerate the innovation of trust functions, focus on building core capabilities, and actively plan for the development of new layout.Therefore, China's trust industry needs to actively adopt big data technology to achieve efficient management and intelligent analysis of trust business.",2020,
82,10.1109/ACCESS.2017.2741105,INDUSTRIAL BIG DATA ANALYSIS IN SMART FACTORY: CURRENT STATUS AND RESEARCH STRATEGIES,"Under the background of cyber-physical systems and Industry 4.0, intelligent manufacturing has become an orientation and produced a revolutionary change. Compared with the traditional manufacturing environments, the intelligent manufacturing has the characteristics as highly correlated, deep integration, dynamic integration, and huge volume of data. Accordingly, it still faces various challenges. In this paper, we summarize and analyze the current research status in both domestic and aboard, including industrial big data collection, modeling of the intelligent product lines based on ontology, the predictive diagnosis based on industrial big data, group learning of product line equipment and the product line reconfiguration of intelligent manufacturing. Based on the research status and the problems, we propose the research strategies, including acquisition schemes of industrial big data under the environment of intelligent, ontology modeling and deduction method based intelligent product lines, predictive diagnostic methods on production lines based on deep neural network, deep learning among devices based on cloud supplements and 3-D selforganized reconfiguration mechanism based on the supplements of cloud. In our view, this paper will accelerate the implementation of smart factory.",2017,21693536
83,10.1109/ISSI.2018.8538098,STUDY OF DATA INTEGRATION ARCHITECTURE FOR WIDEAREA DISTRIBUTED POWER QUALITY OF POWER GRID,"With the increasing degree of interconnection between regional power grids and the diversification of power quality interference sources, the problem of power quality has become a complex problem across provinces and regions. It is necessary to provide an analysis method for solving complex power quality problems between regions by carrying out analysis technology research based on the monitoring data of whole network power quality and exploring the correlation of interregional power quality problems. A new wide-area distributed power quality data fusion architecture is proposed in this paper. It solves the data source problem of the big data analysis of the power quality, and realizes the sharing of the data and information of the whole network power quality, and lays the theoretical foundation for the depth application of the power quality data by researching and designing the architecture aiming at multi-source, heterogeneous and distributed data integration technology and wide area distributed data storage technology.",2018,
84,10.1109/ICIBA52610.2021.9688164,RESEARCH ON RECOMMENDATION SYSTEM USING BIG DATA BASED ON DEEP LEARNING,"According to the recommendation algorithm based on deep learning, this article will verify the validity of the recommendation model through Item2vec and DeepFM models. To analyze the relationship between historical user behavior data and the users of the product are interested in, the Spark SQL scripts can be run according to statistical analysis methods, and generate the training set and test set, which is needed by train the model, and build predictive models, and get the collection of products that each user is interested in and improve the prediction effect through model optimization. Experimental verification and analysis show that the recommendation system can handle massive amounts of user and product data. It can also extract features from these data for training automatically, which is conducive to improving the quality and efficiency of users' shopping decisions. we first complete feature extraction based on the processed data set. At present, the most used feature extraction method is feature extraction based on artificial experience and statistical methods. However, because the actual application scenarios of the recommendation system are often more complicated, we used Spark SQL scripts to extract features in experiments. First, extract the experimental data from the distributed database, and use the workflow scheduling script to generate the data set required for model training and testing.",2021,
85,10.1109/HPCC-SmartCity-DSS.2016.0172,A JOINT GRID SEGMENTATION BASED AFFINITY PROPAGATION CLUSTERING METHOD FOR BIG DATA,"Clustering is useful for discovering underlying groups and identifying interesting patterns in scientific data and engineering systems. Affinity propagation (AP) is an effective clustering algorithm which has been successfully applied to broad areas of computer science. To generate high quality clusters, AP iteratively performs information propagation on the full similarity matrix and requires excessive time to exchange messages between data points. This paper proposes a novel AP clustering method based on grid segmentation. The main ideas of our approach are: (1) to partition the data points into multiple non-overlapping sub-sets to simplify representation of huge data points into smaller sub-sets, (2) to construct sparse similarity matrix to decrease the unnecessary message exchanges. Experimental evaluations on large-scale real-world datasets demonstrate our proposed method has superior performance in effectiveness and efficiency.",2016,
86,10.1109/DAS54948.2022.9786116,TENDENCIES IN THE USE OF BIG DATA ANALYTICS AT A GLOBAL LEVEL,"The current issue of processing big volumes of data leads to a more and more increased interest in the Big Data analytics, as their usage generates solutions for problems in different areas. Taking into consideration the fulminant growth of the data volumes and their complexity, the use of Big Data analytics is becoming a necessity in essential fields such as health, education, banking, marketing and not only. The current society, based on knowledge, makes organizations orient more and more towards obtaining useful information from the data they have with a view to predicting certain evolutions and taking decisions which would offer a certain advantage over their competitors. In this paper, we will expose the current state in the development of Big Data, the main fields in which Big Data has become an indispensable tool, as well as the usefulness of Big Data in developing and implementing the new technologies based on artificial intelligence.",2022,
87,10.1109/ICCCBDA.2018.8386522,A BIG DATA ON PRIVATE CLOUD AGILE PROVISIONING FRAMEWORK BASED ON OPENSTACK,"On the bases of the OpenStack private cloud delivery big data platform, numerous entities yearn for attaining agile and standardized big data delivery platform, reclaiming the resources, managing the total cost of ownership (TCO) and adapting to multiple big data open source or commercial off-the-shelf (COTS) solutions. Nevertheless, as regards the big data platform running on cloud computing, the big data platform is disintegrated from the cloud computing system by virtual machines since neither being based on OpenStack private cloud nor on big data platform can achieve end-to-end resource delivery, together with ensuring that it is quite convenient for the long-term operations. Accordingly, establishing an across framework between private cloud and big data platform is quite essential. The big data on cloud agile provision framework could realize fast resource delivery based on predefined orchestration template of private cloud, operating system, big data platform, monitor, inspection system, etc. Through the deployment of this framework, it is capable of attaining the delivery of agile, low cost, standardized and high adaptability the big data on cloud, as well as the high-quality operation of the big data on cloud with the help of integration configuration management database (CMDB) with the automatic inspection system.",2018,
88,10.1109/CICED.2018.8592252,A NEW CAPACITY INSPECTION METHOD FOR DISTRIBUTION TRANSFORMER BASED ON BIG DATA,"The quality of distribution network equipment is the basis of the construction of intrinsically safe distribution system, and the sample inspection of delivered equipment is an important means of the quality control of distribution network equipment. Centering on the problem that distribution transformer capacity is hard to be inspected accurately at present, this paper proposes a new capacity inspection method for distribution transformer based on big data. This paper accumulates a large number of test data of nearly one thousand distribution transformers from inspection work, and analyzes the distribution of volume, weight, DC resistance, no-load loss, load loss and short-circuit impedance of distribution transformer in different capacity levels through the statistical method, and finally makes a capacity assessment process. This method is simple, practical and reliable, and it can be spread and applied in production practice.",2018,21617481
89,10.1109/TII.2018.2850053,FUZZY-FOLDED BLOOM FILTER-AS-A-SERVICE FOR BIG DATA STORAGE IN THE CLOUD,"With the ongoing trend of smart and Internet-connected objects being deployed across a broad range of applications, there is also a corresponding increase in the amount of data movement across different geographical regions. This, in turn, poses a number of challenges with respect to big data storage across multiple locations, including cloud computing platform. For example, the underlying distributed file system has a large number of directories and files in the form of gigantic trees, which are difficult to parse in polynomial time. Moreover, with the exponential increase of big data streams (i.e., unbounded sets of continuous data flows), challenges associated with indexing and membership queries are compounded. The capability to process such significant amount of data with high accuracy can have significant impact on decision-making and formulation of business and risk-related strategies, particularly in our current Industrial Internet of Things environment (IIoT). However, existing storage solutions are deterministic in nature. In other words, they tend to consume considerable memory and CPU time to yield accurate results. This necessitates the design of efficient quality of service-aware IIoT applications that are able to deal with the challenges of data storage and retrieval in the cloud computing environment. In this paper, we present an effective space-effective strategy for massive data storage using bloom filter (BF). Specifically, in the proposed scheme, the standard BF is extended to incorporate fuzzy-enabled folding approach, hereafter referred to as fuzzy folded BF (FFBF). In FFBF, fuzzy operations are used to accommodate the hashed data of one BF into another to reduce storage requirements. Evaluations on UCI ML AReM and Facebook datasets demonstrate the efficacy of FFBF, in terms of dealing with approximately 1.9 times more data as compared to using the standard BF. This is also achieved without affecting the false positive rate and query time.",2019,19410050
90,10.1109/ETFA.2015.7301625,STATISTICAL DATA MINING FOR EFFICIENT QUALITY CONTROL IN MANUFACTURING,"Extensive use of machines, flexible/re-configurable manufacturing and transition towards the fully automated factories call for intelligent use of information recorded during the manufacturing process. Modern manufacturing processes produce Terabytes of information during different stages of the process e.g sensor measurements, machine readings etc, and the major contributor of these big data sets are different quality control processes. In this article we will present methodology to extract valuable insight from manufacturing data. The proposed methodology is based on comparison of probabilities and extension of likelihood principles in statistics as a performance function for Genetic Algorithm.",2015,19460759
91,10.1109/SCCC.2016.7836014,CONTRIBUTION OF BIG DATA IN E-LEAMING. A METHODOLOGY TO PROCESS ACADEMIC DATA FROM HETEROGENEOUS SOURCES,"Big Data covers a wide spectrum of technologies, which tends to support the processing of big amounts of heterogeneous data. The paper identifies the powerful benefits and the application areas of Big Data in the on-line education context. Considering the boom of academic services on-line, and the free access to the educative content, a great amount of data is being generated in the formal educational field as well as in less formal contexts. In this sense, Big Data can help stakeholders, involved in education decision making, to reach the objective of improving the quality of education and the learning outcomes. In this paper, a methodology is proposed to process big amounts of data coming from the educational field. The current study ends with a specific case study where the data of a well-known Ecuadorian institution that has more than 80 branches is analyzed.",2016,
92,10.1109/ICRIS52159.2020.00070,CONSTRUCTION STRATEGY OF UNIVERSITY INTELLIGENT ART EDUCATION ECOSYSTEM BASED ON BIG DATA,"In order to overcome the problems existing in the development of smart art education in colleges and universities, and to improve the informatization level of art education in colleges and universities, this paper puts forward a construction strategy of the ecosystem of smart art education in colleges and universities based on big data. Based on this, the system realizes the combination of art education and wisdom education from multiple dimensions, which are teaching activities, teaching concepts and teaching subjects. In addition, the system can also give full play to the interaction and immersion of big data technology and artificial intelligence technology, comprehensively improve the quality of art education, and build a perfect education ecological environment. The results show that the construction strategy can strengthen the deep integration of art education and information science and technology, and improve the information level of art education in colleges and universities.",2020,
93,10.1109/CIPAE53742.2021.00050,THE CULTIVATION SYSTEM OF THE THOUGHT-ASSISTED CLASSROOM LEARNING COMMUNITY BASED ON BIG DATA,"In the context of the new curriculum reform led by big data, education staff are facing more and more challenges. In order to meet new challenges and promote the development of the education industry, the country and the government, as well as schools and teachers themselves, are exploring and practicing effective ways to improve the overall quality of teaching. As a result, an ideological-assisted classroom learning community has emerged. This article first elaborates the related concepts of the thought-assisted classroom learning community in detail, then establishes a big data-based thought-assisted classroom detailed community cultivation system, and finally uses the system to use the actual teaching, through the comparison of student performance analyze with satisfaction degree, and the analysis results show that the thought-assisted classroom learning community can help students improve their performance.",2021,
94,10.1109/ICCSAI53272.2021.9609734,ANALYSIS OF BIG DATA IN HEALTHCARE USING DECISION TREE ALGORITHM,"Era of technological developments, big data has been widely implemented in various any company especially healthcare. Big data has opened up new gaps in health care. Big data in healthcare has the potential to improve better healthcare. The effective use of Big data can reduce health care problems such as how to provide proper care, maximum care solutions, and improve existing systems of health care. There are 6 defining domains in Big Data, which are Vol., and etc. Big data represents a variety of opportunities to improve the quality and efficiency of healthcare. Big Data in healthcare need to expanded and explore utilize big data analytics to gain valuable knowledge. Big data analytics is used to catch value any information from all kinds of sources in healthcare that can be used to gain information for the purpose of better decision making in healthcare. Big data analytics in healthcare has the prospect to increased healthcare by discovering decision tree and understanding formats and trends in medical record data. Cardiovascular illness datasets is big data in healthcare which is one or others resources in the health sector and is used as part of facilitating the process of documenting medical records that must be analysed to offer an effective solution to solve problems in healthcare. This paper provides valuable information by using big data analytics from medical data cardiovascular disease to provide effective solutions for the problems in healthcare and also provide how important big data for healthcare is.",2021,
95,10.1109/IWCMC.2016.7577067,BIG VEHICULAR TRAFFIC DATA MINING: TOWARDS ACCIDENT AND CONGESTION PREVENTION,"In 2013, 32,719 people died in traffic crashes in the USA. Almost 90 people on average lose their lives every day and more than 250 are injured every hour. Road safety could be enhanced by decreasing the traffic crashes. Traffic crashes cause traffic congestion as well, which has become unbearable, especially in mega-cities In addition, direct and indirect loss from traffic congestion only is over $124 billion. The existence of the Big Data of traffic crashes, as well as the availability of Big Data analytics tools can help us gain useful insights to enhance road safety and decrease traffic crashes. In this paper we use H2O and WEKA mining tools. We apply the feature selection techniques to find the most important predictors. In addition, we tackle the problem of class imbalance by employing bagging and using different quality measures. Furthermore, we evaluate the performance of five classifiers to: (1) Conduct Big Data analysis on a big traffic accidents dataset of 146322 examples, find useful insight and patterns from the data, and forecast possible accidents in advance (2) Conduct Big Data analysis on a big vehicular casualties dataset of 194477 examples, to study the driver's behavior on the road. From the driver's behavior mining we can predict the driver age, sex as well as the accident severity. The aforementioned analyses, can be used by decision makers and practitioners to develop new traffic rules and policies, in order to prevent accidents, and increase roadway safety.",2016,23766506
96,10.1109/TETC.2018.2869458,"MULTILEVEL GRAPH-BASED DECISION MAKING IN BIG SCHOLARLY DATA: AN APPROACH TO IDENTIFY EXPERT REVIEWER, FINDING QUALITY IMPACT FACTOR, RANKING JOURNALS AND RESEARCHERS","Digital libraries, such as conference papers, journal documents, books and thesis, research patents, and experiments generate a vast amount of data, named as, Scholarly Big Data. It covers scholarly related information for both researcher's perspective as well as publisher's perspective, such as academic activities, author's demography, academic social networks, etc. The relationships among Big Scholarly Data can be worthy of solving researcher as well as journal related concerns, if they are prudently treated to extract knowledge. The best approach to efficiently process these relationships is the graph. However, with the rapid growth in the number of digital articles by various libraries, the relationships raise exponentially, generating large graphs, which have become increasingly challenging to be handled in order to analyze scholarly information. On the other hand, many researchers and publishers/journals have severe concerns about the ranking control mechanisms and the consideration of quantity rather than quality. Therefore, in this paper, we proposed graph-based mechanisms to perform four critical decisions that are the need of the today's scholarly community. To improve the quality of the article, we proposed a mechanism for selecting and recommending suitable reviewers for a submitted paper based on researchers' expertise and their popularity in that particular field while avoiding conflict of interest. Also, due to shortcomings in the existing journal ranking approaches, we also designed a journal ranking mechanism including its new impact factor and relative ranking by using a modified version of traditional page ranking algorithm and excluding self-authors citations as well as self-journal citations. Similarly, researchers ranking is also important for various motives that is calculated based on the expert's field, citation count, and a number of publications while avoiding any loophole to increase the ranking such as, self-citations and wrong citations. Also, to efficiently process big graphs generated by a massive number of scholarly related relationships, we proposed an architecture that uses the parallel processing mechanism of the Hadoop ecosystem over the real-time analysis approach of Apache Spark with GraphX. Finally, the efficiency of the proposed system is evaluated in terms of processing time and throughput while implementing the designed decision mechanisms.",2021,23764562
97,10.1109/ICTON.2013.6602860,"STORE, SCHEDULE AND SWITCH - A NEW DATA DELIVERY MODEL IN THE BIG DATA ERA","The big data era is posing unprecedented challenges on the existing network infrastructure. In today's networks, data are transferred across the network as a combination of a series of packets, delivered one by one, without considering the data in their entirety with respective service level requirements. The so called elephant data, which may be less sensitive to transfer delay, compete precious network resources with mice data, in most cases from interactive and delay sensitive applications. Consequently, the Quality of Service (QoS) of interactive applications is hard to provision, and the utility of network is low. We propose a new data transfer model to complement the existing per-packet forwarding paradigm. In the new data transfer model, a service level requirement is assigned (by the data source) to each big data transfer request. Instead of transferring these data on per-packet bases immediately upon entering the network, the network stores the data until it find necessary, or enough network resource is available for that transfer. The scheduled data delivery is realized through the use of dynamic circuit switching. We also present some preliminary simulation results of SSS networks.",2013,21627339
98,10.1109/BigData52589.2021.9671765,IMBAL-OL: ONLINE MACHINE LEARNING FROM IMBALANCED DATA STREAMS IN REAL-WORLD IOT,"Typically a Neural Networks (NN) is trained on data centers using historic datasets, then a C source file (model as a char array) of the trained model is generated and flashed on IoT devices. This standard process impedes the flexibility of billions of deployed ML-powered devices as they cannot learn unseen/fresh data patterns (static intelligence) and are impossible to adapt to dynamic scenarios. Currently, to address this issue, Online Machine Learning (OL) algorithms are deployed on IoT devices that provide devices the ability to locally re-train themselves -continuously updating the last few NN layers using unseen data patterns encountered after deployment.In OL, catastrophic forgetting is common when NNs are trained using non-stationary data distribution. The majority of recent work in the OL domain embraces the implicit assumption that the distribution of local training data is balanced. But the fact is, the sensor data streams in real-world IoT are severely imbalanced and temporally correlated. This paper introduces Imbal-OL, a resource-friendly technique that can be used as an OL plugin to balance the size of classes in a range of data streams. When Imbal-OL processed stream is used for OL, the models can adapt faster to changes in the stream while parallelly preventing catastrophic forgetting. Experimental evaluation of Imbal-OL using CIFAR datasets over ResNet-18 demonstrates its ability to deal with imperfect data streams, as it manages to produce high-quality models even under challenging learning settings.",2021,
99,10.1109/ICWS55610.2022.00017,A DEVSECOPS-BASED ASSURANCE PROCESS FOR BIG DATA ANALYTICS,"Today big data pipelines are increasingly adopted by service applications representing a key enabler for enterprises to compete in the global market. However, the management of non-functional aspects of the big data pipeline (e.g., security, privacy) is still in its infancy. As a consequence, while functionally appealing, the big data pipeline does not provide a transparent environment, impairing the users’ ability to evaluate its behavior. In this paper, we propose a security assurance methodology for big data pipelines grounded on the DevSecOps development paradigm to increase trustworthiness allowing reliable security and privacy by design. Our methodology models and annotates big data pipelines with non-functional requirements verified by assurance checks ensuring requirements to hold along with the pipeline lifecycle. The performance and quality of our methodology are evaluated in a real walkthrough analytics scenario.",2022,
100,10.1109/TPDS.2017.2758781,MIA: METRIC IMPORTANCE ANALYSIS FOR BIG DATA WORKLOAD CHARACTERIZATION,"Data analytics is at the foundation of both high-quality products and services in modern economies and societies. Big data workloads run on complex large-scale computing clusters, which implies significant challenges for deeply understanding and characterizing overall system performance. In general, performance is affected by many factors at multiple layers in the system stack, hence it is challenging to identify the key metrics when understanding big data workload performance. In this paper, we propose a novel workload characterization methodology using ensemble learning, called Metric Importance Analysis (MIA), to quantify the respective importance of workload metrics. By focusing on the most important metrics, MIA reduces the complexity of the analysis without losing information. Moreover, we develop the MIA-based Kiviat Plot (MKP) and Benchmark Similarity Matrix (BSM) which provide more insightful information than the traditional linkage clustering based dendrogram to visualize program behavior (dis)similarity. To demonstrate the applicability of MIA, we use it to characterize three big data benchmark suites: HiBench, CloudRank-D and SZTS. The results show that MIA is able to characterize complex big data workloads in a simple, intuitive manner, and reveal interesting insights. Moreover, through a case study, we demonstrate that tuning the configuration parameters related to the important metrics found by MIA results in higher performance improvements than through tuning the parameters related to the less important ones.",2018,21619883
101,10.1109/BigData52589.2021.9671487,A PARALLEL CHAIN MAIL APPROACH FOR SCALABLE SPATIAL DATA INTERPOLATION,"Deteriorating air quality is a growing concern that has been linked to many health-related issues. Its monitoring is a good first step to understanding the problem. However, it is not always possible to collect air quality data from every location. Various data interpolation techniques are used to assist with populating sparse maps with more context, but many of these algorithms are computationally expensive. This work introduces a three-step Chain Mail algorithm that uses kriging (without any modifications to the base algorithm) and achieves up to ×100 execution time improvement with minimal accuracy loss (relative RMSE of 3%) by running concurrent interpolation executions. This approach can be described as a multiple-step parallel interpolation algorithm that includes specific regional border data manipulation for achieving greater accuracy. It does so by interpolating geographically defined data chunks in parallel and sharing the results with their neighboring nodes to provide context and compensate for lack of knowledge of the surrounding areas. Combined with a serverless cloud architecture, this approach opens doors to interpolating large data sets in a matter of minutes while remaining cost-efficient. The effectiveness of the three-step Chain Mail approach depends on the equal point distribution among all nodes and the resolution of the parallel configuration. In general, it offers a good balance between execution speed and accuracy.",2021,
102,10.1109/BigData.2015.7364062,DATA VERACITY ESTIMATION WITH ENSEMBLING TRUTH DISCOVERY METHODS,"Estimation of data veracity is recognized as one of the grand challenges of big data. Typically, the goal of truth discovery is to determine the veracity of multi-source, conflicting data and return, as outputs, a veracity label and a confidence score for each data value, along with the trustworthiness score of each source claiming it. Although a plethora of methods has been proposed, it is unlikely a technique dominates all others across all data sets. Furthermore, the performance evaluation of the methods entirely depends on the availability of labeled ground truth data (i.e., data whose veracity has been manually checked). In the context of Big Data, acquiring the complete ground truth data is out-of-reach. In this paper, we propose an ensembling method that mitigates the two problems of method selection and ground truth data sparsity. Our approach combines the results of a set of truth discovery methods and preliminary experiments suggest that it improves the quality performance over the single methods when samples of ground truth data are used.",2015,
103,10.1109/CiSt49399.2021.9357200,TECHNOLOGY AGAINST COVID-19 A BLOCKCHAIN-BASED FRAMEWORK FOR DATA QUALITY,"The effects of COVID-19 have quickly spread around the world, testing the limits of the population and the public health sector. High demand on medical services are offset by disruptions in daily operations as hospitals struggle to function in the face of overcapacity, understaffing and information gaps. Faced with these problems, new technologies are being deployed to fight this pandemic and help medical staff governments to reduce its spread. Among these technologies, we find blockchains and Big Data which have been used in tracking, prediction applications and others. However, despite the help that these new technologies have provided, they remain limited if the data with which they are fed are not of good quality. In this paper, we highlight some benefits of using BIG Data and Blockchain to deal with this pandemic and some data quality issues that still present challenges to decision making. Finally we present a general Blockchain-based framework for data governance that aims to ensure a high level of data trust, security, and privacy.",2020,2327185X
104,10.1109/ITPRO.2014.7029280,PRESENTATION 1. INFORMATION GOVERNANCE IN THE AGE OF BIG DATA,"Organizations have understood the value of their structured data — mostly financial transactions — since the first mainframes were developed in the 40's. Data quality issues have always been a challenge and the increasing numbers of applications consuming and producing structured transactional data has grown exponentially. Unstructured information has been given less significance and strategic importance and therefore fewer resources and less attention on the part of leadership. All of that has changed and is changing faster than anyone imagined. Unstructured content is what humans produce. They create the documents: strategies, proposals, support documents, marketing content, white papers, engineering specifications, etc. that form the intelligence and core knowledge capital of the enterprise. Many organizations have left business units to fend for themselves and “go figure it out” with little guidance or support. These has led to terabytes of content that people cannot find their way through and that leaves the organization open to risks, liabilities and costs of e discovery. According to the Minnesota Journal of Law, Science & Technology, a gig of data costs $30,000 in e discovery costs. The cost of storage is ten cents. The problem is that the enterprise does not understand the hidden costs of not making data accessible and usable — lost time, lost IP, inefficiency, poor customer service that can lead to lost customers, slower growth, etc. As newer collaboration technologies are deployed, they expose the bad habits and sins of the past. Deploying a new search engine shows that the content is not curated. Standing up a new content management application like SharePoint reveals the haphazard shanty town of an information architecture with inconsistencies in models, terminology and applications. Today's landscape of marketing and customer experience technologies is complex and interconnected and requires those upstream knowledge processes that produce unstructured content as the fuel. Customer experience entails everything that happens before you purchase (marketing, education and outreach), when you purchase (e commerce with product content and data), and after you purchase (self-service systems and knowledge bases that support the call center). This is the customer lifecycle and at each step in the process systems and tools need to be harmonized as they gather information about the users using attribute models that are consistent and that serve the business and the customer. They take content and data as input and then output more data. One applications exhaust is another applications fuel. Organizations are also purchasing data streams to enrich their internal information sources. Social media is an enormous virtually untapped reservoir of data about customers and what they think about organizations. This can be mined for sentiment and to gauge marketing effectiveness. Increasingly, much of this is being placed into the hands of the marketing organization. In fact, a study by Gartner Group said that by 2017 the CMO will spend more money on IT than the CIO. What all of this means is that information, content and data governance need to be considered as part of a whole and not as separate initiatives. Elements of good information governance include: • Deployment and Operationalization • Alignment with User Needs • Business Value • Buy-In and Change Management • Sponsorship and Accountability Leads to the following outcomes: • Manages conflicts in business priorities (between initiatives, business units, drivers, etc) • Allows for ongoing input from various stakeholders and constituencies in order to evolve capabilities with the needs of the business • Prioritizes efforts and allocation of resources • Assigns roles and responsibilities with accountability to critical functions • Takes into consideration various levels of maturity in the organization — no one size fits all • Ensures that investments in systems, processes and tools are providing sufficient return to the business • Balances centralized standards with decentralized decision making • Aligns incentives to use a system with business goals This session will review governance concepts, discuss how they apply to various types of data and content and provide a framework for developing governance processes and structures.",2014,
105,10.1145/2737182.2737186,MAKING REAL TIME DATA ANALYTICS AVAILABLE AS A SERVICE,"Conducting (big) data analytics in an organization is not just about using a processing framework (e.g. Hadoop/Spark) to learn a model from data currently in a single file system (e.g. HDFS). We frequently need to pipeline real time data from other systems into the processing framework, and continually update the learned model. The processing frameworks need to be easily invokable for different purposes to produce different models. The model and the subsequent model updates need to be integrated with a product that may require a real time prediction using the latest trained model. All these need to be shared among different teams in the organization for different data analytics purposes. In this paper, we propose a real time data-analytics-as-service architecture that uses RESTful web services to wrap and integrate data services, dynamic model training services (supported by big data processing framework), prediction services and the product that uses the models. We discuss the challenges in wrapping big data processing frameworks as services and other architecturally significant factors that affect system reliability, real time performance and prediction accuracy. We evaluate our architecture using a log-driven system operation anomaly detection system where staleness of data used in model training, speed of model update and prediction are critical requirements.",2015,
106,10.1109/BigData.2017.8258503,USING BIG DATA ANALYTICS AND IOT PRINCIPLES TO KEEP AN EYE ON UNDERGROUND INFRASTRUCTURE,"A Concept Development Study by the Open Geospatial Consortium (OGC) has highlighted the importance of high-quality feature data for underground urban infrastructure (UGI). Analysis of large survey datasets, including both visual and non-visual methods, is essential for creating and maintaining UGI geodata. Connecting hidden features with diverse, high-velocity sensing streams and realistic predictive models that effectively characterize them is key to lower construction costs, efficient infrastructure operation, sound disaster preparedness, and new smart city services. IoT principles that combine OGC geodata and Sensor Web observation standards may offer the best chance for working towards functional “digital twins” of such hidden infrastructure that are both cost effective and scalable with the increasing complexity and instrumentation of the underground built environment. Technical and policy challenges remain, however, before this can be achieved.",2017,
107,10.1109/JPROC.2017.2730585,A NOVEL METHODOLOGY TO LABEL URBAN REMOTE SENSING IMAGES BASED ON LOCATION-BASED SOCIAL MEDIA PHOTOS,"With the rapid development of the internet and popularization of intelligent mobile devices, social media is evolving fast and contains rich spatial information, such as geolocated posts, tweets, photos, video, and audio. Those location-based social media data have offered new opportunities for hazards and disaster identification or tracking, recommendations for locations, friends or tags, pay-per-click advertising, etc. Meanwhile, a massive amount of remote sensing (RS) data can be easily acquired in both high temporal and spatial resolution with a multiple satellite system, if RS maps can be provided, to possibly enable the monitoring of our location-based living environments with some devices like charge-coupled device (CCD) cameras but on a much larger scale. To generate the classification maps, usually, labeled RS image pixels should be provided by RS experts to train a classification system. Traditionally, labeled samples are obtained according to ground surveys, image photo interpretation or a combination of the aforementioned strategies. All the strategies should be taken care of by domain experts, in a means which is costly, time consuming, and sometimes of a low quality due to reasons such as photo interpretation based on RS images only. These practices and constraints make it more challenging to classify land-cover RS images using big RS data. In this paper, a new methodology is proposed to classify urban RS images by exploiting the semantics of location-based social media photos (SMPs). To validate the effectiveness of this methodology, an automatic classification system is developed based on RS images as well as SMPs via big data analysis techniques including active learning, crowdsourcing, shallow machine learning, and deep learning. As the labels of RS training data are given by ordinary people with a crowdsourcing technique, the developed system is named Crowd4RS. The quantitative and qualitative experiments confirm the effectiveness of the proposed Crowd4RS system as well as the proposed methodology for automatically generating RS image maps in terms of classification results based on big RS data made up of multispectral RS images in a high spatial resolution and a large amount of photos from social media sites, such as Flickr and Panoramio.",2017,15582256
108,10.1109/COASE.2017.8256208,A NONPARAMETRIC ADAPTIVE SAMPLING STRATEGY FOR ONLINE MONITORING OF BIG DATA STREAMS,"With the rapid development of sensor techniques, we often face the challenges of monitoring big data streams in modern quality control, which consist of massive series of real-time, continuously and sequentially ordered observations. For example, in manufacturing industries, hundreds or thousands of variables are observed during online production for quality insurance. Also, smart grid infrastructure needs to simultaneously monitor massive access points for intrusion and threat detection. As another example, an image sensing device continuously collects high-resolution images at high frequency for video surveillance and object movement tracking. Ideally, in those applications, it is preferable to detect assignable causes as early as possible, while maintaining a prespecified in-control Average Run Length (ARL).",2017,21618089
109,10.1109/ICITBS.2018.00077,DATA MINING METHOD AND EMPIRICAL RESEARCH FOR EXTENSION ARCHITECTURE DESIGN,"For extension data mining of architecture design, the process and method to convert extension architectural design data to structural data with high quality are discussed in this article. In special knowledge mining of extension architectural design data, the problems of architectural space design, form design, extension knowledge mining for structural design are studied according to cross-industry standard process for data mining(CRISP-DM). They are integrated with operations of extenics, geometry, statistical theory to provide scientific and reasonable design. The results of case study show that our scheme can effectively convert extension architectural design data to computable structural data with unified format and high reliability.",2018,
110,10.1109/ACCESS.2019.2941898,"BIG DATA FEATURES, APPLICATIONS, AND ANALYTICS IN CARDIOLOGY—A SYSTEMATIC LITERATURE REVIEW","In today's digital world the information surges with the widespread use of the internet and global communication systems. Healthcare systems are also facing digital transformations with the enhancement in the utilization of healthcare information systems, electronic records in medical, wearable, smart devices, handheld devices, and so on. A bulk of data is produced from these digital transformations. The recent increase in medical big data and the development of computational techniques in the field of cardiology enables researchers and practitioners to extract and visualize medical big data in a new spectrum. The role of medical big data in cardiology becomes a challenging task. Early decision making in cardiac healthcare system has massive potential for dropping the cost of care, refining quality of care, and reducing waste and error. Therefore, to facilitate this process a detailed report of the existing literature will be feasible to help the doctors and practitioners in decision making for the purpose of identifying and treating cardiac diseases. This detailed study will summarize results from the existing literature on big data in the field cardiac disease. This research uses the systematic literature protocol as presented by Kitchenham et al. The data was collected from the published materials from 2008 to 2018 as conference or journal publications, books, magazines and other online sources. 190 papers were included relying on the defined inclusion, exclusion, and checking the quality criteria. The current study helped to identify medical big data features, the application of medical big data, and the analytics of the big data in cardiology. The results of the proposed research shows that several studies exist that are associated to medical big data specifically to cardiology. This research summarizes and organizes the existing literature based on the defined keywords and research questions. The analysis will help doctors to make more authentic decisions, which ultimately will help to use the study as evidence for treating patients with heart related diseases.",2019,21693536
111,10.1109/TCC.2018.2874944,OPTIMIZING QUALITY-AWARE BIG DATA APPLICATIONS IN THE CLOUD,"The last years witnessed a steep rise in data generation worldwide and, consequently, the widespread adoption of software solutions able to support data-intensive application. Competitiveness and innovation have strongly benefited from these new platforms and methodologies, and there is a great deal of interest around the new possibilities that Big Data analytics promise to make reality. Many companies currently engage in data-intensive processes as part of their core businesses; however, fully embracing the data-driven paradigm is still cumbersome, and establishing a production-ready, fine-tuned deployment is time-consuming, expensive, and resource-intensive. This situation calls for innovative models and techniques to streamline the process of deployment configuration for Big Data applications. In particular, the focus in this paper is on the rightsizing of Cloud deployed clusters, which represent a cost-effective alternative to installation on premises. This paper proposes a novel tool, integrated in a wider DevOps-inspired approach, implementing a parallel and distributed simulation-optimization technique that efficiently and effectively explores the space of alternative Cloud configurations, seeking the minimum cost deployment that satisfies quality of service constraints. The soundness of the proposed solution has been thoroughly validated in a vast experimental campaign encompassing different applications and Big Data platforms.",2021,23720018
112,10.1109/CBFD52659.2021.00083,RESEARCH ON CREDIT RISK CONTROL OF COMMERCIAL BANKS BASED ON DATA MINING TECHNOLOGY,"Credit risk is the most important part of commercial banks' risk management, risk management is the core issue of banks' financial management, and the good or bad credit risk management directly affects banks' efficiency and is one of the effective means to avoid non-performing loans. Under the various business development and promotion of banks, their non-performing loan rate is also growing year by year, which not only affects the rapid and healthy development of banks, but even leads to the expansion of the scale of bank liabilities. In the era of big data, the importance of proper utilization of data has gradually emerged, which brings challenges and opportunities to commercial banks, and the ability to make good use of big data has become the key to risk management for commercial banks. Data mining is a cross-disciplinary field that brings together techniques and methods from databases, statistics, machine learning and other fields. It can uncover potentially useful information and knowledge from a large amount of banking data and provide managers with effective information for decision making, which in turn can prevent and manage risks more effectively. Based on data mining technology, this paper investigates bank credit risk management methods to effectively solve the credit risk control problems of commercial banks, optimize the credit project process, improve the quality of credit delivery, and provide reference for credit risk management related personnel.",2021,
113,10.1109/TSC.2020.2988760,ADVANCING NON-NEGATIVE LATENT FACTORIZATION OF TENSORS WITH DIVERSIFIED REGULARIZATION SCHEMES,"Dynamic relationships are frequently encountered in big data and services computing-related applications, like dynamic data of user-side QoS in Web services. They are modeled into a high-dimensional and sparse (HiDS) tensor, which contain rich knowledge regarding temporal patterns. A non-negative latent factorization of tensors (NLFT) model is very effective in extracting such patterns from an HiDS tensor. However, it commonly suffers from overfitting with improper regularization schemes. To address this issue, this article investigates NLFT models with diversified regularization schemes. Six regularized NLFT models, i.e., $L_{2}, L_{1}$L2,L1, elastic net, log, dropout, and swish-regularized ones, are proposed and carefully investigated. Moreover, owing to their diversified regularization designs, they possess strong model diversity to achieve an effective ensemble. Empirical studies on HiDS QoS tensors from real applications demonstrate that compared with state-of-the-art models, the proposed ones better describe the temporal patterns hidden in an HiDS tensor, thereby achieving significantly higher prediction accuracy for missing data. Moreover, their ensemble further outperforms each of them in terms of prediction accuracy for missing QoS data.",2022,23720204
114,10.1109/BigData50022.2020.9378228,MAINTAINING NOSQL DATABASE QUALITY DURING CONCEPTUAL MODEL EVOLUTION,"Database schemas evolve over time to satisfy changing application requirements. If this evolution is not performed correctly, some quality attributes are at risk such as data integrity, functional correctness, or maintainability. To help developer teams in the design of database schemas, several design methodologies for NoSQL databases have proposed to use conceptual models during this process. The use of an explicit conceptual model can also help developers in the tasks of schema evolution. In this work-in-progress paper, we propose a framework that, given a change in the conceptual model, identifies what must be modified in a NoSQL database schema and the underlying data. We researched several open source projects that use Apache Cassandra to study the benefits of using a conceptual model during the schema evolution process as well as to understand how these models evolve. In this first work, we have focused on studying seven types of conceptual model changes identified in these projects. For each change we describe the transformation required in the database schema to maintain the consistency between the schema and the model as well as the migration of data required to the new schema version.",2020,
115,10.1109/TBDATA.2018.2840222,CITYLINES: DESIGNING HYBRID HUB-AND-SPOKE TRANSIT SYSTEM WITH URBAN BIG DATA,"Rapid urbanization has posed significant burden on urban transportation infrastructures. In today's cities, both private and public transits have clear limitations to fulfill passengers' needs for quality of experience (QoE): Public transits operate along fixed routes with long wait time and total transit time; Private transits, such as taxis, private shuttles and ride-hailing services, provide point-to-point transits with high trip fare. In this paper, we propose CityLines, a transformative urban transit system, employing hybrid hub-and-spoke transit model with shared shuttles. Analogous to Airlines services, the proposed CityLines system routes urban trips among spokes through a few hubs or direct paths, with travel time as short as private transits and fare as low as public transits. CityLines allows both point-to-point connection to improve the passenger QoE, and hub-and-spoke connection to reduce the system operation cost. To evaluate the performance of CityLines, we conduct extensive data-driven experiments using one-month real-world trip demand data (from taxis, buses and subway trains) collected from Shenzhen, China. The results demonstrate that CityLines reduces 12.5-44 percent average travel time, and aggregates 8.5-32.6 percent more trips with ride-sharing over other implementation baselines.",2019,23722096
116,10.1109/ICAICA50127.2020.9182621,DESIGN AND DEVELOPMENT OF SELF-ADAPTIVE LEARNING SYSTEM BASED ON DATA ANALYSIS,"In the era of rapid development of artificial intelligence and big data technology, the education departments are also experiencing the transformation from the traditional classroom to the blending learning mode mixed with online learning and offline classroom. In order to promote the modernization of education and realize the inclusiveness and individuation of future education, the self-adaptive learning platform is rising gradually, which emphasizes service orientation and provides personalized learning contents to students based on data collections and analyses, enhancing the pertinence and efficiency of students' learning, and improving students' learning experience. In this paper, the writer applies to big data and artificial intelligence technology into the educational environment and attempts to build an intelligent self-adaptive learning system to realize personalized teaching model for the purpose of serving for teachers and students. Besides, the writer also carries out some innovative explorations on personalized teaching based on learner-centered and improves teaching qualities with remindings and early warnings.",2020,
117,10.1109/MNET.011.1900393,EDGE QOE: INTELLIGENT BIG DATA CACHING VIA DEEP REINFORCEMENT LEARNING,"In mobile edge networks (MENs), big data caching services are expected to provide mobile users with better quality of experience (QoE) than normal scenarios. However, the increasing types of sensors and devices are producing an explosion of big data. Extracting valuable contents for caching is becoming a vital issue for the satisfaction of QoE. Therefore, it is urgent to propose some rational strategies to improve QoE, which is the major challenge for content-centric caching. This article introduces a novel big data architecture consisting of data management units for content extraction and caching decision, improving quality of service and ensuring QoE. Then a caching strategy is proposed to improve QoE, including three parts: (1) the caching location decision, which means the method of deploying caching nodes to make them closer to users; (2) caching capacity assessment, which aims to seek suitable contents to match the capacity of caching nodes; and (3) caching priority choice, which leads to contents being cached according to their priority to meet user demands. With this architecture and strategy, we particularly use a caching algorithm based on deep reinforcement learning to achieve lower cost for intelligent caching. Experimental results indicate that our schemes achieve higher QoE than existing algorithms.",2020,1558156X
118,10.1109/NPSC.2018.8771733,QUALITY ASSESSMENT OF SMART GRID DATA,"Enormous amount of data gets generated in the Smart Grids (SGs) due to the large number of measuring devices, higher measurement rates and various types of sensors. Smart grid data contains important and critical information about the grid. Data driven applications are being developed for better planning, monitoring and operation of SGs. The outcome of data analytics heavily depends on the quality of SG data. However, not much work has been reported on the quality assessment of SG data. This paper addresses the objective assessment of SG data quality. Various dimensions of SG data quality are identified in this paper. Mathematical formulations are proposed to quantify the SG data quality. Proposed data quality metrics have been applied on the SCADA and PMU measurements collected from the Southern Regional Grid of India to demonstrate their effectiveness.",2018,
119,10.1109/BigData.2016.7840920,SPARK-BASED RARE ASSOCIATION RULE MINING FOR BIG DATASETS,"Nowadays, the quality of wireless network becomes critical for the network service providers (NSP). A poor performed network may lead to customer complaints, even loss of revenue from user churns. To ensure the quality of service, the key quality indicators (KQI) which reflect the quality of specific use cases have been collected alongside the network performance counters (NPC) for network performance analysis. To start, the NSP mines the network performance data to discover KQI anomalies which may cause poor user experience. If there is any KQI anomaly has been detected, the NSP investigates the associated NPCs to identify the possible root causes. Since the number of use cases increases dramatically and the volume of collected network performance data grows tremendously everyday, the wireless network anomaly root cause analysis becomes extremely challenging. How to efficiently discover the relationship between NPCs and KQI outliers becomes the key to identify the root causes of anomalies in the wireless network. To solve this problem, in this paper, we propose an efficient rare association rule mining algorithm called Spark-based Rare Association Rule Mining (SRAM) which leverages not only the efficiency of FP-growth algorithm but also the powerful big data processing mechanism of Spark platform. We have implemented our algorithm on the Spark platform and tested with various of data sets. The result shows our method can efficiently mine rare association rules from big volume of data.",2016,
120,10.1109/BigDataCongress.2015.50,BLOOD PRESSURE MANAGEMENT WITH DATA CAPTURING IN THE CLOUD AMONG HYPERTENSIVE PATIENTS: A MONITORING PLATFORM FOR HYPERTENSIVE PATIENTS,"Hypertension is a significant modifiable risk factor for cardiovascular and kidney disease, and blood pressure (BP) control is a very important step for cardiovascular risk management. Recently, home telemonitoring BP has been suggested as an effective tool for BP control and been commonly used in Western countries. Application of technology for healthcare management becomes a trend. Health data is usually longitudinal and voluminous, an effective data management would improve the quality of healthcare service. In order to deal with the volume, variety and velocity of medical data, cloud technology has opened a new horizon, especially data for medical research. Boosting the current home telemonitoring BP system with an automatic data capturing cloud technology along with healthcare provider alert function would be a pioneer. In this study, a cloud-connected personal-based BP meter will be transformed to a research-based BP data capturing cloud platform and will observe daily use of BP measurement and upload data to the cloud through a USB hub and internet-connected personal computer. All personal identity can be decoded and a study identity number will be assigned to each user for data privacy protection. The cloud platform enables easy access for different parties from anywhere, high speed performance, strong infrastructure support and vigorous data analysis power.",2015,23797703
121,10.1109/QRS-C51114.2020.00096,A BIG DATA BASED DECISION FRAMEWORK FOR PUBLIC MANAGEMENT AND SERVICE IN TOURISM,"In view of the characteristics of the complexity of public management and service content, the universality of service objects and the diversity of demands faced by current tourism industry development into a new stage of popularisation and industrialisation, this paper proposes a big data driven decision-making model to innovate tourism public management and service, and discusses the connotation, decision-making and implementation process under this decision-making mode. Through the construction of tourism public management and service framework based on big data, this paper discusses the elements, environment characteristics and promotion mode of the framework operation. The mode of tourism public management and service are reformed with decision-making and management based on big data. The problems solution efficiency, quality and services in current tourism industry are improved. Further, tourism public service in the sustainable development in tourism industry worldwide is promoted.",2020,
122,10.1109/SERVICES.2014.84,CLOUD COMPUTING FOR BIG DATA ENTREPRENEURSHIP IN THE SUPPLY CHAIN: USING SAP HANA FOR PHARMACEUTICAL TRACK-AND-TRACE ANALYTICS,"This paper focuses on the real-world use of cloud-based analytics for supporting medication track-and-trace and monitoring safety and quality in the pharmaceutical supply chain. The paper describes a track-and-trace system developed by German start-up XQS-Service GmbH and adopted by manufacturers, wholesalers, pharmacies and clinics in Germany. The paper also discusses the benefits of using in-memory computing to process large amounts of big data to support efficient and effective medication track-and-trace.",2014,23783818
123,10.1109/ICMEIM51375.2020.00062,ANALYSES OF THE RESEARCH STATUS QUO OF XI JINPING'S THOUGHT ON ECOLOGICAL CIVILIZATION BASED ON BIG DATA METHOD,"Xi Jinping's thought on ecological civilization, as the guiding ideology of China's ecological civilization construction in the new era, has become one of the hot spots in the academic research. Applying the big data method with the help of VOSviewer and SmartAnalyze, the paper has performed a series of analyses of knowledge graph, including the annual publication trends, the subject term expression changes, the subject distribution, the authors co-occurrence and the keywords co-occurrence on 335 published papers from core journals and CSSCI periodicals database regarding the theme of Xi Jinping's thought on ecological civilization. By analyzing the knowledge map, we find that the quantity and quality of research literatures have the tendency of growth, and subject terms become more and more standardized, but the research perspective needs to be broadened. There are more and more exchanges and cooperation between the authors, but in low frequency. Their topics mainly centre around the five aspects of value, connotation, background, path and theory origin of Xi jinping's thought on ecological civilization.",2020,
124,10.1109/ITCA52113.2020.00145,"RESEARCH ON CHINESE INDIVIDUALIZED READING TEACHING BASED ON ""BIG DATA"" EDUCATION PLATFORM","In the practice of Chinese teaching, reading teaching is a very core content. Reading teaching can scientifically and comprehensively improve students' reading literacy. In order to effectively optimize the reading ability of students, teachers should actively and comprehensively carry out personalized reading teaching. At the same time, teachers also need to rely on the ""big data"" education platform to effectively and efficiently improve the overall effectiveness and level of personalized reading, guarantee the quality of students' reading in an all-round way, and better promote the growth and development of students.",2020,
125,10.1145/3510457.3513034,AN EMPIRICAL STUDY ON QUALITY ISSUES OF EBAY'S BIG DATA SQL ANALYTICS PLATFORM,"Big data SQL analytics platform has evolved as the key infrastructure for business data analysis. Compared with traditional costly commercial RDBMS, scalable solutions with open-source projects, such as SQL-on-Hadoop, are more popular and attractive to enter-prises. In eBay, we build Carmel, a company-wide interactive SQL analytics platform based on Apache Spark. Carmel has been serving thousands of customers from hundreds of teams globally for more than 3 years. Meanwhile, despite the popularity of open-source based big data SQL analytics platforms, few empirical studies on service quality issues (e.g., job failure) were carried out for them. However, a deep understanding of service quality issues and taking right mitigation are significant to the ease of manual maintenance efforts. To fill this gap, we conduct a comprehensive empirical study on 1,884 real-word service quality issues from Carmel. We summa-rize the common symptoms and identify the root causes with typical cases. Stakeholders including system developers, researchers, and platform maintainers can benefit from our findings and implications. Furthermore, we also present lessons learned from critical cases in our daily practice, as well as insights to motivate automatic tool support and future research directions.",2022,
126,10.1109/BigData52589.2021.9671971,CROWDQUAKE+: DATA-DRIVEN EARTHQUAKE EARLY WARNING VIA IOT AND DEEP LEARNING,"In recent years, a low-cost micro-electro-mechanical systems (MEMS) acceleration sensor has been widely used for earthquake early warning (EEW). In our previous work, we introduced a networked earthquake detection system, CrowdQuake with three-hundred smartphones’ acceleration sensors and a deep-learning based earthquake detection model. For one year’s operation, CrowdQuake detected a series of earthquakes and collected various earthquake and non-earthquake data. Based on the successful operation of CrowdQuake, in this paper, we discuss how it can be expanded across the country by addressing the following challenges: (1) sensor deployments for highly dense network, (2) earthquake detection performance using a deep learning model, and (3) high performance and scalable system design for big data processing. The improved system is CrowdQuake+ which can deal with acceleration data sent from 8,000 IoT sensors and detect an earthquake in few seconds using a newly proposed detection model. Moreover, CrowdQuake+ stores all acceleration data sent from sensors and assesses their qualities by calculating noise levels. Then, the collected data are used for deep learning model training, so that its detection performance becomes more accurate.",2021,
127,10.1109/FiCloud.2015.34,OPEN GOVERNMENT DATA AS A SERVICE (GODAAS): BIG DATA PLATFORM FOR MOBILE APP DEVELOPERS,"The next web of open and linked data leverages governmental data openness to improve the quality of social services. This data is a national asset. In this study, we elaborate on this emerging open government movement, together with the underlying data transparency to drive novel business models which utilize these assets under a functioning platform called Open Government Data as a Service (GoDaaS). These business models actively engage civic-minded programmers in developing sustainable applications, contextualizing and utilizing the government open data resources. This leads to an expansive government marketplace, with many civic-minded developers might be new to doing business with the federal or state government. By means of a consultation service prototype, we provide development advices for programmers on how to work out the specific details of their applications business model. Having the business models in focus, this study also proposes a novel abstraction unit called Gov. Data Compute Unit (DCU), so that governments are able to feed developers with formalized, structured and programmable data resource units rather than just data catalogs. Such DCUs enable developers to cope with an increasing heterogeneity of state government data sets, by providing a unified interface on top of diverse data schemata from various states.",2015,
128,10.1109/ACCESS.2019.2963283,SEAPORT DATA SPACE FOR IMPROVING LOGISTIC MARITIME OPERATIONS,"The maritime industry expects several improvements to efficiently manage the operation processes by introducing Industry 4.0 enabling technologies. Seaports are the most critical point in the maritime logistics chain because of its multimodal and complex nature. Consequently, coordinated communication among any seaport stakeholders is vital to improving their operations. Currently, Electronic Data Interchange (EDI) and Port Community Systems (PCS), as primary enablers of digital seaports, have demonstrated their limitations to interchange information on time, accurately, efficiently, and securely, causing high operation costs, low resource management, and low performance. For these reasons, this contribution presents the Seaport Data Space (SDS) based on the Industrial Data Space (IDS) reference architecture model to enable a secure data sharing space and promote an intelligent transport multimodal terminal. Each seaport stakeholders implements the IDS connector to take part in the SDS and share their data. On top of SDS, a Big Data architecture is integrated to manage the massive data shared in the SDS and extract useful information to improve the decision-making. The architecture has been evaluated by enabling a port authority and a container terminal to share its data with a shipping company. As a result, several Key Performance Indicators (KPIs) have been developed by using the Big Data architecture functionalities. The KPIs have been shown in a dashboard to allow easy interpretability of results for planning vessel operations. The SDS environment may improve the communication between stakeholders by reducing the transaction costs, enhancing the quality of information, and exhibiting effectiveness.",2020,21693536
129,10.1109/ACCESS.2019.2930004,FREQUENT PATTERN MINING ON TIME AND LOCATION AWARE AIR QUALITY DATA,"With the advent of big data era, enormous volumes of data are generated every second. Varied data processing algorithms and architectures have been proposed in the past to achieve better execution of data mining algorithms. One such algorithm is extracting most frequently occurring patterns from the transactional database. Dependency of transactions on time and location further makes frequent itemset mining task more complex. The present work targets to identify and extract the frequent patterns from such time and location-aware transactional data. Primarily, the spatio-temporal dependency of air quality data is leveraged to find out frequently co-occurring pollutants over several locations of Delhi, the capital city of India. Varied approaches have been proposed in the past to extract frequent patterns efficiently, but this work suggests a generalized approach that can be applied to any numeric spatio-temporal transactional data, including air quality data. Furthermore, a comprehensive description of the algorithm along with a sample running example on air quality dataset is shown in this work. A detailed experimental evaluation is carried out on the synthetically generated datasets, benchmark datasets, and real world datasets. Furthermore, a comparison with spatio-temporal apriori as well as the other state-of-the-art non-apriori-based algorithms is shown. Results suggest that the proposed algorithm outperformed the existing approaches in terms of execution time of algorithm and memory resources.",2019,21693536
130,10.1109/ICITBS.2018.00091,RESEARCH ON APPLICATION OF DATA MINING IN VIRTUAL COMMUNITY OF FOREIGN LANGUAGE LEARNING,"The construction of virtual community in foreign language learning is a comprehensive foreign language learning environment integrated with foreign language vocabulary database construction and vocabulary retrieval, combining the virtual reality technology to construct the language environment of foreign language learning. The virtual community of foreign language learning can improve the sense of language authenticity in foreign language learning and improve the quality of foreign language teaching. A method of building a virtual community for foreign language learning is proposed based on data mining technology, data acquisition and feature preprocessing model for building semantic vocabulary of foreign language learning is constructed, the linguistic environment characteristics of the semantic vocabulary data of foreign language learning is analyzed, and the semantic noumenon structure model is obtained. Fuzzy clustering method is used for vocabulary clustering and comprehensive retrieval in the virtual community of foreign language learning, the performance of vocabulary classification in foreign language learning is improved, the adaptive semantic information fusion method is used to realize the vocabulary data mining in the virtual community of foreign language learning, information retrieval and access scheduling for virtual communities in foreign language learning are realized based on data mining results. The simulation results show that the accuracy of foreign language vocabulary retrieval is good, improve the efficiency of foreign language learning.",2018,
131,10.1109/PRDC.2015.41,A SURVEY ON DATA QUALITY: CLASSIFYING POOR DATA,"Data is part of our everyday life and an essential asset in numerous businesses and organizations. The quality of the data, i.e., the degree to which the data characteristics fulfill requirements, can have a tremendous impact on the businesses themselves, the companies, or even in human lives. In fact, research and industry reports show that huge amounts of capital are spent to improve the quality of the data being used in many systems, sometimes even only to understand the quality of the information in use. Considering the variety of dimensions, characteristics, business views, or simply the specificities of the systems being evaluated, understanding how to measure data quality can be an extremely difficult task. In this paper we survey the state of the art in classification of poor data, including the definition of dimensions and specific data problems, we identify frequently used dimensions and map data quality problems to the identified dimensions. The huge variety of terms and definitions found suggests that further standardization efforts are required. Also, data quality research on Big Data appears to be in its initial steps, leaving open space for further research.",2015,
132,10.1109/BigData.2014.7004497,"LANGUAGE, CULTURAL INFLUENCES AND INTELLIGENCE IN HISTORICAL GAZETTEERS OF THE GREAT WAR","Historical gazetteers trace locations that have been long forgotten while allowing for the cross-referencing of locations across different documents. In this work, we present the problem of managing a gazetteer of geometries, features and names during the Great War on the Western Front. The careful tracking of provenance information and the novel use of existing semantic web standards allows for the discovery of both the quality of the cartographic work done by both sides and the cultural influences between belligerents.",2014,
133,10.1109/BigData.2015.7364061,CROWDMD: CROWDSOURCING-BASED APPROACH FOR DEDUPLICATION,"Matching dependencies (MDs) were recently introduced as quality rules for data cleaning and entity resolution. They are rules that specify what values should be considered duplicates, and have to be matched. Defining such quality rules on a database instance, is a very expensive and a time consuming process, and requires huge efforts to analyse the whole database. In this demo paper, we present CrowdMD, a hybrid machine-crowd system for generating MDs. It first asks the crowd to determine whether a given pair, from training sample pairs, match or not. Then, it uses data mining techniques to generate attributes constituting an MD. Using a Restaurant database, we will show how the crowders can help to generate MDs by labelling the training sample through the CrowdMD user interface and how MDs can be mined from this training set.",2015,
134,10.1109/AICCSA47632.2019.9035250,ASSESSING CONTEXT-AWARE DATA CONSISTENCY,"Data analysis is a demanding task that involves extracting deep insights hidden in data. Many businesses enforce data analysis irrespective of the domain, as it is crucial in minimizing developmental risks. Raw data cannot be used to perform any analysis as poor-quality data leads to erroneous decision-making. This makes data quality assessment a necessary function before data analysis. Data quality is a multi-dimensional factor that affects the analysis in multiple ways. Among all the dimensions, consistency is one of the most critical dimensions to assess. Context of data plays an important role in consistency assessment, as the records are inherently related within a dataset. Existing studies are computationally expensive and do not consider the context of data. In this paper, we propose a comprehensive context-aware data consistency assessment tool that uses machine learning to evaluate the consistency of data. Our model was developed on Apache Hadoop and Apache Spark to support big data, as well as to boost some computationally intensive algorithms.",2019,21615322
135,10.1109/ISMSIT52890.2021.9604548,DATA TRANSFORMATION FROM SQL TO NOSQL MONGODB BASED ON R PROGRAMMING LANGUAGE,"Owing to their high availability and scalability, NoSQL databases are becoming more popular for Big data applications in web analytics and supporting large websites. Moreover, each NoSQL system has its API which does not support industry standards like SQL and JDBC, integrating these systems with other enterprise and reporting software takes more time. The main requirements of Big data and data analytics are transforming the data from SQL databases to NoSQL data structures to represent the data. In this work, we presented a method to transform the data from different types of SQL databases to the desired NoSQL database based on the R programming language. The proposed work is based on the R environment used to handle the data from the source system to the target databases and meet the data quality requirements in data transformation. The results confirmed that the development provided a good solution for the data transformation from SQL to NoSQL by taking into account the data quality requirements.",2021,
136,10.1109/SusTech.2015.7314320,A DATA-VALUE-DRIVEN ADAPTATION FRAMEWORK FOR ENERGY EFFICIENCY FOR DATA INTENSIVE APPLICATIONS IN CLOUDS,"The emerging of cloud computing and Big Data has been presenting to the world both grand opportunities and challenges. However, the increasing trend in energy consumption in clouds due to the fast growing quantity of data to be transmitted and processed has made cloud computing, together with Big Data phenomenon, becoming the dominant contributor in energy consumption, and consequently in CO2 emission. In this paper, we propose an adaptation framework for data-intensive applications aiming to improve energy efficiency. The adaptation mechanism is driven by the data value extracted from datasets or data streams of the applications. Our main contribution lies in the proposal of treating large amount of data according to their value, i.e., their level of importance.",2015,
137,10.1109/BigData.2017.8258391,EXPLAINABLE DATA-DRIVEN MODELING OF PATIENT SATISFACTION SURVEY DATA,"In the personalized patient-centered healthcare, self-reported patient satisfaction survey data plays an important role. Given the patient survey data, it is necessary to identify the drivers of patient satisfaction and explain them so that such patterns can be used in future as well as necessary corrective actions can be taken. In healthcare, both accuracy and interpretability are important criteria for choosing a reliable predictive model for analyzing patient data. Usually, complex models such as Random Forest, neural networks can achieve high prediction accuracy but lack necessary interpretation to their prediction results. In this paper, we address this problem by proposing a local explanation method to interpret complex model prediction results. First, we build a predictive model using Random Forest to fit the patient satisfaction data. Second, we utilize local explanation method to provide insights into the Random Forest prediction results so as to discover true reasons behind patient experiences and overall ratings. Specifically, our approach allows us to interpret patient's overall rating of a hospital at the individual level, and find out the set of the most influential factors for each patient. We focus on all unhappy patients to investigate the top reasons for patient dissatisfaction. Our approach and findings will help to establish guidelines for a quality healthcare.",2017,
138,10.1109/TNNLS.2019.2958184,HIERARCHICAL QUALITY MONITORING FOR LARGE-SCALE INDUSTRIAL PLANTS WITH BIG PROCESS DATA,"For large-scale industrial plants, quality-related process monitoring is challenging because of the complex features of multiunit, multimode, high-dimension data. Hence, a hierarchical quality monitoring (HQM) algorithm based on the distributed parallel semisupervised Gaussian mixture model (dp-S2GMM) is proposed in this article. In HQM, a large-scale process is first decomposed into a group of unit blocks according to the process structure. Subsequently, in each block, a quality regression model with multimode big process data is built using the dp-S2GMM, which is derived from a scalable stochastic variational inference semisupervised GMM (SVI-S2GMM). With the regression model, a hierarchical fault detection and diagnosis scheme in both quality-related and quality-unrelated subspaces is proposed from the variable level, block level to plant-wide level. Finally, an industrial case study on the Tennessee Eastman process demonstrates the feasibility and effectiveness of the proposed HQM algorithm.",2021,21622388
139,10.1109/i-Society.2014.7009043,SERVICE SCIENCE FACING BIG DATA,"The Big Data is a modification of the traditional view of information organization, particularly view of the data warehouses and databases. Nowadays, business organizations must address a mix of structured, unstructured and streaming data that supports queries and reports. Business recognized the wealth of untapped information in open social media data. Therefore, the goal of this paper is to present the procedural approach on how to cope with massive data sets' management. The proposal included in this paper covers service science application.",2014,
140,10.1109/ACCESS.2020.2973177,SPATIOTEMPORAL PATTERNS OF VISITORS IN URBAN GREEN PARKS BY MINING SOCIAL MEDIA BIG DATA BASED UPON WHO REPORTS,"Green parks in urban areas are believed to enhance the well-being of residents. The importance of green spaces to support health and fitness in urban areas has recently regained interest. Reports released in 2010-2016 by the World Health Organization (WHO) on urban planning, environment, and health stated that green spaces can have a positive impact on physical activity, social and mental well-being, enhance air quality and decrease noise exposure. We analyzed the number of check-ins in various parks of Shanghai by utilizing geotagged social media network check-in data. This article presents a descriptive study using social media data by obtaining the three-year comparison of spatial and temporal patterns of park visits to raise public awareness that green parks provide a healthy environment that can be beneficial for the well-being of urban citizens. We investigated the visitor spatiotemporal behavior in more than 115 green parks in 10 districts of Shanghai with approximately 250,000 check-ins. We examined 3 years of geotagged data and our main findings are: (i) the spatial and temporal variations of users in urban green parks (ii) the gender differences in space and time with relation to urban green parks. The main objective of this article is to present evident data for policymakers on the advantages of providing green spaces access to urban citizens and to facilitate cities with systematic approaches to provide green space access to improve the health of urban citizens.",2020,21693536
141,10.1109/MS.2014.51,"DISTRIBUTION, DATA, DEPLOYMENT: SOFTWARE ARCHITECTURE CONVERGENCE IN BIG DATA SYSTEMS","Big data applications are pushing the limits of software engineering on multiple horizons. Successful solutions span the design of the data, distribution, and deployment architectures. The body of software architecture knowledge must evolve to capture this advanced design knowledge for big data systems. This article is a first step on this path. Our research is proceeding in two complementary directions. First, we're expanding our collection of architecture tactics and encoding them in an environment that supports navigation between quality attributes and tactics, making crosscutting concerns for design choices explicit. Second, we're linking tactics to design solutions based on specific big data technologies, enabling architects to rapidly relate a particular technology's capabilities to a specific set of tactics.",2015,19374194
142,10.1109/NGMAST.2015.15,IMPROVING DATA EXTRACTION EFFICIENCY OF CACHE NODES IN COGNITIVE RADIO NETWORKS USING BIG DATA ANALYSIS,"In cognitive radio networks, unlicensed users are allowed to use underutilized licensed spectrum until licensed users' transmission quality of service is not compromised. As soon as the conflict goes beyond a certain limit, SU must leave the spectrum and move to the other nearby free band. At the time of interruption, sensing the nearby free channels and switching to them will take some time, hence the ongoing data will be interrupted, which will delay the data transmission. To minimize this delay, creating cache of the SU signal at multiple nodes in a cluster has shown significant improvement in reducing the transmission delay if cache placement is done systematically. This systematic and accurate placement of cache is possible if the data accumulated is accessed and processed quickly. Taking into account the vastness of cluster networks, a huge amount of data will be required to be accessed and processed. Cognitive Radio networks are very complex structures when it comes to the information sharing amongst the secondary users and with the cluster head. Taking into account, whether unlicensed users share their information with other secondary users, and in case if they do, how much proportion of it they allow the fusion center to process, several big data scenarios exist. This paper discusses the possible information sharing scenarios in cognitive radio network systems and their possible Big Data Solutions.",2015,
143,10.1109/TII.2020.3001054,A DATA-DRIVEN APPROACH OF PRODUCT QUALITY PREDICTION FOR COMPLEX PRODUCTION SYSTEMS,"In the modern industry, the information has been sufficiently shared among the production equipment, intelligent subsystems, and mobile devices via advanced network technology. For this purpose, many challenges on plant-wide performance evaluation such as product quality prediction have been received considerable attention in complex industrial Internet of Things systems. In this article, an efficient and effective soft sensor based on the semisupervised parallel deepFM model is proposed for the product quality prediction. First, a label broadcasting method is presented to augment labeled samples from unlabeled samples. Then, a data binning method is introduced to discretize process variables for an unbiased estimation. Based on the modified deepFM model, quality information can be separately extracted from different components of the model while high- and low-dimensional features can be obtained. Manifold regularization is embedded into the back propagation algorithm, in which unlabeled samples issue can be further resolved. Experiments on a real-world dataset demonstrate the effectiveness and performance of the proposed methods.",2021,19410050
144,10.1109/AIAM54119.2021.00106,POWER DATA QUALITY OPTIMIZATION AND EVALUATION BASED ON BPNN,"With the continuous improvement of the information technology and communications of Smart Grid, the electric power big data environment has been formed. The data shows diversity and multi-source characteristics. How to ensure the quality of power data in the computer organization under the condition of heterogeneity is the premise of making relevant decisions. This paper firstly gives the definition of Data Space of power enterprises, analyzes the factors affecting the quality of data in the computer environment, and gives the relevant architecture of processing power data in the data space. Secondly, based on business flow and Petri net in the computer environment, this paper constructs the data flow and quality control model of the front and back platforms. The former represents the data flow in the power business and abstracts it to form Petri net computer information flow, so that the data can achieve the effect of cleaning while flowing in the business process. Finally, an evaluation index system is built and back-propagation neural network (BPNN) is used to determine the weight, a case study is given to verify the effectiveness of the proposed method.",2021,
145,10.1109/MiSE.2015.21,DICE: QUALITY-DRIVEN DEVELOPMENT OF DATA-INTENSIVE CLOUD APPLICATIONS,"Model-driven engineering (MDE) often features quality assurance (QA) techniques to help developers creating software that meets reliability, efficiency, and safety requirements. In this paper, we consider the question of how quality-aware MDE should support data-intensive software systems. This is a difficult challenge, since existing models and QA techniques largely ignore properties of data such as volumes, velocities, or data location. Furthermore, QA requires the ability to characterize the behavior of technologies such as Hadoop/MapReduce, NoSQL, and stream-based processing, which are poorly understood from a modeling standpoint. To foster a community response to these challenges, we present the research agenda of DICE, a quality-aware MDE methodology for data-intensive cloud applications. DICE aims at developing a quality engineering tool chain offering simulation, verification, and architectural optimization for Big Data applications. We overview some key challenges involved in developing these tools and the underpinning models.",2015,21567883
146,10.1109/BigDataCongress.2015.94,BIG DATA DRIVEN CYBER ANALYTIC SYSTEM,"An infrastructure for parallel analyzing big data in order to search, pattern recognition and decision-making based on the use of the Boolean metric of cyberspace measurement is proposed. It is characterized by using only logical xor-operation for determining the cyber-distance by means of cyclic closing at least one object, which allows significantly increasing the speed of analysis of large data. A new approach to vector-logical big data processing based on the total removal of arithmetic operations, which impact on the performance and hardware complexity, is offered, the approach can be efficiently implemented based on modern multiprocessor digital systems-on-chips and virtual parallel processors, operating under the control of cyber-physical systems or cloud service-filters. A qubit-vector model of computing automaton is proposed, it is characterized by the transactional interaction of memory components, which represent the combinational and sequential elements and are implemented in the form of a qubit or ""quantum"" primitives needed to create parallel virtual computers and cloud-focused processors. A new structural model for analyzing big data is developed, it is characterized by the use of cloud services, cyber-physical and search systems, virtual parallel multiprocessors with a minimal set of vector-logical operations for accurate information retrieval based on the proposed Boolean metric and non-numerical quality criteria, which makes it possible to create a semantic infrastructure of cyberspace by the competence classification and metric ordering big data across the cyber-ecosystem of the planet.",2015,23797703
147,10.1109/ICInfA.2014.6932715,A FRAMEWORK TO SPECIFY BIG DATA DRIVEN COMPLEX CYBER PHYSICAL CONTROL SYSTEMS,"Big data technology is a new technology that aims to efficiently obtain value from very big volumes of a wide variety of data, by enabling high velocity capture, process, store, discovery and/or analysis, while ensuring their veracity by an automatic quality control in order to obtain a big value and make decision. Big Data is described by what is often represented as a multi-V model. In multi-V model, volume, velocity and variety are the items most commonly recognized. Big data driven cyber physical systems refer to such cyber physical systems that use large quantities of complex data to perform their functions. Data is very important to the correct specification, analysis, design and implementation and operation of these big data driven cyber physical systems. he design of big data driven cyber physical systems requires that new concepts are used to model classical data structures, 4V features of big data, spatio-temporal constraints and moving object, and the dynamic continuous behavior of the physical world. In this paper, we propose an approach to integrate Architecture Analysis & Design Language (AADL) [6], Modelicaml and Hybrid Relation Calculus for big data driven cyber physical system development. We illustrate the proposed method by specifying and modeling the Vehicular Ad doc NETwork (VANET).",2014,
148,10.1109/CSCI49370.2019.00244,APPROXIMATE QUALITY ASSESSMENT WITH SAMPLING APPROACHES,"Data is useful to the extent that it can be quickly analyzed to reveal valuable information. With high-quality data, we can increase revenue, reduce cost, and reduce risk. On the other hand, the consequences of poor-quality data can be severe. It has been estimated that poor quality customer data costs U.S. businesses $611 billion annually in postage, printing, and staff overhead. These issues make data quality assessment a necessary and critical step in any data-related systems. Big data brings new challenges to data quality assessment due to the scale of data, streaming data, and different forms of data. Therefore, we proposed a sampling-based approximate quality assessment model on large data. Sampling large datasets can make all quality assessment processes cheaper and more feasible because of data reduction. The protocol of this work: First, the sample size is determined for estimating a large dataset. Next, sampling techniques are applied to collect samples. Then, these samples are used to estimate the quality of the large dataset. The objective of quality assessment in this work is to evaluate the completeness, accuracy, and timeliness of data and to return fast and approximate scores. Using different sample sizes and different sampling methods, we obtained 72 sets of data and compared them. These results show that the proposed approach is efficient and provides some insight into the quality assessment with samples.",2019,
149,10.1109/ACCESS.2019.2955992,RESEARCH AND ANALYSIS FOR REAL-TIME STREAMING BIG DATA BASED ON CONTROLLABLE CLUSTERING AND EDGE COMPUTING ALGORITHM,"Aiming at the low efficiency, poor performance and weak stability of traditional clustering algorithms and the poor response to the processing of massive data in real time, a real-time streaming controllable clustering edge computing algorithm (SCCEC) is proposed. First, the data tuples that arrive in real time are pre-processed by coarse clustering, the number of clusters, and the position of the center point are determined, and a set formed by macro clusters having differences is formed. Secondly, the macro cluster set obtained by the coarse clustering is sampled, and then K-means parallel clustering is performed with the largest and smallest distances, thereby realizing fine clustering of data. Finally, the completely clustering algorithm and the edge-computing algorithm are combined to realize the clustering analysis under the edge-computing framework. The experimental results show that the proposed algorithm has the advantages of high efficiency, good quality, and strong stability. It can quickly obtain the global optimal solution, and deal with massive data with high real-time performance. It can be used for real-time streaming data aggregation under big data background.",2019,21693536
150,10.1109/CLOUD.2015.107,SCALABLE EUCLIDEAN EMBEDDING FOR BIG DATA,"Euclidean embedding algorithms transform data defined in an arbitrary metric space to the Euclidean space, which is critical to many visualization techniques. At big-data scale, these algorithms need to be scalable to massive data-parallel infrastructures. Designing such scalable algorithms and understanding the factors affecting the algorithms are important research problems for visually analyzing big data. We propose a framework that extends the existing Euclidean embedding algorithms to scalable ones. Specifically, it decomposes an existing algorithm into naturally parallel components and non-parallelizable components. Then, data parallel implementations such as MapReduce and data reduction techniques are applied to the two categories of components, respectively. We show that this can be possibly done for a collection of embedding algorithms. Extensive experiments are conducted to understand the important factors in these scalable algorithms: scalability, time cost, and the effect of data reduction to result quality. The result on sample algorithms: Fast Map-MR and LMDS-MR shows that with the proposed approach the derived algorithms can preserve result quality well, while achieving desirable scalability.",2015,21596182
151,10.1109/HPEC.2015.7322471,BIG DATA STRATEGIES FOR DATA CENTER INFRASTRUCTURE MANAGEMENT USING A 3D GAMING PLATFORM,"High Performance Computing (HPC) is intrinsically linked to effective Data Center Infrastructure Management (DCIM). Cloud services and HPC have become key components in Department of Defense and corporate Information Technology competitive strategies in the global and commercial spaces. As a result, the reliance on consistent, reliable Data Center space is more critical than ever. The costs and complexity of providing quality DCIM are constantly being tested and evaluated by the United States Government and companies such as Google, Microsoft and Facebook. This paper will demonstrate a system where Big Data strategies and 3D gaming technology is leveraged to successfully monitor and analyze multiple HPC systems and a lights-out modular HP EcoPOD 240a Data Center on a singular platform. Big Data technology and a 3D gaming platform enables the relative real time monitoring of 5000 environmental sensors, more than 3500 IT data points and display visual analytics of the overall operating condition of the Data Center from a command center over 100 miles away. In addition, the Big Data model allows for in depth analysis of historical trends and conditions to optimize operations achieving even greater efficiencies and reliability.",2015,
152,10.1109/ICIT52682.2021.9491733,"BIG DATA, CLASSIFICATION, CLUSTERING AND GENERATE RULES: AN INEVITABLY INTERTWINED FOR PREDICTION","Big Data filed is an unsettled standard comparing with a traditional database, data mining, or data warehouse. Stability measure aims to acquire the quality dataset which encourages to use of preprocessing data method to handle instability that miniaturization missing data. Therefore, to increase the data quality in order to achieve an accurate prediction, significant rules are used to provide value and meaningful data. Through, three measures by support, confidence, and the lift to acquire frequently rules. These rules are used to conduct the objective extracting pattern, to estimate each browsing customer's likelihood of making a purchase, and to choose meaningful patterns from the discovered association rules.",2021,
153,10.1109/ITMQIS.2018.8525121,ANALYSIS OF THE APPLICATION OF BIG DATA TECHNOLOGIES IN THE FINANCIAL SPHERE,"The last years' global development is characterized by the mass introduction of information and communication technologies in all spheres of economic development. The implementation of digital technologies acquired explosive character recently. It is no coincidence that the term ""digital economy"", indicating the use of the most advanced digital technologies in various sectors of the world economy, has emerged in past years. Today, digital transformation occurs in virtually all areas of economic development, for example, Industry 4.0 in industry, Fintech in the financial sector. The main breakthrough technologies of digital transformation are cloud computing technology, cyber - physical systems, artificial intelligence, and technologies for analysis and processing of big data. One of the drivers of digital modernization is the big data direction. The rapid development of this sphere is explained by the intermittent growth of information, which leads to the impossibility of applying classical methods and tools for processing available data. The paper analyzes the technologies and methods of processing information in the field of big data. Future development of the sectors of the field of big data is given. The research of the big data market in various sectors of economic development was conducted; the leaders in this field are identified. The main directions of using big data in financial institutions as one of the key players in the market are analyzed. The study of the big data market in the financial sphere was carried out; financial indicators, dynamics of the market growth were determined. The main reasons that prevent the big data introduction in financial institutions are shown. The prognosis for the further development of the use of big data in the financial sphere is given.",2018,
154,10.1109/SITIS.2018.00117,PROCESSING BIG DATA IN STREAMING FOR FAULT PREDICTION: AN INDUSTRIAL APPLICATION,"In the current data-driven industrial scenario, Big Data processing plays a leading role in enhancing business performance. An ever increasing number of working machines are equipped with smart devices (such as sensors and actuators) which are in charge of monitoring machine status in real time and implement corrective actions before the workpiece quality is compromised or machine is damaged. However, many manufacturing companies do not take advantage of the use of Big Data coming from their production systems. In some cases, Big Data analytics is an un-explored issue since it is considered time and resources consuming. Moreover, the real benefits of processing, in real time, industrial data are usually underestimated. The European project TOREADOR wants to extend and facilitate the diffusion of Big Data analytics within industrial contexts, in order to generate greater value for companies. Focusing on the aerospace components manufacturing process (as one of the case studies of the Project), the paper aims to describe the main developments and lessons learned by the customization of TOREADOR Big Data analytics platform to process and analyze data from CNC machines sensors.",2018,
155,10.1109/SMART52563.2021.9675302,IBD: A FEEDBACK FRAMEWORK WITH DEEP-LEARNING FOR IOT-GENERATED BIG DATA PROCESSING,Convolutional Neural Network (CNN) and Recurrent Neural Networks (RNNs)have the ability to find the accurate result in images and text respectively. The best classification results are still awaited due to the high cost of computation and high memory requirements of CNN and RNN. Our work suggests a framework that improves the quality of data at various layers by providing feedback to suggested system. The proposed framework leads to an error free processing system.,2021,27677354
156,10.1109/CCAA.2018.8777571,BIG DATA DEEP LEARNING FRAMEWORK USING KERAS: A CASE STUDY OF PNEUMONIA PREDICTION,"Big Data predictive analytics using machine learning techniques is currently a much active area of research in medical science. With increasing size and complexity of medical data like X-rays, deep learning gained huge success in prediction of many fatal diseases like pneumonia. In this research work, DCNN (deep convolutional neural networks) an efficient predicting model for big data, having deep layers is a proposed, which can classify whether a person is having a pneumonia or not. The experiments are carried after extracting the features of high quality X-ray images data and achieved an prediction accuracy of 84% and AUC of Promising results are found, when the results of the DCNN framework is compared with the regular classifiers like SVM, random forest, etc. using different evaluation metrics like accuracy, sensitivity, etc. With the appearance of increasing cases of pneumonia, tactful implementation of deep learning can play a big part in improving the performance of prediction of many fatal diseases in the future.",2018,26418134
157,10.1109/REPCon.2013.6681858,GRID ANALYTICS: HOW MUCH DATA DO YOU REALLY NEED?,"Electric utilities are experiencing a technology transformation, which includes the deployment of distributed intelligent devices, two-way communications systems, and information technologies to enable greater monitoring and control of their distribution systems. These technologies will increase the volume of data flowing into a utility's Information Systems, which in turn will need to be stored and managed. The amount of data that can be generated from AMI/AMR, SCADA, and other intelligent devices can be significant. One strategy, which seems to be a popular option, is to collect all of the data possible and figure out what to do with the data at a later date. As the amount of data increases that needs to be stored, there is a corresponding increase in Information Technology infrastructure, skill sets, and cost. If collecting all data were possible, it is doubtful the data would be stored or structured in a way that would be useable in the future. An alternative to capturing and storing all data possible is to use Information Engineering methodologies to focus on what data is needed for a given task or application. Information Engineering is defined as a set if interrelated disciplines that are needed to build a computerized enterprise based on information systems. The focus of Information Engineering is the data that is stored and maintained by computers and the information that is distilled from this data. [1] It is foundational to understand that data and information are terms used interchangeably, but are distinctly different terms with different meanings. A kWh is a piece of data that is useful information when added to other data such as meter number, account name, or service location. The amount and kind of data required, as well as the necessary time frames, (i.e., historical, real-time, predicted) depend upon the applications, which may include: 1. Geospatial Information System 2. Meter/billing information 3. Asset management 4. Work and workforce management 5. Network modeling and analysis for planning (e.g., voltage drop, power flow, short circuit, arc flash, contingency studies, reliability metrics, loss analysis, protective device coordination) 6. Operations (outage management, fault location, Volt/VAR control, power quality, demand management, distributed generation and storage) 7. Real-time, active grid management, grid analytics. This paper will discuss how to apply Information Engineering principles to turn data into useful information for a utility as an alternative to the ""Big Data"" approach to capturing and storing data. Using kWh, and Voltage as examples, we will outline the Information Engineering process to turn these data elements into useful information. Once you have the information engineered, the next step is to use data management methodologies to manage the data that is being gathered. Data Management is a detailed topic on its own and will not be covered on this paper. What data has been captured needs to be managed across the enterprise.",2013,07347464
158,10.1109/COMPSAC.2019.00062,MULTI-SOURCE HETEROGENEOUS CORE DATA ACQUISITION METHOD IN EDGE COMPUTING NODES,"As the volume of data grows exponentially, big data brings an unprecedented burden to the current computing infrastructure. How to deal with big data efficiently and concisely and reduce the burden of computing infrastructure has always been a big challenge. Therefore, this paper proposes a high-quality core data extraction method in edge computing nodes. Firstly, heterogeneous data are fused into a unified model, the data characteristics of the original data are retained. Then, a Lanzcos-based incremental tensor decomposition method is proposed to extracted the high quality core tensor dynamically. Finally, the model algorithm is verified using real data. The experimental results show that the approximate tensor reconstructed from the tensor containing 15% of the core data can guarantee 90% accuracy. At the same time, IncLHOSVD is significantly better than non-incremental HOSVD in execution time in guaranteeing the accuracy of approximate equal error.",2019,07303157
159,10.1109/TBDATA.2017.2680460,A MACHINE LEARNING BASED FRAMEWORK FOR VERIFICATION AND VALIDATION OF MASSIVE SCALE IMAGE DATA,"Big data validation and system verification are crucial for ensuring the quality of big data applications. However, a rigorous technique for such tasks is yet to emerge. During the past decade, we have developed a big data system called CMA for investigating the classification of biological cells based on cell morphology which is captured in diffraction images. CMA includes a collection of scientific software tools, machine learning algorithms, and a large-scale cell image repository. In order to ensure the quality of big data system CMA, we developed a framework for rigorously validating the massive scale image data as well as adequately verifying both the software tools and machine learning algorithms. The validation of big data is conducted by iteratively selecting the data using a machine learning approach. An experimental approach guided by a feature selection algorithm is introduced in the framework to select an optimal feature set for improving the machine learning performance. The verification of software and algorithms is developed on the iterative metamorphic testing approach due to the non-testable property of the software and algorithms. A machine learning approach is introduced for developing test oracles iteratively to ensure the adequacy of the test coverage criteria. Performance of the machine learning algorithm is evaluated with a stratified N-fold cross validation and confusion matrix. We describe the design of the proposed big data verification and validation framework with CMA as the case study, and demonstrate its effectiveness through verifying and validating the dataset, the software and the algorithms in CMA.",2021,23722096
160,10.1109/CIBDA50819.2020.00104,WHOLE PROCESS TRACING MODEL OF PIGEON QUALITY IN BLOCK CHAIN ENVIRONMENT,"In order to improve the whole process traceability of meat pigeon quality, a whole process traceability model of meat pigeon quality based on block chain data fusion is proposed. The method comprises the following steps of: constructing a statistical information distribution model for tracing the whole process of meat pigeon quality; reorganizing the structure of information sources for tracing the whole process of meat pigeon quality by adopting a data structure reorganization method; establishing an information source characteristic distribution model for tracing the whole process of meat pigeon quality; carrying out tracking identification and large data mining of meat pigeon quality information by adopting an association rule mining method under a block chain mode; constructing a meat pigeon quality whole process tracing model; and combining information extraction and optimal scheduling of meat pigeon quality. The quantitative feature distribution set of meat pigeon quality is extracted, and the statistical feature analysis of the whole process traceability of meat pigeon quality is realized by combining the information detection and feature positioning methods. The dynamic evaluation of meat pigeon quality information is realized by using the meat pigeon quality statistical large data analysis method. The optimization design of the whole process traceability model of meat pigeon quality is realized by combining the block chain data fusion and the knowledge map feature analysis method. The simulation results show that the method has good real-time performance, strong dynamic tracing ability and good information positioning ability for the quality of meat pigeons.",2020,
161,10.1109/AICCSA.2017.8,A SPECULATIVE CONCURRENCY CONTROL IN REAL-TIME SPATIAL BIG DATA USING REAL-TIME NESTED SPATIAL TRANSACTIONS AND IMPRECISE COMPUTATION,"Real-time spatial applications have become more and more important. These applications are innovating in the way we live. As a result, there is a huge amount of real-time spatial data generated everyday, accessed simultaneously by two types of transactions, Update transactions and User transactions (continuous requests). In real-time spatial Big Data, the performance can be increased by allowing concurrent execution of transactions. This activity is called concurrency control. The concurrency control algorithm must be used to ensure serializability of transaction scheduling and to maintain data consistency.Several works have been done in this area, but without holding into account the existence of a huge volume of data. In this paper, we apply the technique of imprecise computation for real-time spatial nested transactions. We consider that imprecise transaction consist that a User transaction is decomposed logically into a one mandatory sub-transaction and one or more optional sub-transactions and an Update transaction consists only of a single mandatory sub-transaction. We propose an improvement of an existing Two-Shadow Speculative Concurrency Control (SCC-2S) with priority with the use of the imprecise real-time spatial transaction. Our main objectives are: to guarantee the data freshness, to enhance the deadline miss ratio even in the presence of conflicts and unpredictable workloads and finally to satisfy the requirements of users by the improving of the quality of service (QoS).",2017,21615330
162,10.1109/QRS-C.2019.00089,RESEARCH ON SECURITY DETECTION AND DATA ANALYSIS FOR INDUSTRIAL INTERNET,"Industrial Internet platform needs to solve a series of problems, such as access of multi-type industrial equipment, multi-source industrial data integration, massive data management and processing, industrial Internet security and so on. This paper builds industrial big data analysis algorithm library based on domain knowledge modeling and big data analysis of industrial data. Through the analysis of the behavior characteristics of industrial internet network traffic data, this paper studies the method of selecting traffic characteristics of events in the industrial Internet; establishes the propagation and evolution model of security events in the industrial Internet, and builds a traceability map of security event propagation; This study combines the characteristics of large data volume and centralized control of future industrial Internet to reduce the complexity of security event detection and analysis. It has reference value for industrial Internet controller to formulate node routing strategy.",2019,
163,10.1109/IPDPSW.2016.152,OPEN SOURCE INITIATIVES AND FRAMEWORKS ADDRESSING DISTRIBUTED REAL-TIME DATA ANALYTICS,"The continuous evolution of digital services, is resulting in the generation of extremely large data sets that are created in almost real time. Exploring new opportunities for improving the quality of these digital services, as well as providing better-personalized experiences to digital users are two major challenges to be addressed. Different methods, tools, and techniques existed today to generate actionable insights from digital services data. Traditionally, big data problems are handled on historical data-sets. However, there is a growing demand on real-time data analytics to offer new services to users and to provide pro-active customers' care, personalized ads, emergency aids, just to give a few examples. Spite of the fact that there are few existing frameworks for real-time analytics, however, utilizing those for solving distributed real-time big data analytical problems stills remains a challenge. Existing real-time data analytics (RTDA) frameworks are not covering all the features that requires for distributed computation in real-time. Therefore, in this paper, we present a qualitative overview and analysis on some of the mostly used existing RTDA frameworks. Specifically, Apache Spark, Apache Flink, Apache Storm, and Apache Samza are covered and discussed in this paper.",2016,
164,10.1109/PowerTech46648.2021.9495000,BIG DATA ANALYTICS FOR ELECTRICITY THEFT DETECTION IN SMART GRIDS,"In Smart Grids (SG), Electricity Theft Detection (ETD) is of great importance because it makes the SG cost-efficient. Existing methods for ETD cannot efficiently handle data imbalance, missing values, variance and non-linear data problems in the smart meter data. Therefore, an effective integrated strategy is required to address underlying issues and accurately detect electricity theft using big data. In this work, a simple yet effective approach is proposed by integrating two different modules, such as data pre-processing and classification, in a single framework. The first module involves data imputation, outliers handling, standardization and class balancing steps to generate quality data for classifier training. The second module classifies honest and dishonest users with a Support Vector Machine (SVM) classifier. To improve the classifier’s learning trend and accuracy, a Bayesian optimization algorithm is used to tune SVM’s hyperparameters. Simulation results confirm that the proposed framework for ETD significantly outperforms previous machine learning approaches such as random forest, logistic regression and SVM in terms of accuracy.",2021,
165,10.1109/ICBDA55095.2022.9760327,RESEARCH ON INTELLIGENT PLANNING AND SCHEDULING METHOD BASED ON MANUFACTURING BIG DATA,"Intelligent manufacturing is the focus of global attention. The planning and scheduling method of manufacturing big data has become an important factor restricting the development of the manufacturing industry, and its intelligent research has become highlight. Facing the manufacturing big data of small and medium-sized enterprises, under the complex scenario of multi task, multi time-space interleaving and multi constraints, taking the actual manufacturing enterprise data as an example, the multi task intelligent production scheduling method and production part disassembly method based on time series are studied in the paper. It includes intelligent scheduling algorithm based on project and production constraints and production part distribution algorithm based on temporal and spatial characteristics, which are conducive to small and medium-sized manufacturing enterprises to make full use of existing resources and improve speed, efficiency and quality.",2022,
166,10.1109/BigData50022.2020.9377995,GRAPH BASED CIA IN REQUIREMENTS ENGINEERING,We developed a novel Vertical Breadth-First Search All-path graph algorithm that utilizes vertical data structures to find all-length paths (including shortest paths) for all pairs of vertices in a graph. In the current research we propose an approach to apply our All-path algorithm to perform Change Impact Analysis during Requirements Engineering. This article presents a roadmap of our future research plan to apply our All-path algorithm for software quality improvement.,2020,
167,10.1109/TBDATA.2016.2622288,FUZZY BASED SCALABLE CLUSTERING ALGORITHMS FOR HANDLING BIG DATA USING APACHE SPARK,"A huge amount of digital data containing useful information, called Big Data, is generated everyday. To mine such useful information, clustering is widely used data analysis technique. A large number of Big Data analytics frameworks have been developed to scale the clustering algorithms for big data analysis. One such framework called Apache Spark works really well for iterative algorithms by supporting in-memory computations, scalability etc. We focus on the design and implementation of partitional based clustering algorithms on Apache Spark, which are suited for clustering large datasets due to their low computational requirements. In this paper, we propose Scalable Random Sampling with Iterative Optimization Fuzzy c-Means algorithm (SRSIO-FCM) implemented on an Apache Spark Cluster to handle the challenges associated with big data clustering. Experimental studies on various big datasets have been conducted. The performance of SRSIO-FCM is judged in comparison with the proposed scalable version of the Literal Fuzzy c-Means (LFCM) and Random Sampling plus Extension Fuzzy c-Means (rseFCM) implemented on the Apache Spark cluster. The comparative results are reported in terms of time and space complexity, run time and measure of clustering quality, showing that SRSIO-FCM is able to run in much less time without compromising the clustering quality.",2016,23722096
168,10.1109/BigData.2013.6691678,A CASE STUDY ON ENTITY RESOLUTION FOR DISTANT PROCESSING OF BIG HUMANITIES DATA,"At the forefront of big data in the Humanities, collections management can directly impact collections access and reuse. However, curators using traditional data management methods for tasks such as identifying redundant from relevant and related records, a small increase in data volume can significantly increase their workload. In this paper, we present preliminary work aimed at assisting curators in making important data management decisions for organizing and improving the overall quality of large unstructured Humanities data collections. Using Entity Resolution as a conceptual framework, we created a similarity model that compares directories and files based on their implicit metadata, and clusters pairs of closely related directories. Useful relationships between data are identified and presented through a graphical user interface that allows qualitative evaluation of the clusters and provides a guide to decide on data management actions. To evaluate the model's performance, we experimented with a test collection and asked the curator to classify the clusters according to four model cluster configurations that consider the presence of related and duplicate information. Evaluation results suggest that the model is useful for making data management action decisions.",2013,
169,10.1109/ACCESS.2020.3038394,A REAL-TIME QUALITY CONTROL SYSTEM BASED ON MANUFACTURING PROCESS DATA,"Quality prediction is one of the key links of quality control. Benefitting from the development of digital manufacturing, manufacturing process data have grown rapidly, which allows product quality predictions to be made based on a real-time manufacturing process. A real-time quality control system (RTQCS) based on manufacturing process data is presented in this paper. In this study, the relationship between the product real-time quality status and processing task process was established by analyzing the relationship between the product manufacturing resources and the quality status. The key quality characteristics of the product were identified by analyzing the similarity of the product quality characteristic variations in the manufacturing process based on the big data technology, and a quality-resource matrix was constructed. Based on the quality-resource matrix, the RTQCS was established by introducing an association-rule incremental-update algorithm. Finally, the RTQCS was applied in actual production, and the performance of RTQCS was verified by experiments. The experiments showed that the RTQCS can effectively guarantee the quality of product manufacturing and improve the manufacturing efficiency during production.",2020,21693536
170,10.1109/ICIAFS.2016.7946520,USE OF DATA WAREHOUSING TO ANALYZE CUSTOMER COMPLAINT DATA OF CONSUMER FINANCIAL PROTECTION BUREAU OF UNITED STATES OF AMERICA,"The Consumer Financial Protection Bureau was established in USA for enabling the USA consumers to report customer support and complaint related information regarding their financial issues with the US government. The complaint data is freely available for analysis and tracking of how efficiently and effectively the financial institutes handle the complaints lodged against them. Each complaint consists of attributes that can uniquely describe and identify it. These features have been exploited for data mining, analysis and predictions. The data warehouse creation and data analysis was done using Microsoft SQL Server Technologies. The data mining techniques such as Microsoft Decision Tree, Microsoft Naïve Bayes, Microsoft Time Series and Microsoft Neural Network models were used in this study. Based on the results, it was observed that there is a correlation between the growth of complaints in certain financial domains with regards to changes in the economic, political and regulatory forces. Probability predictions also show, how each product can get a particular issue-related complaint, how a particular issue can get a timely response, how a particular issue can cause a consumer dispute and what type of issues are mostly lodged via a particular submission method, etc. This information can be used in prescriptive analysis to enhance financial consumer services and also improve the response quality of automated consumer support systems.",2016,21511810
171,10.1109/COMITCon.2019.8862250,HYBRID PRE-PROCESSING TECHNIQUE FOR HANDLING IMBALANCED DATA AND DETECTING OUTLIERS FOR KNN CLASSIFIER,"Data mining is a technique of examining huge quanta of pre-existing data in order to discover new patterns and relationships among them, which will help to make better decisions. Classification is a data mining technique which organises data into categories. In this paper, in order to enhance the performance of k nearest neighbour (kNN) classifier-a kind of classification technique that is among the most widely used-a new data pre-processing technique has been proposed, which can handle some classification issues such as imbalanced data and outliers. In an imbalanced dataset, the classification categories are not equally distributed. Imbalanced dataset have an inherent issue when it comes to using classifiers on them that have been developed using machine learning algorithms. The basic nature of these algorithms is to reduce errors without relying on balance of classes. Another issue addressed in this paper is the matter of outliers or extreme values. Outlier or extreme values are those values that are outside the expected range of values. The quality of Classification modelling can be greatly enhanced by identifying and excision of these values. In this proposed technique two data pre-processing techniques have been combined to form a hybrid pre-processing technique. The two data pre-processing techniques are resample technique and inter quartile range technique (IQR). Some Imbalanced dataset with outliers that can be considered as yardsticks were taken for this study. It was observed that the classification results obtained were far superior to the classification done without the pre-processing technique.",2019,
172,10.1109/ACCESS.2019.2904248,PERSONAL DATA TRADING SCHEME FOR DATA BROKERS IN IOT DATA MARKETPLACES,"With the widespread use of the Internet of Things, data-driven services take the lead of both online and off-line businesses. Especially, personal data draw heavy attention of service providers because of the usefulness in value-added services. With the emerging big-data technology, a data broker appears, which exploits and sells personal data about individuals to other third parties. Due to little transparency between providers and brokers/consumers, people think that the current ecosystem is not trustworthy, and new regulations with strengthening the rights of individuals were introduced. Therefore, people have an interest in their privacy valuation. In this sense, the willingness-to-sell (WTS) of providers becomes one of the important aspects for data brokers; however, conventional studies have mainly focused on the willingness-to-buy (WTB) of consumers. Therefore, this paper proposes an optimized trading model for data brokers who buy personal data with proper incentives based on the WTS, and they sell valuable information from the refined dataset by considering the WTB and the dataset quality. This paper shows that the proposed model has a global optimal point by the convex optimization technique and proposes a gradient ascent-based algorithm. Consequently, it shows that the proposed model is feasible even if the data brokers spend costs to gather personal data.",2019,21693536
173,10.1109/EI250167.2020.9346938,RESEARCH ON TOPOLOGY IDENTIFICATION OF DISTRIBUTION NETWORK UNDER THE BACKGROUND OF BIG DATA,"Traditional identification of distribution network topology needs to rely on manual identification and verification, which has the problems of low identification rate and poor accuracy. Under the current background of power big data, effective results can be obtained by fully mining the potential effective information of big data and applying it to the identification of distribution network topology structure. This paper presents a line-to-line relationship identification method based on the maximum correlation minimum redundancy method and a line-to-transformer relationship identification method based on the conversion of three-phase unbalanced outlet voltage. The voltage acquisition system is fully utilized, and the maximum correlation minimum redundancy method (mRMR) is used to eliminate the redundancy of feature variables in line-to-line relationship identification to obtain the most accurate correlation results. In the identification of line-transformer relationship, the conversion method of three-phase unbalanced outlet voltage is adopted to eliminate the influence of misjudgment caused by three-phase unbalanced voltage and improve the identification accuracy. The identification method proposed in this paper is applied to the topology identification of a regional distribution network. By comparison with the actual topology, the identification accuracy is up to 99.78% and the effect is remarkable.",2020,
174,10.1109/BigData50022.2020.9378004,DISEASES IDENTIFICATION WITH BIG DATA CONCEPT – THE OLDER PEOPLE COMMUNITY,"The use of the big data in conjunction with artificial intelligence methods used in health areas is increasingly being used. For data capture, smartphones and embedded sensors are an increasingly reliable, accurate and fast way to detect certain types of bio signals. In this study we propose a system for collecting data for the acquisition and study of associated diseases and symptoms to demonstrate how the use of sensors and their connection to a database and later application of methods to detect patterns to remove conclusions and contribute to advances in health and to a higher quality of life especially in the elderly.",2020,
175,10.1109/SISY.2017.8080525,DATA ANALYTICS FOR CLOUDS HEALTH-CARE AND RISK PREDICTIONS BASED ON ENSEMBLE CLASSIFIERS AND SUBJECTIVE PROJECTION,"Discovering patterns from big data attracts a lot of attention due to its importance in discovering accurate patterns and features that are used in predictions of decision making. The challenges in big data analytics are the high dimensionality and complexity in data representation. Granular computing and feature selection are among the challenge to deal with big data analytics that is used for Decision making. We will discuss these challenges in this talk and provide new projection on ensemble learning for health care risk prediction. In decision making most approaches are taking into account objective criteria, however the subjective correlation among different ensembles provided as preference utility is necessary to be presented to provide confidence preference additive among it reducing ambiguity and produce better utility preferences measurement for good quality predictions. Most models in Decision support systems are assuming criteria as independent. Different type of data (time series, linguistic values, interval data, etc.) imposes some difficulties to data analytics due to preprocessing and normalization processes which are expensive and difficult when data sets are raw and imbalanced. We will highlight these issues though project applied to health-care for elderly, by merging heterogeneous metrics for providing health care predictions for elderly at home. We have utilized ensemble learning as multi-classification techniques on multi-data streams that collected from multi-sensing devices. Subjectivity (i.e., service personalization) would be examined based on correlations between different contextual structures that are reflecting the framework of personal context, for example in nearest neighbor based correlation analysis fashion. Some of the attributes incompleteness also may lead to affect the approximation accuracy. Attributes with preference-ordered domain relations properties become one aspect in ordering properties in rough approximations. We outline issues on Virtual Doctor Systems, and highlights its innovation in interactions with elderly patients, also discuss these challenges in granular computing and decision support systems research domains. In this talk I will present the current state of art and focus it on health care risk analysis with examples from our experiments.",2017,19490488
176,10.1109/ISCV49265.2020.9204277,SMART DATA COLLECTION IN MOBILE EDGE COMPUTING ENVIRONMENT,"With the digital transformation, businesses and public administrations must change the place of data in the value chain to serve all areas of the business and open up information systems. The value of the knowledge extracted from this data is directly linked to the quality of data collection. Mobile devices are particularly suitable for reporting data. They are very widespread, very suitable and can be used at any time. These characteristics mean that the use of mobile support for data collection corresponds to a paradigm shift more than a simple new additional technology compared to the panoply of existing tools. The explosion of information sharing and data, which stems from our daily by these devices is stored mostly in the cloud servers. Thus, to reduce the number of data transferred and generated by mobile devices to the cloud servers, the edge computing allows to process data at the network edge where they are generated directly reducing certain characteristics of Big Data. Big data involves the collection of complex data on the “V” dimensions which describe the quantity and type of data collected, as well as their importance and relevance to the challenges of the requester. However, the smart data goes a step further and consist to extract from the data collected only the most relevant information for the client in order to make predictions. Our results show that using an intelligent data collection process in mobile computing could generate savings in terms of data storage and analysis at the cloud level.",2020,
177,10.1109/TNNLS.2020.3001602,A WIDE-DEEP-SEQUENCE MODEL-BASED QUALITY PREDICTION METHOD IN INDUSTRIAL PROCESS ANALYSIS,"Product quality prediction, as an important issue of industrial intelligence, is a typical task of industrial process analysis, in which product quality will be evaluated and improved as feedback for industrial process adjustment. Data-driven methods, with predictive model to analyze various industrial data, have been received considerable attention in recent years. However, to get an accurate prediction, it is an essential issue to extract quality features from industrial data, including several variables generated from supply chain and time-variant machining process. In this article, a data-driven method based on wide-deep-sequence (WDS) model is proposed to provide a reliable quality prediction for industrial process with different types of industrial data. To process industrial data of high redundancy, in this article, data reduction is first conducted on different variables by different techniques. Also, an improved wide-deep (WD) model is proposed to extract quality features from key time-invariant variables. Meanwhile, an long short-term memory (LSTM)-based sequence model is presented for exploring quality information from time-domain features. Under the joint training strategy, these models will be combined and optimized by a designed penalty mechanism for unreliable predictions, especially on reduction of defective products. Finally, experiments on a real-world manufacturing process data set are carried out to present the effectiveness of the proposed method in product quality prediction.",2020,21622388
178,10.1109/TBDATA.2016.2613992,A HIERARCHICAL DISTRIBUTED PROCESSING FRAMEWORK FOR BIG IMAGE DATA,"This paper introduces an effective processing framework nominated Image Cloud Processing (ICP) to powerfully cope with the data explosion in image processing field. While most previous researches focus on optimizing the image processing algorithms to gain higher efficiency, our work dedicates to providing a general framework for those image processing algorithms, which can be implemented in parallel so as to achieve a boost in time efficiency without compromising the results performance along with the increasing image scale. The proposed ICP framework consists of two mechanisms, i.e., Static ICP (SICP) and Dynamic ICP (DICP). Specifically, SICP is aimed at processing the big image data pre-stored in the distributed system, while DICP is proposed for dynamic input. To accomplish SICP, two novel data representations named P-Image and Big-Image are designed to cooperate with MapReduce to achieve more optimized configuration and higher efficiency. DICP is implemented through a parallel processing procedure working with the traditional processing mechanism of the distributed system. Representative results of comprehensive experiments on the challenging ImageNet dataset are selected to validate the capacity of our proposed ICP framework over the traditional state-of-the-art methods, both in time efficiency and quality of results.",2016,23722096
179,10.1109/IC3TSN.2017.8284476,REDEFINING CYBER SECURITY WITH BIG DATA ANALYTICS,"The cyber world is expanding rapidly day by day and more and more people are getting connected to this world, resulting in generation of a large amount of data called Big Data. Along with the cyber world, the number of cyber criminals is also expanding rapidly. To fight against the cyber criminals and safeguard the interest of innocent civilians, we take the help of Big Data Analytics. Big data is large in both quantity and quality and can be efficiently used to analyze certain patterns and behavior anomaly which can help us prevent or be prepared for the thread or any upcoming attack. This proactive and analytical approach will help us greatly reduce the rate of Cyber Crimes and also get the knowledge out of that data which was not previously observable. This paper discusses some of the key characteristics of Big data, architecture, categories of cybercrimes and big data analytic techniques that can be used.",2017,
180,10.1109/ICAIoT53762.2021.00014,A SMART DATA PRE-PROCESSING APPROACH BY USING ML ALGORITHMS ON IOT EDGES: A CASE STUDY,"The internet of things (IoT) is a technology that allows many objects used in daily life to produce a variety of data and transfer those data to other objects or systems. The application domain of this system is increasing day by day, and the technologies used for its infrastructure are also varied. However, to process the huge amount of sensor data effectively, smart and fast filtering solutions are required. As a data pre-processing task, smart data filtering improves not only the data processing speed but also the quality of data as well. In other words, big data management is facilitated by getting more effective results with little noise and meaningful data. In this study, we examined big IoT data stored on IoT edges to detect anomalies in temperature, age, gender, weight, height, and time data. In this context, the Logistic Regression algorithm was applied at both sensing and network layers for anomaly detection purposes. Furthermore, the performance of the classification algorithm in terms of speed and accuracy was reported as the output of the study.",2021,
181,10.1109/BDAI56143.2022.9862611,A SERVICE-BASED INFORMATION SYSTEM FOR AI-SUPPORTED HEALTH INFORMATICS,"Artificial Intelligence (AI) and Big Data Analysis are both trending research areas that can be applied to any application domain to increase efficiency, quality, or bridge supply shortages. An example is the analysis of medical information in health informatics and its subdomains bioinformatics and medical informatics. It aims to explore medical information like, e.g., patient data or clinical studies in order to support medical doctors, healthcare professionals, and ultimately all patients. The increasing level of digitization of people's lives and global pandemics boost the relevance of this research area even further. This paper analyzes a medical information use case in order to extend and improve an existing Information System for AI-based Visual Big Data Analysis with a special focus on user empowerment. In more detail, this paper extends a software architecture, introduces a reference implementation, and refines the underlying reference model AI2VIS4BigData. It focuses on supporting both, expert users with profound technical knowledge and end users without that expertise. The resulting concepts and implementations are consequently evaluated with both target groups: A cognitive walkthrough with a group of AI and Big Data Analysis experts confirms the system's usability and a survey with medical doctors and health domain experts (potential end users) positively evaluates the developed concepts.",2022,
182,10.1109/TBDATA.2016.2637378,LARGE-SCALE DATA POLLUTION WITH APACHE SPARK,"Because of the increasing volume of autonomously collected data objects, duplicate detection is an important challenge in today's data management. To evaluate the efficiency of duplicate detection algorithms with respect to big data, large test data sets are required. Existing test data generation tools, however, are either not able to produce large test data sets or are domain-dependent which limits their usefulness to a few cases. In this paper, we describe a new framework that can be used to pollute a clean, homogeneous and large data set from an arbitrary domain with duplicates, errors and inhomogeneities. To prove its concept, we implemented a prototype which is built upon the cluster computing framework Apache Spark and evaluate its performance in several experiments.",2020,23722096
183,10.1109/IEIT53597.2021.00120,HOW TO CONSTRUCT THE ONLINE TEACHING QUALITY MONITORING SYSTEM OF DESIGN COURSES DRIVEN BY DATA ANALYSIS TECHNOLOGY,"Big data and cloud computing technology play an important role in online teaching, educational data mining and learning behavior analysis. Taking environmental design as an example, based on the analysis of online teaching characteristics, this research uses the Delphi method to construct a digital monitoring system for online teaching quality of engineering design courses, and uses the analytic hierarchy process to determine the weights of the monitoring system indicators. Finally, the teaching data is extracted and quantitatively analyzed through online teaching empirical cases to verify the effectiveness of the monitoring system. The research results could provide technical reference for the monitoring and management of online teaching quality of engineering design courses.",2021,
184,10.1109/ICAIBDA53487.2021.9689727,CLASSIFICATION OF WATER POTABILITY USING MACHINE LEARNING ALGORITHMS,"Clean water is one of the basic needs of everyday life. Recently, an ongoing process has been shown to improve water quality, making water less suitable for use. To solve this problem, research is done using a machine learning model. The Decision Tree Algorithm is used by Naïve Bayes algorithm in this type of machine learning to support drinking water quality. The two types of performance are compared in this work. K-fold cross credentials are used to evaluate our machine learning model. Results obtained in the decision tree algorithm have the best results in the configuration with an accuracy value of 97.23%.",2021,
185,10.1109/ISEF.2017.8090691,BIG DATA OBJECT-ORIENTED REPRESENTATION BASED ON GENOM DATA SEARCHING SYSTEM,"Article presents method of storing and processing Big Data using object-oriented datatype in commercial relational database server. Methodology was presented based on genomic data. Additionally, some methods of pattern matching implemented in the object were described and commented. Due to set of numerical experiments based on real data the efficiency of the system was testified. Obtain results shows good quality and high scalability of presented solutions.",2017,
186,10.1109/ITQMIS51053.2020.9322931,NON-CONFORMING PRODUCTS MANAGEMENT IN A DIGITAL QUALITY MANAGEMENT SYSTEM,"This article addresses the issue of changes in the quality management system, with the digitalization of the company. More specifically, changes regarding the process of managing non-conforming products. The article reflects how the use of digital tools and new technologies affects the process of detecting non-conformities and how the procedure for working with non-conforming products changes: how is the collection of data on the characteristics of the facility, what decisions need to be made regarding the inconsistencies, how the structure of work with non-conformances is changing and to what extent the use of information tools is beneficial.",2020,
187,10.1109/ISEEE.2017.8170630,HARNESSING THE POTENTIAL OF BIG DATA IN ROMANIAN HEALTHCARE,"Big Data technologies provide new opportunities to enable improved methods and facilities for raising the quality and efficiency of healthcare services. The specific characteristics of health data and information, their huge amount and diversity of sources have imposed the intensive use of Big Data Analytics in the health domain. Health software applications have become sustainable and compulsory tools for providing better healthcare delivery. This paper brings forward the boost brought by the potential use of Big Data Analytics inside some Romanian health informatics systems.",2017,
188,10.1109/UIC-ATC.2017.8397405,NEW GPU-BASED SWARM INTELLIGENCE APPROACH FOR REDUCING BIG ASSOCIATION RULES SPACE,"This paper deals with exploration and mining of association rules in big data, with the big challenge of increasing computation time. We propose a new approach based on metarules discovery that gives to the user the summary of the rules' space through a meta-rules representation. This allows the user to decide about the rules to take and prune. We also adapt a pruning strategy of our previous work to keep only the representatives rules. As the meta-rules space is much larger than the rules space, two approaches are proposed for efficient exploitation. The first one uses a bees swarm optimization method in the meta-rules discovery process, which is extended using GPU-based parallel programming to form the second one. The sequential version has been first tested using medium rules set, and the results show clear improvement in terms of the number of returned meta-rules. The two versions have then been compared on large scale rules sets, and the results illustrate the acceleration on the summarization process by the parallel approach without reducing the quality of resulted meta-rules. Further experiments on Webdocs big data instances reveal that the proposed method of pruning rules by summarizing metarules considerably reduces the association rules space compared to state-of-the-art association rules mining-based approaches.",2017,
189,10.1109/BigData47090.2019.9006479,BIG DATA AND TRADITIONAL CHINESE MEDICINE (TCM): WHAT’S STATE OF THE ART?,"Big data and traditional Chinese medicine (TCM) is a new interdisciplinary field quietly emerging in Chinese society. Internet of Things (IoT) sensor system technology is currently being developed to gather large volumes of personal data so that TCM, specifically herbal pharmaceutics, can be applied to treat acute and chronic diseases alike utilizing low cost, safe, and effective treatment protocols that have been prescribed in clinical practice for thousands of years. Through a survey of existing literature, this paper investigates what a future state of medicine may look like with the deployment of a big data TCM system as a means to enhance humankind's health and quality of life.",2019,
190,10.1109/EEBDA53927.2022.9744995,RESEARCH ON TOURISM NETWORK MARKETING MATHEMATICAL STRATEGY MODEL UNDER MOBILE INTERNET PLUS AND BIG DATA,"With the transformation of Chinese tourism from sightseeing to leisure and entertainment, and then to vacation experience, the key points of tourism marketing have also changed from channel and brand to content, with essential changes. With the arrival of the era of “Mobile Internet Plus Big Data”, and the combination of virtual reality technology and network technology, the traditional tourism marketing methods are difficult to meet the needs of the development of the new era. Therefore, in the era of “Mobile Internet Plus Big Data”, tourism marketing needs to develop with new ideas, make full use of the Internet and big data technology, and vigorously develop the new tourism network marketing, so as to realize the high-quality development of tourism. Starting from the era background of the Internet and big data, on the basis of defining and clarifying the concepts of Internet +, big data and tourism network marketing, this paper deeply explores the main channels and advantages of tourism network marketing in China, and then puts forward some countermeasures of tourism network marketing in the new era.",2022,
191,,THE POTENTIAL OF BIG DATA ANALYTICS TO REPLACE MANAGERIAL DECISION-MAKING: FINDINGS OF A SYSTEMATIC REVIEW,"Organisations have been collecting big data - high volume structured and unstructured data - at a rapid rate from a range of different sources. Big data can add value to organisations if processed, analysed and presented correctly. Organisations can benefit significantly from big data analytics if they have the capability to uncover hidden information from fully automated decision-making process. The purpose of this paper is to demonstrate the ability of big data analytics to replace managerial decision-making. A systematic literature review approach is used to review relevant studies that are validated through qualitative data analysis. The potential to replace managerial decision-making with fully automated data driven decisions varies per problem domain. There is evidence that managerial, operational and tactical decision-making can be replaced by big data analytics for stable conditions using structured data. From the study, European countries are the ones benefiting highly from tactical and operational decisions without managerial control. Decision-makers should, therefore, re-evaluate strategic decisions resulting from automated processing from unstable conditions for high quality decisions and prevention of errors. Finding from an African context were based on case studies from Egypt, Morocco and South Africa. The case study from Egypt used an experimental case on retail sector with proven decisions effectiveness.",2021,2576859X
192,10.1109/ICICAS48597.2019.00063,STUDY ON TEACHING MODE SELECTION MODEL BASED ON BIG DATA,"the selection of teaching mode is related to the improvement of teaching quality. In order to improve the quality of teaching and promote the transformation of teaching results, combined with big data statistical analysis method, a choosing teaching method is proposed based on big data's analysis, which constructs the statistical analysis model of teaching mode selection by descriptive statistical analysis method, and extracts the quantity of characteristic information that reflects the optimization of teaching method selection. The big data analysis model based on fuzzy C-means clustering is used to evaluate the performance of teaching methods, and the method of segmental sample detection is used to carry out regression analysis to realize the sample fitting of teaching method selection. The mathematical model of teaching mode selection is designed through the sample fitting result, and the statistical mathematical model based on big data's teaching mode selection is constructed. The simulation results show that the method is used to select the teaching method; it can effectively extract the regular characteristic quantity, which reflects the teaching performance, realizes the association rule mining of the choice of teaching method. The teaching mode is selected according to the result of big data mining and statistical analysis, improving the teaching quality, and promoting the reform of teaching methods.",2019,
193,10.1109/IEIT53597.2021.00031,DESIGN AND FUNCTION OF TEACHING INCENTIVE MECHANISM OF UNIVERSITY TEACHERS UNDER THE BACKGROUND OF BIG DATA,"Data analysis and application have greatly changed the teaching management and evaluation system and provided an opportunity for scientific management. This paper takes the way of teaching incentive of university teachers as the breakthrough point, expounds the improvement path of the teaching incentive mechanism of university teachers under the background of big data. The paper introduces key technology of big data in education, such as educational data mining and learning analysis technology, and at the same time designs a whole-process teaching incentive system for university teachers based on big data. The system is built on the Hadoop big data platform, using the column-oriented distributed open source database HBase as the database system. It carries on the distributed computation to the teacher's scientific research, the teaching administration, the personnel information, the examination reward and punishment, and provides the decision basis for the teacher, s teaching incentive. In the context of big data, positive incentives, such as teaching awards and salary incentives, and negative incentives, such as supervision and assessment, can effectively improve the teaching quality, further improve the teaching level, promote the education and teaching management in colleges and universities, and lay a solid foundation for the implementation of the fundamental task of cultivating people by virtue.",2021,
194,10.1109/ICRIS52159.2020.00104,APPLICATION STRATEGY OF ARMED POLICE FORCE LOGISTICS CONSTRUCTION BASED ON DATA SYSTEM ENGINEERING,"In order to overcome the problems existing in the logistics construction of the armed police force, such as the data standard is not unified and the real-time monitoring is not available, this paper proposes a novel application strategy of the armed police force logistics construction Based on data system engineering. The application strategy starts from three dimensions: logistics command decision, logistics fine management and logistics precise support. In addition, the application strategy is introduced into data system engineering, which makes full use of the advantages of data system engineering, and analyzes the feasibility of data acquisition, data use and data processing. The research results show that the application strategy can improve the efficiency and quality of the armed police force logistics work scientifically and efficiently.",2020,
195,10.1109/BigData.2017.8257965,LOW-RANK SINGULAR VALUE THRESHOLDING FOR RECOVERING MISSING AIR QUALITY DATA,"With the increasing awareness of the harmful impacts of urban air pollution, air quality monitoring stations have been deployed in many metropolitan areas. These stations provide air quality data to the public. However, due to sampling device failures and data processing errors, missing data in air quality measurements is common. Data integrity becomes a critical challenge when such data are employed for public services. In this paper, we investigate the mathematical property of air quality measurements, and attempt to recover the missing data. First, we empirically study the low rank property of these measurements. Second, we formulate the low rank matrix completion (LRMC) optimization problem to reconstruct the missing air quality data. The problem is transformed using duality theory, and singular value thresholding (SVT) is employed to develop sub-optimal solutions. Third, to evaluate the performance of our methodology, we conduct a series of case studies including different types of missing data patterns. The simulation results demonstrate that the proposed SVT methodology can effectively recover missing air quality data, and outperform the existing Interpolation. Finally, we investigate the parameter sensitivity of SVT. Our study can serve as a guideline for missing data recovery in the real world.",2017,
196,10.1109/UCC.2014.111,THE EMERGENT PROJECT: EMERGENCY MANAGEMENT IN SOCIAL MEDIA GENERATION DEALING WITH BIG DATA FROM SOCIAL MEDIA DATA STREAMS,The Emergent project will use social media to support the management of large scale emergencies. The project includes the construction of a big online store of data which will be continuously mined to provide emergency information and alerts. The overall objective is a stronger connection between citizens and emergency management authorities through social media.,2014,
197,10.1109/SEAA51224.2020.00045,FROM A MONOLITHIC BIG DATA SYSTEM TO A MICROSERVICES EVENT-DRIVEN ARCHITECTURE,"Context: Data-intensive systems, a.k.a. big data systems (BDS), are software systems that handle a large volume of data in the presence of performance quality attributes, such as scalability and availability. Before the advent of big data management systems (e.g. Cassandra) and frameworks (e.g. Spark), organizations had to cope with large data volumes with custom-tailored solutions. In particular, a decade ago, Tecgraf/PUC-Rio developed a system to monitor truck fleet in real-time and proactively detect events from the positioning data received. Over the years, the system evolved into a complex and large obsolescent code base involving a costly maintenance process. Goal: We report our experience on replacing a legacy BDS with a microservice-based event-driven system. Method: We applied action research, investigating the reasons that motivate the adoption of a microservice-based event-driven architecture, intervening to define the new architecture, and documenting the challenges and lessons learned. Results: We perceived that the resulting architecture enabled easier maintenance and faultisolation. However, the myriad of technologies and the complex data flow were perceived as drawbacks. Based on the challenges faced, we highlight opportunities to improve the design of big data reactive systems. Conclusions: We believe that our experience provides helpful takeaways for practitioners modernizing systems with data-intensive requirements.",2020,
198,10.1109/NITC.2017.8285647,A CLOUD PLATFORM FOR BIG IOT DATA ANALYTICS BY COMBINING BATCH AND STREAM PROCESSING TECHNOLOGIES,"The Internet of things is a current major developing technology, which is a network of everyday physical objects that enhances the quality of lifestyle. Application of the internet of things encounters dealing with huge amount of data. One of the directions of big data is this huge amount of data with respect to the internet of things. As the name implies, big data refers to the data that cannot be analyzed by a traditional data processing software. The key challenge of this phenomenon is to use a proper way to analyse, which can provide useful features from the data absorbed by the perception layer of the internet of things in order to provide feedback to end users, which helps them in better decision making and improves the performance of the corresponding internet of things network. Analysis of big data in the internet of things is obviously a hard task. Data storages are distributed and there should be parallel data processing. Transmission of the data across the network can slow down because of the massive amount of data. In this regard, this paper focuses on how to analyze the massive and heterogeneous data of the internet of things in a proper way. At first, the internet of things and the big data are discussed separately with architectures, applications, challenges etc. Since these two technologies are interrelated, data analysis in the internet of things is discussed with various methodologies and challenges-Finally, the study discusses a proper framework that can analyze the big data in the internet of things in an efficient way.",2017,
199,10.1109/ICCEAI55464.2022.00162,RESEARCH ON THE CONSTRUCTION AND APPLICATION OF SMART CLASSROOM TEACHING SERVICE PLATFORM BASED ON BIG DATA,"In order to solve the problems of the independence of online autonomous learning and live teaching, and the lack of data analysis on the website of excellent courses, this paper studies and designs an intelligent teaching service platform based on Django + Vue framework. The platform adopts B2B2C online education mode, and designs four functional systems based on PC and mobile terminals, including student terminal, lecturer terminal, enterprise terminal and platform terminal. The platform uses Python as the data processing language, through the user's behavior, using collaborative filtering recommendation algorithm modeling, to provide personalized recommendation for users. By using data visualization technology and data mining technology, the learning progress and knowledge mastery of students are visualized. The project can provide high-quality free online course resources for student users. Improve the learning effect of students, while bringing traffic to the one-to-one teaching business line of enterprises, help to increase the conversion rate of paying users, and is of great significance to the realization of the whole online education ecological closed-loop.",2022,
0,10.1109/GCWOT49901.2020.9391620,BIG DATA PREDICTIVE ANALYTICS FOR APACHE SPARK USING MACHINE LEARNING,"In today's digital world data is producing at a rapid speed and handling this massive diverse data become more challenging. The environment of big data is capable of handling data efficiently from data warehouses and in real-time. In Big data environment, Apache Spark is cluster-based, open-source computing technology explicitly designed for bulky data handling. Apache spark services are to perform composite Analytics through in-memory processing. This plays an active role in making meaningful exploration through machine learning and processes a large amount of data. Machine learning API is known as Mllib. It is highly prominent and efficient for big data platforms also offers excellent functionalities. In this paper, we have performed an experiment to look at the analytical qualities of Mllib in the apache spark environment. Likewise, we have highlighted the modern tendencies of Machine learning in big data studies and provides an understanding of upcoming work.",2020,
1,10.1109/ICSSSM.2013.6602615,BIG DATA: UNLEASHING INFORMATION,"Summary form only given. At present, it is projected that about 4 zettabytes (or 10**21 bytes) of electronic data are being generated per year by everything from underground physics experiments to retail transactions to security cameras to global positioning systems. In the U. S., major research programs are being funded to deal with big data in all five economic sectors (i.e., services, manufacturing, construction, agriculture and mining) of the economy. Big Data is a term applied to data sets whose size is beyond the ability of available tools to undertake their acquisition, access, analytics and/or application in a reasonable amount of time. Whereas Tien (2003) forewarned about the data rich, information poor (DRIP) problems that have been pervasive since the advent of large-scale data collections or warehouses, the DRIP conundrum has been somewhat mitigated by the Big Data approach which has unleashed information in a manner that can support informed - yet, not necessarily defensible or knowledgeable - decisions or choices. Thus, by somewhat overcoming data quality issues with data quantity, data access restrictions with on-demand cloud computing, causative analysis with correlative data analytics, and model-driven with evidence-driven applications, appropriate actions can be undertaken with the obtained information. New acquisition, access, analytics and application technologies are being developed to further Big Data as it is being employed to help resolve the 14 grand challenges (identified by the National Academy of Engineering in 2008), underpin the 10 breakthrough technologies (compiled by the Massachusetts Institute of Technology in 2013) and support the Third Industrial Revolution of mass customization.",2013,21611904
2,10.1109/iThings/GreenCom/CPSCom/SmartData.2019.00147,Q-LEARNING BASED ENERGY HARVESTING FOR HETEROGENEOUS STATISTICAL QOS PROVISIONING OVER MULTIHOP BIG-DATA RELAY NETWORKS,"With the increasing demand for the data-intensive wireless multimedia services over the time-varying wireless channels, the big-data based wireless network problem demands the 5G candidate framework to process such massive amount of multimedia data without causing extra burden to the backhaul links in supporting the heterogeneous statistical delay-bounded quality-of-service (QoS) provisionings. Due to the benefits of energy harvesting (EH) technologies, wireless devices are able to support the data-intensive wireless multimedia services by harvesting energy from the environment. Energy harvesting has emerged as the promising technology to solve the energy supply problem while bringing new challenges due to the stochastic nature of the harvested energy in supporting the heterogeneous statistical quality-of-service (QoS) provisionings. However, due to the unknown dynamics of the harvested energy as well as the channel state information (CSI), it is challenging to design the efficient routing protocol for selecting the optimal routing and power allocation policies under the statistical delay-bounded QoS constraints. To overcome the aforementioned problems, in this paper we propose the Q-learning based optimal routing and power allocation policies through learning from the history of the energy harvesting process while satisfying the heterogeneous statistical delay-bounded QoS constraints over multihop big-data relay networks. In particular, under the heterogeneous statistical delay-bounded QoS requirements, we formulate the end-to-end effective-capacity optimization problem for the battery-free energy harvesting based big-data multihop relay networks. Then, we apply the Markov decision process as well as Q-learning methods for deriving the optimal multihop routing algorithms over big-data multihop relay networks. Also conducted is a set of simulations which evaluate the system performances and show that our proposed Q-learning based multihop routing scheme outperforms the other existing schemes under the heterogeneous statistical delay-bounded QoS constraints over multihop big-data relay networks.",2019,
3,10.1109/ICCCBDA.2018.8386529,TOWARDS AN OPEN SCIENCE PLATFORM FOR THE EVALUATION OF DATA FUSION,"Combining the results of different search engines in order to improve upon their performance has been the subject of many research papers. This has become known as the “Data Fusion” task, and has great promise in dealing with the vast quantity of unstructured textual data that is a feature of many Big Data scenarios. However, no universally-accepted evaluation methodology has emerged in the community. This makes it difficult to make meaningful comparisons between the various proposed techniques from reading the literature alone. Variations in the datasets, metrics, and baseline results have all contributed to this difficulty. This paper argues that a more unified approach is required, and that a centralised software platform should be developed to aid researchers in making comparisons between their algorithms and others. The desirable qualities of such a system have been identified and proposed, and an early prototype has been developed. Re-implementing algorithms published by other researchers is a great burden on those proposing new techniques. The prototype system has the potential to greatly reduce this burden and thus encourage more comparable results being generated and published more easily.",2018,
4,10.1109/ICCES51350.2021.9488980,CROSS-BORDER E-COMMERCE LOGISTICS DEVELOPMENT MODEL BASED ON DATA MINING AND QOS,"The development of cross-border e-commerce has become an important endogenous driving force for China's consumption-driven economic growth. Cross-border e-commerce refers to an international trade activity in which transaction subjects in different countries electronically conclude transactions, make payment and settlement through traditional trade links through e-business platforms and means, and deliver goods and complete transactions through cross-border logistics and long-distance warehousing. Big data mining engine is also limited by its professional use mode, resulting in a skill threshold. At the same time, the popularity of mobile terminals makes people's working environment no longer fixed. In the case that the cluster is often shared by multiple users, a single task often occupies a large amount of computing resources, and the mining of the same data by multiple tasks will inevitably produce repeated operations, which will bring about problems such as low scheduling flexibility, idle resources and low processing efficiency. QoS (Quality of Service) is an important nonfunctional parameter that affects user experience. However, QoS parameter values are generally not easy to obtain, so the typical Web service recommendation system has the problem of sparse QoS data, which reduces the accuracy of prediction. In order to solve this problem, this paper proposes a recommendation method based on user preference to predict the value of QoS to recommend better quality service.",2021,
5,10.1109/EIT48999.2020.9208288,SYSTEMATIC MAPPING STUDY OF NON-FUNCTIONAL REQUIREMENTS IN BIG DATA SYSTEM,"Big data has become the most popular and influential to exist in this competitive digital world. In this regard, the selection of suitable quality attributes in big data software architecture can play a million-dollar solution. In this paper, we work on gathering and understanding key non-functional requirements in the domain of big data systems. Using Systematic Mapping Study (SMS) as a research methodology, we find more than 40 different quality attributes related to big data systems. Then, we implement the ISO/IEC 25010:2011 quality model to map all these NFRs into 8 characteristics of ISO/IEC 25010:2011 model. Finally, we show performance efficiency, functional suitability, reliability, security, usability, and scalability are the most important quality attributes for a data-intensive system. Outcomes from this survey will assist software developers to understand NFRs for data-intensive systems and identify them long before implementing them into practice.",2020,21540357
6,10.1109/I2CT.2018.8529703,DETECTION AND PREVENTION OF HIV AIDS USING BIG DATA TOOL,"Big Data is an accumulation of data sets which are abundant and intricate in character. They comprise both structured and unstructured data that evolve abundant, so speedy they are not convenient by classical relational database systems or current analytical tools. Big Data Analytics is not linearly able to expand. It is a predefined schema. Big data analysis is the demanding one because it contains huge number of records. In today's world, the massive instruction about health care is to be prepared in order to recognize, diagnose, identify and prevent the various diseases. It is projected to develop a centralized patient monitoring system using big data. Health care is the conservation or advancement of health along the avoidance, interpretation and medical care of disorder, bad health, abuse, and other substantial and spiritual deterioration in mortal. One of the best areas where big data can be used to form an advance medical care. Medical care has the potential to reduce costs of treatment, predict outbreaks of epidemics, avoid preventable diseases and improve the quality of life in general. Average human lifespan is increasing along world population, which poses new challenges to today's treatment delivery methods. A disease is a particular abnormal condition that affects part or all of an organism not caused by external force and that consists of a disorder of a structure or function, usually serving as an evolutionary disadvantage. The study of disease is called pathology, which includes the study of cause. This paper mainly focuses on the prediction of disease like HIV/AIDS using R programming.",2018,
7,10.1109/ICVRIS51417.2020.00150,RESEARCH ON STATIC SOFTWARE DEFECT PREDICTION ALGORITHM BASED ON BIG DATA TECHNOLOGY,"The static page processing software is easily disturbed by code defects, which causes the static page processing software to be paralyzed, thus making the accuracy of the static page processing poor. In order to improve the automatic prediction capability of the static page processing software, a code defect prediction technology for the static page processing software based on big data fusion and defect feature location technology algorithm is proposed, and the syntax running state characteristics of the operation and maintenance control management layer and software source code of the static page processing software are analyzed and tested. Using polymorphic software to drive the control program to carry out fault feature monitoring and information fusion of the page static processing software, carrying out polymorphic factor fusion and state feature analysis on the large data of defect fault feature distribution in the pseudo code of the software control program, combining Boehm model and ISO/IEC 9126 model to realize fault feature point location and defect active prediction of the page static processing software, According to the logicality of functions, codes and state variables of the page static processing software, the method of software running program continuity and similarity feature detection is adopted to realize the self-adaptive defect prediction and positioning of the page static processing software, and the global convergence control in defect prediction of the page static processing software is carried out by combining the big data fusion and defect feature positioning algorithm. The simulation results show that the prediction accuracy of page static processing software defects using this method is higher, the localization of defect codes is better, and the reliable operation capability of page static processing software is improved.",2020,
8,10.1109/BigData47090.2019.9006251,DETECTING HARDLY VISIBLE ROADS IN LOW-RESOLUTION SATELLITE TIME SERIES DATA,"Massive amounts of satellite data have been gathered over time, holding the potential to unveil a spatiotemporal chronicle of the surface of Earth. These data allow scientists to investigate various important issues, such as land use changes, on a global scale. However, not all land-use phenomena are equally visible on satellite imagery. In particular, the creation of an inventory of the planet's road infrastructure remains a challenge, despite being crucial to analyze urbanization patterns and their impact. Towards this end, this work advances data-driven approaches for the automatic identification of roads based on open satellite data. Given the typical resolutions of these historical satellite data, we observe that there is inherent variation in the visibility of different road types. Based on this observation, we propose two deep learning frameworks that extend state-of-the-art deep learning methods by formalizing road detection as an ordinal classification task. In contrast to related schemes, one of the two models also resorts to satellite time series data that are potentially affected by missing data and cloud occlusion. Taking these time series data into account eliminates the need to manually curate datasets of high-quality image tiles, substantially simplifying the application of such models on a global scale. We evaluate our approaches on a dataset that is based on Sentinel 2 satellite imagery and OpenStreetMap vector data. Our results indicate that the proposed models can successfully identify large and medium-sized roads. We also discuss opportunities and challenges related to the detection of roads and other infrastructure on a global scale.",2019,
9,10.1109/APARM49247.2020.9209382,RESEARCH ON THE TECHNOLOGY TO BUILD SAFETY INTEGRATION MODEL OF COMPLEX SYSTEM BASED ON RELEVANT FAILURE,"In view of the problems that the existing safety modeling methods can not represent the functions, processes, behaviors and their relevant characteristics of complex system, combined with the requirements for safety work of complex systems, the research on the technology to build safety integrated model of complex system based on the relevant failure is carried out. Firstly, the analysis method for relevant failure characteristics of complex systems is given. Secondly, the method and process to build safety integrated model of complex system is put forward by using the technology of relevant failure analysis, function structure success tree, event sequence diagram and so on. Finally, an application case about a control system in aerospace is given, and the safety integrated model of the control system is built. The feasibility and effectiveness of the technical methods proposed in this paper are verified through comparative analysis.",2020,
10,10.1109/ASONAM.2016.7752415,ARGUMENTS AND INTERPRETATION IN BIG SOCIAL DATA ANALYSIS: A SURVEY OF THE ASONAM COMMUNITY,"Big social data is becoming an important part of human decision making around the world. With the high stakes of decisions based on technical systems, it is important to evaluate the role of researchers in shaping that shared future. I present the results of a survey of 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining participants who performed big social data analyses. The goal was to understand how data scientists use interpretation to complete their projects and how they communicate results to their audience. By looking at research design as both a technical roadmap and an argument, results generated from social media data sets can be evaluated for their quality. Ultimately, these results will assist in the creation of field-dependent evaluation standards than can be used by big social data researchers.",2016,
11,10.1109/TKDE.2020.2992456,LEVERAGING CURRENCY FOR REPAIRING INCONSISTENT AND INCOMPLETE DATA,"Data quality plays a key role in big data management today. With the explosive growth of data from a variety of sources, the quality of data is faced with multiple problems. Motivated by this, we study the multiple data cleaning on incompleteness and inconsistency with currency reasoning and determination in this paper. We introduce a 4-step framework, named ${\sf Imp3C}$Imp3C, for errors detection and quality improvement in incomplete and inconsistent data without timestamps. We achieve an integrated currency determining method to compute the currency orders among tuples, according to currency constraints. Thus, the inconsistent data and missing values are repaired effectively considering the temporal impact. For both effectiveness and efficiency consideration, we carry out inconsistency repair ahead of incompleteness repair. A currency-related consistency distance metric is defined to measure the similarity between dirty tuples and clean ones more accurately. In addition, currency orders are treated as an important feature in the missing imputation training process. The solution algorithms are introduced in detail with case studies. A thorough experiment on three real-life datasets verifies our method ${\sf Imp3C}$Imp3C improves the performance of data repairing with multiple quality problems. ${\sf Imp3C}$Imp3C outperforms the existing advanced methods, especially in the datasets with complex currency orders.",2022,23263865
12,10.1145/2808797.2809367,IMPORTANCE OF DATA MINING IN HEALTHCARE: A SURVEY,"In this survey, we collect the related information that demonstrate the importance of data mining in healthcare. As the amount of collected health data is increasing significantly every day, it is believed that a strong analysis tool that is capable of handling and analyzing large health data is essential. Analyzing the health datasets gathered by electronic health record (EHR) systems, insurance claims, health surveys, and other sources, using data mining techniques is very complex and is faced with very specific challenges, including data quality and privacy issues. However, the applications of data mining in healthcare, advantages of data mining techniques over traditional methods, special characteristics of health data, and new health condition mysteries have made data mining very necessary for health data analysis.",2015,
13,10.1109/BDEE52938.2021.00014,STUDY ON DESIGN AND APPLICATION OF DEVICE FOR SECONDARY INTELLIGENT TICKET CHECKING OF RAILWAY UNDER THE WAVE OF BIG DATA,"In the course of ""passenger transport histology"", the passenger transport process and ticketing equipment are analyzed in detail, and it is found that the AFC system of the railway can not effectively monitor passenger ticket purchase, the existence of short-distance passengers to buy tickets for long-distance vehicles, ticket evasion, ticket chaos and other phenomena. Through innovative guidance to students, teachers and students jointly designed a second ticket checking device on the train, and successfully applied for a patent for a utility model. The device forms an interactive monitoring system with the station ticket system, interactive fusion of large data of passenger flow in railway station. Through the introduction of the structure and principle of the intelligent secondary ticket checking device, this paper makes use of various identification technologies to carry out secondary ticket checking for passengers, By identifying the passenger information, verifying the identity of the passenger, replacing or otherwise dealing with the passenger who does not get on the train correctly, the accuracy of the passenger ""person-ticket-card"" can be guaranteed, the manpower for checking tickets can be saved, and the quality of checking tickets can be improved, improve the technical level of ticketing system and passenger satisfaction.",2021,
14,10.1109/TII.2019.2939573,A BIG DATA-ENABLED CONSOLIDATED FRAMEWORK FOR ENERGY EFFICIENT SOFTWARE DEFINED DATA CENTERS IN IOT SETUPS,"The rapidly evolving industry standards and transformative advances in the field of Internet of Things are expected to create a tsunami of Big Data shortly. This, in turn, will demand real-time data analysis and processing from cloud computing platforms. A substantial part of the computing infrastructure is supported by large-scale and geographically distributed data centers (DCs). Nevertheless, these DCs impose a substantial cost in terms of rapidly growing energy consumption, which in turn adversely affects the environment. In this context, efficient resource utilization is seen as a potential candidate to enhance energy efficiency and minimize the load on the power sector. Nevertheless, in the majority of the public clouds, the resources are idle most of the time (i.e., under-utilized) as the load of the servers is unpredictable; thereby leading to a lofty increase in the energy utilization index and wastage of resources. Thus, it is highly essential to devise a precise and efficient resource management technique. Therefore, in this article, we leverage the advantages of software defined data centers (SDDCs) to minimize energy utilization levels. Precisely, SDDC refers to the process of programmatically abstracting the logical computing, network, and storage resources; and configuring them in real-time based on workload demands. In detail, we demonstrate the possibility of 1) designing a consolidated SDDC-based model to jointly optimize the process of virtual machine (VM) deployment and network bandwidth allocation for reduced energy consumption and guaranteed quality of service (QoS), particularly for heterogeneous computing infrastructures; 2) formulating a multiobjective optimization problem to deduce the optimal allocation of resources for both critical and noncritical applications; and 3) designing an efficient scheme based on heuristics to provide suboptimal results for the formulated multiobjective optimization problem. The proposed article presents a suboptimal approach based on first fit decreasing algorithm. Further, our empirical evaluations suggest that the proposed framework leads to almost 27.9% savings in terms of energy consumptions against the existing schemes with negligible QoS violations (approximately 0.33).",2020,19410050
15,10.1109/SERVICES.2019.00096,IDEAAS: INTERACTIVE DATA EXPLORATION AS-A SERVICE,"Recently servitization has been proposed as a strategic business innovation to enrich products offerings with the delivery of remote services (e.g. remote monitoring services), thus also improving the perception of the product quality. The increasing connections of systems that produce high volumes of real time data have raised the need for advanced Data Exploration techniques able to face the impact of Big Data, in order to make remote monitoring services sustainable. In this paper, the IDEAaS (Interactive Data Exploration As-a-Service) approach is presented, apt to support and ease exploration of real time data in a dynamic context of interconnected systems, where large amounts of data must be incrementally collected, organised and analysed on-the-fly. The proposed approach relies on three main pillars: (i) a multi-dimensional organisation of data, for data exploration according to different analysis dimensions; (ii) data summarisation, based on incremental clustering algorithm, to provide summarised representation of collected data streams; (iii) data relevance evaluation techniques, to attract the users attention on relevant data only during exploration. Finally, the approach has been tested in a Smart Factory context, applying the interactive data exploration techniques in order to assist anomaly detection in remote monitoring services.",2019,23783818
16,10.1109/ICICTA51737.2020.00135,RESEARCH ON DATA SEARCH OPTIMIZATION UNDER BIG DATA BASED ON ASSOCIATION RULE ALGORITHM,"The construction industry is an important pillar industry of our national economy. It has played an enormous role in promoting China's economic development and social progress. With the establishment of the socialist market economy and the continuous deepening of reforms, the competition among construction companies within the construction industry has become increasingly fierce, the construction period has shortened, the quality requirements have become higher, the cost of construction materials has continued to rise, and the labor costs have continued to increase. The survival and development of enterprises have brought great pressure, the competition in the construction industry has become increasingly fierce, and the profit-making space has continued to decrease, making construction companies have to develop towards refinement, information, and integration. And level, in order to improve the management level of construction companies, to achieve the “construction of internal quality, appearance of the tree” of construction companies, and ultimately improve the competitiveness of the construction industry in the world.",2020,
17,10.1109/MCOM.2018.1700231,QOE-DRIVEN BIG DATA ARCHITECTURE FOR SMART CITY,"In the era of big data, the applications/services of the smart city are expected to offer end users better QoE than in a conventional smart city. Nevertheless, various types of sensors will produce an increasing volume of big data along with the implementation of a smart city, where we face redundant and diverse data. Therefore, providing satisfactory QoE will become the major challenge in the big-data-based smart city. In this article, to enhance the QoE, we propose a novel big data architecture consisting of three planes: the data storage plane, the data processing plane, and the data application plane. The data storage plane stores a wide variety of data collected by sensors and originating from different data sources. Then the data processing plane filters, analyzes, and processes the ocean of data to make decisions autonomously for extracting high-quality information. Finally, the application plane initiates the execution of the events corresponding to the decisions delivered from the data processing plane. Under this architecture, we particularly use machine learning techniques, trying to acquire accurate data and deliver precise information to end users. Simulation results indicate that our proposals could achieve high QoE performance for the smart city.",2018,15581896
18,10.1109/BigData.2013.6691561,CLUSIVAT: A MIXED VISUAL/NUMERICAL CLUSTERING ALGORITHM FOR BIG DATA,"Recent algorithmic and computational improvements have reduced the time it takes to build a minimal spanning tree (MST) for big data sets. In this paper we compare single linkage clustering based on MSTs built with the Filter-Kruskal method to the proposed clusiVAT algorithm, which is based on sampling the data, imaging the sample to estimate the number of clusters, followed by non-iterative extension of the labels to the rest of the big data with the nearest prototype rule. Numerical experiments with both synthetic and real data confirm the theory that clusiVAT produces true single linkage clusters in compact, separated data. We also show that single linkage fails, while clusiVAT finds high quality partitions that match ground truth labels very well. And clusiVAT is fast: it recovers the preferred c = 3 Gaussian clusters in a mixture of 1 million two-dimensional data points with 100% accuracy in 3.1 seconds.",2013,
19,10.1109/JIOT.2019.2960526,BLOCKCHAIN-ENABLED DECENTRALIZED TRUST MANAGEMENT AND SECURE USAGE CONTROL OF IOT BIG DATA,"With the fast development of Internet-of-Things (IoT) technologies, IoT big data and its applications are getting more and more useful. However, traditional IoT data management is fragile and vulnerable. Once the gathered data are untrusted or the stored data are tampered with deliberately from the internal users or attacked by an external hacker, then the tampered data have a serious problem to be utilized. To solve the problems of trust and security of IoT big data management, in this article, we propose a permissioned blockchain-based decentralized trust management and secure usage control scheme of IoT big data (called BlockBDM), upon which all the data operations and management, such as data gathering, invoking, transfer, storage, and usage, are processed over the blockchain smart contract. To encourage the IoT client to supply high-quality content, in our scheme, we design public-blockchain-based tokens reward mechanism for the high-quality data supply contribution. All the data processing and usage procedure can be recorded in a cryptography-signed and Merkle tree-based transaction(s) and block(s) with high-level security in a global and distributed ledger with tamper resistance. For data utilization and consumption, we propose secure usage control for digital rights management and token-based data consumption approach of high-value data from being violated or spread without any limitation. We implemented the BlockBDM scheme based on public and permissioned blockchain for IoT big data management. Finally, a large amount of evaluation manifests that the proposed BlockBDM scheme is feasible, secure, and scalable for decentralized trust management of IoT big data.",2020,23722541
20,10.1109/ICCC51557.2021.9454644,IOT BIG DATA MANAGEMENT FOR IMPROVED RESPONSE TIME,"Internet of Things (IoT) impact in our daily life quickly evolves from recording basic data to process and take decisions based on complex data. Data provided by various sources (Health Care IoT, life quality, smart cities etc) came in many formats and will be used for statistical analysis and decision making, often based on data evolution and machine learning. Decisions involving data analysis over a huge database can be very time consuming. In critical time situations, system response time needs to be highly improved. This paper discusses a proposed architecture, suitable to improve the response time on systems using data analysis and decision based on data evolution. Simulation results for a system using sensors, gateways and persistence layers are presented and discussed. Considering all sensor data is sent to a non-relational database (NoSQL) suitable to store various data formats, this paper discusses the particularities of using IoT programmable gateways to send data used in critical time analysis to a fast relational database (SQL) database. In such databases, queries are very fast and, as a result, the decisions based on data evolution have an improved time response. Analysis to detect critical data patterns is triggered when data is inserted in SQL database or based on a time interval. Both strategies are discussed and analysed.",2021,
21,10.1109/ICBASE51474.2020.00102,TEST DATA GENERATION METHOD BASED ON SIAMESE NETWORK IN FACE RECOGNITION SYSTEM,"With the development of AI and big data technology, machine learning system has been widely used in various fields and achieved satisfactory results. However, due to the increasing complexity of machine learning system, there are inevitably a variety of unpredictable problems, errors or failures in machine learning system, which cause fatal damage to the quality and reliability of machine learning system. In order to test machine learning system better, automatic test data generation is the key technology. An improved test data generation method based on GAN and siamese network in face recognition system is proposed. The method focuses on discriminator module composed of siamese network which can better measure the similarity between real and fake images. The results show that the proposed test data generation algorithm can generate images that are very close to the real samples. The test data of machine learning system is enhanced.",2020,
22,10.1145/3278576.3278588,A COMPUTATIONAL EFFICIENT FUZZY CLUSTERING ALGORITHM FOR BIG INCOMPLETE LONGITUDINAL TRIAL DATA,"Our previous research has shown that the enhanced Fuzzy C-Means algorithm (eFCM) can improve the clustering quality in the big cross-sectional ""synthetic data"" [1] where a substantive number of clusters are identified with different degrees of overlap, with enhanced computational efficiency. Here, we provide additional evidence to showcase that our novel initialization method built in MIFuzzy [2-9] [10-12] can improve the computational efficiency in big longitudinal intervention data with missing values. Our numerical analyses further show this improvement in identifying clusters from simulated big incomplete longitudinal data generated using the parameters of our real longitudinal intervention data with missing values. Our findings imply the applicability of this algorithm to similar studies.",2018,
23,10.1109/ICACITE53722.2022.9823810,DETAILED INVESTIGATION OF INFLUENCE OF MACHINE LEARNING (ML) AND BIG DATA ON DIGITAL TRANSFORMATION IN MARKETING,"The study explores with Machine learning (ML), which is a type of neural network (AI) that empowers software programmers to start increasing prediction without being done with full to do so. Because data is so valuable, improving strategies for intelligently having to manage the now-ubiquitous content infrastructures is a necessary part of the process toward completely autonomous agents. Computer vision and computer vision have improved a wide range of industries, including medical diagnoses, data display and procedures, science and research, and so. Just as preparing for a sport may be risky for individuals who are prone to injury, learning from contaminated or erroneous data can be costly. As described in the article Approaching Data Science, incorrectly trained algorithms result in expenses for a corporation rather than savings. Because incorrectly labeled, missing, or irrelevant data might impair the accuracy of any algorithm, organizations must be able to vouch for the quality and integrity of any data sets, along with their sources.",2022,
24,10.1109/ECCE.2017.8095957,MULTI-TIME SCALE FORECAST FOR SCHEDULABLE CAPACITY OF EVS BASED ON BIG DATA AND MACHINE LEARNING,"The application of large-scale electric vehicles (EVs) into the future smart grid may bring about serious power quality problems. But EVs can provide ancillary services for the power system as distributed energy resources through Vehicle-to-grid (V2G) technology. The fast and accurate prediction of schedulable capacity (SC) of EVs is the first step to enable this benefit. In this paper, two different time scales schedulable capacity forecasting (SCF) methods are proposed, including the real-time SCF method and a day head SCF method. Firstly, the real time of SCF of EVs is calculated by collecting the real-time charging or discharging data of individual EVs and a distributed data parallel calculation method to solve problems of the mass data process and storage problem. Secondly, the large amount of results of the real-time SCF, is used as historical data, to train the SCF models one day ahead by parallel decision tree regression algorithm (PDTR) and parallel random forest regression algorithm (PRFR). These proposed methods are evaluated by the real time operation data involving 521 EVs in the filed in China on Spark. The test results show the advantage of using big data technology in the ability and speed of dealing with large-scale data, and PDTR is faster and better than PRFR in one day ahead SCF.",2017,
25,10.1109/ICAIBD49809.2020.9137477,RESEARCH ON RED WINE QUALITY BASED ON DATA VISUALIZATION,"Due to the rapid development of modern society, red wine has gradually become popular. Research on the quality of red wine has turn into an important topic. Red wine contains more than 600 kinds of ingredients, in terms of alcohol, minerals, tannic acid, citric acid, chloride and other substances. This paper analyzes 12 factors affecting red wine quality in the data set and studies the influence of each ingredient on the quality of red wine through data mining algorithm. Based on the data visualization of Python processing, classical visualization tools such as histogram, heat map, box-plot and Pearson correlation coefficient algorithm are used for data mining. The histogram is adopted for univariate analysis and the heat map composed of Pearson coefficient is used for multivariate analysis. Then, the box-plot is used for cross-verification. Finally, it is concluded that alcohol, sulfate, citric acid and volatile acidity are the decisive factors affecting the quality of red wine. This conclusion can not only be used as a reference for consumers to buy, but also provide suggestions for wine manufacturers to improve the quality of red wine.",2020,
26,10.1109/HPCS48598.2019.9188195,OPEN DATA MARKET ARCHITECTURE AND FUNCTIONAL COMPONENTS,"This paper discusses the principles of organisation and infrastructure components of Open Data Markets (ODM) that would facilitate secure and trusted data exchange between data market participants, and other cooperating organisations. The paper provides a definition of the data properties as economic goods and identifies the generic characteristics of ODM as a Service. This is followed by a detailed description of the generic data market infrastructure that can be provisioned on demand for a group of cooperating parties. The proposed data market infrastructure and its operation are employing blockchain technologies for securing data provenance and providing a basis for data monetisation. Suggestions for trust management and data quality assurance are discussed.",2019,
27,10.1109/HPCC-CSS-ICESS.2015.16,IMPROVING POWER GRID MONITORING DATA QUALITY: AN EFFICIENT MACHINE LEARNING FRAMEWORK FOR MISSING DATA PREDICTION,"Big data techniques has been applied to power grid for the evaluation and prediction of grid conditions. However, the raw data quality rarely can meet the requirement of precise data analytics since raw data set usually contains samples with missing data to which the common data mining models are sensitive. Though classic interpolation or neural network methods can been used to fill the gaps of missing data, their predicted data often fail to fit the rules of power grid conditions. This paper presents a machine learning framework (OR_MLF) to improve the prediction accuracy for datasets with missing data points, which mainly combines preprocessing, optimizing support vector machine (OSVM) and refining SVM (RSVM). On top of the OSVM engine, the scheme introduces dedicated data training strategies. First, the original data originating from data generation facilities is preprocessed through standardization. Traditional SVM is then trained to obtain a preliminary prediction model. Next, the optimized SVM predictors are achieved with new training data set, which is extracted based on the preliminary prediction model. Finally, the missing data prediction result depending on OSVM is selectively inputted into the traditional SVM and the refined SVM is lastly accomplished. We test the OR_MLF framework on missing data prediction of power transformers in power grid system. The experimental results show that the predictors based on the proposed framework achieve lower mean square error than traditional ones. Therefore, the framework OR_MLF would be a good candidate to predict the missing data in power grid system.",2015,
28,10.1109/ACIT.2018.8672697,THE EFFECTS OF NATURAL LANGUAGE PROCESSING ON BIG DATA ANALYSIS: SENTIMENT ANALYSIS CASE STUDY,"The social networks are one of the main sources of big data. Continuously, it produce huge volume of variety types of data at high velocity rates. This huge volume of data contains valuable information that requires efficient and scalable analysis techniques to be extracted. Hadoop/MapReduce is considered the most suitable framework for handling big data because of its scalability, reliability and simplicity. One of the basic applications to extract valuable information from data is the sentiment analysis. The sentiment analysis studies peoples' opinion by classifying their written text into positive or negative polarity. In this work, a sentiment analysis method for analyzing a Twitter data set is analyzed. The method uses the Naive Bayes algorithm for classifying the text into positive and negative polarity. Several linguistic and NLP preprocessing techniques were applied on the data set. The aim of these preprocessing techniques is to study their effects on the quality of big data classification. The applied preprocessing techniques have achieved an enhancement in the classification accuracy of the Naive Bayes algorithm. The experiments prove that the performance of the sentiment analysis is enhanced by 5% using NLP and linguistic processing, yielding an accuracy of 73 % on the used data set.",2018,
29,10.1109/CECIT53797.2021.00126,INTEGRATED DESIGN FRAMEWORK OF PRODUCT GENERAL QUALITY CHARACTERISTICS AND FUNCTIONAL CHARACTERISTICS,"In view of the problems existing in the current product development, such as the poor consistency of models and data, the gap between the design of general quality characteristics and functional characteristics, and the lack of systematicness of existing research, this paper studies the concept of integrated design of general quality characteristics and functional characteristics, defines the specific connotation of integrated design, and analyzes and studies the relevant elements of integrated design in combination with the product development process, builds an integrated design framework covering the whole process and characteristics of product development, clarifies the relationship between the elements of the integrated design framework from six dimensions, and as a according, a typical architecture of the integrated design software system based on the framework is given. Finally, take the development process of typical electronic system as an example, a general process of integrated design is given, which lays an important foundation for the implementation of integrated design and the promotion and application of related technologies.",2021,
30,10.1109/ICIS.2018.8466465,DISTANCE-BASED CLUSTERING OF MOVING OBJECTS’ TRAJECTORIES FROM SPATIOTEMPORAL BIG DATA,"With the rapid development of smart sensors, smartphones and social media, distributed sensors and tracking systems are generating overwhelming amounts of high velocity spatiotemporal big data. Clustering spatiotemporal data is an important way to mine hidden information behind moving object sampling data, such as understanding trends in movement patterns, gaining high popularity in geographic information and so on. However, the current approaches for clustering trajectory data generally do not apply for excessive costs in both scalability and computing performance for spatiotemporal big data. To find the clusters of moving objects’ trajectories, the issue is to find a distance measurement method that respects the geographic distance and the semantic similarity for each trajectory. Therefore, a distance measure to compute the spatial similarity between trajectories based on both geographical features and semantic features of motion is proposed in this research, then a framework for clustering moving objects trajectories is also designed to find out the groups of similar paths from big spatiotemporal data. The cluster quality of the proposed method is validated by means of external and internal validation criteria and is practically evaluated by TDrive datasets which is real trajectory dataset.",2018,
31,10.1109/BigData.2017.8258381,TRACK GEOMETRY BIG DATA ANALYSIS: A MACHINE LEARNING APPROACH,"Track geometry has a considerable effect on rail travel comfort and safety and deteriorates with age and tonnage. In order to maintain the track geometry quality, maintenance activities such as tamping, stone blowing and ballast undercutting are usually employed. However, these activities are ineffective if the underlying cause of track deformation such as subgrade failure is not addressed. Geosyn-thetics such as geocells and geogrids can be placed in the subballast which strengthens the layer, lowers the stresses on the weak subgrade and invariably enhances track geometry quality. Machine learning techniques are becoming increasingly imperative in processing and analyzing of large volumes of track geometry data which exhibit the classical attributes of big data. Several unsupervised and supervised learning techniques were used to analyze the effect of geocell installation on track geometry quality. Cluster analysis was used to group the track geometry data with major clusters found to differ by surface and alignment features. Principal component analysis was employed as an effective dimension reduction tool to simplify the track geometry data based on the proportion of variance explained. Supervised learning techniques such as multiple linear regression, decision tree regression, random forest regression and support vector regression were subsequently used to estimate and predict the effect of geocell installation on the track geometry quality. Random forest regression was found to the best performing model for both the original and dimensionally-reduced data.",2017,
32,10.1109/CCWC54503.2022.9720835,SCALABLE HINDSIGHT EXPERIENCE REPLAY BASED Q-LEARNING FRAMEWORK WITH EXPLAINABILITY FOR BIG DATA APPLICATIONS IN FOG COMPUTING,"Nowadays Internet of Things (IoT) applications are proliferating with explosive growth and their computational load is very high. Fog computing is an important structure used for processing the IoT devices data at cloud proximity. Big data applications demand scalable computing with stringent performance requirement. The currently available machine learning models do not match the growing scale of big data applications and lack explainability. In this paper an explainable Q-learning framework with hindsight experience replay (Q-HER) is developed to provide holistic scalability solution for big data applications using minimum number of fog nodes. The reward engineering process is streamlined and each episode with original goal and subset of other goals is repeated to yield high quality scalability policies. The mathematical modeling reveals that the generated scalability decisions satisfy the quality assurance parameters like correctness, robustness, model relevance, and ∊ -Differential Data privacy. The performance of the proposed Q-HER is found to be good towards the performance metrics like accuracy, latency, cost, and average resource wastage under two different scenarios of limited and unlimited processor fog nodes.",2022,
33,10.1109/BigData.2014.7004410,PARALLEL AND QUANTITATIVE SEQUENTIAL PATTERN MINING FOR LARGE-SCALE INTERVAL-BASED TEMPORAL DATA,"Mining frequent subsequences of patterns, or sequential pattern mining, has wide application in customer shopping sequence analysis, web log stream analysis, multi-modal behavioral studies, to name a few. To detect unknown, anomalous, and unexpected patterns from large-scale interval-based temporal data without complete a priori knowledge is challenging. In this paper, we present a framework - PESMiner which allows parallel and quantitative mining of sequential patterns at scale. Whereas most existing sequential mining algorithms can only find sequential orders of temporal events, our work presents a novel interactive temporal data mining algorithm capable of extracting precise temporal properties of sequential patterns. Furthermore, our work provides a unified parallel solution that scales our algorithms to larger temporal data sets by exploiting iterative MapReduce tasks. Comprehensive performance evaluations demonstrate that PESMiner significantly outperforms existing interval-based mining algorithms in terms of both quality (i.e. accuracy, precision, and recall) and scalability.",2014,
34,10.1109/COMPSAC54236.2022.00247,"TRADE-OFF BETWEEN PRIVACY, QUALITY AND RISK: ANONYMIZATION STRATEGY EVALUATION FOR DATA WAREHOUSES","The transformation of big data to the cloud requires us to reconsider trust. Trust in all parties involved in the data management, the infrastructure as well as all groups with an access interest. A common way to mitigate the risk of the identification of individuals in case of privacy breaches is anonymization, which consequently also leads to information loss. Depending on the assumed level of confidence, a data processor can control the risk for privacy breaches in changing the point where anonymization gets applied. We examined anonymization points in data warehouse scenarios to evaluate their effects on utility and re-identification risk. Our evaluation showed that data quality can differ up to 4,80% while the re-identification risk is reduced by up to 16,82 %. With still improved quality, the re-identification risk differs up to 53,49 % in another configuration.",2022,07303157
35,10.1109/CC.2018.8485466,SQOE KQIS ANOMALY DETECTION IN CELLULAR NETWORKS: FAST ONLINE DETECTION FRAMEWORK WITH HOURGLASS CLUSTERING,"The explosive growth of data volume in mobile networks makes fast online diagnose a pressing search problem. In this paper, an object-oriented detection framework with a two-step clustering, named as Hourglass Clustering, is given. Where three object parameters are chosen as Synthetical Quality of Experience (SQoE) Key Quality Indicators (KQIs) to reflect accessibility, integrality, and maintainability of networks. Then, we choose represented Key Performance Indicators (rKPIs) as cause parameters with correlation analysis. For these two kinds of parameters, a hybrid algorithm combining the self-organizing map (SOM) and k-medoids is used for clustering them into different types. We apply this framework to online anomaly detection in Cellular Networks, named SQoE-driven Anomaly Detection and Cause Location System (SQoE-ADCL). Our experiments with real 4G data show that besides fast online detection, SQoE-ADCL makes a better soft decision instead of a traditional hard decision. Furthermore, it is also a general way of being applied to other similar applications in big data.",2018,16735447
36,10.1109/BigData.2014.7004457,DEALING WITH HETEROGENEOUS BIG DATA WHEN GEOPARSING HISTORICAL CORPORA,"It has long been known that `variety' is one of the key challenges and opportunities of big data. This is especially true when we consider the variety of content in historical corpora resulting from large-scale digitisation activities. Collections such as Early English Books Online (EEBO) and the British Library 19th Century Newspapers are extremely large and heterogeneous data sources containing a variety of content in terms of time, location, topic, style and quality. The range of geographical locations referenced in these corpora poses a difficult challenge for state of the art geoparsing tools. In the context of our work on Spatial Humanities analyses, we present our solution for dealing with the variety and scale of these corpora.",2014,
37,10.1109/BigMM.2016.63,SCALABLE FUNCTIONAL DEPENDENCIES DISCOVERY FROM BIG DATA,"Functional dependencies (FDs) represent potentially novel and interesting patterns existent in relational databases. The discovery of functional dependencies has a wide range of applications such as database design, knowledge discovery, data quality assessment, etc. There has been growing interest in the problem of functional dependencies discovery in the last ten years. However, existing functional dependencies discovery algorithms are mainly applied to centralized small data. It is far more challenging to discover functional dependencies from big data. In this paper, we propose an efficient functional dependencies discovery algorithm, for mining functional dependencies from distributed big data. We prune candidate FDs at each node by local fragmented data and batch verify candidate FDs in parallel. Load balance is taken into account when discovering functional dependencies. Experiments show that the proposed algorithm is effective on real dataset and synthetic dataset.",2016,
38,10.1109/IGARSS.2015.7326686,INFERRING AIR QUALITY MAPS FROM REMOTELY SENSED DATA TO EXPLOIT GEOREFERENCED CLINICAL ONSETS: THE PAVIA 2013 CASE,"Recent developments in data acquisition, storage, mining and maintenance have allowed the flourishing of several multi-disciplinary research fields, which can be stated, defined and carried out according to the so-called Big Data paradigm. In this environment, the investigation and analysis of interactions between human phenomena and natural events play a key-role, as they can be fundamental for several applications, from sustainable development to community policy design and short-, medium- and long-range resource allocation planning. In this paper, we provide a study of the interplay between air pollution (as estimated by remotely sensed data processing) and clinical records, so that inferences and correlations among black particulate concentration, micro- and macro-vascular disease onsets and hospitalization tracks can be efficiently drawn. We focused on the second order administrative area of the city of Pavia, Italy, on 2013. Experimental results show how effective connections between the estimated air quality and the hospitalizations behavior can be accurately drawn and derived.",2015,21537003
39,10.1109/ICSA.2019.00013,AN ARCHITECTURE-DRIVEN ADAPTATION APPROACH FOR BIG DATA CYBER SECURITY ANALYTICS,"Big Data Cyber Security Analytics (BDCA) systems leverage big data technologies (e.g., Hadoop and Spark) for collecting, storing, and analyzing large volume of security event data to detect cyber-attacks. Accuracy and response time are the two most important quality concerns for BDCA systems. However, the frequent changes in the operating environment of a BDCA system (such as quality and quantity of security event data) significantly impact these qualities. In this paper, we first study the impact of such environmental changes. We then present ADABTics, an architecture-driven adaptation approach that (re)composes the system at runtime with a set of components to ensure optimal accuracy and response time. We finally evaluate our approach both in a single node and multinode settings using a Hadoop-based BDCA system and different adaptation scenarios. Our evaluation shows that on average ADABTics improves BDCA's accuracy and response time by 6.06% and 23.7% respectively.",2019,
40,10.1109/ICSCC51209.2021.9528292,"""I-CARE"" - BIG-DATA ANALYTICS FOR INTELLIGENT SYSTEMS","A very novel predicament for quantitative data science has been generated by the abundance of large, well-cured data sets in biological and social science, coupled with an extraordinary increase in computational ability. This is the possibility of sophisticated studies combined with remedial understanding. Analytics for intelligent systems should cover architecture of hardware platforms and application of software methods, technique and tools. It is anticipated that adapting dynamic memory information, processing parametric values of large data sheets with optimization, would be faster. The field of Big-Data Analytics under recent trends of Data Science studies various means of pre-processing, analyzing and filtering from huge and semi-structured data sets from different sources which are complex to be handled by traditional data processing systems. In addition to extracting and aggregating data from various main performance measures, this proposal also forecasts potential values for these KPIs (Key Performance Indicators) and alerts them when unfavorable values are about to occur. As AI and ML are implemented through different platforms and sectors including chat-bots, robotics, social media, healthcare, self-driven automobile and space exploration, large companies are investing in these fields, and the demand for ML and AI experts is growing accordingly. Python is becoming the most popular language for AI (Artificial Intelligence and Machine Learning) due to its rich supported tools. This proposed applications ""I-Care"" (Intelligent Care) provide recommendations to improve Quality of Service of Big-data analytics. So, the proposed paper examines the methodology and requirements, architecture, modeling and analytics with implementation and describes the architectural design and the results obtained by the pilot application using Python and its powerful tools like Pandas and Scikit-Learn.",2021,
41,10.1109/ICAIBD55127.2022.9820005,REVIEW OF PANEL DATA CLUSTERING METHODS AND APPLICATIONS,"Panel data has the dual characteristics of cross-sectional data and time series. Cluster analysis with panel data can better reflect the dynamic development trend and development state of indicators. This paper aims to study panel data clustering methods and application scenarios. Based on the analysis of relevant literatures retrieved from Chinese and English databases, the clustering methods of panel data are summarized. Based on the existing research results, the clustering methods of panel data are summarized into three categories: the method based on dimension reduction, the method based on similarity measure index design and the method based on grey correlation analysis. The advantages and disadvantages of each method are compared and analyzed. Then its application in economic analysis, air quality monitoring and other fields is introduced. Finally, the potential research directions in this field are proposed.",2022,
42,10.1109/JSAC.2019.2927088,INFORMATION-CENTRIC VIRTUALIZATION FOR SOFTWARE-DEFINED STATISTICAL QOS PROVISIONING OVER 5G MULTIMEDIA BIG DATA WIRELESS NETWORKS,"The multimedia transmission represents a typical big data application in the fifth-generation (5G) wireless networks. However, supporting multimedia big data transmission over 5G wireless networks imposes many new and open challenges because multimedia big data services are both time-sensitive and bandwidth-intensive over time-varying wireless channels with constrained wireless resources. To overcome these difficulties, in this paper we propose the information-centric virtualization architectures for software-defined statistical delay-bounded quality of service (QoS) provisioning over 5G multimedia big data wireless networks. In particular, our proposed schemes integrate the three 5G-promising candidate techniques to guarantee the statistical delay-bounded QoS for multimedia big data transmissions: 1) information-centric network (ICN), to derive the optimal in-network caching locations for multimedia big data; 2) network functions virtualization (NFV), to abstract the PHY-layer infrastructures into several virtualized networks to derive the optimal multimedia data contents delivery paths; and 3) software-defined networks (SDNs), to dynamically reconfigure wireless resources allocation architectures through the SDN-control plane. Under our proposed architectures, to jointly optimize the implementations of NFV and SDN techniques under ICN architectures, we develop the three virtual network selection and transmit-power allocation schemes to: 1) maximize single user's effective capacity; 2) jointly optimize the aggregate effective capacity and allocation fairness over all users; and 3) coordinate non-cooperative gaming among all users, respectively. By simulations and numerical analyses, we show that our proposed architectures and schemes significantly outperform the other existing schemes in supporting the statistical delay-bounded QoS provisioning over the 5G multimedia big data wireless networks.",2019,15580008
43,10.1109/BigData.2016.7840737,APPLICATION-DRIVEN SENSING DATA RECONSTRUCTION AND SELECTION BASED ON CORRELATION MINING AND DYNAMIC FEEDBACK,"As sensors spread across almost every industry, the Internet of Things (IoT) is going to trigger an era of big data. However, the abundance of available sensing data causes new challenges when building IoT applications. One main challenge is how to select proper data from large amount of sensing data for learning useful information efficiently. Existing approaches require developers to manage data for each specific application, which is very time consuming since the developers may not have enough knowledge about the dynamic changing data quality of different sensors. In this paper, we propose a data management middleware to learn the correlations between time series sensor data without prior knowledge. The learned correlation is then applied to select the useful sensor and reconstruct the incorrect data. To generalize the correlation models for each application, we utilize the dynamic feedback from the application to update the data selection and reconstruction. We evaluate our data management middleware in smart grids. The evaluation results show that our middleware can achieve better application performance with the help of dynamic feedback, data reconstruction and data selection.",2016,
44,10.1109/TrustCom/BigDataSE.2018.00164,PHEDHA: PROTECTING HEALTHCARE DATA IN HEALTH INFORMATION EXCHANGES WITH ACTIVE DATA BUNDLES,"Health Information Exchanges (HIEs) collect and disseminate electronic patient healthcare data (EHRs/EMRs) among different healthcare providers to improve the quality and reduce the cost of healthcare services. However, the dissemination of patient data raises privacy and security concerns due to ease of copying and unauthorized dissemination of electronic data. This paper proposes a HIE system called PHeDHA (Protecting Healthcare Data in HIEs with Active Data Bundles), which provides privacy and security protection for patient data during their transmission via an HIE among different healthcare providers. PHeDHA uses as its basis the scheme named Active Data Bundles with Trusted Third Party (ADB-TTP). As the name suggests, ADB-TTP is based on an integration of a trusted third party (TTP) with Active Data Bundles (ADBs). An ADB is a software object that keeps patient healthcare data as sensitive data; includes metadata describing these sensitive data and prescribing their use (via data access and privacy policies specified within metadata); and encompasses a policy enforcement engine (called a virtual machine or VM), which controls and manages how the ADB behaves. In particular, the VM assures ADB's data integrity and enforces its policies specified as a part of metadata. We describe and discuss the conceptual model for PHeDHA, based on ADB-TTP. We are currently evaluating PHeDHA via simulation experiments.",2018,23249013
45,10.1109/CIBD.2014.7011535,INTEGRATED ANALYTICS OF MICROARRAY BIG DATA REVEALS ROBUST GENE SIGNATURE,"The advance of high throughput biotechnology enables the generation of large amount of biomedical data. The microarray is increasingly a popular approach for the detection of genome-wide gene expression. Microarray data have thus increased significantly in public accessible database repositories, which provide valuable big data for scientific research. To deal with the challenge of microarray big data collected in different research labs using different experimental set-ups and on different bio-samples, this paper presents a primary study to evaluate the impact of two important factors (the origin of bio-samples and the quality of microarray data) on the integrated analytics of multiple microarray data. The aim is to enable the extraction of reliable and robust gene biomarkers from microarray big data. Our work showed that in order to enhance biomarker discovery from microarray big data (i) it is necessary to treat the microarray data differently in terms of their quality, (ii) it is recommended to stratifying (i.e., sub-group) the data according to the origin of bio-samples in the analytics.",2014,
46,10.1109/ICAIBD49809.2020.9137454,IDENTIFICATION OF INFLUENCE FACTORS OF EMERGENCY MANAGEMENT EVALUATION OF AGRO-PRODUCTS QUALITY AND SAFETY BASED ON FUZZY DEMATEL METHOD,"At present, the emergency management of agro-products, which is based on big data, Internet plus and AI technology, is promoting the orderly development. Meanwhile, the emergency management of agro-products will inevitably be affected by different kinds of interaction factors. In this paper, the indicators are sorted out and constructed, and the method combining direct fuzzy information with decision-making laboratory evaluation (DEMATEL) is proposed. The direct influence matrix based on intuitionistic fuzzy information expression is established; the comprehensive evaluation matrix is constructed by combining risk preference coefficients and the score function, and the evaluation indicators are analyzed according to influence degree, center degree and cause degree, so as to find out their interaction relationship. By constructing and analyzing the model, we can effectively quantify and evaluate the emergency management ability of agro-product quality and safety.",2020,
47,10.1109/TII.2021.3123312,A DATA TRADING SCHEME WITH EFFICIENT DATA USAGE CONTROL FOR INDUSTRIAL IOT,"The development of Industrial Internet of Things (IIoT) provides massive abundant data resources for trading and mining. However, the existing data trading schemes achieve data usage control at the cost of high latency, thereby resulting in poor service quality as the values of IIoT data degrade over time. This article proposes a monitor-based usage control model to enforce data usage policies on the user side, which eliminates frequent interactions between owners and users. Based on that, a data trading scheme with efficient usage control for IIoT (called DTSI) is devised, which utilizes blockchain smart contract and software guard extensions (SGX) to enable owners to fully control users’ identities and operations at minimal overhead. Security analysis shows that DTSI effectively prevents data abuse and ensures the fair exchange of data. Meanwhile, extensive experiments are conducted on the DTSI prototype comparing with the state-of-the-art schemes with real-world IIoT datasets, which demonstrates the efficiency of DTSI.",2022,19410050
48,10.1109/TIE.2018.2856200,SCALABLE SEMISUPERVISED GMM FOR BIG DATA QUALITY PREDICTION IN MULTIMODE PROCESSES,"In this paper, a novel variational inference semisupervised Gaussian mixture model (VI-S2GMM) model is first proposed for semisupervised predictive modeling in multimode processes. Parameters of Gaussian components are identified more accurately with extra unlabeled samples, which improve the prediction performance of the regression model. Since all labeled and unlabeled data samples are involved in each iteration of parameter updating, intractable computing problems occur when facing high-dimension datasets. To tackle this problem, a scalable stochastic VI-S2GMM (SVI-S2GMM) is further proposed. Through taking advantage of a stochastic gradient optimization algorithm to maximize the evidence of lower bound, the VI-based algorithm becomes scalable. In the SVI-S2GMM, only one or a minibatch of samples is randomly selected to update parameters in each iteration, which is more efficient than the VI-S2GMM. Since the whole dataset is divided and transferred to iterations batch by batch, the scalable SVI-S2GMM algorithm can easily handle the big data modeling issue. In this way, a large number of unlabeled data can be useful in the modeling, which will further benefit the prediction performance. The SVI-S2GMM is then exploited for the prediction of a quality-related key performance index. Two examples demonstrate the feasibility and effectiveness of the proposed algorithms.",2019,15579948
49,10.1109/EXPAT.2017.7984409,ANALYSIS AND PATTERN IDENTIFICATION ON SMART SENSORS DATA,"This work exemplifies the use of a data analysis technique applied to indoor air quality data obtained in a laboratory. The environment data is acquired with a wireless sensor system, NSensor. The sensing system, developed at the Faculty of Engineering, University of Porto (FEUP), is used for indoor environment monitoring, with the capability to store, in a remotely accessed database, air quality parameters such as temperature, relative humidity, pressure, illuminance, carbon dioxide and volatile organic components. For the current study, it was selected the data from temperature and relative humidity, and a period of ten months was considered. The data analysis uses Fourier transforms to identify patterns on the acquired data. For the temperature data, five main patterns were possible to identify. This work explores the potential of using data analyses techniques for big data on the field of indoor air quality evaluation. To make use of this data, further developments must be carried out so that would be possible to go from the monitoring and identification to the phase of controlling the indoor environment.",2017,
50,10.1109/ICISCT50599.2020.9351494,A RESEARCH ON BIG DATA ANALYTICS IN HEALTHCARE INDUSTRY,"Big Data has changed the way we manage, analyze and leverage data in any industry. One of the most promising areas where it can be applied to make a change is healthcare. Healthcare analytics have the potential to reduce costs of treatment, predict outbreaks of epidemics, avoid preventable diseases and improve the quality of life in general. Average human lifespan is increasing along world population, which poses new challenges to today's treatment delivery methods. In this article, we would like to address the need of big data in health care industry, big data analytics advantages, big data applications and so on.",2020,
51,10.1109/BigData.2014.7004209,METADATA CAPITAL: SIMULATING THE PREDICTIVE VALUE OF SELF-GENERATED HEALTH INFORMATION (SGHI),"Metadata is crucial for understanding data, and can be viewed as a form of capital in the context of Big data. This paper reports on research simulating the potential of SGHI (Self-Generated Health Information) for predicting asthma episodes. A data set of 2,000 cases was generated using the Monte Carlo simulation method, with secondary modifications on air quality and geo-location. The research is being pursued as part of a National Consortium for Data Science (NCDS) effort. The research conducted demonstrates that metadata has an inherent “predictive value” and confirms that metadata is crucial for data analytics. The work presented also provides insights into the best direction for future work in this area.",2014,
52,10.1109/BigData52589.2021.9671677,IMPROVING ALGORITHMIC DECISION–MAKING IN THE PRESENCE OF UNTRUSTWORTHY TRAINING DATA,"Although data quality is of paramount importance in algorithmic decision–making, most existing methods for supervised classification use training data without ever questioning their fidelity. At the same time, counterfactual explanation approaches widely used for post–hoc explanation of algorithmic decisions may result in unrealistic recommendations when left unconstrained. This work highlights a significant research problem, and introduces a novel framework to improve supervised classification in the presence of untrustworthy data, while offering actionable suggestions when an undesirable decision has been made (e.g., loan application rejection). Evaluation results spanning datasets from different domains demonstrate the superiority of the proposed approach, and its comparative advantage as the percentage of mislabeled instances increases.",2021,
53,10.1109/BigData.Congress.2013.41,COST AND TIME AWARE ANT COLONY ALGORITHM FOR DATA REPLICA IN ALPHA MAGNETIC SPECTROMETER EXPERIMENT,"Huge collections of data have been created in recent years. Cloud computing provides a way to enable massive amounts of data to work together as data-intensive services. Considering Big Data and the cloud together, which is a practical and economical way to deal with Big Data, will accelerate the availability and acceptability of analysis of the data. Providing an efficient mechanism for optimized data-intensive services will become critical to meet the expected growth in demand. Because the competition is an extremely important factor in the marketplace, the cost model for data-intensive service provision is the key to provide a sustainable service market. As data play the dominant role in execution of data-intensive service composition, the cost and access response time of data sets influence the quality of the service that requires the data sets. In this paper, a data replica selection optimization algorithm based on an ant colony system is proposed. The performance of the data replica selection algorithm is evaluated by simulations. The background application of the work is the Alpha Magnetic Spectrometer experiment, which involves large amounts of data being transferred, organized and stored. It is critical and challenging to be cost and time aware to manage the data and services in this intensive research environment.",2013,23797703
54,10.1109/IV51561.2020.00080,VISUAL ANALYTICS FOR SPATIO-TEMPORAL AIR QUALITY DATA,"Air pollution is the second biggest environmental concern for Europeans after climate change and the major risk to public health. It is imperative to monitor the spatiotemporal patterns of urban air pollution. The TRAFAIR air quality dashboard is an effective web application to empower decision-makers to be aware of the urban air quality conditions, define new policies, and keep monitoring their effects. The architecture copes with the multidimensionality of data and the real-time visualization challenge of big data streams coming from a network of low-cost sensors. Moreover, it handles the visualization and management of predictive air quality maps series that is produced by an air pollution dispersion model. Air quality data are not only visualized at a limited set of locations at different times but in the continuous space-time domain, thanks to interpolated maps that estimate the pollution at un-sampled locations.",2020,15506037
55,10.23919/CISTI54924.2022.9820084,STRATEGIC MANAGEMENT FOR HEIS BASED ON DATA ANALYTICS: LITERATURE REVIEW : CASE STUDY UISRAEL,"The purpose of this research is to know the current situation of strategic management in Higher Education Institutions, based on Data Analytics - Business Analytics, as a tool for new strategies in predictive models, since, just as companies now store massive data at Big Data level, where they focus on capturing and processing them, Higher Education Institutions do it in the same way, to make decisions. The strategic management in the Higher Education Institutions to be analyzed is based on the deployment model of the competitive strategy to be projected in the operation of the institutions, taking into account the axiomatic framework of strategic planning, a quality work culture and evidencing the results in key indicators. Therefore, a systematic review of the literature is carried out, called a secondary study and consists of five key steps: identification of questions for the review, identification of relevant works, evaluation of the quality of the studies, summary of the evidence, and finally, interpretation of the results. Additionally, one of the Ecuadorian Higher Education Institutions is shown as a case study.",2022,21660727
56,10.1109/BigComp48618.2020.00-27,HIGH QUALITY TRAINING SET COLLECTION USING GENERATIVE ADVERSARIAL NETWORK,"Image classification and object detection using deep learning have evolved with continuous research. In particular, the development of big data and the improvement of computer hardware performance have contributed extremely to the development of deep learning. Deep learning technologies like Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) can train models through training data. In other words, the optimal value of the weight parameter is automatically obtained from the training data. In this way, training data is the most important in deep learning. The quality and amount of training data affects the learning performance of deep learning models. In order to obtain high quality training data, it is difficult for the user to collect it directly. And using a web crawler to collect images for search keywords is also in problem. The images collected by the crawler include many low-resolution images and irrelevant images. These image data are bad for training deep learning model. Therefore, this paper purposes to collect high quality learning data by automatically applying SRGAN to low resolution images after image collection through image crawler and converting high quality images.",2020,2375933X
57,10.1109/ETFA45728.2021.9613600,BIG DATA NEEDS AND CHALLENGES IN SMART MANUFACTURING: AN INDUSTRY-ACADEMIA SURVEY,"The increasing availability of data in Smart Manufacturing opens new challenges and required capabilities in the area of big data in industry and academia. Various organizations have started initiatives to collect and analyse data in their individual contexts with specific goals, e.g., for monitoring, optimization, or decision support in order to reduce risks and costs in their manufacturing systems. However, the variety of available application areas require to focus on most promising activities. Therefore, we see the need for investigating common challenges and priorities in academia and industry from expert and management perspective to identify the state of the practice and promising application areas for driving future research directions. The goal of this paper is to report on an industry-academia survey to capture the current state of the art, required capabilities and priorities in the area of big data applications. Therefore, we conducted a survey in winter 2020/21 in industry and academia. We received 22 responses from different application domains highlighting the need for supporting (a) fault detection and (b) fault classification based on (c) historical and (d) real-time data analysis concepts. Therefore, the survey results reveals current and upcoming challenges in big data applications, such as defect handling based on historical and real-time data.",2021,
58,10.1109/ICBDIE50010.2020.00099,PREDICTION OF AIR QUALITY BASED ON GRADIENT BOOSTING MACHINE METHOD,"With the rapid development of China's economy, the degree of industrialization is gradually deepened, therefore leading to environmental pollution problems. Air is the material basis on which human beings live. Beijing is the capital of China and the national economic, political and cultural center, Beijing's air quality is an important indicator to measure whether the city is livable or not, while PM2.5 has also become an important standard to measure and monitor the air quality of Beijing. In today's era of big data, the use of efficient computing software to conduct data analysis and prediction has become a trend of future environmental detection and data analysis, which can effectively monitor the urban air environment. In this paper, two methods, Light Gradient Boosting Machine (Light GBM) and eXtreme Gradient Boosting (XGB) were used to extract and predict the characteristics of the air monitoring data in Beijing, and the prediction accuracy and operation time of the two methods were evaluated. Finally, conclusion was drawn that the accuracy and operation efficiency of Light GBM was much higher than that of XGB was reached.",2020,
59,10.1109/CSE-EUC.2017.84,QOS-BASED SERVICE SELECTION METHOD FOR BIG DATA SERVICE COMPOSITION,"Different from the traditional web services, the big data services' execution duration vary from the input data volume, so the traditional Quality of Service (QoS) analysis model for traditional web services cannot be directly applied to big data services. Additionally, since many big data or web services provide overlapping or identical functionality, albeit with the description of different QoS, some methods should be taken to determine which service are to participate in a given composite service. To address the problems above, this paper proposes an expanded edition of QoS-based analysis model: (1) Use the linear regression model to estimate the execution duration of the big data services to support the QoS model. (2)Set the weight of QoS using AHP analysis. (3) Improve the service selection algorithm based on backtracking method and validate its effectiveness, which is proved more high-performance than the original backtracking method and Integer Programming method.",2017,
60,10.1109/ICMEW.2015.7169822,IMPART: BIG MEDIA DATA PROCESSING AND ANALYSIS FOR FILM PRODUCTION,"A typical high-end film production generates several terabytes of data per day, either as footage from multiple cameras or as background information regarding the set (laser scans, spherical captures, etc). The EU project IMPART (impart.upf.edu) has been researching solutions that improve the integration and understanding of the quality of the multiple data sources to support creative decisions onset or near it, and an enhanced post-production as well. The main results covered in this paper are: a public multisource production dataset made available for research purposes, monitoring and quality assurance of multicamera set-ups, multisource registration, anthropocentric visual analysis for semantic content annotation, acceleration of 3D reconstruction, and integrated 2D-3D web visualization tools.",2015,
61,10.1109/ICCONS.2017.8250692,SENTIMENT ANALYSIS FOR THE NEWS DATA BASED ON THE SOCIAL MEDIA,"Now social Data are increases very fast, in every area social data play an important role in every angle, social media big data mining area welcomed by researchers from both government, academic and industry. A computing sentiment of news data is a significant component of the social media big data. The computing sentiment of news information may be a major factor of the social media massive information. In current web word range of user use social media and social network to browse and read news connected information. Everyday range of issue area unit occurring and social media influence the news associated with this news. Existing sentiment computing ways area unit primarily supported the feeling wordbook or supervised ways, that aren't climbable to the social media massive information. As a result of once bid information suggests that information size increases this methodology result on potency. Therefore we tend to propose a replacement methodology to try and do the sentiment analysis for news data a lot of specially, supported the social media information and social news (i.e.text and emotions text) of a happening, a Levenshtein algorithm is made to together categorical its linguistics and emotions, that lays the muse for the happening sentiment analysis. The word feeling computation algorithmic rule is planned to get the beginning word feeling that area unit more refined through the quality emotion wordbook. With the word emotions in hand, we are able to reason each sentence sentiments. The proposed method uses Naïve Bayes and Levenshtein algorithm to determine the emotion into different categories from given social media news data. This method provides the excellent performance for real time news data on social media and also provides the better result in terms of accuracy.",2017,
62,10.23919/PICMET.2018.8481834,EXPLORATORY STRATEGIC ROADMAPPING FRAMEWORK FOR BIG DATA PRIVACY ISSUES,"The applications of Big Data continue to expand, due to the many possibilities and unprecedented insights it offers to people, organizations, and communities. However, Big Data poses serious challenges as well, including challenges to the privacy and security of individuals and their data. This paper considers how to best address one concern related to Big Data: the social problems that the pervasiveness of data collection, analysis, and storage create with regard to individuals' ability to control their own data. The paper uses Quality Function Deployment (QFD) and Technology Roadmapping analysis methods to assess the social problems, technologies, resources, and industries that are most relevant to data privacy, and what should be done to address it. The findings indicated that the healthcare industry is one of the most important industries to consider concerning data privacy because of the nature of the data generated through medical processes and technologies. Furthermore, it was found that enforcement mechanisms, specifically in the form of federal enforcement agencies, are the most effective approach to ensure compliance by actors. It was also realized that there are extenuating political circumstances and increased costs that make the implementation of those policies challenging in the United States.",2018,21595100
63,10.1109/BDEIM55082.2021.00108,RESEARCH ON ENTERPRISE FINANCIAL INFORMATION SYSTEMS UNDER ERP ENVIRONMENT — BASED ON DATA VISUALIZATION METHOD,"In this paper, data visualization analysis method is used to deeply analyse the application of ERP system in Haier Group's financial system, find the problems existing in Haier Group's financial sharing centre and clarify the factors restricting Haier group's financial development at the present stage In order to further promote the development of Higher quality of Haier Group, reference suggestions are put forward, that is, organizational reform, optimization of financial system, improvement of assessment indicators, so as to establish a more professional financial management system to meet the requirements of the company's development In addition, this paper introduces in detail the advantages and disadvantages of Haier Group's financial sharing centre from four dimensions of advantages, disadvantages, opportunities and threats by using SWOT analysis tools Finally through the structural design, this paper found that the use of financial sharing centre is conducive to better development of enterprises.",2021,
64,10.1109/QRS-C.2017.107,EXPERIENCES IN TESTING AND ANALYSING DATA INTENSIVE SYSTEMS,"Testing software-intensive systems, for us, has traditionally focused on verifying and validating compliance and conformance to specification, as well as some general non-functional requirements such as performance of different components. In recent years, we have seen a strong move towards more data intensive systems. We have found that these types of systems require a different approach for testing and analysis, moving more towards exploring the system, its elements, behaviour and properties from a big data and analytics perspective. This paper summarizes our experiences in building and developing test and analytics environments for evaluating the performance, reliability, and security of such data-intensive systems.",2017,
65,10.1109/ICDM.2017.141,SYNCHRONIZATION-INSPIRED CO-CLUSTERING AND ITS APPLICATION TO GENE EXPRESSION DATA,"In this paper, we propose a new synchronization-inspired co-clustering algorithm by dynamic simulation, called CoSync, which aims to discover biologically relevant subgroups embedding in a given gene expression data matrix. The basic idea is to view a gene expression data matrix as a dynamical system, and the weighted two-sided interactions are imposed on each element of the matrix from both aspects of genes and conditions, resulting in the values of all element in a co-cluster synchronizing together. Experiments show that our algorithm allows uncovering high-quality co-clusterings embedded in gene expression data sets and has its superiority over many state-of-the-art algorithms.",2017,23748486
66,10.1109/JPHOTOV.2017.2778571,MULTIVARIATE DATA ANALYTICS IN PV MANUFACTURING—FOUR CASE STUDIES USING MANUFACTURING DATASETS,"Many industries are being revolutionized through the use of advanced analytical tools that generate insights from large sets of data. These tools are used as a part of a diversely described but analogous set of pursuits, such as “data science,” “data mining,” and “big data.” In manufacturing, they result in improved quality, improved cost of manufacturing, and more streamlined approaches. Many of these tools are applicable to PV manufacturing data and so present significant opportunities for the industry, but there are limited published studies and limited public domain knowledge of commercial activities in the area. This paper highlights these opportunities for PV manufacturing by describing four case studies applying different analytical approaches to sets of data from different manufacturing facilities. The analyses primarily provide insight into the source of variance in manufacturing, offering manufacturers a detailed and quantifiable way to measure and improve quality. These types of approaches will become more necessary to keep control of manufacturing facilities that continue to grow at high rates, and thus they offer a glimpse for the future operation and organization of large-scale PV manufacturing.",2018,21563403
67,10.1109/TELSIKS52058.2021.9606290,BIG DATA ARCHITECTURE FOR MOBILE NETWORK OPERATORS,"Mobile network operators are faced with tremendous data growth in their networks. Complex and multilayered networks, high-quality signal demands, and strong competition are only some of the aspects that push operators to further optimize their networks. Also, introduction of 5G in their networks enabled mobile operators to offer broad spectrum of 5G services, especially IoT (Internet of Things) and M2M (Machine to Machine) services which further increase traffic volume and the amount of collected data. Traditional data management solutions currently used are not able to successfully respond to such huge amounts of data. Big data technologies represent a modern approach for coping with the enormous data quantities. In this paper, we propose a big data solution that can collect and process huge amounts of data to extract valuable information and help mobile operators to bring their networks to enhanced quality level and offer full IoT solutions to their customers.",2021,
68,10.1109/INFOCT.2018.8356832,A BRIEF REVIEW OF METHODS AND APPROACHES PROPOSED IN EXISTING LITERATURE TO ADDRESS ISSUES IN WIRELESS BIG DATA,"Big data generated from wireless devices bring distinct challenges for the researchers due to its inherent spatial temporal nature. Reviewing existing literature on wireless big data, two major challenges are identified, processing and storage. To address these challenges various methodologies/approaches have been proposed and presented in the recent literature. To present the existing state of research on challenges associated with wireless big data, we present a systematic literature review of selected publications that address these issues. We also present areas which have not been addressed in the existing literature.",2018,
69,10.1109/ICEEM52022.2021.9480634,EMPOWERING GIS WITH BIG DATA: A REVIEW OF RECENT ADVANCES,"In the past few decades, the use of geographic information systems (GIS) was efficient with servers that could handle the amount of data used. However, as geographical big data grows in size and complexity, storing, managing, processing, analyzing, visualizing, and confirming data quality becomes more difficult. Academia, industry, government, and other institutions are increasingly interested in this information. It’s known as Big Data. Since that kind of data recently became massive, there was a need to develop methods to deal with big data and analyze it to keep pace with development. In this paper, we review the previous studies that involve both Big Data and GIS in different applications. Moreover, we focus on the field of agriculture, which is considered one of the most important sources of the economy. Produced results in this research area help decision-makers to make sound executive steps to reach better production.",2021,
70,10.1109/ICRIIS.2017.8002536,FACTORS INFLUENCING TO THE IMPLEMENTATION SUCCESS OF BIG DATA ANALYTICS: A SYSTEMATIC LITERATURE REVIEW,"Big data analytics (BDA) readiness factors have been widely researched; nevertheless, few have investigated the impact and success factors of BDA implementation in the organizational context. The relevance and quality of BDA outcomes have been a significant concern to the organizational leaders in supporting them for strategic decision-making. To that end, the objective of this study is twofold. Firstly, it investigates the factors that influencing the success of BDA implementation for effective decision-making. Secondly, this study adds to the body of knowledge in the information system (IS) domain, especially with its focus on BDA implementation packages. Based on 18 selected papers, this review has established 10 influencing factors that may influence the success of BDA implementation, therefore, contribute to the practice and research of BDA domain and its effectiveness towards the organizational performance enhancement.",2017,23248157
71,10.1109/BigData52589.2021.9672059,TOWARDS REAL-TIME PUBLIC HEALTH: A NOVEL MOBILE HEALTH MONITORING SYSTEM,"Public health monitoring methods have limitations that affect the quality of data. To support traditional data collection efforts, personal smart technologies can be used to collect multimodal, real-time and continuous data. Public health agencies can then study and predict the prevalence of conditions in a population using advanced analytics. Apple Health is one of the most popular sources of health data from personal devices, supporting diverse sensors that collect a wide range of information from heart rate to blood pressure and sleep. This paper introduces a system that uses a mobile health platform to extract Apple Health data to support public health monitoring. Development, security and privacy considerations are discussed, and a pilot study is proposed which collects several objective sensor data from Apple Health as well as self-report perceived stress (both using the platform) to create stress prediction models. Ultimately, the system described can provide public health agencies with novel methods to collect multimodal data from consumer devices as well as implement interventions in real-time to minimize the impact of conditions, such as stress, in a population. The system advances the state-of-the-art in health monitoring by being one of the first works to leverage health data from consumer-level personal devices for public health.",2021,
72,10.1109/BDVA.2018.8534028,VISUAL PATTERN-DRIVEN EXPLORATION OF BIG DATA,"Pattern extraction algorithms are enabling insights into the ever-growing amount of today's datasets by translating reoccurring data properties into compact representations. Yet, a practical problem arises: With increasing data volumes and complexity also the number of patterns increases, leaving the analyst with a vast result space. Current algorithmic and especially visualization approaches often fail to answer central overview questions essential for a comprehensive understanding of pattern distributions and support, their quality, and relevance to the analysis task. To address these challenges, we contribute a visual analytics pipeline targeted on the pattern-driven exploration of result spaces in a semi- automatic fashion. Specifically, we combine image feature analysis and unsupervised learning to partition the pattern space into interpretable, coherent chunks, which should be given priority in a subsequent in-depth analysis. In our analysis scenarios, no ground-truth is given. Thus, we employ and evaluate novel quality metrics derived from the distance distributions of our image feature vectors and the derived cluster model to guide the feature selection process. We visualize our results interactively, allowing the user to drill down from overview to detail into the pattern space and demonstrate our techniques in two case studies on Earth observation and biomedical genomic data.",2018,
73,10.1109/MCC.2014.22,STREAMING BIG DATA PROCESSING IN DATACENTER CLOUDS,"Today, we live in a digital universe in which information and technology are not only around us but also play important roles in dictating the quality of our lives. As we delve deeper into this digital universe, we're witnessing explosive growth in the variety, velocity, and volume of data1,2 being transmitted over the Internet. A zetabyte of data passed through the Internet in the past year; IDC predicts that this digital universe will explode to an unimaginable eight Zbytes by 2015. These data are and will be generated mainly from Internet search, social media, mobile devices, the Internet of Things, business transactions, next-generation radio astronomy telescopes, high-energy physics synchrotron, and content distribution. Government and business organizations are now overflowing with data, easily aggregating to terabytes or even petabytes of information.",2014,23722568
74,10.1109/BigDataCongress.2016.28,FROM BIG DATA TO GREAT SERVICES,"Big Data is increasingly adopted by a wide range of service industries to improve the quality and value of their services, e.g., inventory that matches well the supply and demand, and pricing that reflects well the market needs. Customers benefit from higher quality of service enabled by Big Data. Service providers get higher profits from more precise control of costs and accurate knowledge of customer needs. In this paper, we define the next generation high quality services as Great Services, characterized by 4P Quality-of-Service (QoS) dimensions: Panorama, Penetration, Prediction and Personalization, which go much further than current services. The transformation of Big Data into Great Services would be difficult and expensive without methodical techniques and software tools. We call the intermediate step Deep Knowledge, which is generated by Big Data (with the 4V challenges - Volume, Velocity, Variety, and Veracity) and used in the creation of Great Services. Deep Knowledge is distinguished from traditional Big Data by 4C properties (Complexity, Cross-domain, Customization, and Convergence). In order to achieve the 4P QoS dimensions of Great Services, we need Deep Knowledge with 4C properties. In this paper, we describe an informal characterization of Great Services with 4P QoS dimensions with examples, and outline the techniques and tools that facilitate the transformation of Big Data into Deep Knowledge with 4C properties, and then the use of Deep Knowledge in Great Services.",2016,
75,10.1109/WOCC.2019.8770562,CHALLENGES OF BIG DATA IMPLEMENTATION IN A PUBLIC HOSPITAL,"The digitalization of the healthcare system has resulted in a huge amount of data in medicine. These data help healthcare organizations improve health-process efficiency, enhance healthcare quality, and reduce healthcare cost. In Thailand, several private hospitals obtain numerous benefits by utilizing big data analytics. However, public hospitals might encounter with some difficulties for implementing big data technology. This paper presents challenges of big data implementation in a case study public hospital. According to the analysis, challenges are discussed in aspects of technology, data, human, and organization. The recommendation for implementing big data is proposed as a guideline for big data implementation in the public hospital.",2019,23791268
76,10.1109/CSCloud/EdgeCom.2018.00049,STORAGE AND INDEXING OF BIG DATA FOR POWER DISTRIBUTION NETWORKS,"With the construction of power distribution networks, a large amount of various types of data has accumulated, including automation and information technology system data as well as customer power consumption data, including distribution transformer, distribution transformer station, distribution switch station, meter, and electrical energy quality. The storage and index association of a large amount of different data is the basis for big data analysis. As a massive type of time-series data, customer power consumption load includes large-scale customers and high-density data collection. To improve the efficiency of query and analysis, this study proposed time-series data indexing technology to reduce the time required for data query and retrieval, to improve the efficiency of time-series data analysis, and to enable power companies to deeply analyze and cluster the power consumption behaviors of their customers.",2018,
77,10.1109/ICBASE53849.2021.00128,THE DESIGN AND APPLICATION OF SOFTWARE MEASUREMENT AND EVALUATION MODEL BASED ON PROCESS MANAGEMENT,"This article based on the software management requirements of measuring equipment, establishes the measurement model of measuring equipment software and determines the measurement target. This article defines the metrics for achieving these metrics. This article gives the data acquisition time and data analysis method and standard. According to the measurement target, this article establishes software quality assessment model. By using examples to analyze the various measures of the software development implementation process, this article supports the decision-making data of decision-makers. Besides, this article provides help for forecasting and control of software development processes. According to solve the management problems of software development in limited time and cost conditions, the application of the software development in measuring equipment is of great significance.",2021,
78,10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00302,COMPUTER LAWS CONSIDERATION ON SMART CITY DATA PLANNING OF CHONGLI 2022,"Smart Cities make a good use of modern ICT and innovation idea to provide systematically creative, integrated of variety of services. In order to serve citizen's life a good quality, optimize management and services, enhance the efficiency of resource usage, a good smart city planning is in a big demand. Since 2014, professionals have started to consider Smart City planning and design for Chongli, in order to prepare the 2022 Winter Olympics there properly. Nevertheless, there is a shortage of cyber security consideration in current Smart City planning and designing. Although Yu et al (2018) discussed the Smart Cities development, but more works are still needed. Legal preparation actually is very important for Smart Chongli.",2019,
79,10.1109/ICBDIE52740.2021.00141,DESIGN OF A BIG DATA DRIVEN INTERNAL QUALITY ASSURANCE SYSTEM FOR THE EXTRACURRICULAR ACTIVITIES,"Currently, the construction of the internal quality assurance system for the extracurricular activities is drawing increasing attention from the academic and educational circles. By analyzing the predicaments, the connections between different elements and value congruence of the internal quality assurance for the extracurricular activities, this paper explores the design of a big data driven closed-loop multi-agent internal quality assurance (MQA) system with universal applicability for the extracurricular activities teaching, thereby providing a whole-new possibility for the university management to have a better grasp of the learning behaviors, the adaptability of teaching models, the reasonableness of teaching processes and the effectiveness of teaching activities under the MQA system.",2021,
80,10.23919/TST.2017.7914196,EFFICIENT CURRENCY DETERMINATION ALGORITHMS FOR DYNAMIC DATA,"Data quality is an important aspect in data application and management, and currency is one of the major dimensions influencing its quality. In real applications, datasets timestamps are often incomplete and unavailable, or even absent. With the increasing requirements to update real-time data, existing methods can fail to adequately determine the currency of entities. In consideration of the velocity of big data, we propose a series of efficient algorithms for determining the currency of dynamic datasets, which we divide into two steps. In the preprocessing step, to better determine data currency and accelerate dataset updating, we propose the use of a topological graph of the processing order of the entity attributes. Then, we construct an Entity Query B-Tree (EQB-Tree) structure and an Entity Storage Dynamic Linked List (ES-DLL) to improve the querying and updating processes of both the data currency graph and currency scores. In the currency determination step, we propose definitions of the currency score and currency information for tuples referring to the same entity and use examples to discuss methods and algorithms for their computation. Based on our experimental results with both real and synthetic data, we verify that our methods can efficiently update data in the correct order of currency.",2017,10070214
81,,CORPORATE DATA QUALITY MANAGEMENT: FROM THEORY TO PRACTICE,"It is now assumed that poor quality data is costing large amounts of money to corporations all over the world. Although research on methods and techniques for data quality assessment and improvement have begun in the early nineties of the past century and being currently abundant and innovative, it is noted that the academic and professional communities virtually have no dialogue, which turns out to be harmful to both of them. The challenge of promoting the relevance in information systems research, without compromising the necessary rigor, is still present in the various disciplines of information systems scientific area, including the data quality one. In this paper we present “data as a corporate asset” as a business philosophy, and a framework for the concepts related to that philosophy, derived from the academic and professional literature. According to this framework, we present, analyze and discuss a single explanatory case study, developed in a fixed and mobile telecommunications company, operating in one of the European Union Countries. The results show that, in the absence of data stewardship roles, data quality problems become more of an ""IT problem"" than typically is considered in the literature, owing to Requirements Analysis Teams of the IS Development Units, to become a “quality negotiator” between the various stakeholders. Other findings are their bottom-up approach to data quality management, their biggest focus on motivating employees through innovative forms of communication, which appears to be a critical success factor (CSF) for data quality management, as well as the importance of a data quality champion leadership.",2010,21660735
82,10.1109/ICRITO48877.2020.9197808,EVALUATE THE ROLE OF BIG DATA IN ENHANCING STRATEGIC DECISION MAKING FOR E-GOVERNANCE IN E-OMAN PORTAL,"This research study is to analyse the role of big data for enhancing the decision making for E-Oman. E-Oman is one of the It's organisation in Oman which provides key to It solutions includes application, infrastructure in Oman. Big data has become a forthcoming part of all trades and business segment Oman in electronic portal is for the citizens which make easy use of a transactions. Enhancement of big data in e-Oman Company supports in providing perfect public services and also searching of big term in process. The range which is used in this topic is exemplified in a crucial panel conversation a recent big data conference. We present a cooperative big data analytics stage for big data as a service. It takes longer time to achieve wrinkle data, progress events and investigative services. The outdate technologies do not become an appropriate solution to process a big data platform has begun to appear. The quality of big data is of great significance is more significant because the quality of material is affected by size, speed and format in which the data is generated. The main benefit of the e-service is user-friendly. By implementing the e-services it makes easy communication between the government and the citizens. The quality of big data is great pertinent and it is more significant. Quality of information is affected by the size, speed, and the data in which the format is generated.",2020,
83,10.1109/AEEICB.2017.7972391,A NOVEL PATTERN CLASSIFIER APPROACH TOWARDS THE PERFORMANCE OPTIMIZATION OF BIG DATA ANALYSIS IN DISTRIBUTED ENVIRONMENT,"Big data is a collection of massive amount of data whose characteristics exceeds the capabilities of conventional algorithm and techniques to derive the useful value. The real power lies not just in having colossal data but in what insights can be drawn from this data to facilitate better and faster decisions. Hadoop Map Reduce is emerged as powerful and cost effective open source framework for processing large data set distributed computing environment. The load imbalance, which occurs while processing the Big Data is common problem with this framework which reduces the efficiency of the processing, may impact the QOS of the application. In this paper a new pattern classifier algorithm has been proposed to tackle these problems. The proposed algorithm integrated with the traditional environment and experimentally it has been proved that response time of Map Reduce jobs are amplified by minimizing the load imbalance factor, thereby QOS requirement of the application is achieved.",2017,
84,10.1109/ICSGEA53208.2021.00114,CREDIT DECISION SYSTEM OF SMALL AND MEDIUM SIZED MICRO ENTERPRISES BASED ON BIG DATA TECHNOLOGY AND RISK ASSESSMENT THINKING,"In order to solve the common credit decision-making problem of most small and medium-sized enterprises, this paper proposes a new credit decision-making system for small and medium-sized enterprises based on big data technology and risk assessment thinking. The decision-making system fully analyzes the problems existing in the credit decision-making of most small and medium-sized micro enterprises, and gives full play to the powerful function of big data technology in data processing. Based on this, the decision-making system also constructs a comprehensive evaluation model, cluster analysis model, adds matlab algorithm, and from the perspective of risk assessment thinking, insists on minimizing the risk of credit decision-making. The results show that the decision-making system can analyze the credit problems of most small and medium-sized enterprises from a comprehensive and scientific perspective, and obtain the optimal credit strategy of enterprises through qualitative and quantitative analysis.",2021,
85,10.1109/ICCSE51940.2021.9569413,RESEARCH ON TEACHING EVALUATION SYSTEM OF HIGHER VOCATIONAL COLLEGES BASED ON OKR AND BIG DATA,"In the era of “Internet =” in order to meet the needs of society, vocational colleges are developing rapidly, but the problems of backward teaching evaluation methods and low enthusiasm of teachers are gradually exposed. In order to improve this phenomenon, this paper proposes a teaching evaluation method based on OKR (Objectives and Key Results) concept and big data technology. This method uses real teaching data and big data technology to build a teaching evaluation system that is truly suitable for higher vocational colleges. The results of data analysis show that using this method can improve teachers' initiative, improve the quality of education and teaching, and standardize the development of higher vocational colleges.",2021,24716146
86,10.1109/ICSGEA51094.2020.00109,CONSTRUCTION AND APPLICATION OF ACCOUNTING INFORMATION PLATFORM BASED ON BIG DATA ENVIRONMENT,"With the wide application of big data technology in accounting work and the continuous development of Internet technology, accounting informatization has become an inevitable trend in the development of accounting work. This paper mainly studies the service composition architecture of the application system based on the SOA idea, which not only completes the accounting integrated service management platform, but also provides the related web services related to accounting information for other internal independent systems. At the same time, the application service engine provided by cloud computing platform is used to realize all kinds of required financial applications. By using the SOA solution of Hessian, a distributed computing framework that provides users with more appropriate instant information services and related information query functions, and presents the problems encountered in the implementation process and solutions. It realizes the interface and service release between the accounting integrated service platform and other information management systems, as well as the information sharing and resource integration among the independent systems within the enterprise, which finally promotes the efficiency of the accounting staff.",2020,
87,10.1109/JPROC.2015.2496111,BIG DATA ANALYSIS FOR MEDIA PRODUCTION,"A typical high-end film production generates several terabytes of data per day, either as footage from multiple cameras or as background information regarding the set (laser scans, spherical captures, etc). This paper presents solutions to improve the integration of the multiple data sources, and understand their quality and content, which are useful both to support creative decisions on-set (or near it) and enhance the postproduction process. The main cinema specific contributions, tested on a multisource production dataset made publicly available for research purposes, are the monitoring and quality assurance of multicamera set-ups, multisource registration and acceleration of 3-D reconstruction, anthropocentric visual analysis techniques for semantic content annotation, and integrated 2-D–3-D web visualization tools. We discuss as well improvements carried out in basic techniques for acceleration, clustering and visualization, which were necessary to deal with the very large multisource data, and can be applied to other big data problems in diverse application fields.",2016,15582256
88,10.23919/JCC.2019.08.009,PECS: TOWARDS PERSONALIZED EDGE CACHING FOR FUTURE SERVICE-CENTRIC NETWORKS,"Mobile operators face the challenge of how to best design a service-centric network that can effectively process the rapidly increasing number of bandwidth-intensive user requests while providing a higher quality of experience (QoE). Existing content distribution networks (CDN) and mobile content distribution networks (mCDN) have both latency and throughput limitations due to being multiple network hops away from end-users. Here, we first propose a new Personalized Edge Caching System (PECS) architecture that employs big data analytics and mobile edge caching to provide personalized service access at the edge of the mobile network. Based on the proposed system architecture, the edge caching strategy based on user behavior and trajectory is analyzed. Employing our proposed PECS strategies, we use data mining algorithms to analyze the personalized trajectory and service usage patterns. Our findings provide guidance on how key technologies of PECS can be employed for current and future networks. Finally, we highlight the challenges associated with realizing such a system in 5G and beyond.",2019,16735447
89,10.1109/ICBDIE50010.2020.00084,RESEARCH ON THE RELATIONSHIP BETWEEN TEACHERS' AND STUDENTS' BEHAVIOR AND THE MEASURES TO IMPROVE THE TEACHING QUALITY,"With the approach of artificial intelligence to education, big data analysis has played a very important role in higher education and higher education management. However, due to the complex and changeable characteristics of teachers and students' classroom behavior, especially the interpretation of classroom behavior has obvious subjective color, so the use of big data behavior data analysis and AI assisted teaching behavior interpretation will become the only way for the development of education. This paper use a variety of technical means to collect the behavior data of teachers and students in classroom teaching, and cross judge the multi-source data, introduce complete random number algorithm and transformation regression model to regularize the classroom data and build a prediction model. The positive and negative feedback of teachers' teaching behavior in classroom teaching is carried out, and the mapping relationship between students' behavior data and teachers' behavior data in classroom teaching is used to further judge teachers' teaching behavior, form evaluation mechanism in real time, and establish quantitative evaluation basis for teachers' teaching ability improvement.",2020,
90,10.1109/ICALT49669.2020.00007,QUALITY PREDICTION OF OPEN EDUCATIONAL RESOURCES A METADATA-BASED APPROACH,"In the recent decade, online learning environments have accumulated millions of Open Educational Resources (OERs). However, for learners, finding relevant and high quality OERs is a complicated and time-consuming activity. Furthermore, metadata play a key role in offering high quality services such as recommendation and search. Metadata can also be used for automatic OER quality control as, in the light of the continuously increasing number of OERs, manual quality control is getting more and more difficult. In this work, we collected the metadata of 8,887 OERs to perform an exploratory data analysis to observe the effect of quality control on metadata quality. Subsequently, we propose an OER metadata scoring model, and build a metadata-based prediction model to anticipate the quality of OERs. Based on our data and model, we were able to detect high-quality OERs with the F1 score of 94.6%.",2020,21613761
91,10.1109/BigData.2018.8621867,DYNAMIC ONLINE PERFORMANCE OPTIMIZATION IN STREAMING DATA COMPRESSION,"Compression is essential to high bandwidth applications such as scientific simulations and sensing applications to reduce resource burden such as storage, network transmission, and more recently I/O. Existing lossy compression methods attempt to minimize the Euclidean distance between original data and reconstructed data, which significantly limits either compression performance or reconstruction quality since original and reconstructed data sequences should be aligned. Substituting the Euclidean distance for a statistical similarity maximizes the compression performance while retaining essential data features. By implementing this methodology, IDEALEM has recently demonstrated compression ratios far exceeding 100:1, better than best-known compression methods, while preserving reconstruction quality. This work proposes an online algorithm for streaming data compression which takes account of generally concave trend of compression ratio curve, and optimizes key operation parameters. We demonstrate that the proposed algorithm successfully adapts one of the key parameters in IDEALEM to the optimal value and yields near maximum compression ratios for time series data.",2018,
92,10.1109/ICAA53760.2021.00102,A METHOD OF APPLYING BIG DATA IN ELECTRICITY TO INCLUSIVE FINANCE,"Aiming at the difficulties of small and micro enterprises in obtaining loans and the implementation of inclusive credit, a solution based on big data of power grid to Inclusive Finance is proposed. With the support of big data technology, a comprehensive risk control system of Inclusive Credit has been established, which has improved the ability to identify and control the risks of Inclusive Credit. In addition, an inclusive credit user profile model based on electricity data has been established. It has been verified that this model can accurately identify target customers and effectively improve the quality and efficiency of inclusive credit.",2021,
93,10.1109/JSEN.2015.2471802,A SCALABLE AND FLEXIBLE REPOSITORY FOR BIG SENSOR DATA,"Data generation rates of sensors are rapidly increasing, reaching a limit such that storage expansion cannot keep up with the data growth. We propose a new big data archiving scheme that handles the huge volume of sensor data with an optimized lossy coding. Our scheme leverages spatial and temporal correlations inherent in typical sensor data. The spatio-temporal correlations, observed in quality adjustable sensor data, enable us to compress a massive amount of sensor data without compromising distinctive attributes in sensor signals. Sensor data fidelity can also be decreased gradually. In order to maximize storage efficiency, we derive an optimal storage configuration for this data aging scenario. Experiments show outstanding compression ratios of our scheme and the optimality of storage configuration that minimizes system-wide distortion of sensor data under a given storage space.",2015,23799153
94,10.1109/IWQoS54832.2022.9812905,AIQOSER: BUILDING THE EFFICIENT INFERENCE-QOS FOR AI SERVICES,"The AI inspired methods have entirely changed the network QoS landscape and brought better demand-guided experiences for the end-users. However, the increasing demands of satisfactory experiences require larger AI models, whose inference efficiency becomes the non-negligible drawback in the time-sensitive network QoS. In this work, we defined this challenge as the inference-QoS (iQoS) problem of the network QoS itself, which balances inference efficiency and performance for AI services. We design a unified iQoS metric to evaluate the AI-enhanced QoS frameworks with considerations on model performance, inference latency, and input scale. Then, we propose a two-stage pipeline as the exemplar for leveraging the iQoS metric in QoS-aware AI services: (i) enhance reconstruction ability, pretraining masked autoencoder extracts intrinsic data correlations by multi-scale masking; (ii) improve inference efficiency, forecasting masked decoder uses the data scale pruning in terms of spatial and temporal dimension for prediction. Comprehensive experiments on our method demonstrate its superior inference latency and overwhelming traffic matrix prediction performance.",2022,1548615X
95,10.1109/ICSC.2018.00071,BIG DATA INFRASTRUCTURE FOR AGRICULTURAL TOMOGRAPHIC IMAGES RECONSTRUCTION,"A single agricultural soil sample obtained by a tomograph is composed of too many projections. In addition, considering that a sample is scanned at different angles a big set of projections is formed. Therefore, when using micro-resolution tomographic instruments, for a single soil sample it is necessary to deal with amounts of data in the order of gigabytes. On the other hand, the quantity of samples contributes to quality of information, for example, in the construction of maps to study agricultural soils. In general, in order to get improvements in the quality of soil analyzes it is required to increase exponentially the amount of the soil samples, i. e., increasing the amount of data to be reconstructed, which exceed the order of terabytes. This massive amount of data suggests new emerging methods and technologies. In this sense, Big Data has shown the great potential in optimizing data, making decisions, spotting business trends in various fields such agriculture. In this work, the infrastructure for the process of tomographic reconstruction of agricultural soil samples based on Big Data is presented. We introduced the Big Data architecture which uses the Hadoop framework. Additionally, we present the Filtered Back-Projection (FBP) algorithm adapted to the MapReduce model. The use of Big Data environment allows reconstructing a greatest number of agricultural soil tomographic images in the same time-frame and, consequently, it allows increasing the number of analysis contributing to improvement of quality of information about agricultural soils. Furthermore, the developed application has required both interpretation and language generation to allow the organization of knowledge, as well as the establishment of an adequate computational semantics for its operation.",2018,
96,10.1109/IICSPI48186.2019.9095877,APPLICATION OF DATA MINING TECHNOLOGY IN THE RECALL OF DEFECTIVE AUTOMOBILE PRODUCTS IN CHINA ——A TYPICAL CASE OF THE CONSTRUCTION OF DIGITAL CHINA,"According to multisource quality safety data of defective automobile products, key quality safety factors of defective automobile products are extracted, a defect information indicator system for automobile products is systematically constructed and a correlated graph is established between quality safety factors. Based on the optimization and correlation of the quality safety factor indicator system, Big Data technology is used to design a data structure for multisource quality safety information cluster, develop a data platform for the defect information analysis of automobile products and achieve information clustering and correlation analysis based on multisource quality safety data, providing technical support for the recall management of defective automobile products.",2019,
97,10.26599/BDMA.2020.9020001,ERROR DATA ANALYTICS ON RSS RANGE-BASED LOCALIZATION,"The quality of measurement data is critical to the accuracy of both outdoor and indoor localization methods. Due to the inevitable measurement error, the analytics on the error data is critical to evaluate localization methods and to find the effective ones. For indoor localization, Received Signal Strength (RSS) is a convenient and low-cost measurement that has been adopted in many localization approaches. However, using RSS data for localization needs to solve a fundamental problem, that is, how accurate are these methods? The reason of the low accuracy of the current RSS-based localization methods is the oversimplified analysis on RSS measurement data. In this proposed work, we adopt a generalized measurement model to find optimal estimators whose estimated error is equal to the Cramér-Rao Lower Bound (CRLB). Through mathematical techniques, the key factors that affect the accuracy of RSS-based localization methods are revealed, and the analytics expression that discloses the proportional relationship between the localization accuracy and these factors is derived. The significance of our discovery has two folds: First, we present a general expression for localization error data analytics, which can explain and predict the accuracy of range-based localization algorithms; second, the further study on the general analytics expression and its minimum can be used to optimize current localization algorithms.",2020,20960654
98,10.1109/DASC-PICom-CBDCom-CyberSciTech49142.2020.00110,SCALABLE TAXONOMY GENERATION AND EVOLUTION ON APACHE SPARK,"Big data mainly refers to a huge volume of rapidly growing data over size exabytes (1018). A major chunk of this data is unstructured text data produced from several sources. In order to use such data effectively, they need to be processed and organized. Taxonomy, a hierarchical structure, is considered an effective way of organizing the data. In the past, many techniques have been proposed to generate taxonomy automatically. Recently some attempts have also been made to evolve the static structure of taxonomy to deal with the rapidly changing nature of data. However, the voluminous nature of today's data currently exceeds the processing capabilities of conventional techniques. In this regard, there is a need for a scalable technique that potentially speeds up the process of taxonomy generation and evolution and caters to a large amount of unstructured big data. This paper presents a technique for both the generation and the evolution of taxonomy on the Apache Spark framework. The technique is tested on a text dataset belonging to a computing domain. The test results show that the scalable taxonomy generation and evolution technique proposed in this paper is not only time- efficient but also produces a good quality taxonomy as compared to state-of-the-art techniques.",2020,
99,10.1109/BigData52589.2021.9671374,FEDTRINET: A PSEUDO LABELING METHOD WITH THREE PLAYERS FOR FEDERATED SEMI-SUPERVISED LEARNING,"Federated Learning has shown great potentials for the distributed data utilization and privacy protection. Most existing federated learning approaches focus on the supervised setting, which means all the data stored in each client has labels. However, in real-world applications, the client data are impossible to be fully labeled. Thus, how to exploit the unlabeled data should be a new challenge for federated learning. Although a few studies are attempting to overcome this challenge, they may suffer from information leakage or misleading information usage problems. To tackle these issues, in this paper, we propose a novel federated semi-supervised learning method named FedTriNet, which consists of two learning phases. In the first phase, we pre-train FedTriNet using labeled data with FedAvg. In the second phase, we aim to make most of the unlabeled data to help model learning. In particular, we propose to use three networks and a dynamic quality control mechanism to generate high-quality pseudo labels for unlabeled data, which are added to the training set. Finally, FedTriNet uses the new training set to retrain the model. Experimental results on three publicly available datasets show that the proposed FedTriNet outperforms state-of-the-art baselines under both IID and Non-IID settings.",2021,
100,10.1109/ICTC.2018.8539474,PERFORMANCE STUDY OF DISTRIBUTED BIG DATA ANALYSIS IN YARN CLUSTER,"In the 4-th Industrial Revolution era, various intelligent solutions and services have been emerging recently. To provide high quality service in those intelligent applications, the big data should be collected without any loss and comprehensively analyzed. Especially, when using machine and deep learning techniques, the big data processing delays should be minimized in order to guarantee the freshness of models. In this paper, we evaluate the performance of Apache Spark which is one of the most popular big data processing and analysis frameworks. Beyond the performance analysis of Spark in distributed cluster environment, we evaluate the performance of TensorFlowOnSpark which is the promising distributed deep learning framework designed to handle big data efficiently. From the experimental results, we can conclude that Spark on YARN is a solid underlying framework that guarantees the performance and scalability of distributed machine and deep learning by efficiently processing its data and algorithms in a parallel and distributed manner.",2018,21621233
101,10.1109/RE.2016.14,URBAN COMPUTING: TACKLING URBAN CHALLENGES USING BIG DATA,"Urban computing is a process of acquisition, integration, and analysis of big and heterogeneous data generated by a diversity of sources in cities to tackle urban challenges, e.g. air pollution, energy consumption and traffic congestion. Urban computing connects unobtrusive and ubiquitous sensing technologies, advanced data management and analytics models, and novel visualization methods, to create win-win-win solutions that improve urban environment, human life quality, and city operation systems. Urban computing is an inter-disciplinary field where computer science meets urban planning, transportation, economy, the environment, sociology, and energy, etc., in the context of urban spaces. In this talk, I will overview the framework of urban computing, discussing its key challenges and methodologies from computer science's perspective. This talk will also present a diversity of urban computing applications, ranging from big data-driven environmental protection to transportation, from urban planning to urban economy. The research has been not only published at prestigious conferences but also deployed in the real world. More details can be found on the homepage of urban computing1.",2016,23326441
102,10.1109/ITNT52450.2021.9649302,MODELING AND DATA ANALYSIS FOR ASSESSING THE SENSITIVITY OF COMPETITIVE PERFOMANCE OF TERRITORIES IN THE IMPLEMENTATION OF INVESTMENT PROJECTS,"The purpose of this study is modeling and analysis in the implementation of investment projects to improve the competitiveness of territories. The tasks of this study include: determine the factors of competitiveness, develop a model of the competitiveness of territories using expert assessments, research the analysis of the competitiveness sensitivity of territories to the effectiveness of investment projects for further software development using BIG DATA technology. The research results include: models for making managerial decisions on the competitiveness of territories with detailed parameters of the investment group of factors. The efficiency of investment processes is determined on the basis of innovative projects. The practical results include improving the quality and timeliness of decision-making on the management of territories based on the management model of the innovative component of the competitiveness model.",2021,
103,10.1109/ICCTEC.2017.00091,RESEARCH ON ERROR DETECTION TECHNOLOGY OF ELECTRIC POWER DISPATCH BASED ON BIG DATA,"With the gradual increase of power grid access, the risk of wrong operation of power grid dispatching is gradually increasing. It is very important to study the power dispatching error prevention technology to avoid the unstable operation of the power system and the illegal operation of the illegal network. Therefore, this paper proposes the use of big data technology and the overall scheduling work to improve the quality of power dispatching.",2017,
104,10.1109/ICCCT2.2019.8824850,A SYSTEMATIC ANALYSIS OF DATA-INTENSIVE MOOCS AND THEIR KEY CHALLENGES,"Big Data blends modern technologies with numerous data management techniques to handle a wide variety of concerns that occur when operating with data of huge volume, variety and velocity. Big data deals with complex semi-structured and unstructured data from several sources and formats which include Social Media content in free form, data from E-commerce sites, Weather forecasting statistics, Clinical Diagnosis, Share Market Transactions and Smart Computing Environments. In the same way, big data offers substantial prospects in the discipline of Education, E-Learning and Learning Analytics. Application of big data analytics in E-Learning helps to assess the quality of Teaching, Development of Curriculum, predict learning outcomes, Career Development and Readiness, Attrition Risks and Feedback Analysis. The Massive Open Online Courses (MOOCs) have produced a major influence on E-Learning with the availability of Live and pre-recorded Lectures, Easy-to-learn Tutorials, Novel Assessment Methodologies, Quick feedback and results. In this paper, we present the various Technologies that formulate the MOOCs and address the learning paradigms and key challenges.",2019,
105,10.1109/CLOUD.2019.00034,SLA-AWARE AND DEADLINE CONSTRAINED PROFIT OPTIMIZATION FOR CLOUD RESOURCE MANAGEMENT IN BIG DATA ANALYTICS-AS-A-SERVICE PLATFORMS,"Discovering optimal data analytics solutions to extract value from data for better and faster decision making is essential for many application domains, especially in the big data era. Big data analytics typically requires a tremendous amount of computational resources to process large data volumes that can be very expensive and time consuming. Our research focuses on providing optimization solutions for Analytics-as-a-Service (AaaS) platforms that automatically and elastically provision cloud resources to execute queries guaranteeing Service Level Agreements (SLAs) across a range of Quality of Service (QoS) requirements. We propose admission control and resource scheduling algorithms for AaaS platforms to maximize profits while providing time-minimized query execution plans to meet user demands and expectations. To enable timely responses as required for many domains, the algorithms utilize data splitting-based query admission and resource scheduling offering parallel processing on the split datasets. Extensive experiments are conducted to evaluate the algorithm performance compared to state-of-the-art optimization algorithms. Experimental results show that our algorithms perform significantly better from a range of perspectives, including increasing query admission rates and creating higher profits, whilst supporting efficient resource configurations that are able to support big data processing demands under tight deadlines.",2019,21596182
106,10.1109/ICCSE49874.2020.9201656,STUDY OF HIGH-DIMENSIONAL DATA ANALYSIS BASED ON CLUSTERING ALGORITHM,"With the rapid development of big data, the scale, dimensions, diversity and sparsity of high-dimensional data restrict the effectiveness of traditional clustering algorithms. This paper mainly focuses on high-dimensional data clustering. Starting from the traditional K-means clustering algorithm and subspace clustering algorithm based on self-representation model, an improved algorithm is designed and implemented based on the existing clustering algorithm in this paper. The improved algorithm has better clustering quality by combining the ""distance optimization method"" and the ""density method"" to determine the initial clustering center. The feasibility and effectiveness of improved algorithm are verified through simulation experiments.",2020,24716146
107,10.1109/ICBAIE49996.2020.00024,RESEARCH ON RURAL DEVELOPMENT INNOVATION AND DEVELOPMENT BASED ON BIG DATA,"Big data provides brand-new means and tools for comprehensive rural revitalization. It is a powerful support for the comprehensive rural revitalization and an important way to achieve the integrated development of the digital economy and rural revitalization and improve the quality and level of rural revitalization. At present, some places in China have carried out fruitful practical explorations on the promotion of rural revitalization by big data, which has vigorously promoted rural revitalization in depth. However, in the course of practice, there are also imperfect supporting measures, weak infrastructure, lack of applied talents, and data information. There are outstanding problems, such as institutional obstacles, in sharing and integration, and further innovations are needed in platform construction, system improvement, and personnel training to fully play the role of big data in rural revitalization.",2020,
108,10.1109/ICSGEA.2019.00074,RESEARCH ON QUALITY EVALUATION METHOD OF DIGITAL TEACHING RESOURCES DESIGN CAPABILITY BASED ON CLOUD COMPUTING,"To improve the intelligent evaluation ability of digital teaching resource design ability and optimize the quality evaluation model, a cloud computing-based digital teaching resource design capability quality intelligent evaluation method was proposed. Digital data collection and statistical analysis methods were used for digitization. Teaching resource design capability quality statistical sample sequence sampling, using digital teaching resource design capability quantitative evaluation method in cloud computing environment, constructing big data distribution model of digital teaching resource design ability quality statistical sample sequence, combined with quantitative regression analysis method for big data characteristics Extraction and information regression analysis, constructing the feature extraction model of digital teaching resource design ability quality statistical analysis, taking the distribution status of teaching resources as the evaluation object, combined with quantitative recursive analysis method to carry out adaptive evaluation of digital teaching resource design ability quality statistical sample sequence. Adopting bus design and sensing quantitative tracking and recognition technology to carry out the system construction of digital teaching resource design capability quality, using local bus control method to carry out digital teaching resource design ability quality intelligence Estimated load instructions, to achieve design evaluation system. The test results show that the design of digital learning resources designed to assess the ability of intelligent quality assessment system has good performance, good intelligence.",2019,
109,10.1109/BigDataService49289.2020.00020,DWRECK: A DATA WRECKER FRAMEWORK FOR GENERATING UNCLEAN DATASETS,"In this paper, we present DWreck, a data wrecker framework for generating unclean datasets by counterproductively applying different data quality dimensions. In a typical data-analysis pipeline, data cleaning is the most cost-intensive, laborious, and time-consuming step. Unclean dataset or partially cleaned dataset can lead to incorrect training of machine learning models and result in wrong conclusions. Generally, data-scientists examine null, missing, or duplicate values, and the dataset is cleaned by removing the entire record or imputing the values. However, deleting the records, or imputing the values cannot be termed as comprehensive cleaning, as these cleaning techniques may result in a reduction in the population of data, and increased error in estimation due to biased values. For systematically cleaning an unclean dataset, it is necessary to comply with the data quality dimensions such as completeness, validity, consistency, accuracy, and conformity. The errors described as violations of expectations for completeness, accuracy, timeliness, consistency and other dimensions of data quality often impede the successful completion of information processing streams and consequently degrade the dependent business processes. Therefore, educating a data-scientist for comprehensively cleaning a raw-dataset acquired for analysis is an incremental learning process. Moreover, for extensive training on cleaning a dataset on different quality dimensions, it is necessary to provide a variety of datasets that are unclean on various data quality dimensions. Hence, in this paper, we present DWreck, a data wrecker framework for generating unclean datasets by counterproductively applying different data quality dimensions. The DWreck framework is designed on the principles of microservices architecture pattern. For allowing function-specific extensibility, the DWreck comprises four groups of microservices: (a) Dataset Profiling, (b) Data type Processing, (c) Counterproductive Dimensions, and (d) Miscellaneous. The orchestrator coordinates the different microservices in a complex workflow that is further split into three sub-workflows to generate an unclean (wrecked) dataset as an output. Finally, we evaluate the DWreck framework on twenty seed-datasets to generate corresponding wrecked datasets.",2020,
110,10.1109/ICBDIE50010.2020.00077,AN ERROR FEEDBACK METHOD TO ENHANCE TEACHING ABILITY OF YOUNG TEACHERS FOR CLASSROOM TEACHING PROCESS,"Teaching ability is a hot issue of teachers' team construction, and plays an important role in establishing teachers' teaching quality evaluation system, changing classroom teaching models and establishing teaching supervision and operation mechanisms. According to the main problems in teaching management and classroom teaching process, such as the difficulty in analyzing teaching data and quantitative evaluation, this paper proposes an error feedback method of teaching information based on classroom teaching process data, which is generated by teachers, students, teaching administrators and teaching supervisors. This method can generate teaching error information by comparing teaching process data and teaching evaluation data, and it is helpful for young teachers to improve teaching methods. Besides, the teaching information feedback method has important reference significance for the concrete implementation of the teaching quality monitoring system in the teaching closed loop.",2020,
111,10.1109/BigData.2017.8258453,DEVELOPING SUSTAINABLE TRADING STRATEGIES USING DIRECTIONAL CHANGES WITH HIGH FREQUENCY DATA,"Market prices are traditionally recorded in fixed time intervals. Directional Change is an alternative approach to summarize price movements in financial markets that is consistent with across all time scales. Unlike time series, directional change summarizes the big data in finance by focusing on the intrinsic time of the data. This captures deeper intrinsic data qualities and thus trading strategies based on directional change are more sustainable and less disruptive. In this paper, we propose four trading strategies using the concept of directional change and explore the combination with technical analysis. The trading strategies are tested using EUR/USD and GBP/USD high frequency FX market data. Empirical results show good performance of our trading strategies based on thresholds, and that combining with technical analysis brings further improvement.",2017,
112,10.1109/HPBDIS.2019.8735465,DECISION SUPPORT SYSTEM FOR RISK ASSESSMENT USING FUZZY INFERENCE IN SUPPLY CHAIN BIG DATA,"Currently, organisations find it difficult to design a Decision Support System (DSS) that can predict various operational risks, such as financial and quality issues, with operational risks responsible for significant economic losses and damage to an organisation's reputation in the market. This paper proposes a new DSS for risk assessment, called the Fuzzy Inference DSS (FIDSS) mechanism, which uses fuzzy inference methods based on an organisation's big data collection. It includes the Emerging Association Patterns (EAP) technique that identifies the important features of each risk event. Then, the Mamdani fuzzy inference technique and several membership functions are evaluated using the firm's data sources. The FIDSS mechanism can enhance an organisation's decision-making processes by quantifying the severity of a risk as low, medium or high. When it automatically predicts a medium or high level, it assists organisations in taking further actions that reduce this severity level.",2019,
113,10.1109/MWC.2018.1700224,VEHICULAR CONTENT DELIVERY: A BIG DATA PERSPECTIVE,"The appearance of the Internet of Vehicles enables comfort driving experiences and content- rich multimedia services for in-vehicle users. The vehicular network provides specific scenario- centric content delivery services involving data of vehicle status, user behaviors, and environmental features. In this article, we focus on vehicular content delivery from a big data perspective. After a comprehensive review of state-of-the-art works, we elaborate the potential value of big data in vehicular information and content services by introducing several typical application scenarios. According to the data characteristics, we classify the vehicular data into three categories, that is, location-centric, user-centric, and vehicle-centric, and then illustrate an implementation of big data collection and analysis. A real-world big data application in social-based vehicular networks is presented, and simulation results show that the big-data-enabled content delivery strategy can obtain a performance gain of user satisfaction with the delivered contents compared to the case without consideration of social big data. Finally, we conclude the article with several future research topics.",2018,15580687
114,10.1109/RI2C56397.2022.9910310,BIG DATA ANALYTICS FOR PRE-TEACHER PREPARATION OF HIGHER EDUCATION IN THAILAND,"Digital technology is really important in the present world for driving the national economy with the data-driven development at the age of big data. The arrival of the new Big Data Technology becomes an important instrument for driving organizations to be effective with higher competitive ability. Therefore, big data is popularly used in all industries as well as in graduate production of the universities for improving educational quality. In this paper, the focus is on big data for higher education by presenting big data analytics for producing pre-teacher of higher education in accordance with the need for pre-teacher preparation in Thailand framework.",2022,
115,10.1109/PIMRC.2018.8581037,LEARNING ABOUT FASHION EXPLOITING THE BIG MULTIMEDIA DATA,"We here propose to collect and analyze large amounts of multimedia data from different public and private sources, in the form of text, image, video, to predict relevant information about specific quantities related to fashion brands, such as their sales volumes and/or trends. To this aim, we deem deep learning techniques as the suitable instrument capable of managing extremely large amounts of multimedia data. While a few works exist in literature on learning applications in the fashion area, where text is used to perform sentiment analysis operations, limited research has also considered images and videos in this context. In this paper, starting with an overview of the state of the art of the applications of artificial intelligence for fashion, we set the stage for a holistic approach for the deep learning based analysis of multimedia data related to fashion.",2018,21669570
116,10.1109/TCYB.2017.2685521,INCORPORATION OF EFFICIENT SECOND-ORDER SOLVERS INTO LATENT FACTOR MODELS FOR ACCURATE PREDICTION OF MISSING QOS DATA,"Generating highly accurate predictions for missing quality-of-service (QoS) data is an important issue. Latent factor (LF)-based QoS-predictors have proven to be effective in dealing with it. However, they are based on first-order solvers that cannot well address their target problem that is inherently bilinear and nonconvex, thereby leaving a significant opportunity for accuracy improvement. This paper proposes to incorporate an efficient second-order solver into them to raise their accuracy. To do so, we adopt the principle of Hessian-free optimization and successfully avoid the direct manipulation of a Hessian matrix, by employing the efficiently obtainable product between its Gauss-Newton approximation and an arbitrary vector. Thus, the second-order information is innovatively integrated into them. Experimental results on two industrial QoS datasets indicate that compared with the state-of-the-art predictors, the newly proposed one achieves significantly higher prediction accuracy at the expense of affordable computational burden. Hence, it is especially suitable for industrial applications requiring high prediction accuracy of unknown QoS data.",2018,21682275
117,10.1109/ICIBA50161.2020.9276830,RESEARCH ON FILM DATA PREPROCESSING AND VISUALIZATION,"Data is the core of information, and good data quality is a prerequisite for many data analysis. Data cleaning is to increase the fault tolerance rate by correcting the error value of detected data. This paper aims to solve the problem of data set processing and visualization in the recommendation algorithm, so as to better apply in the field of recommendation algorithm. The recommendation algorithm and data sets Movielens and IMDB are analyzed theoretically. First, data set A was processed from data reading and movie score calculation; Again, the IMDB is processed in four steps to make it more suitable for the recommendation algorithm field; Finally, the plot function is used to visualize the key information. experiment shows: The data set sorted out by the above methods can effectively improve the quality and availability of data and provide relevant basis for better application in the algorithm.",2020,
118,10.1109/HPCS.2018.00154,ARE NDN CONGESTION CONTROL SOLUTIONS COMPATIBLE WITH BIG DATA TRAFFIC?,"Big Data refers to analyzing the massive volume of data by combining different applications in order to save time, efficiency and quality when interpreting data. Controlling the data transfer along the network is a fundamental question in Big Data. In this context, the transport model of the Named Data Networking (NDN) architecture introduces several new features, especially name-based retrieval policy and smaller data transfer time thanks to the interest aggregation and in-network caching. These distinguishing features make the NDN a suitable communication model for Big Data transfer. But, since in NDN content can be retrieved from multiple caches and through multiple paths, the traditional host-to-host congestion control schemes become inconsistent. Hence there is a need for an efficient congestion control scheme that takes into account the tremendous volume of data generated by Big Data processing and NDN characteristics. In this paper, we give a detailed understanding of NDN benefits over traditional TCP/IP stack for Big Data transfer, then we focus on efficient control of Big Data transfer over NDN. We give a comprehensive overview of recent Named Data congestion control solutions and evaluate and discuss their relevance for Big Data applications.",2018,
119,10.1109/TAAI48200.2019.8959927,CASE STUDY: PARAMETERS OPTIMIZATION PROCESS FOR FLAT PANEL DISPLAY MANUFACTURE,"Taiwan's flat panel display manufacturing is famous internationally. Its outstanding manufacturing capability can produce high quality panels in the shortest time. In the production process of flat panel displays, the process yield is the best indicator reflecting the good and bad of manufacturers. The yield is direct. The decision is to the company's profit and reputation. Add to this, it is also the trust and acceptance of the company's customers. When the products on the production line are abnormal in quality, the process and equipment engineers must immediately discuss the crux of the problem and improve it. For decomposition of analysis and process management to find out the method, it needs to carry out the problem in linear verification of confirmation whether the problem is a single problem or not. Furthermore, it will produce customer complaints, dissatisfaction with the company, causing the company's losses. Therefore, using Knowledge Discovery from Data (KDD) to establish a process of big data analysis is core technology in our work. We proposed the improvement process that how to quickly find the variation factor and couple with machine learning. Hence the research is based on data pre-processing, modeling. The experiment shows that we find out the right parameters and optimze them. Finally, the yield improvement was increasing to 66%.",2019,23766816
120,10.1109/BigData50022.2020.9378036,PAIRS AUTOGEO: AN AUTOMATED MACHINE LEARNING FRAMEWORK FOR MASSIVE GEOSPATIAL DATA,"An automated machine learning framework for geospatial data named PAIRS AutoGeo is introduced on IBM PAIRS Geoscope big data and analytics platform. The frame-work simplifies the development of industrial machine learning solutions leveraging geospatial data to the extent that the user inputs are minimized to merely a text file containing labeled GPS coordinates. PAIRS AutoGeo automatically gathers required data at the location coordinates, assembles the training data, performs quality check, and trains multiple machine learning models for subsequent deployment. The framework is validated using a realistic industrial use case of tree species classification. Open-source tree species data are used as the input to train a random forest classifier and a modified ResNet model for 10-way tree species classification based on aerial imagery, which leads to an accuracy of 59.8% and 81.4%, respectively. This use case exemplifies how PAIRS AutoGeo enables users to leverage machine learning without extensive geospatial expertise.",2020,
121,10.1109/ICBDA55095.2022.9760331,THE PREDICTION METHOD OF KPIS BY USING LS-TSVR,"Closely monitoring service performance and making predictions of Key Performance Indicators (KPIs) are critical for Internet-based services. However, fast yet accurate prediction of these seasonal KPIs with various patterns and data quality has been a great challenge. This paper tackles this challenge through a novel approach based on auto-regressive Least Square Twin Support Vector Regression (LS-TSVR). As an improved version of SVR, LS-TSVR can handle big data without any external optimization, and meanwhile, the prediction accuracy is better than that of SVR. For seasonal KPI data in a production dataset, our methods satisfy or approximate a mean average error (MAE) of around 0.013, which is significantly lower than the baseline method.",2022,
122,10.1109/RTSI.2016.7740644,ANOMALY DETECTION IN AEROSPACE PRODUCT MANUFACTURING: INITIAL REMARKS,"Manufacturing companies need to acquire, analyze and share large amounts of information and data to sustain competitive advantage in complex environments. In the context of complex manufacturing, an increasing number of devices, sensors and people are connected to internal networks dramatically changing the ability to generate, communicate, share and access data. Therefore, the data volume has become so large that it cannot be processed using conventional methods. Many companies have dramatically boosted profits and have met consumer demands more proactively, by utilizing automated data collection to feed information into a big data analytics program. In the aerospace manufacturing sector, there is a growing need to consider Big Data solutions to add value to their business services and to optimize their internal production processes. Manufacturing data are an important source of knowledge that can be recorded from different data sources such as sensors and enterprise. The majority of this data are stream processed i.e., they are produced by analytics performed on “in-motion” data. A real-time predictive analysis can help detecting manufacturing anomalies thus improving the production processes and the quality of product. This paper aims to shortly describe the initial findings of an action research study performed in the aerospace industry pilot of the TOREADOR European project.",2016,
123,10.1109/ICC.2018.8422106,QOE-BASED BIG DATA ANALYSIS WITH DEEP LEARNING IN PERVASIVE EDGE ENVIRONMENT,"In the age of big data, the services in pervasive edge environment are expected to offer end-users better Quality of Experience (QoE) than that in a normal edge environment. Nevertheless, various types of edge devices with storage, delivery, and sensing are coming into our environment and produce the high-dimensional big data accompanied by a volume of pervasive big data increasingly with a lot of redundancy. Therefore, the satisfaction of QoE becomes the primary challenge in high dimensional big data on the basis of pervasive edge environment. In this paper, we first propose a QoE model to evaluate the quality of service in pervasive edge environment. The value of QoE does not only include the accurate data, but also the transmission rate. Then, on the basis of the accuracy, we propose a Tensor-Fast Convolutional Neural Network (TF-CNN) algorithm based on Deep Learning, which is suitable for pervasive edge environment with high-dimensional big data analysis. Simulation results reveal that our proposals could achieve high QoE performance.",2018,19381883
124,10.1109/BDEIM55082.2021.00095,RESEARCH ON STREET SPACE QUALITY COMBINED WITH ATTENTION MULTI-TASK DEEP LEARNING,"In the context of advocating human-oriented urbanism, in order to more accurately evaluate the urban scene. This paper uses the sentiment evaluation of millions of street view images in the Place Pulse 2.0 data set to design a multi-task deep learning network (Mtl-attn-vgg) fused with attention to analyze the urban spatial quality attributes. Multi-task learning is used to simultaneously learn different attributes and the attention mechanism to focus on the characteristics of the object to improve the effect of the convolutional layer on the attribute feature extraction, combined with deep relative attribute learning, use the structural sparsity of the feature matrix to compare the attribute features In addition, classification loss and ranking loss are introduced to constrain the parameters of the deep learning network. Finally, six attributes of street view are beautiful, boring, depressed, lively, safe, and rich. The results show that the Mtl-attn-vgg model improves the feature extraction effect and improves the accuracy of attribute sorting to 78.48%, which is more conducive to urban designers and planners to understand the attribute characteristics of the street scene environment.",2021,
125,10.1109/IWIES.2013.6698559,ADVANCED ANALYTICS FOR HARNESSING THE POWER OF SMART METER BIG DATA,"Smart meters or advanced metering infrastructure (AMI) are being deployed in many countries around the world. Smart meters are the basic building block of the smart grid and governments have invested vast amounts in smart meter deployment targeting wide economic, social and environmental benefits. The key functionality of the smart meter is the capture and transfer of data relating to the consumption (electricity, gas) and events such as power quality and meter status. Such capability has also resulted in the generation of an unprecedented data volume, speed of collection and complexity, which has resulted in the so called big data challenge. To realize the hidden value and power in such data, it is important to use the appropriate tools and technology which are currently being called advanced analytics. In this paper we define a smart metering landscape and discuss different technologies available for harnessing the smart meter captured data. Main limitations and challenges with existing techniques with big data are also highlighted and several future directions in smart metering are presented.",2013,
126,10.1109/ICBDACI.2017.8070824,BIG DATA IN WEATHER FORECASTING: APPLICATIONS AND CHALLENGES,"Increasing evidence of climate change worldwide is becoming the reason to understand a lot more about the weather — everything from what's going to happen tomorrow to what's coming next year. To forecast weather we need to analyze a large set of data therefore use of big data in weather forecasting will provide numerous advantages such as saving lives, improving the quality of life, reducing risks, enhancing profitability and humanity. Some examples of these domains include Forecasting Solar Power for Utility Operations, large-scale crop production forecasts for global food security, in precision agriculture for future farming and space weather forecasting. In order to know how these applications could impact normal operations this paper defines various weather forecasting applications and technical challenges.",2017,
127,10.1109/AiIC54368.2022.9914026,TOWARDS HEALTHCARE ORGANIZATIONAL PERFORMANCE DERIVING BY BIG DATA ANALYTICS QUALITY FACTORS: A SYSTEMATIC LITERATURE REVIEW,"Quality factors of Big Data Analytics (BDA) have been moderately studied; however, few have examined the effect of BDA quality factors in the organizational context; yet, there is a lack of research in the healthcare domain. Healthcare organizations have been concerned about the BDA quality factors in ensuring an effective BDA system and application in healthcare. To that purpose, this study has a dual objective. Firstly, to identify the BDA quality factors that enhance organizational performance. Second, this work contributes to the body of knowledge in the information system (IS) domain by concentrating on BDA quality factors. This review has revealed nine prominent BDA quality factors that may affect organizational performance in our study on healthcare organizational performance. Thus, this study will contribute to the practice and research of BDA and its effectiveness in enhancing organizational performance, based on the twenty-three papers chosen.",2022,
128,10.1109/IS3C.2016.107,SOCIAL MEDIA ANALYTICS BASED PRODUCT IMPROVEMENT FRAMEWORK,"In the traditional world, marketing studies managed to employ various techniques to explore customers' consensual experiences toward products with limited information available. The uses of surveys, focus groups or regular individual interviews are some of the frequently used methods by marketers. We are now entering in the era of Big Data. The explosion and profusion of the unprecedented scale of heterogeneous data available in this new era allow us to acquire further insights and knowledge about the market for improving the quality of products. In this paper, we present a Social Media-based Product Improvement Framework (SM-PIF) which is capable of deriving recommendations for product improvement and subsequently increase the product's market competitiveness. The recommendation generated by the SM-PIF is expected to be more accurate and less biased than traditional methods due to its ""Big Data"" nature.",2016,
129,10.1109/ICBDA51983.2021.9403027,RESEARCH ON THE LEARNING BEHAVIOR OF STUDENTS IN BLENDED LEARNING MODE,"With the advent of the information age coming, the Blended Learning mode combining real classrooms and online teaching has gradually become a research hotspot in the field of education at home and abroad. In the process of implementing the teaching mode, students have produced a great deal of data related to their leaming behavior, thus data mining and students' behavior analysis can play a guiding role in the learning process. Taking the “Data Structure” course as a practical case, this paper investigates the relationship between the learning behaviors and effects of this course, analyzes the deficiency in the Blended Learning mode, which provides strong support for improving the quality of education.",2021,
130,10.1109/MysuruCon52639.2021.9641609,A SMART ENVIRONMENT MONITORING FRAMEWORK USING BIG DATA AND IOT,"The impact of air pollution is rapidly increasing, causing a slew of health issues as well as a decline in agricultural productivity, thereby affecting all forms of life. This addresses the need for a smart framework for monitoring air pollution, which promotes a smart environment and can be widely adopted in smart cities. This paper proposes a system for monitoring ambient air quality on roads and tracking vehicles that emit pollution above a predetermined limit by installing a visual Air Quality Meter (AQM) and thus protecting the smart environment. Real-time AQMs installed along roadsides are an effective way of informing the public about air pollution and raising awareness. The paper proposes a method using the Internet of Things (IoT), which can supervise the quality of air contamination at several locations by integrating Electrochemical Toxic Gas Sensors, Radio Frequency Identification (RFID), and IoT devices to solve this problem. The proposed approach also makes use of Big Data techniques to extract insights from patterns, which provides the necessary information to monitor and protect the environment in order to keep it safe and green in the future. This framework also assists in intelligent traffic light control and deviation thereby emission pollutant can be reduced and saves fuel and time. Environmental engineering and IoT technology will also be integrated into the proposed solution. The installation of this technology provides relative air quality data with solutions and suggestions for public display, as well as AQMs to avoid the critical stages. The proposed work is intended to be implemented as a prototype, with the results revealed as an extension of this paper. Given the design and development constraints, this system appears to be more effective in monitoring and achieving a suitable environment and a healthy society for a smart living environment.",2021,
131,10.1109/ICBDSS51270.2020.00032,THE IMPACT OF CROSS-BORDER E-COMMERCE ON CHINA’S AGRICULTURAL PRODUCTS EXPORT : —AN EMPIRICAL STUDY BASED ON BIG DATA PROCESSING,"In recent years, China's agricultural product export trade has achieved rapid growth with the help of cross-border e-commerce platforms, which has played a positive role in China's agricultural and economic development. Based on the construction of China's cross-border e-commerce development index through the Entropy method, this paper uses 2008-2019 agricultural product import and export big data to establish a model, and then analyzes the relationship between cross-border e-commerce and China's agricultural product export trade. The results show that cross-border e-commerce has indeed promoted the export trade of agricultural products. However, the agricultural product logistics system is not sound enough, and the quality of agricultural products needs to be improved, which restricts the development of China's agricultural export. Based on the analysis, this article believes that the problem can be solved in terms of improving logistics capabilities, improving quality, and increasing talent training.",2020,
132,10.1109/IIAI-AAI.2015.219,QUALITY MEASUREMENT BEYOND BIBLIOMETRY,"Research Administration has always been an important part of research organization, as it has the authority to boost research activities in selected fields. One of the challenges is, to identify those directions of research and those spontaneously popping up findings as early as possible, which will become of high importance for economy and research in the nearer or more distant future. Without good and well tested technical aids, this task may be impossible to meet. For many years, bibliometry was the method of first choice, to identify persons and groups of high impact (=quality?) research. The problem with using bibliometry is, that publishing is a slow process, resulting in measuring the quality and impact of those activities years back. It also results in support for those groups and individuals, who already had been supported for a long time, moving those researchers out of focus, who may come up with new, innovative findings. This paper gives an overview of research activities for measuring research quality beyond the bibliometric paradigm, but making use of big data, research data and computer based learning in Germany. It also gives some short but generally understandable introduction into the used technologies to enable the audience to discuss on the shown methodologies and techniques.",2015,
133,10.1109/BCD.2019.8884820,THERMAL VISION BASED FALL DETECTION VIA LOGICAL AND DATA DRIVEN PROCESSES,"Inadvertent falls can cause serious, and potentially fatal injuries, to at risk individuals. One such community of at-risk individuals is the elderly population where age related complications, such as osteoporosis and dementia, can further increase the incidence and negative impact of such falls. Notably, falls within that community has been identified as the leading cause of injury related preventable death, hospitalization and reduction to quality of life. In such cases, rapid detection of, and reaction, to fall events has shown to be critical to reduce the negative effects of falls within this community. Currently, a range of fall detection solutions exist, however, they have several deficiencies related to the core approach that has been adopted. This study has developed an ensemble of thermal vision-based, big data facilitated, solutions which aim to address some of these deficiencies. An evaluation of these logical and data-driven processes has occurred with the promising results presented within this manuscript. Finally, opportunities future work and real-world evaluation have occurred and are underway.",2019,
134,10.1109/ICISE51755.2020.00034,RESEARCH ON THE QUALITY OF THE MASTER TRAINING IN THE CONTEXT OF BIG DATA———BASED ON THE ECONOMIC MODEL OF PUBLIC TRUST,"With the development of my country's information technology, it can be found that my country's master's degree is increasing in a certain proportion every year. Affected by the novel coronavirus epidemic, policies will be introduced in 2020 to increase the enrollment expansion of masters by 20%. Big data has found that the proportion of enrollment expansion has increased significantly compared with previous years. The large increase in the proportion of enrollment has raised concerns about whether the gold content of master's degree will decline in this era of big data. Based on the SPSS of statistical analysis by network computers, this paper constructs an economic model to explore whether the current quality of master's training can solve the public's concerns, and thus introduce educational policies that are in line with the development of information technology.",2020,
135,10.23919/ICACT.2019.8702037,AN IMPROVEMENT OF A CHECKPOINT-BASED DISTRIBUTED TESTING TECHNIQUE ON A BIG DATA ENVIRONMENT,"The advancement of storage technologies and the fast-growing number of generated data have made the world moved into the Big Data era. In this past, we had many data mining tools but they are inadequate to process Data-Intensive Scalable Computing workloads. The Apache Spark framework is a popular tool designed for Big Data processing. It leverages in-memory processing techniques that make Spark up to 100 times faster than Hadoop. Testing this kind of Big Data program is time consuming. Unfortunately, developers lack a proper testing framework, which cloud help assure quality of their data-intensive processing programs while saving development time and storage usages.We propose Distributed Test Checkpointing (DTC) for Apache Spark. DTC applies unit testing to the Big Data software development life cycle and reduce time spent for each testing loop with checkpoint. By using checkpoint technique, DTC keeps quality of Big Data processing software while keeps an inexpensive testing cost by overriding original Spark mechanism so that developers no pain to learn how to use DTC. Moreover, DTC has no addition abstraction layers. Developers can upgrade to a new version of Spark seamlessly. From the experimental results, we found that in the subsequence rounds of unit testing, DTC dramatically speed the testing time up to 450-500% faster. In case of storage, DTC can cut unnecessary data off and make the storage 19.7 times saver than the original checkpoint of Spark. DTC can be used either in case of JVM termination or testing with random values.",2019,17389445
136,10.1109/IPFA53173.2021.9617238,DATA ANALYTICS AND MACHINE LEARNING: ROOT-CAUSE PROBLEM-SOLVING APPROACH TO PREVENT YIELD LOSS AND QUALITY ISSUES IN SEMICONDUCTOR INDUSTRY FOR AUTOMOTIVE APPLICATIONS,"Quality requirements in the semiconductor industry for automotive products are increasing rapidly with the movement to autonomous vehicles and higher levels of safety. It is no longer possible to express maximum failure requirements in parts per million (ppm). Individual failing parts observed in the field and reported by customers can trigger a significant quality response. ‘Zero-defect’ (ZD) is no longer considered a utopian ideal, but a required potentially reachable goal for semiconductor manufacturers. Projects and studies that include artificial intelligence and big data, are seen as key drivers to reach a ZD level of quality. Competing objectives targeted in any industrial project, such as quality improvement and gross margin, must also be considered. Initial projects in machine learning (ML), focusing on yield-loss issues, are being deployed within the manufacturing sites. These projects interconnect typical internal data collected from the manufacturing and assembly lines with engineering, qualification and reliability data. For a specific case study of unexpected abnormally high variability on some parameters, this paper presents a problem-solving approach in a big-data environment. Models implemented and results obtained towards root-cause problem solving for this issue, are discussed. This overall approach may be replicated in other ML projects.",2021,19461542
137,10.1109/ICoDT255437.2022.9787394,INTEGRATED PRODUCT-PROCESS DESIGN: CONCEPTUAL FRAMEWORK FOR DATA DRIVEN MANUFACTURING RESOURCE SELECTION,"With the extensive development of new technologies and the application of information in the manufacturing industry, immense volumes of distinct data are being generated and collected daily. However, this data is largely unusable as its not meticulously cleaned and processed. The effective utilization of such complex data is the cornerstone of data analytics, as successful analysis leads to useful, relevant, and actionable knowledge, which in the long run can prove to be revolutionary for any field and open new avenues. Although the application of data analytics in areas such as sales & marketing, healthcare, cybersecurity, and climate change is largely prevalent, the implementation of data analysis and its tools for efficient product & process design is an unexplored opportunity with large volumes of data generated by major stakeholders throughout the manufacturing and product-process design activity remaining underutilized. This paper, therefore, defines a novel conceptual framework that applies data analysis to integrated product-process design (IPPD) for weighted data-driven IPPD that amalgamates data generated from multiple streams. Primarily from the user perspective, supply chain network, current & upcoming technological processes, and competitor process and product designs, will be utilized. The proposed framework can be further used to create new products better aligned with customer requirements, enhance the overall quality of the product, improve production efficiency through new technological advancements, support the supply chain network, and give the applicant industry a competitive advantage against its competitors.",2022,
138,10.1109/ISS1.2017.8389462,SYSTEMATIC REVIEW OF BIG DATA ANALYTICS IN GOVERNANCE,"With advent of technology, data is increasing abruptly day by day. Traditional database systems are not capable of processing and handling such a voluminous data. Big data analytics has the capability of processing, handling and analyzing the large datasets or stream of data. Big data analytics play important role in fields such as healthcare, agriculture, smart grid and policy making. Big data analytics with governance improves planning and decision making phases for government projects. It helps to improve the quality of government services. This paper focuses on the different application areas of governance in which big data analytics play a role and tools used to handle big data management problem. The existing work is classified into different categories and is presented using visualizations. This paper also deals with challenges related to governance field.",2017,
139,10.1109/MSIEID52046.2020.00010,EVALUATION OF MATERIAL SUPPLIERS BASED ON BP NEURAL NETWORK UNDER THE BACKGROUND OF BIG DATA,"Nowdays for construction enterprises, there are more material suppliers to choose under the background of big data application in construction industry. Therefore how to evaluate and select the material suppliers correctly and wisely become a particularly important work for manager to control the cost and quality of a project. Firstly, the material supplier evaluation index system is constructed, then the method of obtaining relevant data is put forward. Based on the above, the material supplier evaluation model is constructed by using the BP neural network algorithm. Finally, the applicability and reliability of this model are demonstrated through an example, which provides reference for the evaluation of material suppliers in the practice of building materials procurement.",2020,
140,10.1109/ICCC51557.2021.9454608,UTILIZATION OF DATA MINING METHODS IN MANUFACTURING INDUSTRY,"The paper presents data mining as a suitable tool for analyzing data from industrial processes. The data mining methods offer a wide range of uses in the current age of digitalization, big data processing and analysis. Apart from discovering patterns and detecting relationship between individual characteristics, assuring quality of products, prediction and optimization of process performance, data mining techniques also contribute to the transition from a reactive to a predictive approach in problem solving. The first part of the paper presents the possibilities of utilization of data mining methods and techniques to analyze data from industrial processes. The second part of the paper deals with a selection of proper data mining method and its practical application on data from manufacturing industry.",2021,
141,10.1109/BigData.2017.8258061,PREDICTING REGIONAL ECONOMIC INDICES USING BIG DATA OF INDIVIDUAL BANK CARD TRANSACTIONS,"For centuries quality of life was a subject of studies across different disciplines. However, only with the emergence of a digital era, it became possible to investigate this topic on a larger scale. Over time it became clear that quality of life not only depends on one, but on three relatively different parameters: social, economic and well-being measures. In this study we focus only on the first two, since the last one is often very subjective and consequently hard to measure. Using a complete set of bank card transactions recorded by Banco Bilbao Vizcaya Argentaria (BBVA) during 2011 in Spain, we first create a feature space by defining various meaningful characteristics of a particular area performance through activity of its businesses, residents and visitors. We then evaluate those quantities by considering available official statistics for Spanish provinces (e.g., housing prices, unemployment rate, life expectancy) and investigate whether they can be predicted based on our feature space. For the purpose of prediction, our study proposes a supervised machine learning approach. Our finding is that there is a clear correlation between individual spending behavior and official socioeconomic indexes denoting quality of life. Moreover, we believe that this modus operandi is useful to understand, predict and analyze the impact of human activity on the wellness of our society on scales for which there is no consistent official statistics available (e.g., cities and towns, districts or smaller neighborhoods).",2017,
142,10.1109/BigData.2018.8621870,A DENSITY-BASED PREPROCESSING TECHNIQUE TO SCALE OUT CLUSTERING,"Clustering big data is a challenging task, because the majority of high-quality clustering algorithms do not scale well with respect to the data set cardinality. To tackle the scalability problem, we propose a general-purpose density-based preprocessing technique, called SCOUT, implemented in the Spark framework. It allows compacting the original data by means of a set of representative points, while still preserving the original data distribution and density information. This small set of representative points may become the input to almost any clustering algorithm. Thus, also complex, high-quality in-memory algorithms can be applied. A thorough experimental evaluation shows that the proposed approach is efficient and at the same time effective.",2018,
143,10.1109/ICECAA55415.2022.9936159,ANALYSIS OF STABILITY BIG DATA ENVIRONMENT OF INTELLIGENT FINANCIAL DATA ABNORMAL QOS SYSTEM BASED ON WOLF PACK ALGORITHM,"A new swarm intelligence algorithm, the Wolf Pack Algorithm has been proposed in this paper, and the convergence of the algorithm is proved based on the Markov chain theory. It reduces the risk of the algorithm falling into local optimum due to the excessively large penalty parameter. Inspired by the reproduction mode of wol ves, a big data environment analysis for the stability of the QoS system for abnormal data is proposed based on the binary wolf pack algorithm. Moreover, the Convolutional Neural Network with 4 hidden layers is used to classify and evaluate the constructed time series financial data. Data testing and analysis are performed using actual financial data. It is believed that the supervision system and relevant laws and regulations need to be improved first; secondly, the big data is used to collect personal credit records so as to establish a sound credit system as soon as possible; finally, through big data and computer technology, risk control methods are innovated to enhance the stability of Internet finance.",2022,
144,10.1109/PERCOM.2018.8444581,A PLATFORM SOLUTION OF DATA-QUALITY IMPROVEMENT FOR INTERNET-OF-VEHICLE SERVICES,"Interconnection and intelligence have become the latest trends of the new generation of vehicle and transportation technologies. Applications built upon platforms of cloud-centered vehicle networking, i.e., Internet-of-Vehicles (IoVs), have been increasingly developed and deployed to provide data-centric services (e.g., driving assistance). Because these services are often safety critical, assuring service dependability has become an important requirement. In this paper, we propose DQI, a platform-level solution of Data-Quality Improvement designed to assure service dependability for Internet-of-Vehicle services. As an example, DQI is deployed in CarStream, an industrial system of big data processing designed for chauffeured car services. Via CarStream, over 30,000 vehicles are organized in a virtual vehicle network by sharing vehicle-status data in a near real-time manner. Such data often have low-quality issues and compromise the dependability of data-centric services. DQI includes techniques of data-quality improvement, including detecting outliers, extracting frequent patterns, and interpolating sequences. DQI enhances the dependability of data-centric services in IoVs by addressing the common data-quality requirements at the platform level. Upper-level services can benefit from DQI for data-quality improvement and reduce the complexity of service logic. We evaluate DQI by using a three-year dataset of vehicles and real applications deployed in CarStream. The result shows that compared with existing approaches, DQI can effectively restore missing data and correct anomalies with more than 30.0% improvement in precision. By studying multiple real applications, we also show that this data-quality improvement can indeed enhance the dependability of IoV services.",2018,2474249X
145,10.1109/ACCESS.2021.3069449,"TOWARDS PADDY RICE SMART FARMING: A REVIEW ON BIG DATA, MACHINE LEARNING, AND RICE PRODUCTION TASKS","Big Data (BD), Machine Learning (ML) and Internet of Things (IoT) are expected to have a large impact on Smart Farming and involve the whole supply chain, particularly for rice production. The increasing amount and variety of data captured and obtained by these emerging technologies in IoT offer the rice smart farming strategy new abilities to predict changes and identify opportunities. The quality of data collected from sensors greatly influences the performance of the modelling processes using ML algorithms. These three elements (e.g., BD, ML and IoT) have been used tremendously to improve all areas of rice production processes in agriculture, which transform traditional rice farming practices into a new era of rice smart farming or rice precision agriculture. In this paper, we perform a survey of the latest research on intelligent data processing technology applied in agriculture, particularly in rice production. We describe the data captured and elaborate role of machine learning algorithms in paddy rice smart agriculture, by analyzing the applications of machine learning in various scenarios, smart irrigation for paddy rice, predicting paddy rice yield estimation, monitoring paddy rice growth, monitoring paddy rice disease, assessing quality of paddy rice and paddy rice sample classification. This paper also presents a framework that maps the activities defined in rice smart farming, data used in data modelling and machine learning algorithms used for each activity defined in the production and post-production phases of paddy rice. Based on the proposed mapping framework, our conclusion is that an efficient and effective integration of all these three technologies is very crucial that transform traditional rice cultivation practices into a new perspective of intelligence in rice precision agriculture. Finally, this paper also summarizes all the challenges and technological trends towards the exploitation of multiple sources in the era of big data in agriculture.",2021,21693536
146,10.1109/ICITBS.2019.00118,THE ANALYSIS AND DESIGN OF SHIP MONITORING SYSTEM BASED ON HYBRID REPLICATION TECHNOLOGY,"As the core of informatization, data has a huge significance to the development of information-based enterprises. Data replication technology is an important approach to solve the problem of enterprise data sharing based on distributed database system. It plays a crucial role in promoting business integration of enterprises and institutions, improving data quality, enhancing data sharing and improving the application level of back-end big data analysis [1]. It is necessary to do research for making a good data management of the distributed database application system, synchronizing the data to the data, preventing data conflicting and being able to synchronize or asynchronous replication. Combining with the database design model of the ship monitoring and control system, this paper mainly described how to complete the construction of distributed database system using Oracle, based on advanced replication technology named as the combination of multi-agent replication and materialized views hybrid replication technology.",2019,
147,10.1109/TC.2016.2550454,BIG-DATA STREAMING APPLICATIONS SCHEDULING BASED ON STAGED MULTI-ARMED BANDITS,"Several techniques have been recently proposed to adapt Big-Data streaming applications to existing many core platforms. Among these techniques, online reinforcement learning methods have been proposed that learn how to adapt at run-time the throughput and resources allocated to the various streaming tasks depending on dynamically changing data stream characteristics and the desired applications performance (e.g., accuracy). However, most of state-of-the-art techniques consider only one single stream input in its application model input and assume that the system knows the amount of resources to allocate to each task to achieve a desired performance. To address these limitations, in this paper we propose a new systematic and efficient methodology and associated algorithms for online learning and energy-efficient scheduling of Big-Data streaming applications with multiple streams on many core systems with resource constraints. We formalize the problem of multi-stream scheduling as a staged decision problem in which the performance obtained for various resource allocations is unknown. The proposed scheduling methodology uses a novel class of online adaptive learning techniques which we refer to as staged multi-armed bandits (S-MAB). Our scheduler is able to learn online which processing method to assign to each stream and how to allocate its resources over time in order to maximize the performance on the fly, at run-time, without having access to any offline information. The proposed scheduler, applied on a face detection streaming application and without using any offline information, is able to achieve similar performance compared to an optimal semi-online solution that has full knowledge of the input stream where the differences in throughput, observed quality, resource usage and energy efficiency are less than 1, 0.3, 0.2 and 4 percent respectively.",2016,23263814
148,10.1109/ITAIC54216.2022.9836678,RESEARCH ON TEACHING QUALITY EVALUATION SYSTEM BASED ON DATA MINING,"Starting from the problems existing in the current teaching quality evaluation process in colleges and universities, this paper discusses a teaching quality evaluation system based on data mining. The paper puts forward the design scheme of the teaching quality evaluation system in colleges and universities. Furthermore, by using data mining technology, it discusses the data mining implementation method, system architecture, and system evaluation process in the teaching quality and evaluation system in colleges and universities. Implementing the system can provide a reference for the teaching quality evaluation work in colleges and universities, which has significant application value.",2022,26932873
149,,ONLINE ANOMALY DETECTION IN BIG DATA,"In this paper, the problem of online anomaly detection in multi-attributed, asynchronous data from a large number of individual devices is considered. It has become increasingly common for many services, such as video-on-demand (VOD), to have connected customers where hundreds of millions of subscribers access a cluster of content servers for online services. It is important to monitor these transactions online, in order to ensure acceptable quality of experience to the customers as well as for detecting any abnormal or undesirable activities. Our proposed anomaly detection strategy works in two phases: First we perform intermittent anomaly detection in space, using data from the entire set of devices for a short duration in time. This phase employs principal component analysis (PCA) for data reduction and captures models of normal and abnormal features. Then, these identified models are used to monitor each subscriber's devices online in order to quickly detect any abnormalities. The proposed approach is demonstrated on Comcast's Xfinity video streaming data.",2014,
150,10.1109/BigData.2013.6691760,BIG DATA SOLUTIONS FOR PREDICTING RISK-OF-READMISSION FOR CONGESTIVE HEART FAILURE PATIENTS,"Developing holistic predictive modeling solutions for risk prediction is extremely challenging in healthcare informatics. Risk prediction involves integration of clinical factors with socio-demographic factors, health conditions, disease parameters, hospital care quality parameters, and a variety of variables specific to each health care provider making the task increasingly complex. Unsurprisingly, many of such factors need to be extracted independently from different sources, and integrated back to improve the quality of predictive modeling. Such sources are typically voluminous, diverse, and vary significantly over the time. Therefore, distributed and parallel computing tools collectively termed big data have to be developed. In this work, we study big data driven solutions to predict the 30-day risk of readmission for congestive heart failure (CHF) incidents. First, we extract useful factors from National Inpatient Dataset (NIS) and augment it with our patient dataset from Multicare Health System (MHS). Then, we develop scalable data mining models to predict risk of readmission using the integrated dataset. We demonstrate the effectiveness and efficiency of the open-source predictive modeling framework we used, describe the results from various modeling algorithms we tested, and compare the performance against baseline non-distributed, non-parallel, non-integrated small data results previously published to demonstrate comparable accuracy over millions of records.",2013,
151,10.1109/ICHQP.2016.7783434,ADDITIONAL INFORMATION FROM VOLTAGE DIPS,"This paper presents some methods to extract additional information from voltage dip recordings, beyond residual voltage and duration. Additionally it discusses some issues related to the massive amount of data obtained from modern measurements that, is referred to as Big Data. The paper proposes some Deep Learning based algorithms as good candidates to extract complex features from big data as a step towards additional information. The applications of the information include predicting individual equipment performance, fault type and location, protection operation, and overall load behavior. Individual equipment and overall load include production as well as consumption.",2016,21640610
152,10.1109/IAS44978.2020.9334900,BIG DATA COMPRESSION IN SMART GRIDS VIA OPTIMAL SINGULAR VALUE DECOMPOSITION,"The smart grid is a fully automatic delivery grid for electricity power with a two-way reliable flow of electricity and information among different equipment on the grid. With the rapid development of smart grids, smart meters and sensors are used to monitor the system and provide a wide reporting which produce a huge amount of data in various part of the grid. To logical manage this trouble, the presented paper proposes a new lossy data compression approach for big data compression. In the proposed method, at the first step, the optimal singular value decomposition (OSVD) is applied to a matrix that achieves the optimal number of singular values to the sending process and the other ones will be neglected. This goal is done due to the quality of retrieved data and the rate of compression ratio. In the presented scheme, to implementation of the optimization framework, various intelligent optimization methods are used to determine the number of optimal values in the elimination stage. The efficiency and capabilities of the proposed method are examined using the experimental dataset of several residential microgrid consumers and market dataset. Simulation results show the high performance and efficiency of the proposed model in smart grids with big data.",2020,01972618
153,10.1109/BigData52589.2021.9671366,LEARNING FROM FAILURES IN LARGE SCALE SOFT SENSING,"Scientific publications typically favor studies that show successful results. Things that did not work either are relegated to footnotes, or worse, never get mentioned. The publication of failures can potentially shorten the learning curve of research because they are discoveries in their own right. Here we share those things that have not worked in our soft sensing deep learning experiments in the hope that future researchers and application engineers can learn from these lessons. This article is compiled from our experiments with multiple deep learning algorithms including Autoencoder, CNN, GNN and Transformer to predict the quality control variables in the wafer manufacturing process in Seagate, based on the sensor data collected during deposition and etching processes. We experimented on various pre-processing and modeling methods and here are some of the methods that did not show a good performance: using focal loss or hinge loss to replace cross entropy, using standard or robust scaler to replace min-max scaler, scaling the data by groups, using oversampling methods such as random sampling or SMOTE, and selecting features based on domain knowledge. We hope that these experiments will give some intuition to future researchers and save their time for experiments. We recognize that these conclusions are drawn from our experiments only, and accept that it is possible that these things may work in a different context. We welcome all kinds of discussion or collaboration.",2021,
154,10.1109/TKDE.2018.2844176,NORMALIZATION OF DUPLICATE RECORDS FROM MULTIPLE SOURCES,"Data consolidation is a challenging issue in data integration. The usefulness of data increases when it is linked and fused with other data from numerous (Web) sources. The promise of Big Data hinges upon addressing several big data integration challenges, such as record linkage at scale, real-time data fusion, and integrating Deep Web. Although much work has been conducted on these problems, there is limited work on creating a uniform, standard record from a group of records corresponding to the same real-world entity. We refer to this task as record normalization. Such a record representation, coined normalized record, is important for both front-end and back-end applications. In this paper, we formalize the record normalization problem, present in-depth analysis of normalization granularity levels (e.g., record, field, and value-component) and of normalization forms (e.g., typical versus complete). We propose a comprehensive framework for computing the normalized record. The proposed framework includes a suit of record normalization methods, from naive ones, which use only the information gathered from records themselves, to complex strategies, which globally mine a group of duplicate records before selecting a value for an attribute of a normalized record. We conducted extensive empirical studies with all the proposed methods. We indicate the weaknesses and strengths of each of them and recommend the ones to be used in practice.",2019,23263865
155,10.1109/ICSGEA.2019.00028,RISK ASSESSMENT MODEL AND EXPERIMENTAL ANALYSIS OF ELECTRIC POWER PRODUCTION BASED ON BIG DATA,"This paper studies the characteristics of big data of power, and aims at the data quality problems faced by power system. It puts forward an assessment method of power system data quality. Based on the characteristics of large power data, a series of indicators influencing the data are analyzed and hierarchically divided to determine the measurement standard of power production data during the process of risk management, namely, the risk index system. Then, the risk assessment model of power data is established by referring to the assessment model in other fields or the rules of deduction and induction in data mining. It can be used to evaluate the quality of power system data, and find a framework and solution suitable for large data quality assessment. Finally, the model is implemented on Hadoop platform, which proves that it takes into account the completeness of the index system, the objectivity of the assessment method and the rapidity of the calculation method.",2019,
156,10.1109/AICCSA53542.2021.9686922,MODELING BIG DATA-CENTRIC SERVICES USING KNOWLEDGE GRAPHS,"Big services have recently emerged from the synergy between big data and cloud computing paradigms. This new big data-centric service model aims to provide customer-oriented massive services by combining both physical and virtualized resources from different domains. Although such complex ecosystem is able to process, encapsulate and offer huge volumes of data as services, its management operations are beyond the ability of human administrators, due to several challenges including the big services’ large-scale nature and complexity, the heterogeneity of their components (e.g., services, data sources, connected things), the dynamicity and uncertainty of their hosting cloud environments. To cope with the lack of understanding regarding big services capabilities, we propose to describe them using a novel meta-model for the quality of big services (QoBS). We also take advantage of a recent technology called knowledge graphs, to represent the big service information (service descriptions, services’ and data sources’ quality levels, management policies) as a heterogeneous information network. Finally, a multi-view representation learning approach is proposed to infer additional knowledge regarding big services capabilities.",2021,21615322
157,10.1109/BigData.2018.8622583,ENSEMBLE MACHINE LEARNING SYSTEMS FOR THE ESTIMATION OF STEEL QUALITY CONTROL,"Recent advances in the steel industry have encountered challenges in soliciting decision making solutions for quality control of products based on data mining techniques. In this paper, we present a steel quality control prediction system encompassing with real-world data as well as comprehensive data analysis results. The core process is cautiously designed as a regression problem, which is then best handled by grouping various learning algorithms with their massive resource of historical production datasets. The characteristics of the currently most popular learning models used in regression problem analysis are as well investigated and compared. The performance indicates our steel quality control prediction system based on ensemble machine learning model can offer promising result whilst delivering high usability for local manufacturers to address the production problem by aid of development of machine learning techniques. Furthermore, real-world deployment of this system is demonstrated and discussed. Finally, future directions and the performance expectation are pointed out.",2018,
158,10.1109/ICAICT.2015.7338514,ANALYSIS OF FINITE FLUCTUATIONS FOR SOLVING BIG DATA MANAGEMENT PROBLEMS,The paper is related to the solution of problems of regulated city intersections and assessment of quality of teaching staff activity. The proposed method of solving these problems - the method of Analysis of finite fluctuations - showed good results represented in the examples below.,2015,
159,10.1109/ICESC54411.2022.9885609,BIG-DATA IN HEALTHCARE MANAGEMENT AND ANALYSIS: A REVIEW ARTICLE,"Big data in healthcare is used to describe a massive amount of information about the collection of patient records created by digital technology. It also helps in managing hospital performance. The main aim and objective of this article are to show how big data is used in healthcare and management with the analysis of big data. Mainly this article narrates the special characteristics and different phases of big data as it is an essential process because it allows health care providers to go down and learn more about the care they have provided to their patients. In a health care section, big data analytics offers many benefits for detecting severe disorders in the starting phase and delivering higher quality healthcare facilities to the correct client at the proper time to improve the quality of life of the patient. If data is managed and analyzed correctly, it will provide a piece of meaningful information for the patients. It is also beneficial, reducing medical errors, preventing mass disease, detecting diseases, and focusing on patients’ personalized care. As this big data in healthcare has a characteristic too based on that, they are defining themselves. Big data analytics in healthcare allow for analyzing large amounts of information from more than a thousand patients.",2022,
160,10.1109/ICWS.2016.49,BIG DATA ANALYTIC SERVICE DISCOVERY USING SOCIAL SERVICE NETWORK WITH DOMAIN ONTOLOGY AND WORKFLOW AWARENESS,"In the era of Big Data, data analysis gives strong competition power to enterprises. As services for Big Data Analysis (BDA) become prevalent, analysis services with intelligence and autonomy using automatic service composition show very bright prospects in the BDA market. Service composition consists of four stages: workflow generation, discovery, selection, and execution. In this paper, we propose a novel service discovery approach that considers two key concerns in the discovery domain towards better quality as well as effective service composition. BDA services are fine grained according to the domain and functional behaviors. The services need a domain context-aware and precision-guided discovery approach. Therefore, we propose domain ontology-based service discovery. It is mainly focused on the BDA domain for precise service discovery considering all behavioral signatures between queries and services. As for the second concern, components in composed services depend greatly on each other in situations such as workflow for data analysis. We show that linking services together considering sociability or user preference gives better discovery performance. We propose a Linked Social Service Network (LSSN) with multiple feature attribute-based service discovery for BDA. Our approach combines two advantages, the precision and sociability of Web services. The experimental results show that both of these methods perform well based on their perspectives, better than previous approaches.",2016,
161,10.1109/ICOMITEE.2019.8920823,BIG DATA TECHNOLOGIES USING SVM (CASE STUDY: SURFACE WATER CLASSIFICATION ON REGIONAL WATER UTILITY COMPANY IN SURABAYA),"How important to the role of water for the survival of living beings, not only for human but also the other living beings need water as one of the elements that support the continuity of life in every living creature. To maintain the necessity of water resources such as river, recently the need for monitoring systems that able to take the parameter of water quality using sensors important. In the previous paper, we built the Internet of Things to get the data using a passive sensor and an active sensor. As additionally, we built Big-Data system equipped with machine learning algorithm that can perform water quality classification with the Support Vector Machine method. This system monitoring every activity in the Karang Pilang area and applying classification. The result of this system that the big data system can perform the classification of river water quality in interactive and accurate. The result discusses that we were able to classify by using Support Vector Machine with accuracy level 0.9138 by using Linear kernel and 0.8372 by using RBF kernel. From the ROC result, we achieved AUC value until 0.93. It's mean we achieved an excellent result.",2019,
162,10.1109/CCEM.2018.00020,BIG DATA SCIENCE IN BUILDING MEDICAL DATA CLASSIFIER USING NAÏVE BAYES MODEL,"currently, maintenance of clinical databases has become a crucial task in the medical field. The patient data consisting of various features and diagnostics related to disease should be entered with the utmost care to provide quality services. As the data stored in medical databases may contain missing values and redundant data, mining of the medical data becomes cumbersome. As it can affect the results of mining, it is essential to have good data preparation and data reduction before applying data mining algorithms. Prediction of disease becomes quick and easier if data is precise and consistent and free from noise. One of the key specialty of Naive Bayes classifiers is that they are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Evaluation of closed-form expression can be achieved by Maximum-likelihood training. Which requires linear time, rather than by expensive iterative approximation as used for many other types of classifiers. This research uses data science approach to diognize the medical data. In this article, a study has been conducted by using naïve Bayes classifier to classify the medical data. The suitability of the classifier and the accuracy of the classifier are measured using different performance criteria. This study is useful for researchers and developers in understanding and using a classification technique in medical diagnosis.",2018,
163,10.1109/SmartWorld.2018.00310,BANKING COMPREHENSIVE RISK MANAGEMENT SYSTEM BASED ON BIG DATA ARCHITECTURE OF HYBRID PROCESSING ENGINES AND DATABASES,"Banks are shifting from a simple credit risk management model to the comprehensive risk management model. Banking risks come from many channels and systems. Big data technology provides an innovative and effective solution for data management, and thus is suitable to be applied in the risk management scenarios that require high-quality data and complex data analysis. This paper firstly proposes big data architecture of hybrid processing engines and databases. This architecture uses Hadoop ecosystem with ETL and Spark processing engines, and using massive parallel processing databases (MPP), transactional databases, and HDFS. Then a banking comprehensive risk management system prototype based on the proposed big data architecture is implemented. Comparisons and evaluations clearly demonstrate that the proposed system has better performance.",2018,
164,10.1109/JAS.2020.1003384,BIG DATA ANALYTICS IN HEALTHCARE − A SYSTEMATIC LITERATURE REVIEW AND ROADMAP FOR PRACTICAL IMPLEMENTATION,"The advent of healthcare information management systems (HIMSs) continues to produce large volumes of healthcare data for patient care and compliance and regulatory requirements at a global scale. Analysis of this big data allows for boundless potential outcomes for discovering knowledge. Big data analytics (BDA) in healthcare can, for instance, help determine causes of diseases, generate effective diagnoses, enhance QoS guarantees by increasing efficiency of the healthcare delivery and effectiveness and viability of treatments, generate accurate predictions of readmissions, enhance clinical care, and pinpoint opportunities for cost savings. However, BDA implementations in any domain are generally complicated and resource-intensive with a high failure rate and no roadmap or success strategies to guide the practitioners. In this paper, we present a comprehensive roadmap to derive insights from BDA in the healthcare (patient care) domain, based on the results of a systematic literature review. We initially determine big data characteristics for healthcare and then review BDA applications to healthcare in academic research focusing particularly on NoSQL databases. We also identify the limitations and challenges of these applications and justify the potential of NoSQL databases to address these challenges and further enhance BDA healthcare research. We then propose and describe a state-of-the-art BDA architecture called Med-BDA for healthcare domain which solves all current BDA challenges and is based on the latest zeta big data paradigm. We also present success strategies to ensure the working of Med-BDA along with outlining the major benefits of BDA applications to healthcare. Finally, we compare our work with other related literature reviews across twelve hallmark features to justify the novelty and importance of our work. The aforementioned contributions of our work are collectively unique and clearly present a roadmap for clinical administrators, practitioners and professionals to successfully implement BDA initiatives in their organizations.",2021,23299274
165,10.1109/RAINS.2016.7764382,ANALYSIS OF HADOOP PERFORMANCE AND UNSTRUCTURED DATA USING ZEPPELIN,"Big data has been designed as next 20 years for innovation, competition and productivity. It helps to top opportunities and address the top challenges. Every person has 320 * times data in their library and this data can't be examined by traditional data processing application devices, within the acceptable time. The difficulties incorporate the zones of capture, storage, search, sharing, exchange, examination, and representation of this data. Management of this data which ensures that the data from varied sources is processed error free and good quality to perform analysis. Hadoop is designed to handle the extremely high volumes of data in any structure. This paper explores the Hadoop cluster on Amazon Elastic Cloud, perform the benchmark of data load time with traditional data processing application and Hadoop. Secondly we analyze the unstructured data in Zeppelin with Spark.",2016,
166,10.1109/EIConRus49466.2020.9038949,ERP DATA ANALYSIS AND VISUALIZATION IN HIGH-PERFORMANCE COMPUTING ENVIRONMENT,"In the era of the fourth industrial revolution the enterprise resource planning system (ERP) becomes a foundation for interconnection between logistics systems, production facilities, smart machines, IoT-enabled devices and other enterprise data sources. The paper proposes an approach to extend the ERP integrated analytical tools capabilities by processing ERP data in a multi-tenant GPU-enabled high-performance computing (HPC) environment. Corporate analytic features in conjunction with GPU in-memory processing of big structured and unstructured data increase the performance and analysis effectiveness for enterprise machine learning (ML) tasks. The approach proposes sharing the data in GPU memory using open analytic platform along with existed ERP analytical capabilities on example of SAP S/4Hana. Considered solution accelerates data scientists work with ERP data sets and could be used for faster quality AI model creation and easier data interaction in unspecific for ERP visualization way like immersive learning with virtual or augmented reality (VR/AR).",2020,23766557
167,10.1109/BigData47090.2019.9006281,IMPROVING K-NEAREST NEIGHBOR PATTERN RECOGNITION MODELS FOR PRIVACY-PRESERVING DATA ANALYSIS,"Supervised learning classification models use labeled data to train models on a discrete form for generating predictions. A major challenge addressed in this paper is training a machine learning model to the recognition of a pattern data perspective of the original datasets and privacy-preserving datasets to improve predictive models. The model training process, the training datasets, and validation datasets are mixed with data and privacy-preserving data cause overfitting from high variance in the machine learning algorithm. This paper addresses a k-Nearest Neighbor algorithm to build models, apply an automated hyperparameter tuning method to determine the optimal parameters based on the characteristics before the training process of a large volume datasets. Evaluating the model to achieve goals based on a high score of accuracy results on quality prediction and performance models. The experiments from our real datasets and the UCI machine learning repository show the best method for all of the training data and conduct difference experiments for improving accuracy, feasibility, correctness and reliability of the scheme.",2019,
168,10.1109/LGRS.2018.2856514,LINE STRUCTURE-BASED INDOOR AND OUTDOOR INTEGRATION USING BACKPACKED AND TLS POINT CLOUD DATA,"This letter presents a line structure-based method for integration of centimeter-level indoor backpacked scanning point clouds and millimeter-level outdoor terrestrial laser scanning point clouds. Using 3-D lines for registration, instead of matching points directly, can improve the robustness of the method and adapt to multisource point cloud data of different qualities. Considering the limited overlapping between indoor and outdoor scenes, line structures are extracted from overlapped wall areas that may be included in interior and exterior data. Here, a patch-based method labels a point cloud into wall, ceiling, floor categories, as well as assigning the candidate overlapping walls. Then, lines structures are extracted from the wall plane point cloud. Potential door and window line structures are detected and refined for point cloud registration. Last, an iterative closest point-based method is used to fine tune the registration results. Our results show that the proposed method effectively integrates a promising map of indoor and outdoor scenes.",2018,15580571
169,10.1109/TNNLS.2017.2649101,LEARNING A NO-REFERENCE QUALITY ASSESSMENT MODEL OF ENHANCED IMAGES WITH BIG DATA,"In this paper, we investigate into the problem of image quality assessment (IQA) and enhancement via machine learning. This issue has long attracted a wide range of attention in computational intelligence and image processing communities, since, for many practical applications, e.g., object detection and recognition, raw images are usually needed to be appropriately enhanced to raise the visual quality (e.g., visibility and contrast). In fact, proper enhancement can noticeably improve the quality of input images, even better than originally captured images, which are generally thought to be of the best quality. In this paper, we present two most important contributions. The first contribution is to develop a new no-reference (NR) IQA model. Given an image, our quality measure first extracts 17 features through analysis of contrast, sharpness, brightness and more, and then yields a measure of visual quality using a regression module, which is learned with big-data training samples that are much bigger than the size of relevant image data sets. The results of experiments on nine data sets validate the superiority and efficiency of our blind metric compared with typical state-of-the-art full-reference, reduced-reference and NA IQA methods. The second contribution is that a robust image enhancement framework is established based on quality optimization. For an input image, by the guidance of the proposed NR-IQA measure, we conduct histogram modification to successively rectify image brightness and contrast to a proper level. Thorough tests demonstrate that our framework can well enhance natural images, low-contrast images, low-light images, and dehazed images. The source code will be released at https://sites.google.com/site/guke198701/publications.",2018,21622388
170,10.1109/CCGrid.2015.175,CROSS-LAYER SLA MANAGEMENT FOR CLOUD-HOSTED BIG DATA ANALYTICS APPLICATIONS,"As we come to terms with various big data challenges, one vital issue remains largely untouched. That is service level agreement (SLA) management to deliver strong Quality of Service (QoS) guarantees for big data analytics applications (BDAA) sharing the same underlying infrastructure, for example, a public cloud platform. Although SLA and QoS are not new concepts as they originated much before the cloud computing and big data era, its importance is amplified and complexity is aggravated by the emergence of time-sensitive BDAAs such as social network-based stock recommendation and environmental monitoring. These applications require strong QoS guarantees and dependability from the underlying cloud computing platform to accommodate real-time responses while handling ever-increasing complexities and uncertainties. Hence, the over-reaching goal of this PhD research is to develop novel simulation, modelling and benchmarking tools and techniques that can aid researchers and practitioners in studying the impact of uncertainties (contention, failures, anomalies, etc.) on the final SLA and QoS of a cloud-hosted BDAA.",2015,
171,10.1109/ICWS.2017.104,TOWARDS RELIABLE ONLINE SERVICES ANALYZING MOBILE SENSOR BIG DATA,"Sensors are pervasively deployed on mobile devices with the development of Internet of Things technology. Value-added services are innovated and developed by analyzing data streams from massive number of mobile sensors in online mode. Due to dynamic working condition of mobile sensors and the high data rate, back end analytic services confront incoming streams with large rate fluctuation and out-of-order time series. This puts forward special challenges in service implementation for commercial applications, where good reliability/scalability performance is a must. In this paper, a data ingestion and scheduling framework is proposed to enable large-scale tempo-spatial streams analysis in a reliable and cost-effective way. A case study on a real world application adopting this framework is introduced and its pilot result is presented.",2017,
172,10.1109/ACCESS.2019.2939158,OPTIMIZING THE ELECTRONIC HEALTH RECORDS THROUGH BIG DATA ANALYTICS: A KNOWLEDGE-BASED VIEW,"Many hospitals are suffering from ineffective use of big data analytics with electronic health records (EHRs) to generate high quality insights for their clinical practices. Organizational learning has been a key role in improving the use of big data analytics with EHRs. Drawing on the knowledge-based view and big data lifecycle, we investigate how the three modes of knowledge can achieve meaningful use of big data analytics with EHRs. To test the associations in the proposed research model, we surveyed 580 nurses of a large hospital in China in 2019. Structural equation modelling was used to examine relationships between knowledge mode of EHRs and meaningful use of EHRs. The results reveal that know-what about EHRs utilization, know-how EHRs storage and utilization, and know-why storage and utilization can improve nurses' meaningful use of big data analytics with EHRs. This study contributes to the existing digital health and big data literature by exploring the proper adaptation of analytical tools to EHRs from the different knowledge mode in order to shape meaningful use of big data analytics with EHRs.",2019,21693536
173,10.23919/SpliTech49282.2020.9243708,AIR QUALITY VISUAL ANALYTICS WITH KIBANA,"The recent studies report that the short-term fluctuations of air pollution levels are directly related to the hospital admissions of patients with pneumonia and bronchitis. In addition, the long-term exposure to air pollution causes significant health problems, including cardiovascular disease, lung cancer and respiratory disease such as emphysema. At the same time, a huge amount of air quality data is collected by public air quality monitoring systems in different parts of the world. Official reporting from government is one of reliable sources of data. The European Environment Agency for Europe's Air Quality and Clean Air Asia databases, the Global Burden of Disease epidemiological study, and peer-reviewed journal articles are other sources. Multidimensional visualization of such big amounts of data, including temporal granularities and spatial distribution, is a challenging question. In order to address this challenge, the paper proposes a software solution for visual analytics of air quality data using the potential of Big Data technologies. Its architecture, implementation with Elasticsearch and Kibana, and actual results from data visualization are presented. The findings of the paper show that the proposed solution provides more intuitive perception and valuable insight through multi-perspective air pollution graphs.",2020,
174,10.1109/BigData.2018.8622378,"CONSOLIDATING BILLIONS OF TAXI RIDES WITH AWS EMR AND SPARK IN THE CLOUD : TUNING, ANALYTICS AND BEST PRACTICES","Saving nature using Big Data Analytics is a very noble goal. Using New York taxi rides data, we decided to learn how many rides could be consolidated. It was a journey we would like to share. First, we had to choose the platform for calculation between Amazon Athena, Serverless Microservices, SQL or NoSql databases, Hadoop and Spark. Then, we had to find an optimal solution for the platform using assorted tuning and optimization techniques. Although the problem seems to be straight forward, it turned out that the solution is quite challenging because of the input size, data quality, calculation complexities and numerous EMR/Spark tuning options. We have been using New York taxi data from 2009 to 2017 to quantify the rides that can be joined together. The taxi rides were consolidated based on pickup location, pickup time and drop-off location. We have been calculating the percentage of taxi rides that can be joined. The benchmark originally set was rides within five minutes with a pickup and drop-off locations within half a kilometer. Then we started experimenting with different times and locations. We have been using parquet format, parallel Scala collections, compression, filtering, new column introduction, tuning parameters, I/O overhead tuning, bucketing, timeouts and partitioning. Over 1.2 billion rides were processed using Amazon EMR with Spark. We have been optimizing calculation time and processing price. Spark has hundreds of parameters, EMR has over fifty instances to choose from. It was challenging to process our data within reasonable time. We were able to find the optimal Spark queries (plans), tested different types of joins and compared their performances. Also, we were able to compare I/O and in-memory operations during partitioning and large files manipulation (the input file sizes were hundreds of Gigabytes). The results were amazing - we could consolidate around thirty five percent of total rides, saving tons of gas and improving environment and traffic in New York City.",2018,
175,10.1109/BIGCOMP.2016.7425974,AUTOMATED DISCOVERY OF SMALL BUSINESS DOMAIN KNOWLEDGE USING WEB CRAWLING AND DATA MINING,"It has become an era where everything is on the web with ever more chances of data utilization on the web. Still, there are obstacles to make the use of the web efficiently. With too much information, Internet users have often come across information that are not relevant for their use. On top of that, until recently, most of web content have not contained semantic information, posing difficulties for mechanical analysis. The Semantic Web emerged as a way to tackle those poor qualities of the web. Adopting formal languages such as RDF or OWL, the semantic web has made the Internet become more highly available for computer-based analysis. In this study, what we aimed at is building a small business knowledge base to provide useful information for small business owners for their marketing strategies or dynamic QA systems for their restaurant recommendation services. The knowledge base was built according to the concept of the Semantic Web. To build the knowledge base, first, it is needed to conduct web crawling from different web sources including social media. However, the crawled data typically come in informal and do not have any semantic information. So we devised text mining techniques to catch useful information from them and generate formal knowledge for the knowledge base.",2016,23759356
176,10.1109/CLOUD53861.2021.00081,PARA: HARVESTING CPU TIME FRAGMENTS IN BIG DATA ANALYTICS,"Modern data analytics typically run tasks on statically reserved resources (e.g., CPU and memory), which is prone to over-provision to guarantee the Quality of Service (QoS), leading to a large amount of resource time fragments. As a result, the resource utilization of a data analytics cluster is severely under-utilized. Workload co-location on shared resources has been substantially studied, but they are unaware the sizes of resource time fragments, making them hard to improve the resource utilization and guarantee QoS at the same time. In this paper, we propose Para, an event-driven scheduling mechanism, to harvest the CPU time fragments in co-located big data analytic workloads. Para innovates three techniques: 1) identifying the Idle CPU Time Window (ICTW) associated with each CPU core by capturing the task-switch event; 2) designing a runtime communication mechanism between each task execution of a workload and the underlying resource management system; 3) designing a pull-based scheduler to schedule a workload to run in the ICTW of another workload. We implement Para based on Apache Mesos and Spark. And the experimental results show that Para improves the CPU utilization by 44% and 30% on average relative to the original Mesos and enhanced Mesos under Spark's dynamic mode (MSDM), respectively. Moreover, Para increases the averaged task throughput of Mesos and MSDM by 4.8x and 1.7x, respectively, while guaranteeing the execution time of the primary applications.",2021,21596182
177,10.1109/ICDE.2013.6544913,MACHINE LEARNING ON BIG DATA,"Statistical Machine Learning has undergone a phase transition from a pure academic endeavor to being one of the main drivers of modern commerce and science. Even more so, recent results such as those on tera-scale learning [1] and on very large neural networks [2] suggest that scale is an important ingredient in quality modeling. This tutorial introduces current applications, techniques and systems with the aim of cross-fertilizing research between the database and machine learning communities. The tutorial covers current large scale applications of Machine Learning, their computational model and the workflow behind building those. Based on this foundation, we present the current state-of-the-art in systems support in the bulk of the tutorial. We also identify critical gaps in the state-of-the-art. This leads to the closing of the seminar, where we introduce two sets of open research questions: Better systems support for the already established use cases of Machine Learning and support for recent advances in Machine Learning research.",2013,10636382
178,10.1109/AERO.2015.7119147,TRANSMISSION OF BIG DATA OVER MANETS,"Big data recently has gained tremendous importance in the way information is being disseminated. Transaction based data, unstructured data streaming to and fro from social media, increasing amounts of sensor and machine-to-machine data and many such examples rely on big data in conjunction with cloud computing. It is desirable to create wireless networks on-the-fly as per the demand or a given situation. In such a scenario reliable transmission of big data over mobile Ad-Hoc networks plays a key role. Limitations like low bandwidth, congestion and loss of packets pose a challenge for such systems. Hence an effective routing mechanism plays an important role. The proposed protocol is Multi-path QoS Routing (MPQR) protocol. Existing protocols try to establish a single path for communication. The proposed paper focuses on distributing tickets in the network. Also it can be divided into sub-tickets to get an optimum multi-path. The principal advantage is its high performance in the case of bandwidth limited environments when compared to existing protocols.",2015,1095323X
179,10.1109/ICSGEA51094.2020.00107,AN OPTIMIZATION SCHEME OF NETWORK MARKETING BASED ON BIG DATA,"In this paper, from the perspective of consumers, the personalized information service mode of e-commerce is systematically studied through the research on the influencing factors of users' purchase of personalized recommended goods under the background of big data. Based on the analysis of the influencing factors of personalized online marketing recommendation service, the influencing factor model of personalized recommendation service is established, and a targeted questionnaire survey is formed. Then, we use the recommendation technology based on collaborative filtering to compare the marketing effect. Finally, combining the theory of big data and precision marketing, it is comprehensively applied to the marketing plan of e-commerce enterprises. By optimizing the precision marketing strategy of a large-scale shopping website, it can solve the actual problems in current online marketing. It is hoped that the strategy can provide a reference for other enterprises in the same type of e-commerce field during the process of precision marketing.",2020,
180,10.1109/TBDATA.2016.2597149,CLOUD INFRASTRUCTURE RESOURCE ALLOCATION FOR BIG DATA APPLICATIONS,"Increasing popular big data applications bring about invaluable information, but along with challenges to industrial community and academia. Cloud computing with unlimited resources seems to be the way out. However, this panacea cannot play its role if we do not arrange fine allocation for cloud infrastructure resources. In this paper, we present a multi-objective optimization algorithm to trade off the performance, availability, and cost of Big Data application running on Cloud. After analyzing and modeling the interlaced relations among these objectives, we design and implement our approach on experimental environment. Finally, three sets of experiments show that our approach can run about 20 percent faster than traditional optimization approaches, and can achieve about 15 percent higher performance than other heuristic algorithms, while saving 4 to 20 percent cost.",2018,23722096
181,10.1109/CSCI51800.2020.00277,AN AUTO OPTIMIZED PAYMENT SERVICE REQUESTS SCHEDULING ALGORITHM VIA DATA ANALYTICS THROUGH MACHINE LEARNING,"Traditional customer payment service scheduling approaches cannot cope with the modern demand for timely, high-quality service due to the disruption of big data within small and medium-sized payment solution providers (SaMS-PSP). While many customers have access to modern technologies to lodge their service requests easily and fast, SaMS-PSPs do not have equally automated big data-driven capabilities to handle the growing demands of these service requests. To effectively improve SaMS-PSP’s customer payment service requests processing speeds, personnel optimization, throughput, and low latency scheduling, we have developed a new customer payment service request scheduling algorithm via matching request priority with the best personnel to handle the request based on data analytics through machine learning. Our experiments and testing have confirmed the merits of this new algorithm. We are also in the process of applying this new algorithm in real-world payment operations.",2020,
182,10.1109/BigData.2017.8257992,VIGAN: MISSING VIEW IMPUTATION WITH GENERATIVE ADVERSARIAL NETWORKS,"In an era when big data are becoming the norm, there is less concern with the quantity but more with the quality and completeness of the data. In many disciplines, data are collected from heterogeneous sources, resulting in multi-view or multi-modal datasets. The missing data problem has been challenging to address in multi-view data analysis. Especially, when certain samples miss an entire view of data, it creates the missing view problem. Classic multiple imputations or matrix completion methods are hardly effective here when no information can be based on in the specific view to impute data for such samples. The commonly-used simple method of removing samples with a missing view can dramatically reduce sample size, thus diminishing the statistical power of a subsequent analysis. In this paper, we propose a novel approach for view imputation via generative adversarial networks (GANs), which we name by VIGAN. This approach first treats each view as a separate domain and identifies domain-to-domain mappings via a GAN using randomly-sampled data from each view, and then employs a multi-modal denoising autoencoder (DAE) to reconstruct the missing view from the GAN outputs based on paired data across the views. Then, by optimizing the GAN and DAE jointly, our model enables the knowledge integration for domain mappings and view correspondences to effectively recover the missing view. Empirical results on benchmark datasets validate the VIGAN approach by comparing against the state of the art. The evaluation of VIGAN in a genetic study of substance use disorders further proves the effectiveness and usability of this approach in life science.",2017,
183,10.1109/ICDCECE53908.2022.9792800,RANKING THE CUSTOMER REVIEWS FROM MOBILE COMMERCE BIG DATA: K MEANS CLUSTERING,"Big data analytics in the field of mobile commerce gathers huge measures of data, yet it doesn't use the information to settle on constant choices. Rather, there is ordinarily a slack between when the data is gathered and when the data is dissected. In short, such data is so substantial and complex that none of the conventional data the executives’ devices can store it or procedure it effectively. The moto of this article is to analyze the big data analytics in mobile commerce field. In m commerce area customer reviews is an important thing to purchase products. Here we mine the high customer reviews based on K means clustering algorithm to cluster the reviews as per the features. The proposed work optimizes the features by using Salp Swarm Algorithm (SSA) to find the efficient features. The performance of the proposed work relates to group the reviews, and ranking the reviews for particular sites based on some products. The result depicted that Amazon and flip kart performs better reviews from customers in mobile commerce sites compared to other shopping sites. The proposed result gives minimum cost, high quality and best brand performs in Amazon platform than others and recognize optimally utilizing the K-means clustering algorithm.",2022,
184,10.1109/GET.2016.7916864,QUALITY BASED CLUSTERING USING MAPREDUCE FRAMEWORK,The problem of data deluge is prevailing everywhere. Analyzing voluminous and variety of data is a great challenge to the researchers. The MapReduce framework is adapted to many computational methodologies to overcome these issues. Clustering is one of the most commonly used data mining techniques in various pattern analysis applications. This paper is mainly focuses on quality based data clustering using MapReduce framework for fast processing. In order to satisfy the many pattern analysis research applications.,2016,
185,10.1109/LSC.2018.8572185,BIG DATA INTEGRATION CASE STUDY FOR RADIOLOGY DATA SOURCES,"Today's digitized world urgently needs Big Data integration and analysis. Healthcare records are responsible for generating petabytes of data in a single day. Such data is heterogeneous in nature, captured in different files and formats, and varies from hospital to hospital. By integrating data from different sources and extracting meaningful information for the medical community, we can improve the overall quality of patient care. Our research targets the problem of integration for health records. To start, we already developed the Integrated Radiology Image search (IRIS) engine, which could represent a data integration framework for the healthcare domain. IRIS provided support for multiple public data sources and incorporated medical ontologies which would help radiologists and improve search interpretation by considering the meaning of the search query terms. In this paper, we describe a case study of data integration for radiology data sources. While the need for data integration is self-evident, we learned that rather than being a single step, data integration is an iterative process that requires continuous integration of metadata and additional supporting data sources. Our results show that an each step of data integration further improved IRIS engine results.",2018,
186,10.1109/CBMS52027.2021.00078,BEHEALTHIER: A MICROSERVICES PLATFORM FOR ANALYZING AND EXPLOITING HEALTHCARE DATA,"The era of big data is surrounded by plenty of challenges, concerning aspects related to data quality, data management, and data analysis. Plenty of these challenges are met in several domains, such as the healthcare domain, where the corresponding healthcare platforms not only have to deal with managing and/or analyzing a tremendous quantity of health data, but also have to accomplish these actions in the most efficient and secure way possible. Towards this direction, medical institutions are paying attention to the replacement of traditional approaches such as the Monolithic and Service Oriented Architecture (SOA), which deal with many difficulties for handling the increasing amount of healthcare data. This paper presents a platform for overcoming these issues, by adopting the Microservice Architecture (MSA), being able to efficiently manage and analyze these vast amounts of data. More specifically, the proposed platform, namely beHEALTHIER, offers the ability to construct health policies out of data of collective knowledge, by utilizing a newly proposed kind of electronic health records (i.e., eXtended Health Records (XHRs)) and their corresponding networks, through the efficient analysis and management of ingested healthcare data. In order to achieve that, beHEALTHIER is architected based upon four (4) discrete and interacting pillars, namely the Data, the Information, the Knowledge and the Actions pillars. Since the proposed platform is based on MSA, it fully utilizes MSA's benefits, achieving fast response times and efficient mechanisms for healthcare data collection, processing, and analysis.",2021,2372918X
187,10.1109/BigData52589.2021.9671968,POSITION-BASED HASH EMBEDDINGS FOR SCALING GRAPH NEURAL NETWORKS,"Graph Neural Networks (GNNs) bring the power of deep representation learning to graph and relational data and achieve state-of-the-art performance in many applications. GNNs compute node representations by taking into account the topology of the node’s ego-network and the features of the ego-network’s nodes. When the nodes do not have high-quality features, GNNs learn an embedding layer to compute node embeddings and use them as input features. However, the size of the embedding layer is linear to the product of the number of nodes in the graph and the dimensionality of the embedding and does not scale to big data and graphs with hundreds of millions of nodes. To reduce the memory associated with this embedding layer, hashing-based approaches, commonly used in applications like NLP and recommender systems, can potentially be used. However, a direct application of these ideas fails to exploit the fact that in many real-world graphs, nodes that are topologically close will tend to be related to each other (homophily) and as such their representations will be similar.In this work, we present approaches that take advantage of the nodes’ position in the graph to dramatically reduce the memory required, with minimal if any degradation in the quality of the resulting GNN model. Our approaches decompose a node’s embedding into two components: a position-specific component and a node-specific component. The position-specific component models homophily and the node-specific component models the node-to-node variation. Extensive experiments using different datasets and GNN models show that our methods are able to reduce the memory requirements by 88% to 97% while achieving, in nearly all cases, better classification accuracy than other competing approaches, including the full embeddings.",2021,
188,10.1109/ICCCBDA.2016.7529531,BIG DATA STREAM COMPUTING IN HEALTHCARE REAL-TIME ANALYTICS,"The healthcare industry is changing at a dramatic rate. There are multiple processes going on within the health sector. These processes not only impact the care of individuals but also help medical practitioners and the delivery of care and services. The industry can take advantage of big data analytics to ensure that all the multiple processes within the industry are running smoothly. Big data analytics is not just an opportunity but a necessity. Recently, big data stream computing has been studied in order to improve the quality of healthcare services and reduce costs by capability support prediction, thus making decisions in real-time. This paper proposes a generic architecture for big data healthcare analytic by using open sources, including Hadoop, Apache Storm, Kafka and NoSQL Cassandra. The combination of high throughput publish-subscribe messaging for streams, distributed real-time computing, and distributed storage system can effectively analyze a huge amount of healthcare data coming with a rapid rate.",2016,
189,10.1109/ICC40277.2020.9148843,LEARNING-BASED ONLINE QUERY EVALUATION FOR BIG DATA ANALYTICS IN MOBILE EDGE CLOUDS,"The rise of big data brings extraordinary benefits and opportunities to businesses and governments. Enterprise users can analyze their consumers' data and infer the business value obtained, such as purchasing goods correlations, customer preferences, and hidden patterns. Meanwhile, with the emerge of big data processing frameworks, such as Hadoop and Tensor-flow, more and more mobile users are embracing big data analytics by issuing queries to analyze their data. In this paper, we investigate the problem of Quality-of-Service (QoS) aware query evaluation for big data analytics in a mobile edge cloud to maximize the system throughput while minimizing the query evaluation time of each admitted query, by exploring the materialization of intermediate query results. We consider dynamic big-data query evaluations where user queries arrive one by one without the knowledge of future arrivals, and the system needs to respond to each query by accepting or rejecting the query immediately. We propose an online algorithm for query admissions within a finite time horizon, the proposed algorithm can intelligently determine whether some immediate results during a query evaluation need to be materialized for later use of other queries, by making use of the Reinforcement Learning (RL) method with predictions. We finally investigate the performance of the proposed algorithm by simulations, and results show that the performance of the proposed algorithm is promising, by achieving a higher system throughput while reducing the average evaluation cost per query by from 20% to 52% compared to the comparison benchmarks.",2020,15503607
190,10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00120,A COLLABORATIVE MULTI-MODALITY SELECTION METHOD BASED ON DATA UTILITY ASSESSMENT,"Multimodal fusion is more and more widely used in the field of machine learning, but it faces a prominent problem in practical application: data utility is not stable. The data of different modalities may be missing and noisy randomly, which will interfere the machine learning model of multi-modal fusion. Most of the existing multi-modal fusion methods neglect data utility problems or only adopt simple data denoising methods to improve data utility. To solve the problem of unstable data utility, we propose a data selection method based on the evaluation of data utility. By training a special machine learning model, the optimal modal combination is predicted according to the quality evaluation of multi-modal data samples to accomplish the dynamic selection of data modalities. The experimental results show that the proposed method can effectively improve the accuracy of multi-modal recognition under low data utility.",2019,
191,10.1109/CONFLUENCE.2016.7508130,ANALYSIS OF DATA SECURITY BY USING ANONYMIZATION TECHNIQUES,"Modern technology generates such a huge amount of public and private datasets that its security becomes an inevitable task. Initially the priority was provided for data security for the data of organization's and firm's, but nowadays it is necessary to provide security for personal data as well. So to achieve the data security, it is mandatory to preserve the privacy of personal information for that we use anonymization technique such as generalization, bucketization, multi set-based generalization, one-attribute-per-column slicing, slicing and suppression with slicing are applied to avoid retrieval of data from database. Thus, privacy preservation means to protect the data value and it is used for data mining in order to get the valid and accurate results. These are discussed and successfully analyzed with different parameters such as revealed co-relation quality (linkage property), loss of information, type of data, security (privacy preserved) and membership disclosure in this paper. The analysis shows that suppression with slicing is an innovative technique that preserves the privacy of identity of an individual in a database better than previously mentioned techniques.",2016,
192,10.1109/ICITISEE53823.2021.9655853,MEASURING THE IMPACT OF AN IOT TEMPERATURE SENSOR FRAMEWORK FOR TRACKING CONTAGIOUS DISEASES,"Due to the COVID-19 pandemic, much computer science research has been dedicated to utilizing sensor readings for medical purposes. Throughout this period, the need for virus symptom tracking has become a promising area for remotely deployed sensor networks and platforms. Our research goal is to prove that the temperature readings from these sensor network platforms can be statistically linked to public record, medical case study data. The expected outcome of our project is to prove the correlation between sensor network tracking of remote human temperature data and medical records for COVID cases. The results of this study will prove that tracking human temperature can assist in tracking disease outbreaks in various populations. Our framework platform is comprised of four main modules: (1) Temperature Collection, (2) Internal Data Validation (3) Internal-External data merger, (4) Data Analytics. The temperature data are collected from internal databases, mobile sensing devices and medical health professionals. After collection, the internal data are validated by our software, TAU-FIVE, a multi-tier data quality validation system, then merged with external data sources into a data analytic based data warehouse. The data mart queries are designed to compare the location and date of temperature sensor data with known data sets from government officials. Once blended into a fully operational data warehouse, these data marts produce high quality data analysis linking remotely sensed human temperature readings to sources of disease outbreaks.",2021,
193,10.1109/BigData.2017.8258006,QUALITY-EFFICIENCY TRADE-OFFS IN MACHINE LEARNING FOR TEXT PROCESSING,"As the amount of available digital documents keeps growing rapidly, extracting useful information from them has become a major challenge. Data mining, natural language processing, and machine learning are powerful techniques that can be used together to deal with this problem. Depending on the task at hand, there are many different approaches that can be used. The methods available are continuously improved, but not all of them have been tested and compared in a set of coherent problems using supervised machine learning algorithms. For example, what happens to the quality of the methods if we increase the training data size from, say, 100 MB to over 1 GB? Moreover, are quality gains worth it when the rate of data processing diminishes? Can we trade quality for time efficiency and recover the quality loss by just being able to process more data? We attempt to answer these questions in a general way for text processing tasks, considering the trade-offs involving training data size, learning time, and quality obtained. For this, we propose a performance trade-off framework and apply it to three important tasks: Named Entity Recognition, Sentiment Analysis and Document Classification. These problems were also chosen because they have different levels of object granularity: words, paragraphs, and documents. For each problem, we selected several supervised machine learning algorithms and we evaluated the trade-offs of them on large publicly available data sets (news, reviews, patents). To explore these trade-offs, we use different data subsets of increasing size ranging from 50 MB to several GB. For the last two tasks, we also consider similar algorithms with two different data sets and two evaluation techniques, to study their impact on the resulting trade-offs. We find that the results do not change significantly and that most of the time the best algorithms are the ones with fastest processing time. However, we also show that the results for small data (say less than 100 MB) are different from the results for big data and in those cases the best algorithm is much harder to determine.",2017,
194,10.1109/ICISCE48695.2019.00053,DATA CLEANING OPTIMIZATION FOR GRAIN BIG DATA PROCESSING USING TASK MERGING,"Data quality has exerted important influence over the application of grain big data, so data cleaning is a necessary and important work. In MapReduce frame, we can use parallel technique to execute data cleaning in high scalability mode, but due to the lack of effective design there are amounts of computing redundancy in the process of data cleaning, which results in lower performance. In this research, we found some tasks often are carried out multiple times on same input files, or require same operation results in the process of data cleaning. For this problem, we proposed a new optimization technique that is based on task merge. By merging simple or redundancy computations on same input files, the number of the loop computation in MapReduce can be reduced greatly. The experiment shows, by this means, the overall system runtime is significantly reduced, which proves that the process of data cleaning is optimized. In this paper, we optimized several modules of data cleaning such as entity identification, inconsistent data restoration, and missing value filling. Experimental results show that the proposed method in this paper can increase efficiency for grain big data cleaning.",2019,
195,10.1109/BigData50022.2020.9439788,SCALABLE MULTI-CRITERIA DECISION-MAKING: A MAPREDUCE DEPLOYED BIG DATA APPROACH FOR SKILL ANALYTICS,"The main question in today's rapidly changing world is how fast and what sort of corresponding knowledge should an agent be adopted to?! This can be defined as knowledge mapping problem for decision based on large scale datasets with veracity and accuracy as key criteria, especially in safety-critical systems. The following paper proposes a hybrid ans scalable approach for Multi-Criteria Decision Making (MCDM) problems that is deployed in MapReduce. The main sector specific problem that is solved is to recommend training resources that efficiently improves skill gaps of job seekers. The main innovations of this work are: (1) the use of large scale semi-real skill analytics and training resources dataset (Dataset Perspective), (2) a hybrid MCDM approach that resolves skill gaps by matching required skills to the training resources (Decision Support Perspective). This can be applied to any other sector with the context of matching problems. (3) the use of MapReduce as scalable processing approach to deliver lower processing latency and higher quality for large scale datasets (Big Data and Scalability Perspective). The experimental results showed 89% accuracy in the clustering and matching results. The recommendation results have been tested and verified with the industrial partner.",2020,
196,10.1109/ACCTCS52002.2021.00065,DESIGN OF VISUAL MODEL AND SOLUTIONS FOR PUBLIC OPINION MONITORING AND ANALYSIS FOR BIG DATA,"Use the big data visual analysis perspective to examine the government's public opinion monitoring and analysis work, and solve the problem of public opinion service quality and efficiency improvement under the new situation. Through status analysis of public opinion monitoring and analysis subject, environment, means, presentation form, design visualization model and visualization scheme of government public opinion monitoring and analysis. The visualization concept system of government public opinion monitoring analysis includes four levels: mechanism level, model level, application level and support level. Taking the monitoring information as the information classification object, the single index driving force structural model is selected as the visual data structure expression model. Set the public opinion analysis index to the total amount of public opinion, the driving force of public opinion, the growth rate of public opinion, and the percentage. Through mathematical formulas, the total amount of public opinion and the driving force index are decomposed into the driving force, growth rate and percentage of the slave nodes according to the dimensions. The total amount of public opinion is superimposed on the decomposed first-dimension and second-dimension dimensions to formulate a modelling plan. Select static data and multi-view associated interaction as the presentation form of the visual solution.",2021,
197,10.1109/IWECAI50956.2020.00025,PATH ANALYSIS OF USING BIG DATA TO INNOVATE ARCHIVES MANAGEMENT MODEL AND IMPROVE SERVICE ABILITY,"Archives management is an important part of the administrative institutions. Under the new situation, archives management departments at all levels must make full use of various high-quality resources and advanced science and technology, so as to improve the overall quality and service of archives management in China. The article mainly analyzes the current status under the background of big data, and analyzes the innovative path of model with the relevant principles, hoping to help archives management in public institutions.",2020,
198,10.1109/ICACITE53722.2022.9823542,THE ROLE OF SMART GRID DATA ANALYTICS IN ENHANCING THE PARADIGM OF ENERGY MANAGEMENT FOR SUSTAINABLE DEVELOPMENT,"The energy industry is witnessing massive growth due to increase demand of power, hence the business enterprises, stakeholders and governments are focusing in implementing novel measures so as to enhance the overall efficiency of operations. The implementation of various technologies in the grids supports the energy providers, users and others in enhancing the efficiency of operations, identify the areas of latency, enable in deploying the resources effectively for better future. The application of smart grids in the energy sector is replacing the traditional and outdated grids, this has enabled in implementing smart meters, enhance the free flow of large volume of data and information. These data re collected using BDA models for analysing the current pattern and forecast the future requirements based on the growing needs and requirements. Hence, this offers different opportunities that are associated in sustainable development and energy management in critical way. It has been noted that the academic practitioners stated that the implementation of big data analytics support the energy companies, users and others to optimise the usage of smart grids, enable in supporting the application of renewable energy sources, also involve in understanding the latency classification and identification, supporting in power quality monitoring and forecasting the pattern for better capacity planning. These aspects tend to support in augmenting overall nature of energy management and support in better growth and development. This study focuses in using the primary data sources in collecting the data from the respondents to understand the overall impact of BDA in enhancing the paradigm of energy management for sustainable development. The researchers intend to use secondary data source for presenting comprehensive understanding on the chosen area of study.",2022,
199,10.1109/ICSGEA51094.2020.00116,RESEARCH ON E-COMMERCE CREDIT INFORMATION EVALUATION BASED ON SOCIAL BIG DATA,"In this paper, the complexity of business data in social business environment is analyzed. Firstly, the POS rule matching method is used to extract feature points from user evaluation text. Then, we use extended matching method to select effective POS rules from a large number of training comment texts, use effective POS rules to extract feature viewpoint pairs from test comment texts, and provide pruning method to delete invalid feature viewpoint pairs. A product recommendation model based on feature view pairs in online reviews is proposed. In the feature view pair extraction, the feature view pairs are extracted by dynamic window according to the matching relationship of multiple sentence structures and feature views. Combined with the viewpoint of product feature tree aggregation feature, the product feature score is given. Finally, it is found that the improved comprehensive model has better topic recognition effect than standard LDA model. The improved model can help e-commerce enterprises to recommend hot topics on micro-blog and help users to make purchase decisions.",2020,
