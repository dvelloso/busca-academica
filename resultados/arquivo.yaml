!!python/object:pandas.core.frame.DataFrame
_metadata: []
_mgr: !!python/object/new:pandas.core.internals.managers.BlockManager
  state: !!python/tuple
  - &id004
    - !!python/object/apply:pandas.core.indexes.base._new_Index
      - &id003 !!python/name:pandas.core.indexes.base.Index ''
      - data: !!python/object/apply:numpy.core.multiarray._reconstruct
          args:
          - &id001 !!python/name:numpy.ndarray ''
          - !!python/tuple
            - 0
          - !!binary |
            Yg==
          state: !!python/tuple
          - 1
          - !!python/tuple
            - 5
          - &id002 !!python/object/apply:numpy.dtype
            args:
            - O8
            - false
            - true
            state: !!python/tuple
            - 3
            - '|'
            - null
            - null
            - null
            - -1
            - -1
            - 63
          - false
          - - doi
            - abstract
            - year
            - title
            - author
        name: null
    - !!python/object/apply:pandas.core.indexes.base._new_Index
      - !!python/name:pandas.core.indexes.numeric.Int64Index ''
      - data: !!python/object/apply:numpy.core.multiarray._reconstruct
          args:
          - *id001
          - !!python/tuple
            - 0
          - !!binary |
            Yg==
          state: !!python/tuple
          - 1
          - !!python/tuple
            - 5
          - !!python/object/apply:numpy.dtype
            args:
            - i8
            - false
            - true
            state: !!python/tuple
            - 3
            - <
            - null
            - null
            - null
            - -1
            - -1
            - 0
          - false
          - !!binary |
            AAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAACAAAAAAAAAA==
        name: null
  - - &id005 !!python/object/apply:numpy.core.multiarray._reconstruct
      args:
      - *id001
      - !!python/tuple
        - 0
      - !!binary |
        Yg==
      state: !!python/tuple
      - 1
      - !!python/tuple
        - 5
        - 5
      - *id002
      - false
      - - 10.1002/9781119690962.ch1
        - 10.1109/BigData.2017.8258380
        - 10.1109/WISA.2017.29
        - 10.1109/BigDataCongress.2017.73
        - 10.1109/IBDAP50342.2020.9245455
        - This chapter discusses the different aspects of designing Big Data platforms,
          in order to define what makes a big platform and to set expectations for
          these platforms. The solutions for Big Data processing vary based on the
          company strategy. A modern Big Data platform has several requirements, and
          to meet them correctly, expectations with regard to data should be set.
          Securing data has become a crucial aspect of a modern Big Data platform.
          The data quality depends on factors such as accuracy, consistency, reliability,
          and visibility. One of the hard problems of Big Data is backups as the vast
          amount of storage needed is overwhelming for backups. The Big Data platform
          should provide an extract, transform, and load (ETL) solution/s that manages
          the experience end to end. ETL developers should be able to develop, test,
          stage, and deploy their changes. Big Data platforms are quite complex as
          they are built based on distributed systems.
        - The railways worldwide are increasingly looking to the integration of their
          data resources coupled with advanced analytics to enhance traffic management,
          to provide new insights on the health of infrastructure assets, to provide
          soft linkages to other transport modes, and ultimately to enable them to
          better serve their customers. As in many industrial sectors, over the past
          decade the rail industry has been investing heavily in sensing technologies
          that record every aspect of the operation of the railway network. However,
          as any data scientist knows, it does not matter how good an algorithm is,
          if you put rubbish in, you get rubbish out; and as the traditional industry
          model of working with data only within the system that it was collected
          by becomes increasingly fragile, the industry is discovering that it knows
          less than it thought about the data it is gathering. When coupled with legacy
          data resources of unknown accuracy, such as design diagrams for assets that
          in many cases are decades old, the rail industry now faces a crisis in which
          its data may become essentially worthless due to a poor understanding of
          the quality of its data. This paper reports the findings of the first phase
          of a three-phase systematic review of literature about how data quality
          can be managed and evaluated in the rail domain. It begins by discussing
          why data quality matters in a rail context, before going on to define the
          quality, introduce and expand the concept of a data quality schema.
        - Since a low-quality data may influence the effectiveness and reliability
          of applications, data quality is required to be guaranteed. Data quality
          assessment is considered as the foundation of the promotion of data quality,
          so it is essential to access the data quality before any other data related
          activities. In the electric power industry, more and more electric power
          data is continuously accumulated, and many electric power applications have
          been developed based on these data. In China, the power grid has many special
          characteristic, traditional big data assessment frameworks cannot be directly
          applied. Therefore, a big data framework for electric power data quality
          assessment is proposed. Based on big data techniques, the framework can
          accumulate both the real-time data and the history data, provide an integrated
          computation environment for electric power big data assessment, and support
          the storage of different types of data.
        - In the Big Data Era, data is the core for any governmental, institutional,
          and private organization. Efforts were geared towards extracting highly
          valuable insights that cannot happen if data is of poor quality. Therefore,
          data quality (DQ) is considered as a key element in Big data processing
          phase. In this stage, low quality data is not penetrated to the Big Data
          value chain. This paper, addresses the data quality rules discovery (DQR)
          after the evaluation of quality and prior to Big Data pre-processing. We
          propose a DQR discovery model to enhance and accurately target the pre-processing
          activities based on quality requirements. We defined, a set of pre-processing
          activities associated with data quality dimensions (DQD's) to automatize
          the DQR generation process. Rules optimization are applied on validated
          rules to avoid multi-passes pre-processing activities and eliminates duplicate
          rules. Conducted experiments showed an increased quality scores after applying
          the discovered and optimized DQR's on data.
        - "Data Profiling and data quality management become a more significant part\
          \ of data engineering, which an essential part of ensuring that the system\
          \ delivers quality information to users. In the last decade, data quality\
          \ was considered to need more managing. Especially in the big data era that\
          \ the data comes from many sources, many data types, and an enormous amount.\
          \ Thus it makes the managing of data quality is more difficult and complicated.\
          \ The traditional system was unable to respond as needed. The data quality\
          \ managing software for big data was developed but often found in a high-priced,\
          \ difficult to customize as needed, and mostly provide as GUI, which is\
          \ challenging to integrate with other systems. From this problem, we have\
          \ developed an opensource package for data quality managing. By using Python\
          \ programming language, Which is a programming language that is widely used\
          \ in the scientific and engineering field today. Because it is a programming\
          \ language that is easy to read syntax, small, and has many additional packages\
          \ to integrate. The software developed here is called \u201CSakdas\u201D\
          \ this package has been divided into three parts. The first part deals with\
          \ data profiling provide a set of data analyses to generate a data profile,\
          \ and this profile will help to define the data quality rules. The second\
          \ part deals with data quality auditing that users can set their own data\
          \ quality rules for data quality measurement. The final part deals with\
          \ data visualizing that provides data profiling and data auditing report\
          \ to improve the data quality. The results of the profiling and auditing\
          \ services, the user can specify both the form of a report for self-review.\
          \ Or in the form of JSON for use in post-process automation."
        - '2021'
        - '2017'
        - '2017'
        - '2017'
        - '2020'
        - 'An Introduction: What''s a Modern Big Data Platform'
        - 'Understanding data quality: Ensuring data quality by design in the rail
          industry'
        - A Big Data Framework for Electric Power Data Quality Assessment
        - 'Big Data Pre-Processing: Closing the Data Quality Enforcement Loop'
        - 'Sakdas: A Python Package for Data Profiling and Data Quality Auditing'
        - Aytas, Yusuf
        - Fu, Qian and Easton, John M.
        - Liu, He and Huang, Fupeng and Li, Han and Liu, Weiwei and Wang, Tongxun
        - Taleb, Ikbal and Serhani, Mohamed Adel
        - Loetpipatwanich, Sakda and Vichitthamaros, Preecha
  - - !!python/object/apply:pandas.core.indexes.base._new_Index
      - *id003
      - data: !!python/object/apply:numpy.core.multiarray._reconstruct
          args:
          - *id001
          - !!python/tuple
            - 0
          - !!binary |
            Yg==
          state: !!python/tuple
          - 1
          - !!python/tuple
            - 5
          - *id002
          - false
          - - doi
            - abstract
            - year
            - title
            - author
        name: null
  - 0.14.1:
      axes: *id004
      blocks:
      - mgr_locs: !!python/object/apply:builtins.slice
        - 0
        - 5
        - 1
        values: *id005
_typ: dataframe
attrs: {}
