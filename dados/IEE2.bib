@INBOOK{9821748,
author={Aytas, Yusuf},
booktitle={Designing Big Data Platforms: How to Use, Deploy, and Maintain Big Data Systems},
title={An Introduction: What's a Modern Big Data Platform},
year={2021},
volume={},
number={},
pages={1-9},
abstract={This chapter discusses the different aspects of designing Big Data platforms, in order to define what makes a big platform and to set expectations for these platforms. The solutions for Big Data processing vary based on the company strategy. A modern Big Data platform has several requirements, and to meet them correctly, expectations with regard to data should be set. Securing data has become a crucial aspect of a modern Big Data platform. The data quality depends on factors such as accuracy, consistency, reliability, and visibility. One of the hard problems of Big Data is backups as the vast amount of storage needed is overwhelming for backups. The Big Data platform should provide an extract, transform, and load (ETL) solution/s that manages the experience end to end. ETL developers should be able to develop, test, stage, and deploy their changes. Big Data platforms are quite complex as they are built based on distributed systems.},
keywords={Big Data;Task analysis;Cloud computing;Business;Security;Monitoring;Data mining},
doi={10.1002/9781119690962.ch1},
ISSN={},
publisher={Wiley},
isbn={9781119690948},
url={https://ieeexplore.ieee.org/document/9821748},}
@INPROCEEDINGS{8258380,
author={Fu, Qian and Easton, John M.},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Understanding data quality: Ensuring data quality by design in the rail industry},
year={2017},
volume={},
number={},
pages={3792-3799},
abstract={The railways worldwide are increasingly looking to the integration of their data resources coupled with advanced analytics to enhance traffic management, to provide new insights on the health of infrastructure assets, to provide soft linkages to other transport modes, and ultimately to enable them to better serve their customers. As in many industrial sectors, over the past decade the rail industry has been investing heavily in sensing technologies that record every aspect of the operation of the railway network. However, as any data scientist knows, it does not matter how good an algorithm is, if you put rubbish in, you get rubbish out; and as the traditional industry model of working with data only within the system that it was collected by becomes increasingly fragile, the industry is discovering that it knows less than it thought about the data it is gathering. When coupled with legacy data resources of unknown accuracy, such as design diagrams for assets that in many cases are decades old, the rail industry now faces a crisis in which its data may become essentially worthless due to a poor understanding of the quality of its data. This paper reports the findings of the first phase of a three-phase systematic review of literature about how data quality can be managed and evaluated in the rail domain. It begins by discussing why data quality matters in a rail context, before going on to define the quality, introduce and expand the concept of a data quality schema.},
keywords={Industries;Rails;Data models;Rail transportation;Systematics;Decision making;data quality;rail;quality by design;data quality schema},
doi={10.1109/BigData.2017.8258380},
ISSN={},
month={Dec},}